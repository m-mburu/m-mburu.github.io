---
title: "Supervised Learning in R: Classification"
date: "23/02/2023"
output:
  html_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## k-Nearest Neighbors (kNN)

### Recognizing a road sign with kNN

After several trips with a human behind the wheel, it is time for the self-driving car to attempt the test course alone.

As it begins to drive away, its camera captures the following image:

Stop Sign

Can you apply a kNN classifier to help the car recognize this sign?

The dataset signs is loaded in your workspace along with the dataframe next_sign, which holds the observation you want to classify.

```{r}
library(tidyverse)
library(data.table)
library(here)
# Load the 'class' package
library(class)

signs_data <- fread(here("supervised_learning_R/data/knn_traffic_signs.csv"))

id_nms <- c("id", "sample", "sign_type")

train_set <- signs_data[sample == "train"]
test_set <- signs_data[sample == "test"]
# Create a vector of labels
sign_types <- train_set$sign_type

# Classify the next sign observed
knn(train = train_set[, .SD, .SDcols = !id_nms],
    test = test_set[1, .SD, .SDcols = !id_nms],
    cl = sign_types)
```

#### Thinking like kNN

With your help, the test car successfully identified the sign and stopped safely at the intersection.

How did the knn() function correctly classify the stop sign?

-   kNN isn't really learning anything; it simply looks for the most similar example.

### Exploring the traffic sign dataset

To better understand how the knn() function was able to classify the stop sign, it may help to examine the training dataset it used.

Each previously observed street sign was divided into a 4x4 grid, and the red, green, and blue level for each of the 16 center pixels is recorded as illustrated here.

Stop Sign Data Encoding

The result is a dataset that records the sign_type as well as 16 x 3 = 48 color properties of each sign.

```{r}
# Examine the structure of the signs dataset
signs <- copy(train_set)
str(signs)
# Count the number of signs of each type
table(signs$sign_type)

# Check r10's average red level by sign type
aggregate(r10 ~ sign_type, data = signs, mean)
```

### Classifying a collection of road signs

Now that the autonomous vehicle has successfully stopped on its own, your team feels confident allowing the car to continue the test course.

The test course includes 59 additional road signs divided into three types:

Stop Sign Speed Limit Sign Pedestrian Sign

At the conclusion of the trial, you are asked to measure the car's overall performance at recognizing these signs.

The class package and the dataset signs are already loaded in your workspace. So is the dataframe test_signs, which holds a set of observations you'll test your model on.

```{r}
# Use kNN to identify the test road signs
sign_types <- signs$sign_type
signs_pred <- knn(train = train_set[, .SD, .SDcols = !id_nms],
    test = test_set[, .SD, .SDcols = !id_nms],
    cl = sign_types)

# Create a confusion matrix of the predicted versus actual values
signs_actual <- test_set$sign_type
table(signs_actual,signs_pred)

# Compute the accuracy
mean(signs_actual == signs_pred)
```

#### Understanding the impact of 'k'

There is a complex relationship between k and classification accuracy. Bigger is not always better.

Which of these is a valid reason for keeping k as small as possible (but no smaller)?

-   A smaller k may utilize more subtle patterns

### Testing other 'k' values

By default, the knn() function in the class package uses only the single nearest neighbor.

Setting a k parameter allows the algorithm to consider additional nearby neighbors. This enlarges the collection of neighbors which will vote on the predicted class.

Compare k values of 1, 7, and 15 to examine the impact on traffic sign classification accuracy.

The class package is already loaded in your workspace along with the datasets signs, signs_test, and sign_types. The object signs_actual holds the true values of the signs.

```{r}
# Compute the accuracy of the baseline model (default k = 1)
k_1 <- knn(train = train_set[, .SD, .SDcols = !id_nms],
    test = test_set[, .SD, .SDcols = !id_nms],
    cl = sign_types)

mean(signs_actual == k_1)

# Modify the above to set k = 7
k_7 <-  knn(train = train_set[, .SD, .SDcols = !id_nms],
    test = test_set[, .SD, .SDcols = !id_nms],
    cl = sign_types, k = 7)
mean(signs_actual == k_7)

# Set k = 15 and compare to the above
k_15 <- knn(train = train_set[, .SD, .SDcols = !id_nms],
    test = test_set[, .SD, .SDcols = !id_nms],
    cl = sign_types, k = 15)
mean(signs_actual == k_15)
```

### Seeing how the neighbors voted

When multiple nearest neighbors hold a vote, it can sometimes be useful to examine whether the voters were unanimous or widely separated.

For example, knowing more about the voters' confidence in the classification could allow an autonomous vehicle to use caution in the case there is any chance at all that a stop sign is ahead.

In this exercise, you will learn how to obtain the voting results from the knn() function.

The class package has already been loaded in your workspace along with the datasets signs, sign_types, and signs_test.

```{r}
# Use the prob parameter to get the proportion of votes for the winning class
sign_pred <-  knn(train = train_set[, .SD, .SDcols = !id_nms],
    test = test_set[, .SD, .SDcols = !id_nms],
    cl = sign_types, k = 7, prob = T)

# Get the "prob" attribute from the predicted classes
sign_prob <- attr(sign_pred, "prob")

# Examine the first several predictions
head(sign_pred )

# Examine the proportion of votes for the winning class
head(sign_prob)
```

#### Why normalize data?

Before applying kNN to a classification task, it is common practice to rescale the data using a technique like min-max normalization. What is the purpose of this step? - Rescaling reduces the influence of extreme values on kNN's distance function.

## Naive Bayes

### Computing probabilities

The where9am data frame contains 91 days (thirteen weeks) worth of data in which Brett recorded his location at 9am each day as well as whether the daytype was a weekend or weekday.

Using the conditional probability formula below, you can compute the probability that Brett is working in the office, given that it is a weekday.

Calculations like these are the basis of the Naive Bayes destination prediction model you'll develop in later exercises.

```{r}
locations <- fread(here("supervised_learning_R/data/locations.csv"))
where9am <- locations[hour==9]
p_A <- nrow(subset(where9am, location == "office"))/nrow(where9am)

# Compute P(B)
p_B <- nrow(subset(where9am, daytype == "weekday"))/nrow(where9am)

# Compute the observed P(A and B)
p_AB <- nrow(subset(where9am, location == "office" & daytype == "weekday" ))/nrow(where9am)

# Compute P(A | B) and print its value
p_A_given_B <- p_AB /p_B
p_A_given_B
```

#### Understanding dependent events

In the previous exercise, you found that there is a 60% chance Brett is in the office at 9am given that it is a weekday. On the other hand, if Brett is never in the office on a weekend, which of the following is/are true?

### A simple Naive Bayes location model

The previous exercises showed that the probability that Brett is at work or at home at 9am is highly dependent on whether it is the weekend or a weekday.

To see this finding in action, use the where9am data frame to build a Naive Bayes model on the same data.

You can then use this model to predict the future: where does the model think that Brett will be at 9am on Thursday and at 9am on Saturday?

The data frame where9am is available in your workspace. This dataset contains information about Brett's location at 9am on different days.

```{r}
# Load the naivebayes package
library(naivebayes)

# Build the location prediction model
locmodel <- naive_bayes(location~daytype, data = where9am)
thursday9am <- locations[weekday=="thursday" & hour==9]
# Predict Thursday's 9am location
predict(locmodel, newdata = thursday9am)

saturday9am <- locations[weekday=="saturday" & hour==9]
# Predict Saturdays's 9am location
predict(locmodel, newdata = saturday9am)

```

### Examining "raw" probabilities

The naivebayes package offers several ways to peek inside a Naive Bayes model.

Typing the name of the model object provides the a priori (overall) and conditional probabilities of each of the model's predictors. If one were so inclined, you might use these for calculating posterior (predicted) probabilities by hand.

Alternatively, R will compute the posterior probabilities for you if the type = "prob" parameter is supplied to the predict() function.

Using these methods, examine how the model's predicted 9am location probability varies from day-to-day. The model locmodel that you fit in the previous exercise is in your workspace.

```{r}
# The 'naivebayes' package is loaded into the workspace
# and the Naive Bayes 'locmodel' has been built

# Examine the location prediction model

locmodel
# Obtain the predicted probabilities for Thursday at 9am
predict(locmodel,newdata = thursday9am , type = "prob")

# Obtain the predicted probabilities for Saturday at 9am
predict(locmodel,newdata = saturday9am , type = "prob")

```

#### Understanding independence

Understanding the idea of event independence will become important as you learn more about how "naive" Bayes got its name. Which of the following is true about independent events?

-   *Yes! One event is independent of another if knowing one doesn't give you information about how likely the other is. For example, knowing if it's raining in New York doesn't help you predict the weather in San Francisco. The weather events in the two cities are independent of each other*.

#### Who are you calling naive?

The Naive Bayes algorithm got its name because it makes a "naive" assumption about event independence.

What is the purpose of making this assumption?

-   *Yes! The joint probability of independent events can be computed much more simply by multiplying their individual probabilities.*

### A more sophisticated location model

The locations dataset records Brett's location every hour for 13 weeks. Each hour, the tracking information includes the daytype (weekend or weekday) as well as the hourtype (morning, afternoon, evening, or night).

Using this data, build a more sophisticated model to see how Brett's predicted location not only varies by the day of week but also by the time of day. The dataset locations is already loaded in your workspace.

You can specify additional independent variables in your formula using the + sign (e.g. y \~ x + b).

```{r}
# The 'naivebayes' package is loaded into the workspace already

# Build a NB model of location
locmodel <- naive_bayes(location~ daytype + hourtype,data = locations)

weekday_afternoon <- locations[daytype =="weekday" &  hourtype == "afternoon"]
# Predict Brett's location on a weekday afternoon
predict(locmodel, weekday_afternoon)

weekday_evening <- locations[daytype =="weekday" &  hourtype == "evening"]
# Predict Brett's location on a weekday evening
predict(locmodel, weekday_evening)
```

### Preparing for unforeseen circumstances

While Brett was tracking his location over 13 weeks, he never went into the office during the weekend. Consequently, the joint probability of P(office and weekend) = 0.

Explore how this impacts the predicted probability that Brett may go to work on the weekend in the future. Additionally, you can see how using the Laplace correction will allow a small chance for these types of unforeseen circumstances.

The model locmodel is already in your workspace, along with the dataframe weekend_afternoon.

```{r}
# The 'naivebayes' package is loaded into the workspace already
# The Naive Bayes location model (locmodel) has already been built

# Observe the predicted probabilities for a weekend afternoon
weekend_afternoon <- locations[daytype =="weekday" &  hourtype == "afternoon"]
predict(locmodel,newdat = weekend_afternoon ,type = "prob")

# Build a new model using the Laplace correction
locmodel2 <- naive_bayes(location~ daytype + hourtype,data = locations, laplace = 1)

# Observe the new predicted probabilities for a weekend afternoon
predict(locmodel2,newdat = weekend_afternoon ,type = "prob")
```

-   *Adding the Laplace correction allows for the small chance that Brett might go to the office on the weekend in the future.*

#### Understanding the Laplace correction

By default, the naive_bayes() function in the naivebayes package does not use the Laplace correction. What is the risk of leaving this parameter unset? - *The small probability added to every outcome ensures that they are all possible even if never previously observed.*
