---
title: "Feature Selection Comparison"
author: ''
date: "9/22/2020"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)
```



```{r}
library(tidyverse)
library(data.table)
library(knitr)
library(caret)
library(glmnet)
library(ggthemes)

cancer <- fread("data.csv")

cancer[, V33 := NULL]
cancer[, diagnosis := factor(diagnosis)]
nms <- names(cancer)
nms <- gsub(" ", "_", nms)
names(cancer) <- nms
str(cancer)
cancer[, id := NULL]
```



```{r}
cancer[, .(freq = .N),
       by = diagnosis] %>% 
    .[, perc := round(100 * freq/sum(freq), 2)] %>%
  
ggplot(aes(x=diagnosis, y=perc, fill = diagnosis)) + 
    geom_bar(stat = "identity", width  = 0.5)+ theme_hc() +
    geom_text(aes(x=diagnosis, y=perc, label = paste(perc, "%")),
              position =  position_dodge(width = 0.5),
              vjust = 0.05, hjust = 0.5, size = 5)+
    scale_fill_hc(name = "")+
    labs(x = "Cancer Type",
         y = "Percentage",
         title = "Percentage of women with benign or malignant breast bancer")+
    theme(legend.position = "none",
          axis.title = element_text(size =12))

```

## Test train

```{r,}
set.seed(100)
train_sample <- sample(1:nrow(cancer), round(0.7*nrow(cancer)))
train_set <- cancer[train_sample,]
test_set <- cancer[-train_sample,]
```

## Fit model


```{r}
library(broom)
glm_mod <- glm(diagnosis ~ .,
               data = train_set, 
               family = binomial())


tidy(glm_mod) %>% kable
```


## Forward


```{r}
forward_select <- step(glm_mod, direction = "forward")
```



## Backward


```{r}
back_select <- step(glm_mod, direction = "backward")
```

## Using Entropy-Based Feature Selection Algorithms 

```{r}
library(FSelectorRcpp)
x <- information_gain(diagnosis ~ ., train_set)
x %>% arrange(desc(importance)) %>%
  kable()
```


## Recursive Feature Elimination (RFE)

```{r}
ctrl <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 5,
                   verbose = FALSE)

lmProfile <- rfe(diagnosis ~ ., 
                 data = train_set,
                 rfeControl = ctrl)

lmProfile

lmProfile$optVariables
```


```{r}
var
```

## Model


```{r}
cv_fold <- createFolds(train_set$diagnosis, k = 5)

train_ctrl <- trainControl(method = "cv",
                        number = 5,
                        summaryFunction = twoClassSummary,
                        classProbs = TRUE,
                        allowParallel=T,
                        index = cv_fold,
                        verboseIter = FALSE,
                        savePredictions = TRUE,
                        search = "grid")
glm_grid <- expand.grid(
                       alpha = 0:1,
                       lambda = seq(0.0001, 1, length = 10)
                      )

```



```{r}
full_model <- train(
    diagnosis~.,
    data = train_set,
    method = "glmnet",
    metric = "ROC",
    trControl = train_ctrl,
    tuneGrid = glm_grid
)

full_model
```

## Forward model

```{r}
forward_model <- train(
    forward_select$formula,
    data = train_set,
    method = "glmnet",
    metric = "ROC",
    trControl = train_ctrl,
    tuneGrid = glm_grid
)

forward_model
```

## Fit model with variables selected from backward selection

```{r}
back_model <- train(
    back_select$formula,
    data = train_set,
    method = "glmnet",
    metric = "ROC",
    trControl = train_ctrl,
    tuneGrid = glm_grid
)

back_model
```


## Fit model with variables selected from backward selection

```{r}
back_model <- train(
    back_select$formula,
    data = train_set,
    method = "glmnet",
    metric = "ROC",
    trControl = train_ctrl,
    tuneGrid = glm_grid
)

back_model
```


## Fit model with variables selected from entropy

```{r}
setDT(x)
#selector predictors with importance of more than 0.05
predictors <- x[importance > 0.05, attributes]

entropy_predctors <- train_set[, ..predictors]
entropy_y <- train_set$diagnosis
entropy_model <- train(
    entropy_predctors,
    entropy_y,
    method = "glm",
    metric = "ROC",
    trControl = train_ctrl
)

entropy_model 
```


## Fit model with variables selected Recursive Feature Elimination

```{r}

recu_pred <- lmProfile$optVariables
recursive_predctors <- train_set[, ..recu_pred]
recursive_y <- train_set$diagnosis
recu_model <- train(
    recursive_predctors,
    recursive_y,
    method = "glm",
    metric = "ROC",
    trControl = train_ctrl
)

recu_model 
```


## Full model test accuracy

```{r}

for_glm <- predict(full_model, test_set, type = "prob")


for_glm1 <- ifelse(for_glm[, "M"] > 0.5, "M", "B")
for_glm1 <- factor(for_glm1, levels = levels(test_set$diagnosis))



confusionMatrix(for_glm1, test_set$diagnosis,positive = "M") 
  
```


## Forward test accuracy

```{r}

for_glm <- predict(forward_model, test_set, type = "prob")


for_glm1 <- ifelse(for_glm[, "M"] > 0.5, "M", "B")
for_glm1 <- factor(for_glm1, levels = levels(test_set$diagnosis))



confusionMatrix(for_glm1, test_set$diagnosis,positive = "M") 
  
```


## Backward test accuracy

```{r}

for_glm <- predict(back_model, test_set, type = "prob")


for_glm1 <- ifelse(for_glm[, "M"] > 0.5, "M", "B")
for_glm1 <- factor(for_glm1, levels = levels(test_set$diagnosis))



confusionMatrix(for_glm1, test_set$diagnosis,positive = "M") 
  
```





## entropy method test accuracy

```{r}

for_glm <- predict(entropy_model, test_set, type = "prob")


for_glm1 <- ifelse(for_glm[, "M"] > 0.5, "M", "B")
for_glm1 <- factor(for_glm1, levels = levels(test_set$diagnosis))



confusionMatrix(for_glm1, test_set$diagnosis,positive = "M") 
  
```


## Recursive Feature Elimination method test accuracy

```{r}

for_glm <- predict(recu_model, test_set, type = "prob")


for_glm1 <- ifelse(for_glm[, "M"] > 0.5, "M", "B")
for_glm1 <- factor(for_glm1, levels = levels(test_set$diagnosis))



confusionMatrix(for_glm1, test_set$diagnosis,positive = "M") 
  
```

