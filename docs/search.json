[
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html",
    "href": "myblog/breast_cancer_prediction/cancer_data.html",
    "title": "Cancer Data",
    "section": "",
    "text": "In this tutorial I’m going to predict whether a breast cancer tumor is benign or malignant. Using Wiscosin breast cancer data set available on Kaggle. The 30 predictors are divided into three parts first is Mean ( variables 3-13), Standard Error(13-23) and Worst(23-32) and each contain 10 parameter (radius, texture,area, perimeter, smoothness,compactness,concavity,concave points,symmetry and fractal dimension of the tumor). When predicting cancer breast tumor types there two types of cost;\n\nThe cost of telling someone who has malignant tumor that they have benign these are the false negatives in this case someone might not seek medical help which is can cause death.\nTelling someone that they have malignant type of tumor but they don’t which is usually false positives. In this case you subject someone to unnecessary stress\n\nSo it’s highly desirable that our model has good accuracy $ f_1 score$ and high recall.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(xgboost)\nlibrary(knitr)\nlibrary(broom)\nlibrary(caret)\nlibrary(e1071)\nlibrary(kableExtra)\nlibrary(ggthemes)\nlibrary(DT)\nlibrary(glmnet)\n\noptions(scipen = 1, digits = 4)\ncancer <- fread(\"data.csv\")\n\ncancer[, V33 := NULL]\n\n\nhead(cancer)  %>%\n  datatable(options = list(scrollX = TRUE))"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#percentage-of-women-with-malignant-tumor",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#percentage-of-women-with-malignant-tumor",
    "title": "Cancer Data",
    "section": "Percentage of women with malignant tumor",
    "text": "Percentage of women with malignant tumor\nThe percentage of women with malignant tumor is 37.26%(212 out 569) while the rest 62.74%(357) had benign tumors.\n\ncancer[, .(freq = .N),\n       by = diagnosis] %>% \n    .[, perc := round(100 * freq/sum(freq), 2)] %>%\n  \nggplot(aes(x=diagnosis, y=perc, fill = diagnosis)) + \n    geom_bar(stat = \"identity\", width  = 0.5)+ theme_hc() +\n    geom_text(aes(x=diagnosis, y=perc, label = paste(perc, \"%\")),\n              position =  position_dodge(width = 0.5),\n              vjust = 0.05, hjust = 0.5, size = 5)+\n    scale_fill_hc(name = \"\")+\n    labs(x = \"Cancer Type\",\n         y = \"Percentage\",\n         title = \"Percentage of women with benign or malignant breast bancer\")+\n    theme(legend.position = \"none\",\n          axis.title = element_text(size =12))"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#boxplots",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#boxplots",
    "title": "Cancer Data",
    "section": "Boxplots",
    "text": "Boxplots\nFrom the boxplots we can identify variables where we expect there is a significance difference between the two groups of cancer tumors. When using a boxplot if two distributions do not averlap or more than 75% of two boxplot do not overlap then we expect that there is a significance difference in the mean/median between the two groups. Some of the variables where the distribution of two cancer tumors are significantly different are radius_mean, texture_mean etc. The visible differences between malignant tumors and benign tumors can be seen in means of all cells and worst means where worst means is the average of all the worst cells. The distribution of malignant tumors have higher scores than the benign tumors in this cases.\n\ncancerm <- melt(cancer[, -1, with = F], id.vars = \"diagnosis\")\n\nggplot(cancerm, aes(x = diagnosis, y = value))+\n    geom_boxplot() + facet_wrap(~variable, scales = \"free_y\")"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#features-scaling",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#features-scaling",
    "title": "Cancer Data",
    "section": "Features Scaling",
    "text": "Features Scaling\nWe find that some variables are highly correlated. We can use principle component analysis for dimension reduction. Since variables are correlated it’s evident that we can use a smaller set of features to build our models.\n\ncancer[, id := NULL]\npredictors <- names(cancer)[3:31]\ncancer[, (predictors) := lapply(.SD, function(x) scale(x)), .SDcols = predictors ]\ncancer[, diagnosis := as.factor(diagnosis)]"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#correlation-matrix",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#correlation-matrix",
    "title": "Cancer Data",
    "section": "Correlation matrix",
    "text": "Correlation matrix\n\ncor(cancer[, -(1:2), with = F]) %>%\n  datatable(options = list(scrollX = TRUE), style = \"bootstrap4\")"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#principle-component-analysis",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#principle-component-analysis",
    "title": "Cancer Data",
    "section": "Principle Component Analysis",
    "text": "Principle Component Analysis\nUsing the elbow rule we can use the first 5 principle components. Using 15 principle components we will have achieved al most 100% of the variance from the original data set.\n\npca <- prcomp(cancer[, predictors, with = F], scale. = F)"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#variance-explained",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#variance-explained",
    "title": "Cancer Data",
    "section": "Variance Explained",
    "text": "Variance Explained\nSince PCA forms new characteristics the variance explained plot shows the amount of variation of the original features captured by each principle component. The new features are simply linear combinations of the old features.\n\nstdpca <- pca$sdev\n\nvarpca <- stdpca^2\n\nprop_var <- varpca/sum(varpca)\nprop_var * 100\n\n [1] 43.706363 18.472237  9.716239  6.816736  5.676223  4.161723  2.292352\n [8]  1.643434  1.363238  1.191515  1.011032  0.897368  0.832105  0.539193\n[15]  0.323823  0.269517  0.198317  0.178851  0.153573  0.107095  0.102579\n[22]  0.093821  0.082603  0.058725  0.053331  0.027514  0.022985  0.005110\n[29]  0.002394\n\nsum(prop_var[1:15])\n\n[1] 0.9864"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#scree-plot",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#scree-plot",
    "title": "Cancer Data",
    "section": "Scree plot",
    "text": "Scree plot\nScree plot shows the variance explained by each principle component which reduces as the number of principle components increase.\n\nplot(prop_var, xlab = \"Principal Component\",\n     ylab = \"Proportion of Variance Explained\",\n     type = \"b\", xlim = c(0, 30))"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#cumulative-variance-explained",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#cumulative-variance-explained",
    "title": "Cancer Data",
    "section": "Cumulative Variance Explained",
    "text": "Cumulative Variance Explained\nThe cumulative of variance plot helps to choose the number of features based on the amount of variation from original data set you want captured. In this case, I wanted to use number of principle components that capture almost 100% of the variation. After trying with different number of principle components I found out that the accuracy of the models did not increase after the 15th principle components.\n\ncum_var <- cumsum(prop_var)\nplot(cum_var, xlab = \"Principal Component\",\n     ylab = \"Cumulative Proportion of Variance Explained\",\n     type = \"b\", xlim = c(0, 30))"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#construct-new-data-set",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#construct-new-data-set",
    "title": "Cancer Data",
    "section": "Construct new data set",
    "text": "Construct new data set\nWe use the first 15 principle components as our new predictors, then we randomly split data into training and test set in 7:3 ratio.\n\nset.seed(100)\ntrain_sample <- sample(1:nrow(cancer), round(0.7*nrow(cancer)))\n\npcadat <- data.table( label = cancer$diagnosis, pca$x[,1:15]) \npcadat[, label := factor(label, levels = c(\"M\", \"B\"))]\ntrain <- pcadat[train_sample,]\ntest <- pcadat[-train_sample,]"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#logistic-regression",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#logistic-regression",
    "title": "Cancer Data",
    "section": "Logistic regression",
    "text": "Logistic regression\nThis is one of generalized linear models which deals with binary data. There is a generalization of this model which is called multinomial regression where you can fit multi class data. The equation for logistic regression model is:\n\\[log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1*X_1 + ... \\beta_n * X_n\\] and using mle the cost function can be derived as: \\[J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m} y^i log(h_\\theta(x^i)) + (1-y^i) log(1 - h_\\theta(x^i))\\] Given that \\[y = 0\\] \\[y = 1\\] . Finding \\[\\beta\\] s we minimizing the cost function.\n\nfit_glm <- glm(label ~., data = train, family = binomial)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#regularization-in-logistic-regression",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#regularization-in-logistic-regression",
    "title": "Cancer Data",
    "section": "Regularization in logistic regression",
    "text": "Regularization in logistic regression\nThe warning “glm.fit: fitted probabilities numerically 0 or 1 occurred” shows that there is a perfect separation/over fitting. In this case you can load glmnet library and fit a regularized logistic regression. These can be achieved by adding a regularization term to the cost function.The L1 regularization(Lasso) adds a penalty equal to the sum of the absolute values of the coefficients.\n\\[J(\\theta) = -\\frac{1}{m}\\sum_{i=0}^{m} y^i log(h_\\theta(x^i)) + (1-y^i) log(1 - h_\\theta(x^i)) + \\frac {\\lambda}{2m}\\sum_{j=1}^{n} |\\theta^i|\\]\n\ntrainx <- train[,-1]\n\ny_train <- factor(train$label, levels = c(\"B\", \"M\"), labels = 0:1)\n#y <- as.numeric(as.character(y))\n\ny_test <- factor(test$label, levels = c(\"B\", \"M\"), labels = 0:1) %>% as.character() %>% as.numeric()\n#ytest <- as.numeric(as.character(ytest))\n\ntestx <- data.matrix(test[, -1]) \n\nTo find the optimal values \\(\\lambda\\) we use cross validation. We choose \\(\\lambda\\) which gives the highest cross validation accuracy.\n\ncv_fold <- createFolds(train$label, k = 10)\n\nmyControl <- trainControl(\n  method = \"cv\", \n  number = 10,\n  summaryFunction = twoClassSummary,\n  savePredictions = \"all\",\n  classProbs = TRUE,\n  verboseIter = FALSE,\n  index = cv_fold,\n  allowParallel = TRUE\n  \n)\n\ntuneGrid <-  expand.grid(\n    alpha = 0:1,\n    lambda = seq(0.001, 1, length.out = 10))\n    \nglmnet_model <- train(\n  label ~.,\n  data = train,\n  method = \"glmnet\",\n  metric = \"ROC\",\n  trControl = myControl,\n  tuneGrid = tuneGrid\n)\n\ns\n\nplot(glmnet_model) \n\n\n\n#lamda_min <- cv_glm$lambda.min\n\n\nresample_glmnet <- thresholder(glmnet_model, \n                              threshold = seq(.2, 1, by = 0.05), \n                              final = TRUE, \n                              statistics = \"all\")\n\nggplot(resample_glmnet , aes(x = prob_threshold, y = F1)) + \n  geom_point() + \n  geom_point(aes(y = Sensitivity), col = \"blue\")\n\n\n\n\n\nlibrary(caTools)\n\npred_glm <- predict(glmnet_model, test, type = \"prob\")\n\ncolAUC(pred_glm , test$label, plotROC = TRUE)\n\n\n\n\n             M      B\nM vs. B 0.9683 0.9683\n\npred_glm1 <- ifelse(pred_glm[, \"M\"] > 0.4, \"M\", \"B\")\n#pred_glm1 <- predict(glmnet_model, test, type = \"raw\")\n\n\npred_glm1 <- factor(pred_glm1, levels = levels(test$label))\n\n\nconfusionMatrix(pred_glm1, test$label,positive = \"M\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   M   B\n         M  53   7\n         B   7 104\n                                        \n               Accuracy : 0.918         \n                 95% CI : (0.866, 0.955)\n    No Information Rate : 0.649         \n    P-Value [Acc > NIR] : <2e-16        \n                                        \n                  Kappa : 0.82          \n                                        \n Mcnemar's Test P-Value : 1             \n                                        \n            Sensitivity : 0.883         \n            Specificity : 0.937         \n         Pos Pred Value : 0.883         \n         Neg Pred Value : 0.937         \n             Prevalence : 0.351         \n         Detection Rate : 0.310         \n   Detection Prevalence : 0.351         \n      Balanced Accuracy : 0.910         \n                                        \n       'Positive' Class : M"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#svm",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#svm",
    "title": "Cancer Data",
    "section": "SVM",
    "text": "SVM\nSupport Vector Machines is a type of supervised learning algorithm that is used for classification and regression. Most of the times however, it’s used for classification.\nTo understand how SVM works consider the following example of linearly separable data. It’s clear that we can separate the two classes using a straight line(decision boundary). Which is normally referred to a separating hyperplane.\n\n\n\n\n\nThe question is, since there exists many lines that can separate the red and the black classes which is the best one. This introduces us to the maximal margin classification, In short SVM finds the hyperplane/line that gives the biggest margin/gap between the two classes. In this case SVM will choose the solid line as the hyperplane while the margins are the dotted lines. The circled points that lie directly on the margin, or on the wrong side of the margin for their class, are known as support vectors. This shows that SVM uses this points to come up with a the decision boundary, the other points are not used. In this case since it’s a two dimensional space the equation of the separating line will be \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2\\]. Then when equations evaluates to more than 0 then 1 is predicted \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2 > 0, y = 1\\] and when it evaluates to less than zero then predicted class is -1 \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2 < 0, \\; y = -1\\] This becomes maximisation problem \\[width \\; of \\;the \\; margin = M \\] \\[\\sum_{j=1}^{n}\\beta_j = 1\\]\n\\[y_i(\\beta_0 + \\beta_1X_1 + \\beta_2X_2) >= M\\]\n\n\n\n\n\nThis is a best case scenario but in most cases the classes are noisy. Consider the plot below no matter which line you choose some points are bound to be on the wrong side of the desicion boundary. Thus maximal margin classification would not work.\n\n\n\n\n\nSVM then introduces what is called a soft margin. In naive explanation you can think of this as a margin that allows some points to be on the wrong side. By introducing an error term we allow for some slack. Thus in a two case the maximisation becomes \\[y_i(\\beta_0 + \\beta_1X_1 + \\beta_2X_2) >= M(1- \\epsilon)\\]\n\\[\\sum_{i=0}^{n} \\epsilon_i <= C\\] C is a tuning parameter which determines the width of the margin while \\[\\epsilon_i  \\;'s\\] are slack variables. that allow individual observations to fall on the wrong side of the margin. In some cases the decision boundary maybe non linear. In case your are dealing with logistic regression you will be forced to introduce polynomial terms which might result in a very large feature space. SVM then introduces what are called kernels"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#tuning-svm",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#tuning-svm",
    "title": "Cancer Data",
    "section": "Tuning SVM",
    "text": "Tuning SVM\n\nsvm_tune <-  expand.grid(\n    C =c(1 ,5 ,  10, 100, 150),\n    sigma = seq(0, .01, length.out = 5))\n    \nsvm_model <- train(\n  label ~.,\n  data = train,\n   metric=\"ROC\",\n  method = \"svmRadial\",\n  trControl = myControl,\n  tuneGrid = svm_tune,\n  verbose = FALSE\n)\n\n\nresample_svm <- thresholder(svm_model, \n                              threshold = seq(.0, 1, by = 0.05), \n                              final = TRUE, \n                              statistics = \"all\")\n\nggplot(resample_svm , aes(x = prob_threshold, y = F1, col = \"F1\")) + \n  geom_point() + \n  geom_point(aes(y = Sensitivity,  col = \"Sensitivity\"))+\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1))\n\n\n\n#mean(pred_svm == ytest)\n\n\npred_svm <-predict(svm_model, newdata = test, type = \"prob\")\n\npred_svm <- ifelse(pred_svm[, \"M\"] > 0.40, \"M\", \"B\")\n\npred_svm <- factor(pred_svm, levels = levels(test$label))\n\nconfusionMatrix(test$label, pred_svm, positive = \"M\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   M   B\n         M  58   2\n         B   2 109\n                                        \n               Accuracy : 0.977         \n                 95% CI : (0.941, 0.994)\n    No Information Rate : 0.649         \n    P-Value [Acc > NIR] : <2e-16        \n                                        \n                  Kappa : 0.949         \n                                        \n Mcnemar's Test P-Value : 1             \n                                        \n            Sensitivity : 0.967         \n            Specificity : 0.982         \n         Pos Pred Value : 0.967         \n         Neg Pred Value : 0.982         \n             Prevalence : 0.351         \n         Detection Rate : 0.339         \n   Detection Prevalence : 0.351         \n      Balanced Accuracy : 0.974         \n                                        \n       'Positive' Class : M"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#xgboost",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#xgboost",
    "title": "Cancer Data",
    "section": "Xgboost",
    "text": "Xgboost\nXGBoost is a type of an ensemble learner. Ensemble learning is where multiple machine learning algorithms are used at the same time for prediction. A good example will be Random Forests. In random Forest multiple decision trees are used together for prediction. There are two main types of ensemble learners, bagging and boosting. Random forest use the bagging approach. Trees are built from random subsets(rows and columns) of training set and then the final prediction is the weighted sum of all decision trees functions. Boosting methods are similar but in boosting samples are selected sequentially. For instance the first sample is selected and a decision tree is fitted, The model then picks the examples that were hard to learn and using this examples and a few others selected randomly from the training set the second model is fitted, Using the first model and the second model prediction is made, the model is evaluated and hard examples are picked and together with another randomly selected new examples from training set another model is trained. This is the process for boosting algorithms which continues for a specified number of n.\nIn gradient boosting the first model is fitted to the original training set. Let say your fitting a simple regression model for ease of explanation. Then your first model will be $ y = f(x) + $. When you find that the error is too large one of the things you might try to do is add more features, use another algorithm, tune your algorithm, look for more training data etc. But what if the error is not white noise and it has some relationship with output \\(y\\) . Then we can fit a second model. $ = f_1(x) + _1$. then this process can continue lets say until n times. Then the final model will be\n$ n = f*{n}(x) + _{n-1}$.\nThen the final step is to add this models together with some weighting criteria $ weights = ’s$ which gives us the final function used for prediction.\n\\(y = \\alpha * f(x) + \\alpha_1 * f_1(x) + \\alpha_2 * f_2(x)...+ \\alpha_n * f_n + \\epsilon\\)\n\n# \"subsample\" is the fraction of the training samples (randomly selected) that will be used to train each tree.\n# \"colsample_by_tree\" is the fraction of features (randomly selected) that will be used to train each tree.\n# \"colsample_bylevel\" is the fraction of features (randomly selected) that will be used in each node to train each tree.\n#eta learning rate\n\n\n\nxgb_ctrl <- trainControl(method = \"cv\",\n                        number = 5,\n                        summaryFunction = twoClassSummary,\n                        classProbs = TRUE,\n                        allowParallel=T,\n                        index = cv_fold,\n                        verboseIter = FALSE,\n                        savePredictions = TRUE,\n                        search = \"grid\")\n\nxgb_grid <- expand.grid(nrounds = c(10, 50, 100),\n                        eta = seq(0.06, .2, length.out = 3),\n                        max_depth = c(50, 80),\n                        gamma = c(0,.01, 0.1),\n                        colsample_bytree = c(0.6, 0.7,0.8),\n                        min_child_weight = 1,\n                        subsample =  .7\n                        \n    )\n\nxgb_model <-train(label~.,\n                 data=train,\n                 method=\"xgbTree\",\n                 trControl= xgb_ctrl,\n                 tuneGrid=xgb_grid,\n                 verbosity=0,\n                 metric=\"ROC\",\n                 nthread =4\n                     \n    )\n\nIncreasing cut of increases the precision. A greater fraction of those who will be predicted that they have cancer will turn out that they have, but the algorithm is likely to have lower recall. If we want to avoid too many cases of people cancer being predicted that they do not have cancer. It will be very bad to tell someone that they do not have cancer but they have. If we lower the probability let say to 0.3 then we want to make sure that even if there is a 30% chance you have cancer then you should be flagged.\n\nresample_xgb <- thresholder(xgb_model, \n                              threshold = seq(.0, 1, by = 0.01), \n                              final = TRUE, \n                              statistics = \"all\")\n\nggplot(resample_xgb , aes(x = prob_threshold, y = F1, col = \"F1\")) + \n  geom_point() + \n  geom_point(aes(y = Sensitivity, col = \"Sensitivity\"))+\n  scale_x_continuous(breaks = seq(0, 1, by =.1))\n\n\n\n\n\npred_xgb <-predict(xgb_model, newdata = test, type = \"prob\")\npred_xgb1 <- ifelse(pred_xgb[, \"M\"] > 0.4, \"M\", \"B\")\npred_xgb1 <- factor(pred_xgb1, levels = levels(test$label))\n\nconfusionMatrix(pred_xgb1,test$label,  positive = \"M\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   M   B\n         M  59   2\n         B   1 109\n                                       \n               Accuracy : 0.982        \n                 95% CI : (0.95, 0.996)\n    No Information Rate : 0.649        \n    P-Value [Acc > NIR] : <2e-16       \n                                       \n                  Kappa : 0.962        \n                                       \n Mcnemar's Test P-Value : 1            \n                                       \n            Sensitivity : 0.983        \n            Specificity : 0.982        \n         Pos Pred Value : 0.967        \n         Neg Pred Value : 0.991        \n             Prevalence : 0.351        \n         Detection Rate : 0.345        \n   Detection Prevalence : 0.357        \n      Balanced Accuracy : 0.983        \n                                       \n       'Positive' Class : M"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#learning-curves",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#learning-curves",
    "title": "Cancer Data",
    "section": "Learning Curves",
    "text": "Learning Curves\n\nsets <- seq(from = 50, to = nrow(train), by = 50)\nsets[length(sets)] <-nrow(train) \ntrain.err <- c()\ntest.err<- c()\ntune_grid <- expand.grid( nrounds = 50, max_depth = 50, eta = 0.06, gamma = 0.01, \n                         colsample_bytree = 0.6, min_child_weight = 1, subsample = 0.7)\nfor (i in 1:length(sets)) {\n    \n    traini = train[1:sets[i],]\n    fit_svm <- train(label ~., data = traini, metric=\"Accuracy\", method = \"svmRadial\",\n                 trControl = trainControl(method = \"none\", summaryFunction = twoClassSummary,\n                                          classProbs = TRUE),\n                 tuneGrid = expand_grid( sigma = 0.0075, C = 5),\n                 )\n    \n    # fit_svm <-train(label~.,\n    #              data=traini,\n    #              method=\"xgbTree\",\n    #              trControl= xgb_ctrl,\n    #              tuneGrid= tune_grid ,\n    #              verbose=T,\n    #              metric=\"ROC\",\n    #              nthread =3\n    #                  \n    # )\n    pred_train = predict(fit_svm, newdata = traini, type = \"prob\")\n    pred_train = ifelse(pred_train[[\"M\"]] > 0.4, \"M\", \"B\")\n    train.err[i] =1 -  mean(pred_train == traini$label)\n    pred_test = predict(fit_svm, newdata = test, type = 'prob')\n    pred_test = ifelse(pred_test[, \"M\"] > 0.4, \"M\", \"B\")\n    test.err[i] = 1 - mean(test$label == pred_test)\n    \n    cat(i,\" \")\n    \n}\n\n1  2  3  4  5  6  7  \n\ntrain.err\n\n[1] 0.00000 0.03000 0.03333 0.01500 0.02000 0.02000 0.02261\n\nmatplot(sets, cbind(test.err, train.err), pch = 19, col = c(\"red\", \"blue\"),\n        type = \"b\", ylab = \"Error\", xlab = \"Train sample size\", main = \"SVM Learning Curves\")\nlegend(\"topright\", legend = c(\"Test\", \"Train\"), pch = 19, col = c(\"red\", \"blue\"))"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#error-analysis",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#error-analysis",
    "title": "Cancer Data",
    "section": "Error Analysis",
    "text": "Error Analysis\nLook at the examples that the algorithm misclassified to see if there is a trend. Generally you are trying to find out the weak points of your algorithm. Checking why your algorithm is making those errors. For instance, from the boxplots below the malignant tumors that were misclassified had lower radius mean compared to mislassified benign tumors. This contrary to what we saw in the first boxplots graph.\n\ndf <- data.frame(cancer[-train_sample,], pred_svm) %>%\n    setDT()\n\n\ntest_mis_svm <- df[(diagnosis == \"M\" & pred_svm == 0) |( diagnosis == \"B\" & pred_svm == \"M\")]\n\n\n# test_mis_svm_m <- melt(test_mis_svm, \n#                 id.vars = c(\"diagnosis\", \"pred_svm\"))\n# \n# ggplot(test_mis_svm_m , aes(x = pred_svm, y = value))+\n#     geom_boxplot() + facet_wrap(~variable, scales = \"free_y\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mburu Blog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Okay I’m excited to finally sit down and work on my blog!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/association_analysis/association_analysis.html",
    "href": "posts/association_analysis/association_analysis.html",
    "title": "Association analysis",
    "section": "",
    "text": "Maybe you have heard of a grocery store that carried out analysis and found out that men who buy diapers between 5pm to 7pm were more likely to buy beer. The grocery store then moved the beer isle closer to the diaper isle and beer sales increased by 35%. This is called association analysis which was motivated by retail store data.\nIn this blog I will explore the basics of association analysis. The goal is to find out:\n\nItems frequently bought together in association analysis this is called support. Let say you have ten transactions and in those ten 3 transactions have maize floor, rice and bread the the support for maize floor, rice and bread is 3/10 = 0.3. This is just marginal probability. In other terms the percentage of transactions these items were bought together.\nIn this example the support is written as Support({bread, maize floor} –> {rice} ). In general this is written as Support of item one and item 2 is Support({item1} –> {item 2}). Item 1 and item 2 may contain one or more items.\nWe also want to find out if someone bought a set of items what other set of item(s) were they likely to buy. In association analysis this is called confidence. In our above example let say that you find the proportion of transactions that contained maize floor and bread are 0.4. Then the confidence is the proportion of those transactions with maize floor, bread and rice/proportion of transactions that contained maize floor and bread. Then the confidence is 0.3/0.4 which is 0.75. In other word 75% of those who bought maize floor and bread also bought rice.\n\nConfidence in this example is denoted as Confidence({bread, maize floor} –> {rice} ) and in general this is Confidence({item 1} –> {item 2} ).\n\nThe lift refers to how the chances of rice being purchased increased given that maize floor and bread are purchased. So the lift of rice is confidence of rice/support(rice). Support of rice is the number of transactions that contain rice.\n\nLift({Item 1} -> {Item 2 }) = (Confidence(Item1 -> Item2)) / (Support(Item2))\n\n\nTo make sense of all these I’m going to use a bakery to find association rules between items bought manually and then towards the end I will use r package arules which uses apriori algorithm to find association between items bought. The data set is available on kaggle as BreadBasket_DMS. We start by first having a glimpse of this data set.\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(data.table)\nlibrary(DT)\n\ndat <- setDT(read.csv( \"BreadBasket_DMS.csv\" ))\n\ndat <- dat[Item != \"NONE\"]\nhead(dat[sample(1:nrow(dat), 10)]) %>% datatable()\n\n\n\n\n\n\n\n\nFirst step is to transform the data set into wide format. Column headers will be items sold in the bakery and the rows will be populated with 1 and 0 indicating whether that item was bought for that transaction.\n\ndat2 <- dcast(Date+Time+Transaction~Item, data = dat, fun.aggregate = length)\n#dat2[, NONE := NULL]\n\nsample_cols <- sample(4:ncol(dat2), 5)\n\nitem_names <- names(dat2)[4:97]\n\ndat2[, (item_names) := lapply(.SD, function(x) ifelse(x==0, 0, 1)), .SDcols = item_names]\n\nhead(dat2[, c(1:3, sample_cols), with = F]) %>% datatable()\n\n\n\n\n\n\n\n\n\nOn average a transaction has 2 items. The median is also 2, this shows that atleast 50% of the transactions contained 2 or more items and atleast 25% of the transactions have 1 item.\n\nnumber_items <- rowSums(dat2[, 4:97, with =F])\n\ndat2[, number_items := number_items]\n\nhist(number_items, col = \"black\", main = \"Number of items bought\")\n\n\n\nsumStats <- dat2 %>%\n    summarise(Average = round(mean(number_items), 2), Stdev = round(sd(number_items), 2),\n              Median= median(number_items),\n              Minimum = min(number_items), Maximum = max(number_items),\n              First_qurtile = quantile(number_items,0.25,  na.rm = T),\n              Third_qurtile=quantile(number_items,0.75,  na.rm = T))\n\ndatatable(sumStats)\n\n\n\n\n\n\n\n\n\nTable below shows top ten most bought items and about 47.84% of the transactions contained coffee. Coffee was the most popular item in this bakery followed by bread.\n\nn_transacts_item_in <- colSums(dat2[, item_names, with = F])\n\ndata.frame(item = names(n_transacts_item_in),\n           number =n_transacts_item_in) %>%\n    mutate(Percentage = round(number/nrow(dat2)*100, 2)) %>%\n    arrange(desc(number)) %>% head(10) %>% datatable()\n\n\n\n\n\n\nSince we have transformed the data in the wide format and every transaction is in it’s row we can visualize how the baskets look like. This is done by extracting the column names for the transactions where the value is 1. For each transaction, 1 represent that item being in that transaction.\n\nitems_bought <- apply(dat2[, 4:97, with =F], 1, paste, collapse = \"\", sep = \"\")\n\nlist_items <- vector(mode = \"list\", length = length(items_bought))\nfor (i in 1:length(items_bought)) {\n    index <- unlist(gregexpr(\"1\", items_bought[i]))\n    items_transaction_i <- item_names[index]\n    items_transaction_i <- paste(items_transaction_i, sep = \" \", collapse = \" , \")\n    list_items[[i]] <- items_transaction_i\n}\n\nhead(unlist(list_items))\n\n[1] \"Bread\"                         \"Scandinavian\"                 \n[3] \"Cookies , Hot chocolate , Jam\" \"Muffin\"                       \n[5] \"Bread , Coffee , Pastry\"       \"Medialuna , Muffin , Pastry\"  \n\n\nData frame below shows how the baskets look like. Only 10 randomly selected rows are displayed. Items_bought column shows the baskets.\n\ndat2[, items_bought := unlist(list_items) ]\n\nhead(dat2[sample(1:nrow(dat2), 10),\n          .(Transaction, number_items,items_bought)]) %>%\n  datatable()\n\n\n\n\n\n\nA small example which I will work out manually to see what is the support for ({coffee, bread} –> {jam}). Generally I want to see how many transactions contained these 3 items. 0.12% of the transactions contained {bread, coffee, jam}\n\nmy_item_set <- Hmisc::Cs(Coffee , Jam , Bread)\n\nidx_sample <- grep( \"Transactio|^Coffee$|^Jam$|^Bread$\", names(dat2))\n\n\n\nitem_set_dat <- dat2[, idx_sample, with = F] \n\n#some transaction bought more thanone of \nitem_set_dat[, (my_item_set) := lapply(.SD, function(x) ifelse(x==0, 0, 1)), .SDcols = my_item_set]\n\nitem_set_dat[, total_items := rowSums(item_set_dat[, 2:4, with = F]) ]\n\nsupport_coffee_bread_jam <- table(item_set_dat$total_items)[\"3\"]/9531 \n\nsupport_coffee_bread_jam\n\n          3 \n0.001154129 \n\n\nTo calculate confidence({bread, coffee} –> {jam}) we should also calculate the support of ({bread, coffee}) which is the prorpotion of bread and coffee appearing together in the transactions which is about 8.9% of the transactions.\n\nitem_set_dat[, coffee_bread := rowSums(item_set_dat[, c(\"Bread\", \"Coffee\"), with = F]) ]\n\nhead(item_set_dat, 2)\n\n   Transaction Bread Coffee Jam total_items coffee_bread\n1:           1     1      0   0           1            1\n2:           2     0      0   0           0            0\n\nsupport_coffee_bread <- table(item_set_dat$coffee_bread)[\"2\"]/9531 \n\nsupport_coffee_bread\n\n         2 \n0.08939251 \n\n\n1.3% of the people who bought bread and coffee also bought jam. This is the confidence of({bread, coffee} –> {jam}) For statisticians this can be translated as conditional probability. In conditional probability notations P(Jam/bread, coffee) which is probability you will buy jam given that you have already bought bread and coffee. In association analysis we have {bread, coffee} >>{jam} bread and coffee implies jam. So the confidence measures the strength/probability of this implication.\n\nconfidence <- support_coffee_bread_jam/support_coffee_bread \n\nconfidence * 100\n\n      3 \n1.29108 \n\n\nUsing package arules we find the 10 rules with the highest confidence in descending order. Confidence({Toast} –>{Coffee}) had the highest confidence of 0.70440252. About 70.44% of the transactions that contained toast also contained coffee.\n\nlibrary(arules)\n\ntransactions <- as(split(dat$Item, dat$Transaction), \"transactions\")\n\nassoc_rules <- apriori(transactions,\n                 parameter = list(supp = 0.02, conf = 0.04, target = \"rules\"))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n       0.04    0.1    1 none FALSE            TRUE       5    0.02      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 189 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[94 item(s), 9465 transaction(s)] done [0.00s].\nsorting and recoding items ... [19 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 done [0.00s].\nwriting ... [38 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\nassoc_rules <- sort(assoc_rules, by='confidence', decreasing = TRUE)\n\n\ninspect(assoc_rules[1:30]) #%>% broom::tidy() %>% kable\n\n     lhs                rhs         support    confidence coverage   lift     \n[1]  {Toast}         => {Coffee}    0.02366614 0.70440252 0.03359746 1.4724315\n[2]  {Medialuna}     => {Coffee}    0.03518225 0.56923077 0.06180666 1.1898784\n[3]  {Pastry}        => {Coffee}    0.04754358 0.55214724 0.08610671 1.1541682\n[4]  {Juice}         => {Coffee}    0.02060222 0.53424658 0.03856313 1.1167500\n[5]  {Sandwich}      => {Coffee}    0.03824617 0.53235294 0.07184363 1.1127916\n[6]  {Cake}          => {Coffee}    0.05472795 0.52695829 0.10385631 1.1015151\n[7]  {Cookies}       => {Coffee}    0.02820919 0.51844660 0.05441099 1.0837229\n[8]  {Hot chocolate} => {Coffee}    0.02958267 0.50724638 0.05832013 1.0603107\n[9]  {}              => {Coffee}    0.47839408 0.47839408 1.00000000 1.0000000\n[10] {Tea}           => {Coffee}    0.04986793 0.34962963 0.14263074 0.7308402\n[11] {Pastry}        => {Bread}     0.02916006 0.33865031 0.08610671 1.0349774\n[12] {}              => {Bread}     0.32720549 0.32720549 1.00000000 1.0000000\n[13] {Bread}         => {Coffee}    0.09001585 0.27510494 0.32720549 0.5750592\n[14] {Cake}          => {Tea}       0.02377179 0.22889115 0.10385631 1.6047813\n[15] {Cake}          => {Bread}     0.02334918 0.22482197 0.10385631 0.6870972\n[16] {Tea}           => {Bread}     0.02810354 0.19703704 0.14263074 0.6021813\n[17] {Coffee}        => {Bread}     0.09001585 0.18816254 0.47839408 0.5750592\n[18] {Tea}           => {Cake}      0.02377179 0.16666667 0.14263074 1.6047813\n[19] {}              => {Tea}       0.14263074 0.14263074 1.00000000 1.0000000\n[20] {Coffee}        => {Cake}      0.05472795 0.11439929 0.47839408 1.1015151\n[21] {Coffee}        => {Tea}       0.04986793 0.10424028 0.47839408 0.7308402\n[22] {}              => {Cake}      0.10385631 0.10385631 1.00000000 1.0000000\n[23] {Coffee}        => {Pastry}    0.04754358 0.09938163 0.47839408 1.1541682\n[24] {Bread}         => {Pastry}    0.02916006 0.08911850 0.32720549 1.0349774\n[25] {}              => {Pastry}    0.08610671 0.08610671 1.00000000 1.0000000\n[26] {Bread}         => {Tea}       0.02810354 0.08588957 0.32720549 0.6021813\n[27] {Coffee}        => {Sandwich}  0.03824617 0.07994700 0.47839408 1.1127916\n[28] {Coffee}        => {Medialuna} 0.03518225 0.07354240 0.47839408 1.1898784\n[29] {}              => {Sandwich}  0.07184363 0.07184363 1.00000000 1.0000000\n[30] {Bread}         => {Cake}      0.02334918 0.07135938 0.32720549 0.6870972\n     count\n[1]   224 \n[2]   333 \n[3]   450 \n[4]   195 \n[5]   362 \n[6]   518 \n[7]   267 \n[8]   280 \n[9]  4528 \n[10]  472 \n[11]  276 \n[12] 3097 \n[13]  852 \n[14]  225 \n[15]  221 \n[16]  266 \n[17]  852 \n[18]  225 \n[19] 1350 \n[20]  518 \n[21]  472 \n[22]  983 \n[23]  450 \n[24]  276 \n[25]  815 \n[26]  266 \n[27]  362 \n[28]  333 \n[29]  680 \n[30]  221 \n\n\nI hope with this small example you can now understand how association analysis works."
  },
  {
    "objectID": "posts/busara_task/busara data analysis.html",
    "href": "posts/busara_task/busara data analysis.html",
    "title": "Busara Data Analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\nlibrary(data.table)\nlibrary(knitr)\nlibrary(ggthemes)\n#library(kableExtra)\n\n\nI was not sure if the presentation is about insights from the data or how I solved the problem.\nI decided to to combine both with R presentation.\n\n\nTask 1\n\nUnderstanding the demographics of company xyz.\nAtleast half are youth average age = 33.5\nAtleast half earn 5557\n\n\nxyz <- setDT(read_csv(\"XYZ.csv\"))\n\nxyz_sub <- xyz[, .(Gender, Age, Income)]\n\nxyz_subm <- melt(xyz_sub, id.vars = \"Gender\")\n\n\n\nSummary Statistics Age and Income\n\nxyz_subm %>% group_by(variable) %>%\n    summarise(Average = mean(value), Median = median(value),\n              Min = min(value), Max = max(value)) %>%\n    \n    kable() #%>% kable_styling() %>%\n\n\n\n\nvariable\nAverage\nMedian\nMin\nMax\n\n\n\n\nAge\n33.506\n33\n18\n50\n\n\nIncome\n5498.844\n5557\n1000\n9897\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\n% gender Gap\n\nMale/Female % al most equal\nThere are 5.2% more men than women\n\n\ngender <- xyz %>% group_by(Gender) %>%\n    summarise(freq = n()) %>%\n    mutate(Perc = round(freq/sum(freq) * 100, 2))\n\ngender %>% kable() #%>% kable_styling() %>%\n\n\n\n\nGender\nfreq\nPerc\n\n\n\n\nFemale\n237\n47.4\n\n\nMale\n263\n52.6\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\n% gender Gap\n\ngender[, -2] %>% spread(Gender, Perc) %>% \n    mutate(Percentage_Gender_Gap = Male - Female) %>% \n    kable() #%>% kable_styling() %>%\n\n\n\n\nFemale\nMale\nPercentage_Gender_Gap\n\n\n\n\n47.4\n52.6\n5.2\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n#Single Ladies Nyeri\n\n12(2.4% of the company employees) single ladies from Nyeri county\n\n\nsingle_nyeri <- xyz[Gender == \"Female\" & Marital_Status == \"Single\" & County == \"Nyeri\",]\nnrow(xyz)\n\n[1] 500\n\ncat(\"The Number of single ladies in Nyeri is \", nrow(single_nyeri))\n\nThe Number of single ladies in Nyeri is  12\n\n\n\n\nSummary Statistics Single Ladies Nyeri\n\nAverage age 36 and medium income is about $50\n\n\nsingle_nyeri %>%\n    summarise(Average_Age = mean(Age), Median_Income = median(Income)) %>%\n    kable() #%>%  kable_styling() %>%\n\n\n\n\nAverage_Age\nMedian_Income\n\n\n\n\n36\n5557\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nNumber of Juniors\n\n28 juniors\n\n\njuniors_26 <-xyz[!grepl(\"Operartions|Data\",Department)  & xyz$Age < 26 & grepl(\"Junior\", Role),]\n\ncat(\"The Number of juniors \", nrow(juniors_26))\n\nThe Number of juniors  28\n\njuniors_26 %>% group_by(Department) %>%\n    summarise(freq = n()) %>%\n    mutate(Perc = round(freq/sum(freq) * 100, 2)) %>%\n    kable() #%>% kable_styling() %>%\n\n\n\n\nDepartment\nfreq\nPerc\n\n\n\n\nAssociate\n5\n17.86\n\n\nFinance\n11\n39.29\n\n\nOperations\n8\n28.57\n\n\nResearch Analyst\n4\n14.29\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"40%\")\n\n\n\nDifference in mean income between male and female\n\nThe Operations has the biggest difference in mean income\nFemale/Males average earnings in different departments\n\n\nincome_gender <- xyz %>% group_by(Gender, Department) %>%\n    summarise(Average = mean(Income))\n\n\nincome_gender_dcast <- dcast(Department ~ Gender, data = income_gender) \n\nincome_gender_dcast %>% mutate( Difference = Male - Female) %>%\n    kable()#%>% kable_styling() %>%\n\n\n\n\nDepartment\nFemale\nMale\nDifference\n\n\n\n\nAssociate\n5345.047\n5071.941\n-273.1053\n\n\nData\n5613.420\n5270.396\n-343.0238\n\n\nFinance\n5574.714\n5936.750\n362.0357\n\n\nOperations\n5043.286\n6284.854\n1241.5685\n\n\nResearch Analyst\n5264.522\n5533.327\n268.8055\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"45%\")\n\n\n\nFunction to plot categorical variables\n\nbar_plot <- function(data, title,...) {\n    #load ggplot2\n    #function takes a data frame\n    #and other arguments that ggplot\n    #function from ggplot2 takes\n    # the other arguments are aesthetic mappings\n    require(ggplot2)\n    ggplot(data) + geom_bar(aes(...))+\n        ggtitle(title)+\n        ggthemes::theme_hc()+\n        ggthemes::scale_fill_hc()+\n        theme(legend.position = \"none\")\n        \n}\n\n\n\nFunction to plot categorical variables test 1\n\nbar_plot(xyz, Department, title = \"Department Distribution\", fill = Department)\n\n\n\n\n\n\nFunction to plot categorical variables test 2\n\nbar_plot(xyz, Gender, title = \"Gender Distibution\", fill = Gender)\n\n\n\n\n\n\nTask 2\nRead Files\n\nRead files using the patterns\n\n\nmy_files <- dir(path = \"Education\",pattern = \"^Chi|^Sch|^Persi|Secon|^Progr|Pri\")\n\nmy_files <- paste0(\"Education/\", my_files)\n\nlibrary(readxl)\n\nlist_files <- list()\n\nfor (i in 1:length(my_files)) {\n    \n    \n    x = read_excel(my_files[i]) \n    id = grep(\"Country Name\", x$`Data Source`)\n    nms <- x[id,]\n    names(x) <- nms %>% as.character()\n    list_files[[i]] <- x[-c(1:id),] \n    cat(\"...\")\n    \n}\n\n......................................................\n\n\n\n\nCombine Files\n\nSince files are stored in a list combine them\n\n\ndf_world <- rbindlist(list_files) %>% setDT()\n\ndf_world_melt <- melt(df_world, id.vars = names(df_world)[1:4])\n\nnms2 <- Hmisc::Cs(Country_Name, Country_Code,   \n          Indicator_Name,   Indicator_Code, Year,   Indicator_value)\n\nnames(df_world_melt) <- nms2\n\ndf_world_melt[,  Year := as.numeric(as.character(df_world_melt$Year))]\n\n\n\nHead output data frame\n\nhead(df_world_melt) %>% kable() #%>% \n\n\n\n\n\n\n\n\n\n\n\n\nCountry_Name\nCountry_Code\nIndicator_Name\nIndicator_Code\nYear\nIndicator_value\n\n\n\n\nAruba\nABW\nChildren out of school, primary, female\nSE.PRM.UNER.FE\n1960\nNA\n\n\nAfghanistan\nAFG\nChildren out of school, primary, female\nSE.PRM.UNER.FE\n1960\nNA\n\n\nAngola\nAGO\nChildren out of school, primary, female\nSE.PRM.UNER.FE\n1960\nNA\n\n\nAlbania\nALB\nChildren out of school, primary, female\nSE.PRM.UNER.FE\n1960\nNA\n\n\nAndorra\nAND\nChildren out of school, primary, female\nSE.PRM.UNER.FE\n1960\nNA\n\n\nArab World\nARB\nChildren out of school, primary, female\nSE.PRM.UNER.FE\n1960\nNA\n\n\n\n\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nHead output kenya data\n\nkenya_2011 <- df_world_melt[Country_Name == \"Kenya\" & Year >= 2011]\n\nhead(kenya_2011) #%>% kable() %>%\n\n   Country_Name Country_Code\n1:        Kenya          KEN\n2:        Kenya          KEN\n3:        Kenya          KEN\n4:        Kenya          KEN\n5:        Kenya          KEN\n6:        Kenya          KEN\n                                               Indicator_Name    Indicator_Code\n1:                    Children out of school, primary, female    SE.PRM.UNER.FE\n2: Persistence to last grade of primary, female (% of cohort) SE.PRM.PRSL.FE.ZS\n3:   Persistence to last grade of primary, male (% of cohort) SE.PRM.PRSL.MA.ZS\n4:  Primary completion rate, female (% of relevant age group) SE.PRM.CMPT.FE.ZS\n5:    Primary completion rate, male (% of relevant age group) SE.PRM.CMPT.MA.ZS\n6:                Progression to secondary school, female (%) SE.SEC.PROG.FE.ZS\n   Year Indicator_value\n1: 2011            <NA>\n2: 2011            <NA>\n3: 2011            <NA>\n4: 2011            <NA>\n5: 2011            <NA>\n6: 2011            <NA>\n\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"70%\")\n\n\n\nHead output kenya data and saving files\n\nwrite.csv(head(kenya_2011, 15), file = \"kenya data.csv\", row.names = F)\n\nkenya_2011_na <- kenya_2011[!is.na(kenya_2011$Indicator_value),]\nwrite.csv(head(kenya_2011_na, 15), file = \"kenya data without na.csv\", row.names = F)\n\nhead(kenya_2011_na) %>% kable() #%>%\n\n\n\n\n\n\n\n\n\n\n\n\nCountry_Name\nCountry_Code\nIndicator_Name\nIndicator_Code\nYear\nIndicator_value\n\n\n\n\nKenya\nKEN\nChildren out of school, primary, female\nSE.PRM.UNER.FE\n2012\n537736\n\n\nKenya\nKEN\nSchool enrollment, primary (gross), gender parity index (GPI)\nSE.ENR.PRIM.FM.ZS\n2012\n1.0080599784851101\n\n\nKenya\nKEN\nSchool enrollment, primary, female (% gross)\nSE.PRM.ENRR.FE\n2012\n112.41464233398401\n\n\nKenya\nKEN\nSchool enrollment, primary, male (% gross)\nSE.PRM.ENRR.MA\n2012\n111.51609802246099\n\n\nKenya\nKEN\nPrimary completion rate, female (% of relevant age group)\nSE.PRM.CMPT.FE.ZS\n2014\n100.183967590332\n\n\nKenya\nKEN\nPrimary completion rate, male (% of relevant age group)\nSE.PRM.CMPT.MA.ZS\n2014\n98.815101623535199\n\n\n\n\n   # kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"70%\")\n\n#Task 3\n\nfigari_sheet1 <- read_excel(\"Figari Bank.xlsx\" ) %>% setDT()\n\nfigari_sheet2 <- read_excel(\"Figari Bank.xlsx\", sheet = 2 ) %>% setDT()\n\nfigari_sheet2[,  Dates := as.Date(Dates, origin = \"1900-01-01\")]\n\nfigari_sheet2[,  year := year(Dates)]\n\n\nfigari_sheet2[,  month := month(Dates)]\nfigari_sheet2[, month := ifelse(nchar(month) == 1 ,paste(0, month), month)]\n\nfigari_sheet2[,  week_day := as.POSIXlt(Dates)$wday+1]\nfigari_sheet2[, week_day := ifelse(nchar(week_day) == 1 ,paste(0, week_day), week_day)]\nfigari_sheet2[,  week_no := week(Dates)]\n\nfigari_sheet2[, week_no := ifelse(nchar(week_no) == 1 ,paste(0, week_no), week_no)]\n\nfigari_sheet2[,  day_month := format(Dates, \"%d\")]\n\nfigari_sheet2_m <- melt(figari_sheet2[, c(3:9), with = F], id.vars = c(\"Amount\", \"Saving Mode\"))\n\n\n\nTask 3 Plots\n\nTime series will enable us too see if there is seasonal/cyclic effects/trend\nweek number after every two weeks, maybe end month\nSmoothing/decoposing often needed to see trend\n\n\nfigari_dat <- figari_sheet2_m %>% group_by(`Saving Mode`,variable, value) %>%\n    summarise(Average = mean(Amount)) \ntitles <- levels(as.factor(figari_dat$`Saving Mode`))\ntitles <- paste(\"Average Savings for\", titles)\nfigari_dat_split <- split(figari_dat, figari_dat$`Saving Mode`)\nplots_figari <- list()\nfor ( i in 1:length(figari_dat_split)) {\n    this = figari_dat_split[[i]]\n    #write.csv(this, file = \"this.csv\", row.names = F)\n   plots_figari[[i]] <- ggplot(this, aes(value, Average)) +\n       facet_wrap(~variable, scales = \"free_x\", ncol = 1)+\n       geom_line(data = this, aes(value, Average, group = 1)) +\n       ggthemes::theme_hc()+\n       labs(x = \"\", y = \"Average amount saved (KES)\", title = titles[i])# +\n       #theme(axis.text.x = element_text(angle = 30, vjust = 1, hjust = 1))\n    \n}\n\n\n\nAverage Amount saved at the Bank\n\nYear no visible trend/ few years\nfirst two months higher savings\nlowest between month 3 to month 8\nSave more from day 2 - day 4\nWeek 1- 3 more savings drops to week 10\nmore save less in around day 10 of the month\ndecreasing trend trend from day 4 -10\n\n\nplots_figari[[2]]\n\n\n\n\n\n\nAverage Amount saved at the Agent\n\nSave more from March to June\nIncreasing trend from week 1 to 23 then decreasing\nSave less towards end of a month\n\n\nplots_figari[[1]]\n\n\n\n\n\n\nAverage Amount saved Mobile money\n\non average\nsave less from month 3 to 6\nsave less from week 9 to 22\n\n\nplots_figari[[3]]\n\n\n\n\n\n\nEnd Month Savings Favourite tool\n\nI’m thinking about the number of times someone saves. Average maybe skewed.\nWomen prefer to save using agent\nIn regions no Nyeri\n\n\nnames(figari_sheet2)[1] = names(figari_sheet1)[1]\n\nfigari_comb <- merge(figari_sheet2, figari_sheet1, by = \"CustomerID\")\n\nend_month <- figari_comb %>% \n    group_by(day_month, `Saving Mode`) %>%\n    summarise(Freq = n()) %>%\n    mutate(perc = round(100 * Freq/sum(Freq), 2)) %>% ungroup()\n#The number of times one deposits\nggplot(end_month, aes(day_month, Freq )) +\n    geom_line(aes(color =`Saving Mode`, group =`Saving Mode` ), size = 1)+\n    theme_hc()+\n    scale_color_hc(name = \"\") +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\nHistogram Deposits\n\nWhat you would expect.\n\n\ndeposits <- figari_sheet2[, .(freq = .N), by = CustomerID] \n\n#approximmately poison\nhist(deposits$freq, col = \"black\",\n     main = \"Deposits\", \n     xlab = \"Deposits\")\n\n\n\n\n\n\nSubset People who have made one deposit\n\nfigari_deposits <- merge(deposits, figari_sheet1, by = \"CustomerID\")\nfigari_deposits_one <- figari_deposits[freq == 1] \n\n\n\nDemographic characteristics of those who have only made one deposit\n\n\nGender\n\nfigari_deposits_one %>% group_by(Gender) %>%\n    summarise(freq= n()) %>%\n    mutate(Perc =round(freq/sum(freq) *100, 2) ) %>%\n    kable()# %>%\n\n\n\n\nGender\nfreq\nPerc\n\n\n\n\nFemale\n141\n49.3\n\n\nMale\n145\n50.7\n\n\n\n\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nRegion\n\nfigari_deposits_one %>% group_by(Region) %>%\n    summarise(freq= n()) %>% \n    mutate(Perc =round(freq/sum(freq) *100, 2) ) %>%\n    kable() #%>%\n\n\n\n\nRegion\nfreq\nPerc\n\n\n\n\nBondo\n25\n8.74\n\n\nGatitu\n53\n18.53\n\n\nKawangware\n21\n7.34\n\n\nKayole\n13\n4.55\n\n\nKibera\n31\n10.84\n\n\nKilimani\n41\n14.34\n\n\nKirinyaga\n18\n6.29\n\n\nRongai\n10\n3.50\n\n\nRuai\n45\n15.73\n\n\nTaita\n29\n10.14\n\n\n\n\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"100%\")\n\n\n\nAge\n\nfigari_deposits_one %>% \n    summarise(Mean= round(mean(Age), 2), Median  = median(Age)) %>%\n    kable() #%>%\n\n\n\n\nMean\nMedian\n\n\n\n\n54.71\n56\n\n\n\n\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nTask 4\n\n\nProject Motivation\nData has the potential to transform business and drive the creation of business value. It can be used for a range of tasks such visualization relationships between variables to predicting if an event will occur. The later is one of the heavily reaserched areas in recent times. The reason for this is that data has grown exponentially and so does the computing power. Banks and financial institutions used data analytics for a range of value such as fraud detetction customer segment, recruiting, credit scoring and so on.\nIn this study I will use Bogoza data set to build a credit model where an applicat will be avaluated on whether they will default or not.\nHigh accuracy for this model will be required because predicting false positives will eventually cause a business to make a loss and false negatives means that the financial instituion looses business.\n\n\nData Cleaning\nFirst step is data cleaning. This ensures that columns are consistent. For instance the target variable had values such as Y y yes where all of them represent yes.\n\n#some algorithms like xgboost take numeric data\n#you can convert binary vars to 1,0\n# and form dummie variables using library dummies\n#for variables with more than 2 categories\nborogoza <- setDT(read_csv( \"Bagorogoza Loan.csv\"))\n\nborogoza[, Target := ifelse(grepl(\"y|Y\", Target), 1, 0)]\n\nborogoza[, Gender := ifelse(grepl(\"^m$|^male$\", tolower(Gender)), 0, 1)]\n\nborogoza[, Married := ifelse(grepl(\"Yes\",Married), 1, 0)]\n\nborogoza[, Education := ifelse(grepl(\"not\", tolower(Education)), 0, 1)]\n\nborogoza[, Self_Employed := ifelse(grepl(\"Yes\",Self_Employed), 1, 0)]\n\nborogoza[, Property_Area := ifelse(grepl(\"rural\",tolower(Property_Area)), \"Rural\", Property_Area)]\n\nborogoza[, Property_Area := ifelse(grepl(\"semi\",tolower(Property_Area)), \"Semi-urban\", Property_Area)]\n\nborogoza[, Property_Area := ifelse(grepl(\"^urban$\",tolower(Property_Area)), \"Urban\", Property_Area)]\n\n\n\nVariable Selection\n\nWhere we run descripte statistics\n\n\n\nVisualize Categorical variables\nVisualization and summary statistics is an impostant step before fitting any model as this will give you a glimpse of how the variables are associated with target variable. In this case I will use stacked barplot as from them you can see if the prorpotions of defaulters and non defaulters is equal in defferent categories of a variable. From the graphs we can see that the prorpotion of defaulters and non defaulters is defferent for the different credit history categories. This is aslo seen in the prorpety area. From the categorical variables we can therefore conclude that one of the best predictors is credit history.\n\nnumeric_vars <- Hmisc::Cs(ApplicantIncome,CoapplicantIncome, LoanAmount )\n\nnms_bo <- names(borogoza)[-1]\n\ncat_vars <- nms_bo[!nms_bo %in% numeric_vars]\n\n\nborogoza_catm <- melt(borogoza[, cat_vars, with = F], id.vars = \"Target\")\n\nborogoza_catm_perc <-borogoza_catm  %>%  group_by(variable, value, Target) %>%\n    summarise(freq= n()) %>% mutate(perc =round(freq/sum(freq) *100, 2) )\n\nlibrary(ggthemes)\nggplot(borogoza_catm_perc, aes(value, perc, fill = factor(Target) )) +\n    geom_bar(stat = \"identity\") +facet_wrap(~variable, scales = \"free_x\")+\n    scale_fill_economist(name = \"\")+\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\nVisualize numeric variables\nFor the numeric variables boxplot help us visualize which distribution is different from the other. Non overlapping boxplot for defaulters and non defaulters may indicate that the mean/median values in the two groups was significantly different. From this we can see that it’s unlikely that education and self employment affect loan repayment and for this we drop this two variables\n\nborogoza_numm <- melt(borogoza[, c(numeric_vars, \"Target\"), with = F], id.vars = \"Target\")\n\nggplot(borogoza_numm, aes(as.factor(Target), scale(value ))) +\n    geom_boxplot() +facet_wrap(~variable, scales = \"free_y\")\n\n\n\n\n\n\nOne-Hot Encoding for categorical variables with more than 2 levels\nIn this step variables with more than two categories are converted to dummies variables. The first column in each category is dropped as it’s linearly depedent with the second column.\n\nchars <- unlist(lapply(borogoza[, -1, with = F], is.character)) \n\nchars <- nms_bo[chars]\n\nlibrary(dummies)\nborogoza_dummy <- dummy.data.frame(borogoza, names = c(chars, \"Loan_Amount_Term\")) %>%\n    setDT()\n\n\nborogoza_dummy[, Loan_ID := NULL]\nborogoza_dummy[, Loan_Amount_Term36 := NULL]\nborogoza_dummy[, `Property_AreaSemi-urban` := NULL]\nborogoza_dummy[, `Dependents1` := NULL]\n\n\n\nScale variables\nIt’s important to scale your variables since it leads to faster convergence and since some algorithm use distances to find decision boundary this means that variables with big values will have a big influence.\n\nxvars <- names(borogoza_dummy)[!names(borogoza_dummy) %in% \"Target\"]\nborogoza_dummy[, (xvars) := lapply(.SD, function(x) scale(x)), .SDcols = xvars ]\n\n\n\nSplit test and train sets\nThis is important as it helps evaluate your model on data it has never seen. The model will be trained on one set(training set) and tested using test set.\n\nset.seed(200) # for reproducibility\ntrain_sample <- sample(1:nrow(borogoza_dummy), round(0.7*nrow(borogoza_dummy)))\n\ntrain <- borogoza_dummy[train_sample,]\ntest <- borogoza_dummy[-train_sample,]\n\n\n\nFit Logistic Regression\nLogistic regression was fit to predict the probability of someone defaulting. The advantages of logistic regression is interprettable, ie you can see the association between a predictor and response value, it also gives a probability. This is very improtant when you want to have your own cut off point eg you want to label someone as a defulter if you the predicted probability is more than 0.7. This increases precision but lowers recall. Using stepwise selection the model was used to select the variables that best predict loan deafult.\n\nfit_glm <- glm(Target ~ Married + CoapplicantIncome + Loan_Amount_Term60 + \n    Loan_Amount_Term180 + Loan_Amount_Term300 + \n     Loan_Amount_Term360 + Credit_History + Property_AreaRural + \n     Property_AreaUrban ,data = train, family = binomial)\n\nborogoza_dummy <- borogoza_dummy[, .(Target,Married , CoapplicantIncome , Loan_Amount_Term60 , \n     Loan_Amount_Term180 , Loan_Amount_Term300 , \n     Loan_Amount_Term360 , Credit_History , Property_AreaRural , \n     Property_AreaUrban)]\n\n\ntrain <- borogoza_dummy[train_sample,]\ntest <- borogoza_dummy[-train_sample,]\nsummary(fit_glm) %>% xtable::xtable() %>% kable()# %>%\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nz value\nPr(>|z|)\n\n\n\n\n(Intercept)\n1.1415592\n5.3056959\n0.2151573\n0.8296447\n\n\nMarried\n0.1436465\n0.1721719\n0.8343199\n0.4041008\n\n\nCoapplicantIncome\n-0.1281571\n0.1406808\n-0.9109779\n0.3623070\n\n\nLoan_Amount_Term60\n1.1450622\n73.3785540\n0.0156049\n0.9875496\n\n\nLoan_Amount_Term180\n0.5524587\n0.2784012\n1.9843977\n0.0472115\n\n\nLoan_Amount_Term300\n0.0344328\n0.1576171\n0.2184586\n0.8270718\n\n\nLoan_Amount_Term360\n0.5348545\n0.2558935\n2.0901448\n0.0366048\n\n\nCredit_History\n1.4385841\n0.2260642\n6.3636094\n0.0000000\n\n\nProperty_AreaRural\n-0.4079017\n0.2058167\n-1.9818693\n0.0474939\n\n\nProperty_AreaUrban\n-0.4665362\n0.2048849\n-2.2770650\n0.0227823\n\n\n\n\n    # kable_styling() %>%\n    # scroll_box(width = \"100%\", height = \"100%\")\n\n#MASS::stepAIC(fit_glm)\n\nThe estimate column shows the log odds. Positive values means that the variable makes it more likely for a person to repay their loan negative values means that the person is less likely to repay.\n\n\nConfusion Matrix Logistic regression\nThe confusion matrix evaluate correctly classified cases. A perfect fit will have all values in the main diagnol while the entries of lower/upper triangulars should be zeros. In this case we have 14 cases of false positives and 7 cases of false negatives the accuracy of the model is 0.82 with and f1 score of 0.87. F1 score is a very important evaluation metric where there is unbalanced classes.\n\nlibrary(caret)\npred_glm <- predict(fit_glm,newdata = test)\n\npred_glm <- ifelse(pred_glm>0.7, 1 , 0)\n\ntable(test$Target, pred_glm) %>% kable()# %>%\n\n\n\n\n\n0\n1\n\n\n\n\n0\n20\n19\n\n\n1\n3\n73\n\n\n\n\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nAccuracy Logistic regression\n\nlibrary(broom)\nlibrary(pROC)\ntable(test$Target, pred_glm) %>% \n    confusionMatrix(positive = \"1\") %>% \n    tidy() %>% kable()# %>%\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nclass\nestimate\nconf.low\nconf.high\np.value\n\n\n\n\naccuracy\nNA\n0.8086957\n0.724814\n0.8760546\n0.4628434\n\n\nkappa\nNA\n0.5258621\nNA\nNA\nNA\n\n\nmcnemar\nNA\nNA\nNA\nNA\n0.0013838\n\n\nsensitivity\n1\n0.7934783\nNA\nNA\nNA\n\n\nspecificity\n1\n0.8695652\nNA\nNA\nNA\n\n\npos_pred_value\n1\n0.9605263\nNA\nNA\nNA\n\n\nneg_pred_value\n1\n0.5128205\nNA\nNA\nNA\n\n\nprecision\n1\n0.9605263\nNA\nNA\nNA\n\n\nrecall\n1\n0.7934783\nNA\nNA\nNA\n\n\nf1\n1\n0.8690476\nNA\nNA\nNA\n\n\nprevalence\n1\n0.8000000\nNA\nNA\nNA\n\n\ndetection_rate\n1\n0.6347826\nNA\nNA\nNA\n\n\ndetection_prevalence\n1\n0.6608696\nNA\nNA\nNA\n\n\nbalanced_accuracy\n1\n0.8315217\nNA\nNA\nNA\n\n\n\n\n    # kable_styling() %>%\n    # scroll_box(width = \"100%\", height = \"100%\")\n\n\n\nArea under curve\nThis is important as it will help you know if the sufferes from high false negatives or false positives. A value greater than 0.8 is normally desired in this case we achieve 0.74.\n\nroc(as.numeric(test$Target), pred_glm, print.auc=T, print.auc.y=0.5, levels =0:1 ) \n\n\nCall:\nroc.default(response = as.numeric(test$Target), predictor = pred_glm,     levels = 0:1, print.auc = T, print.auc.y = 0.5)\n\nData: pred_glm in 39 controls (as.numeric(test$Target) 0) < 76 cases (as.numeric(test$Target) 1).\nArea under the curve: 0.7367\n\n\n\n\nCross Validation SVM\nNext we fit Support vector machine model. We start by finding the best parameters using cross validation. We use 10 fold this where train set is randomly split into 10 sets. In each cases one of the 1 set is used as a valiadation/test set while the other 9 are used to train the model.\n\nlibrary(e1071)\ntune.out = tune(svm, as.factor(Target)~., data = train, kernel =\"radial\", \n                type =\"C-classification\",\n                ranges =list (cost=c(0.01, 0.1, 1 ,5 ,  10),\n                              gamma = c(0.01,  0.1, 1 ,5 )))\n\nsummary(tune.out)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost gamma\n    5  0.01\n\n- best performance: 0.1595442 \n\n- Detailed performance results:\n    cost gamma     error dispersion\n1   0.01  0.01 0.2747863 0.06706580\n2   0.10  0.01 0.2747863 0.06706580\n3   1.00  0.01 0.1596866 0.06510749\n4   5.00  0.01 0.1595442 0.06956599\n5  10.00  0.01 0.1595442 0.06956599\n6   0.01  0.10 0.2747863 0.06706580\n7   0.10  0.10 0.1967236 0.07767111\n8   1.00  0.10 0.1670940 0.06780670\n9   5.00  0.10 0.1633903 0.06543082\n10 10.00  0.10 0.1633903 0.06543082\n11  0.01  1.00 0.2747863 0.06706580\n12  0.10  1.00 0.2747863 0.06706580\n13  1.00  1.00 0.2041311 0.06754684\n14  5.00  1.00 0.2004274 0.07183672\n15 10.00  1.00 0.2078348 0.07591537\n16  0.01  5.00 0.2747863 0.06706580\n17  0.10  5.00 0.2747863 0.06706580\n18  1.00  5.00 0.2078348 0.06963224\n19  5.00  5.00 0.2152422 0.07506115\n20 10.00  5.00 0.2189459 0.08024003\n\n\n\n\nConfusion Matrix SVM\n\nfit_svm <- svm(as.factor(Target)~., data = train, cost =5  , gamma = .01, \n               kernel = \"radial\", type =\"C-classification\")\n\n\npred_svm <-predict(fit_svm, newdata = test)\ntable(test$Target, pred_svm) %>% kable() #%>%\n\n\n\n\n\n0\n1\n\n\n\n\n0\n16\n23\n\n\n1\n2\n74\n\n\n\n\n   # kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nArea under curve\n\nroc(test$Target, as.numeric(pred_svm), print.auc=T, print.auc.y=0.5, levels =0:1 ) \n\n\nCall:\nroc.default(response = test$Target, predictor = as.numeric(pred_svm),     levels = 0:1, print.auc = T, print.auc.y = 0.5)\n\nData: as.numeric(pred_svm) in 39 controls (test$Target 0) < 76 cases (test$Target 1).\nArea under the curve: 0.692\n\n\n\n\nAccuracy SVM\n\ntable(test$Target, pred_svm) %>% \n    confusionMatrix(positive = \"1\") %>% \n    tidy() %>% kable() #%>%\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nclass\nestimate\nconf.low\nconf.high\np.value\n\n\n\n\naccuracy\nNA\n0.7826087\n0.6960357\n0.8541027\n0.9685321\n\n\nkappa\nNA\n0.4418560\nNA\nNA\nNA\n\n\nmcnemar\nNA\nNA\nNA\nNA\n0.0000633\n\n\nsensitivity\n1\n0.7628866\nNA\nNA\nNA\n\n\nspecificity\n1\n0.8888889\nNA\nNA\nNA\n\n\npos_pred_value\n1\n0.9736842\nNA\nNA\nNA\n\n\nneg_pred_value\n1\n0.4102564\nNA\nNA\nNA\n\n\nprecision\n1\n0.9736842\nNA\nNA\nNA\n\n\nrecall\n1\n0.7628866\nNA\nNA\nNA\n\n\nf1\n1\n0.8554913\nNA\nNA\nNA\n\n\nprevalence\n1\n0.8434783\nNA\nNA\nNA\n\n\ndetection_rate\n1\n0.6434783\nNA\nNA\nNA\n\n\ndetection_prevalence\n1\n0.6608696\nNA\nNA\nNA\n\n\nbalanced_accuracy\n1\n0.8258877\nNA\nNA\nNA\n\n\n\n\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"100%\")\n\n\n\nValidation Curves\nThe two models almost give equal results based on accuracy, f1 score and area under the curve. In this section we will evaluate the models using learning curves to see if they suffer from high variance or bias. In this case the model sufferes from high bias. It’s evident that adding more data won’t solve accuracy problems. In this case additional features would help.\n\nsets <- seq(from = 50, to = nrow(train), by = 50)\nsets[length(sets)] <-nrow(train) \ntrain.err <- c()\ntest.err<- c()\nfor (i in 1:length(sets)) {\n    \n    traini = train[1:sets[i],]\n    fit_svm <- svm(as.factor(Target)~., data = traini, cost =5  , gamma = .01, \n               kernel = \"radial\", type =\"C-classification\")\n\n    \n    pred_train = predict(fit_svm, newdata = traini)\n    train.err[i] =1 -  mean(pred_train == traini$Target)\n    pred_test <- predict(fit_svm, newdata = test)\n    test.err[i] = 1 - mean(test$Target == pred_test)\n    \n    cat(i,\" \")\n    \n}\n\n1  2  3  4  5  \n\nmatplot(sets, cbind(test.err, train.err), pch = 19, col = c(\"red\", \"blue\"),\n        type = \"b\", ylab = \"Error\", xlab = \"Train sample size\", main = \"SVM Training and Validation errors\")\nlegend(\"topright\", legend = c(\"Test\", \"Train\"), pch = 19, col = c(\"red\", \"blue\"))\n\n\n\n\n\n\nDeployment\nOther model like Xgboost which uses boosting and bagging could first be used to see if the model performs better on this data. The problem could after this be intergrated with a loan evaluation software where it can help loan officers decide if the will award a loan."
  }
]