[
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html",
    "title": "ML with tree based models in r",
    "section": "",
    "text": "Let’s get started and build our first classification tree. A classification tree is a decision tree that performs a classification (vs regression) task. You will train a decision tree model to understand which loan applications are at higher risk of default using a subset of the German Credit Dataset. The response variable, called “default”, indicates whether the loan went into a default or not, which means this is a binary classification problem (there are just two classes). You will use the rpart package to fit the decision tree and the rpart.plot package to visualize the tree.\n\n# Look at the data\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(rpart)\nlibrary(rpart.plot)\ncredit <- fread(\"credit.csv\", stringsAsFactors = TRUE) \ncredit[, default := factor(default)]\ncreditsub <- credit[sample(1:nrow(credit), 522),]\nstr(creditsub)\n\nClasses 'data.table' and 'data.frame':  522 obs. of  17 variables:\n $ checking_balance    : Factor w/ 4 levels \"1 - 200 DM\",\"< 0 DM\",..: 4 4 4 2 1 4 4 1 2 1 ...\n $ months_loan_duration: int  18 4 15 12 18 18 18 11 12 36 ...\n $ credit_history      : Factor w/ 5 levels \"critical\",\"good\",..: 2 1 2 2 1 2 5 4 2 4 ...\n $ purpose             : Factor w/ 6 levels \"business\",\"car\",..: 5 2 5 4 1 2 2 5 2 2 ...\n $ amount              : int  3422 3380 2221 795 3590 2662 6458 4771 3651 7432 ...\n $ savings_balance     : Factor w/ 5 levels \"100 - 500 DM\",..: 3 3 2 3 3 5 3 3 4 3 ...\n $ employment_duration : Factor w/ 5 levels \"1 - 4 years\",..: 4 2 1 3 5 2 4 2 1 1 ...\n $ percent_of_income   : int  4 1 2 4 3 4 2 2 1 2 ...\n $ years_at_residence  : int  4 1 4 4 3 3 4 4 3 2 ...\n $ age                 : int  47 37 20 53 40 32 39 51 31 54 ...\n $ other_credit        : Factor w/ 3 levels \"bank\",\"none\",..: 1 2 2 2 2 2 1 2 2 2 ...\n $ housing             : Factor w/ 3 levels \"other\",\"own\",..: 2 2 3 2 2 2 2 2 2 3 ...\n $ existing_loans_count: int  3 1 1 1 3 1 2 1 1 1 ...\n $ job                 : Factor w/ 4 levels \"management\",\"skilled\",..: 2 2 2 2 3 2 1 2 2 2 ...\n $ dependents          : int  2 2 1 1 2 1 2 1 2 1 ...\n $ phone               : Factor w/ 2 levels \"no\",\"yes\": 2 1 1 1 2 1 2 1 1 1 ...\n $ default             : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 2 1 1 2 1 1 1 ...\n - attr(*, \".internal.selfref\")=<externalptr> \n\n# Create the model\ncredit_model <- rpart(formula = default ~ ., \n                      data = creditsub, \n                      method = \"class\")\n\n# Display the results\nrpart.plot(x = credit_model, yesno = 2, type = 0, extra = 0)"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#traintest-split",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#traintest-split",
    "title": "ML with tree based models in r",
    "section": "Train/test split",
    "text": "Train/test split\nFor this exercise, you’ll randomly split the German Credit Dataset into two pieces: a training set (80%) called credit_train and a test set (20%) that we will call credit_test. We’ll use these two sets throughout the chapter.\n\n# Total number of rows in the credit data frame\nn <- nrow(credit)\n\n# Number of rows for the training set (80% of the dataset)\nn_train <- round(.8 * n) \n\n# Create a vector of indices which is an 80% random sample\nset.seed(123)\ntrain_indices <- sample(1:n, n_train)\n\n# Subset the credit data frame to training indices only\ncredit_train <- credit[train_indices, ]  \n  \n# Exclude the training indices to create the test set\ncredit_test <- credit[-train_indices, ]  \n\n# Train the model (to predict 'default')\ncredit_model <- rpart(formula = default ~., \n                      data = credit_train, \n                      method = \"class\")\n\n# Look at the model output                      \nprint(credit_model)\n\nn= 800 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 800 230 no (0.7125000 0.2875000)  \n    2) checking_balance=> 200 DM,unknown 365  48 no (0.8684932 0.1315068) *\n    3) checking_balance=1 - 200 DM,< 0 DM 435 182 no (0.5816092 0.4183908)  \n      6) months_loan_duration< 22.5 259  85 no (0.6718147 0.3281853)  \n       12) credit_history=critical,good,poor 235  68 no (0.7106383 0.2893617)  \n         24) months_loan_duration< 11.5 70  11 no (0.8428571 0.1571429) *\n         25) months_loan_duration>=11.5 165  57 no (0.6545455 0.3454545)  \n           50) amount>=1282 112  30 no (0.7321429 0.2678571) *\n           51) amount< 1282 53  26 yes (0.4905660 0.5094340)  \n            102) purpose=business,education,furniture/appliances 34  12 no (0.6470588 0.3529412) *\n            103) purpose=car,renovations 19   4 yes (0.2105263 0.7894737) *\n       13) credit_history=perfect,very good 24   7 yes (0.2916667 0.7083333) *\n      7) months_loan_duration>=22.5 176  79 yes (0.4488636 0.5511364)  \n       14) savings_balance=> 1000 DM,unknown 29   7 no (0.7586207 0.2413793) *\n       15) savings_balance=100 - 500 DM,500 - 1000 DM,< 100 DM 147  57 yes (0.3877551 0.6122449)  \n         30) months_loan_duration< 47.5 119  54 yes (0.4537815 0.5462185)  \n           60) amount>=2313.5 93  45 no (0.5161290 0.4838710)  \n            120) amount< 3026 19   5 no (0.7368421 0.2631579) *\n            121) amount>=3026 74  34 yes (0.4594595 0.5405405)  \n              242) percent_of_income< 2.5 38  15 no (0.6052632 0.3947368)  \n                484) purpose=business,car,education 23   6 no (0.7391304 0.2608696) *\n                485) purpose=car0,furniture/appliances,renovations 15   6 yes (0.4000000 0.6000000) *\n              243) percent_of_income>=2.5 36  11 yes (0.3055556 0.6944444) *\n           61) amount< 2313.5 26   6 yes (0.2307692 0.7692308) *\n         31) months_loan_duration>=47.5 28   3 yes (0.1071429 0.8928571) *"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#compute-confusion-matrix",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#compute-confusion-matrix",
    "title": "ML with tree based models in r",
    "section": "Compute confusion matrix",
    "text": "Compute confusion matrix\nAs discussed in the previous video, there are a number of different metrics by which you can measure the performance of a classification model. In this exercise, we will evaluate the performance of the model using test set classification error. A confusion matrix is a convenient way to examine the per-class error rates for all classes at once. The confusionMatrix() function from the caret package prints both the confusion matrix and a number of other useful classification metrics such as “Accuracy” (fraction of correctly classified instances). The caret package has been loaded for you.\n\nlibrary(caret)\n# Generate predicted classes using the model object\nclass_prediction <- predict(object = credit_model,  \n                        newdata = credit_test,   \n                        type = \"class\")  \n                            \n# Calculate the confusion matrix for the test set\n\nclass_prediction <- factor(class_prediction, levels = levels(credit_test$default) )\nconfusionMatrix(data = class_prediction,       \n                reference = credit_test$default, positive = \"yes\")  \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  117  44\n       yes  13  26\n                                          \n               Accuracy : 0.715           \n                 95% CI : (0.6471, 0.7764)\n    No Information Rate : 0.65            \n    P-Value [Acc > NIR] : 0.03046         \n                                          \n                  Kappa : 0.3023          \n                                          \n Mcnemar's Test P-Value : 7.08e-05        \n                                          \n            Sensitivity : 0.3714          \n            Specificity : 0.9000          \n         Pos Pred Value : 0.6667          \n         Neg Pred Value : 0.7267          \n             Prevalence : 0.3500          \n         Detection Rate : 0.1300          \n   Detection Prevalence : 0.1950          \n      Balanced Accuracy : 0.6357          \n                                          \n       'Positive' Class : yes"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#compare-models-with-a-different-splitting-criterion",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#compare-models-with-a-different-splitting-criterion",
    "title": "ML with tree based models in r",
    "section": "Compare models with a different splitting criterion",
    "text": "Compare models with a different splitting criterion\nTrain two models that use a different splitting criterion and use the validation set to choose a “best” model from this group. To do this you’ll use the parms argument of the rpart() function. This argument takes a named list that contains values of different parameters you can use to change how the model is trained. Set the parameter split to control the splitting criterion.\n\n# Train a gini-based model\ncredit_model1 <- rpart(formula = default ~ ., \n                       data = credit_train, \n                       method = \"class\",\n                       parms = list(split = \"gini\"))\n\n# Train an information-based model\ncredit_model2 <- rpart(formula = default ~ ., \n                       data = credit_train, \n                       method = \"class\",\n                       parms = list(split = \"information\"))\n\n# Generate predictions on the validation set using the gini model\npred1 <- predict(object = credit_model1, \n             newdata = credit_test,\n             type = \"class\")    \n\n# Generate predictions on the validation set using the information model\npred2 <- predict(object = credit_model2, \n             newdata = credit_test,\n             type = \"class\") \n\ndt_preds <- predict(object = credit_model2, \n             newdata = credit_test,\n             type = \"prob\") \n\n# Compare classification error\nlibrary(Metrics)\nce(actual = credit_test$default, \n   predicted = pred1)\n\n[1] 0.285\n\nce(actual = credit_test$default, \n   predicted = pred2)  \n\n[1] 0.285"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#the-response-is-final_grade-numeric-from-0-to-20-output-target.",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#the-response-is-final_grade-numeric-from-0-to-20-output-target.",
    "title": "ML with tree based models in r",
    "section": "The response is final_grade (numeric: from 0 to 20, output target).",
    "text": "The response is final_grade (numeric: from 0 to 20, output target).\nAfter initial exploration, split the data into training, validation, and test sets. In this chapter, we will introduce the idea of a validation set, which can be used to select a “best” model from a set of competing models. In Chapter 1, we demonstrated a simple way to split the data into two pieces using the sample() function. In this exercise, we will take a slightly different approach to splitting the data that allows us to split the data into more than two parts (here, we want three: train, validation, test). We still use the sample() function, but instead of sampling the indices themselves, we will assign each row to either the training, validation or test sets according to a probability distribution. The dataset grade is already in your workspace.\n\ngrade <- read.csv(\"grade.csv\")\n# Look at the data\nstr(grade)\n\n'data.frame':   395 obs. of  8 variables:\n $ final_grade: num  3 3 5 7.5 5 7.5 5.5 3 9.5 7.5 ...\n $ age        : int  18 17 15 15 16 16 16 17 15 15 ...\n $ address    : chr  \"U\" \"U\" \"U\" \"U\" ...\n $ studytime  : int  2 2 2 3 2 2 2 2 2 2 ...\n $ schoolsup  : chr  \"yes\" \"no\" \"yes\" \"no\" ...\n $ famsup     : chr  \"no\" \"yes\" \"no\" \"yes\" ...\n $ paid       : chr  \"no\" \"no\" \"yes\" \"yes\" ...\n $ absences   : int  6 4 10 2 4 10 0 6 0 0 ...\n\n# Set seed and create assignment\nset.seed(1)\nassignment <- sample(1:3, size = nrow(grade), prob = c(.7, .15, .15), replace = TRUE)\n\n# Create a train, validation and tests from the original data frame \ngrade_train <- grade[assignment == 1, ]    # subset grade to training indices only\ngrade_valid <- grade[assignment == 2, ]  # subset grade to validation indices only\ngrade_test <- grade[assignment == 3, ]   # subset grade to test indices only"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-regression-tree-model",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-regression-tree-model",
    "title": "ML with tree based models in r",
    "section": "Train a regression tree model",
    "text": "Train a regression tree model\nIn this exercise, we will use the grade_train dataset to fit a regression tree using rpart() and visualize it using rpart.plot(). A regression tree plot looks identical to a classification tree plot, with the exception that there will be numeric values in the leaf nodes instead of predicted classes. This is very similar to what we did previously in Chapter 1. When fitting a classification tree, we use method = “class”, however, when fitting a regression tree, we need to set method = “anova”. By default, the rpart() function will make an intelligent guess as to what the method value should be based on the data type of your response column, but it’s recommened that you explictly set the method for reproducibility reasons (since the auto-guesser may change in the future). The grade_train training set is loaded into the workspace.\n\n# Train the model\ngrade_model <- rpart(formula = final_grade ~ ., \n                     data = grade_train, \n                     method = \"anova\")\n\n# Look at the model output                      \nprint(grade_model)\n\nn= 282 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 282 1519.49700 5.271277  \n   2) absences< 0.5 82  884.18600 4.323171  \n     4) paid=no 50  565.50500 3.430000  \n       8) famsup=yes 22  226.36360 2.272727 *\n       9) famsup=no 28  286.52680 4.339286 *\n     5) paid=yes 32  216.46880 5.718750  \n      10) age>=17.5 10   82.90000 4.100000 *\n      11) age< 17.5 22   95.45455 6.454545 *\n   3) absences>=0.5 200  531.38000 5.660000  \n     6) absences>=13.5 42  111.61900 4.904762 *\n     7) absences< 13.5 158  389.43670 5.860759  \n      14) schoolsup=yes 23   50.21739 4.847826 *\n      15) schoolsup=no 135  311.60000 6.033333  \n        30) studytime< 3.5 127  276.30710 5.940945 *\n        31) studytime>=3.5 8   17.00000 7.500000 *\n\n# Plot the tree model\nrpart.plot(x = grade_model, yesno = 2, type = 0, extra = 0)"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-a-regression-tree-model",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-a-regression-tree-model",
    "title": "ML with tree based models in r",
    "section": "Evaluate a regression tree model",
    "text": "Evaluate a regression tree model\nPredict the final grade for all students in the test set. The grade is on a 0-20 scale. Evaluate the model based on test set RMSE (Root Mean Squared Error). RMSE tells us approximately how far away our predictions are from the true values.\n\n# Generate predictions on a test set\npred <- predict(object = grade_model,   # model object \n                newdata = grade_test)  # test dataset\n\n# Compute the RMSE\nrmse(actual = grade_test$final_grade, \n     predicted = pred)\n\n[1] 2.278249"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#tuning-the-model",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#tuning-the-model",
    "title": "ML with tree based models in r",
    "section": "Tuning the model",
    "text": "Tuning the model\nTune (or “trim”) the model using the prune() function by finding the best “CP” value (CP stands for “Complexity Parameter”).\n\n# Plot the \"CP Table\"\nplotcp(grade_model)\n\n\n\n# Print the \"CP Table\"\nprint(grade_model$cptable)\n\n          CP nsplit rel error    xerror       xstd\n1 0.06839852      0 1.0000000 1.0066743 0.09169976\n2 0.06726713      1 0.9316015 1.0185398 0.08663026\n3 0.03462630      2 0.8643344 0.8923588 0.07351895\n4 0.02508343      3 0.8297080 0.9046335 0.08045100\n5 0.01995676      4 0.8046246 0.8920489 0.08153881\n6 0.01817661      5 0.7846679 0.9042142 0.08283114\n7 0.01203879      6 0.7664912 0.8833557 0.07945742\n8 0.01000000      7 0.7544525 0.8987112 0.08200148\n\n# Retrieve optimal cp value based on cross-validated error\nopt_index <- which.min(grade_model$cptable[, \"xerror\"])\ncp_opt <- grade_model$cptable[opt_index, \"CP\"]\n\n# Prune the model (to optimized cp value)\ngrade_model_opt <- prune(tree = grade_model, \n                         cp = cp_opt)\n                          \n# Plot the optimized model\nrpart.plot(x = grade_model_opt, yesno = 2, type = 0, extra = 0)"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#generate-a-grid-of-hyperparameter-values",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#generate-a-grid-of-hyperparameter-values",
    "title": "ML with tree based models in r",
    "section": "Generate a grid of hyperparameter values",
    "text": "Generate a grid of hyperparameter values\nUse expand.grid() to generate a grid of maxdepth and minsplit values.\n\n# Establish a list of possible values for minsplit and maxdepth\nminsplit <- seq(1, 4, 1)\nmaxdepth <- seq(1, 6, 1)\n\n# Create a data frame containing all combinations \nhyper_grid <- expand.grid(minsplit = minsplit, maxdepth = maxdepth)\n\n# Check out the grid\nhead(hyper_grid)\n\n  minsplit maxdepth\n1        1        1\n2        2        1\n3        3        1\n4        4        1\n5        1        2\n6        2        2\n\n# Print the number of grid combinations\nnrow(hyper_grid)\n\n[1] 24"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#generate-a-grid-of-models",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#generate-a-grid-of-models",
    "title": "ML with tree based models in r",
    "section": "Generate a grid of models",
    "text": "Generate a grid of models\nIn this exercise, we will write a simple loop to train a “grid” of models and store the models in a list called grade_models. R users who are familiar with the apply functions in R could think about how this loop could be easily converted into a function applied to a list as an extra-credit thought experiment.\n\n# Number of potential models in the grid\nnum_models <- nrow(hyper_grid)\n\n# Create an empty list to store models\ngrade_models <- list()\n\n# Write a loop over the rows of hyper_grid to train the grid of models\nfor (i in 1:num_models) {\n\n    # Get minsplit, maxdepth values at row i\n    minsplit <- hyper_grid$minsplit[i]\n    maxdepth <- hyper_grid$maxdepth[i]\n\n    # Train a model and store in the list\n    grade_models[[i]] <- rpart(formula = final_grade ~ ., \n                               data = grade_train, \n                               method = \"anova\",\n                               minsplit = minsplit,\n                               maxdepth = maxdepth)\n}\n\nEvaluate the grid Earlier in the chapter we split the dataset into three parts: training, validation and test.\nA dataset that is not used in training is sometimes referred to as a “holdout” set. A holdout set is used to estimate model performance and although both validation and test sets are considered to be holdout data, there is a key difference:\nJust like a test set, a validation set is used to evaluate the performance of a model. The difference is that a validation set is specifically used to compare the performance of a group of models with the goal of choosing a “best model” from the group. All the models in a group are evaluated on the same validation set and the model with the best performance is considered to be the winner. Once you have the best model, a final estimate of performance is computed on the test set. A test set should only ever be used to estimate model performance and should not be used in model selection. Typically if you use a test set more than once, you are probably doing something wrong.\n\n# Number of potential models in the grid\nnum_models <- length(grade_models)\n\n# Create an empty vector to store RMSE values\nrmse_values <- c()\n\n# Write a loop over the models to compute validation RMSE\nfor (i in 1:num_models) {\n\n    # Retrieve the i^th model from the list\n    model <- grade_models[[i]]\n    \n    # Generate predictions on grade_valid \n    pred <- predict(object = model,\n                    newdata = grade_valid)\n    \n    # Compute validation RMSE and add to the \n    rmse_values[i] <- rmse(actual = grade_valid$final_grade, \n                           predicted = pred)\n}\n\n# Identify the model with smallest validation set RMSE\nbest_model <- grade_models[[which.min(rmse_values)]]\n\n# Print the model paramters of the best model\nbest_model$control\n\n$minsplit\n[1] 2\n\n$minbucket\n[1] 1\n\n$cp\n[1] 0.01\n\n$maxcompete\n[1] 4\n\n$maxsurrogate\n[1] 5\n\n$usesurrogate\n[1] 2\n\n$surrogatestyle\n[1] 0\n\n$maxdepth\n[1] 1\n\n$xval\n[1] 10\n\n# Compute test set RMSE on best_model\npred <- predict(object = best_model,\n                newdata = grade_test)\nrmse(actual = grade_test$final_grade, \n     predicted = pred)\n\n[1] 2.124109"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-bagged-tree-model",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-bagged-tree-model",
    "title": "ML with tree based models in r",
    "section": "Train a bagged tree model",
    "text": "Train a bagged tree model\nLet’s start by training a bagged tree model. You’ll be using the bagging() function from the ipred package. The number of bagged trees can be specified using the nbagg parameter, but here we will use the default (25). If we want to estimate the model’s accuracy using the “out-of-bag” (OOB) samples, we can set the the coob parameter to TRUE. The OOB samples are the training obsevations that were not selected into the bootstrapped sample (used in training). Since these observations were not used in training, we can use them instead to evaluate the accuracy of the model (done automatically inside the bagging() function).\n\nlibrary(ipred)\n# Bagging is a randomized model, so let's set a seed (123) for reproducibility\nset.seed(123)\n\n# Train a bagged model\ncredit_model <- bagging(formula = default ~ ., \n                        data = credit_train,\n                        coob = TRUE)\n\n# Print the model\nprint(credit_model)\n\n\nBagging classification trees with 25 bootstrap replications \n\nCall: bagging.data.frame(formula = default ~ ., data = credit_train, \n    coob = TRUE)\n\nOut-of-bag estimate of misclassification error:  0.2537"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#prediction-and-confusion-matrix",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#prediction-and-confusion-matrix",
    "title": "ML with tree based models in r",
    "section": "Prediction and confusion matrix",
    "text": "Prediction and confusion matrix\nAs you saw in the video, a confusion matrix is a very useful tool for examining all possible outcomes of your predictions (true positive, true negative, false positive, false negative). In this exercise, you will predict those who will default using bagged trees. You will also create the confusion matrix using the confusionMatrix() function from the caret package. It’s always good to take a look at the output using the print() function.\n\n# Generate predicted classes using the model object\nclass_prediction <- predict(object = credit_model,    \n                            newdata = credit_test,  \n                            type = \"class\")  # return classification labels\n\n# Print the predicted classes\nprint(class_prediction)\n\n  [1] no  no  no  no  yes no  no  no  no  no  no  no  no  yes no  no  no  no \n [19] no  no  yes no  no  no  no  no  yes no  no  no  no  no  no  no  no  no \n [37] yes yes no  yes no  yes no  no  no  no  no  no  no  yes no  yes no  yes\n [55] yes no  yes no  yes no  no  yes no  no  yes yes no  yes no  no  no  yes\n [73] yes no  no  no  no  no  no  yes no  no  no  no  yes no  no  yes no  no \n [91] no  no  no  yes yes no  no  no  no  no  no  yes no  no  yes no  no  no \n[109] no  no  no  no  no  no  no  no  no  no  no  no  yes no  yes no  no  yes\n[127] yes no  yes no  no  no  no  no  yes no  yes yes no  no  no  no  yes no \n[145] no  no  yes no  no  no  no  yes no  no  no  no  no  no  no  yes no  no \n[163] yes no  yes no  no  no  no  no  no  no  no  no  no  no  no  no  no  no \n[181] no  no  yes yes yes no  yes no  no  no  no  no  yes no  no  no  yes no \n[199] no  yes\nLevels: no yes\n\n# Calculate the confusion matrix for the test set\nconfusionMatrix(data =  class_prediction,     \n                reference =  credit_test$default, positive = \"yes\")  \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  119  33\n       yes  11  37\n                                          \n               Accuracy : 0.78            \n                 95% CI : (0.7161, 0.8354)\n    No Information Rate : 0.65            \n    P-Value [Acc > NIR] : 4.557e-05       \n                                          \n                  Kappa : 0.4787          \n                                          \n Mcnemar's Test P-Value : 0.001546        \n                                          \n            Sensitivity : 0.5286          \n            Specificity : 0.9154          \n         Pos Pred Value : 0.7708          \n         Neg Pred Value : 0.7829          \n             Prevalence : 0.3500          \n         Detection Rate : 0.1850          \n   Detection Prevalence : 0.2400          \n      Balanced Accuracy : 0.7220          \n                                          \n       'Positive' Class : yes"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#predict-on-a-test-set-and-compute-auc",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#predict-on-a-test-set-and-compute-auc",
    "title": "ML with tree based models in r",
    "section": "Predict on a test set and compute AUC",
    "text": "Predict on a test set and compute AUC\nIn binary classification problems, we can predict numeric values instead of class labels. In fact, class labels are created only after you use the model to predict a raw, numeric, predicted value for a test point. The predicted label is generated by applying a threshold to the predicted value, such that all tests points with predicted value greater than that threshold get a predicted label of “1” and, points below that threshold get a predicted label of “0”. In this exercise, generate predicted values (rather than class labels) on the test set and evaluate performance based on AUC (Area Under the ROC Curve). The AUC is a common metric for evaluating the discriminatory ability of a binary classification model.\n\n# Generate predictions on the test set\npred <- predict(object = credit_model,\n                newdata = credit_test,\n                type = \"prob\")\n\n# `pred` is a matrix\nclass(pred)\n\n[1] \"matrix\" \"array\" \n\n# Look at the pred format\nhead(pred)\n\n       no  yes\n[1,] 0.92 0.08\n[2,] 0.92 0.08\n[3,] 1.00 0.00\n[4,] 1.00 0.00\n[5,] 0.16 0.84\n[6,] 0.84 0.16\n\n# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)\ncredit_ipred_model_test_auc <- auc(actual = ifelse(credit_test$default == \"yes\", 1, 0), \n    predicted = pred[,\"yes\"])  \n\ncredit_ipred_model_test_auc\n\n[1] 0.8084066"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#cross-validate-a-bagged-tree-model-in-caret",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#cross-validate-a-bagged-tree-model-in-caret",
    "title": "ML with tree based models in r",
    "section": "Cross-validate a bagged tree model in caret",
    "text": "Cross-validate a bagged tree model in caret\nUse caret::train() with the “treebag” method to train a model and evaluate the model using cross-validated AUC. The caret package allows the user to easily cross-validate any model across any relevant performance metric. In this case, we will use 5-fold cross validation and evaluate cross-validated AUC (Area Under the ROC Curve).\n\n# Specify the training configuration\nctrl_bag <- trainControl(method = \"cv\",     # Cross-validation\n                     number = 5,      # 5 folds\n                     classProbs = TRUE,                  # For AUC\n                     summaryFunction = twoClassSummary)  # For AUC\n\n# Cross validate the credit model using \"treebag\" method; \n# Track AUC (Area under the ROC curve)\nset.seed(1)  # for reproducibility\ncredit_caret_model <- train(default ~ .,\n                            data = credit_train, \n                            method = \"treebag\",\n                            metric = \"ROC\",\n                            trControl = ctrl_bag)\n\n# Look at the model object\nprint(credit_caret_model)\n\nBagged CART \n\n800 samples\n 16 predictor\n  2 classes: 'no', 'yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 640, 640, 640, 640, 640 \nResampling results:\n\n  ROC       Sens       Spec     \n  0.744508  0.8736842  0.4173913\n\n# Inspect the contents of the model list \nnames(credit_caret_model)\n\n [1] \"method\"       \"modelInfo\"    \"modelType\"    \"results\"      \"pred\"        \n [6] \"bestTune\"     \"call\"         \"dots\"         \"metric\"       \"control\"     \n[11] \"finalModel\"   \"preProcess\"   \"trainingData\" \"ptype\"        \"resample\"    \n[16] \"resampledCM\"  \"perfNames\"    \"maximize\"     \"yLimits\"      \"times\"       \n[21] \"levels\"       \"terms\"        \"coefnames\"    \"contrasts\"    \"xlevels\"     \n\n# Print the CV AUC\ncredit_caret_model$results[,\"ROC\"]\n\n[1] 0.744508"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#generate-predictions-from-the-caret-model",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#generate-predictions-from-the-caret-model",
    "title": "ML with tree based models in r",
    "section": "Generate predictions from the caret model",
    "text": "Generate predictions from the caret model\nGenerate predictions on a test set for the caret model.\n\n# Generate predictions on the test set\nbag_preds <- predict(object = credit_caret_model, \n                newdata = credit_test,\n                type = \"prob\")\n\n# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)\ncredit_caret_model_test_auc <- auc(actual = ifelse(credit_test$default == \"yes\", 1, 0), \n                    predicted = pred[,\"yes\"])\n\ncredit_caret_model_test_auc\n\n[1] 0.8084066"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#compare-test-set-performance-to-cv-performance",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#compare-test-set-performance-to-cv-performance",
    "title": "ML with tree based models in r",
    "section": "Compare test set performance to CV performance",
    "text": "Compare test set performance to CV performance\nIn this excercise, you will print test set AUC estimates that you computed in previous exercises. These two methods use the same code underneath, so the estimates should be very similar.\nThe credit_ipred_model_test_auc object stores the test set AUC from the model trained using the ipred::bagging() function. The credit_caret_model_test_auc object stores the test set AUC from the model trained using the caret::train() function with method = “treebag”. Lastly, we will print the 5-fold cross-validated estimate of AUC that is stored within the credit_caret_model object. This number will be a more accurate estimate of the true model performance since we have averaged the performance over five models instead of just one.\nOn small datasets like this one, the difference between test set model performance estimates and cross-validated model performance estimates will tend to be more pronounced. When using small data, it’s recommended to use cross-validated estimates of performance because they are more stable.\n\n# Print ipred::bagging test set AUC estimate\nprint(credit_ipred_model_test_auc)\n\n[1] 0.8084066\n\n# Print caret \"treebag\" test set AUC estimate\nprint(credit_caret_model_test_auc)\n\n[1] 0.8084066\n\n# Compare to caret 5-fold cross-validated AUC\ncredit_caret_model$results[, \"ROC\"]\n\n[1] 0.744508"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-random-forest-model",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-random-forest-model",
    "title": "ML with tree based models in r",
    "section": "Train a Random Forest model",
    "text": "Train a Random Forest model\nHere you will use the randomForest() function from the randomForest package to train a Random Forest classifier to predict loan default.\n\nlibrary(randomForest)\n# Train a Random Forest\nset.seed(1)  # for reproducibility\ncredit_model <- randomForest(formula = default ~ ., \n                             data = credit_train)\n                             \n# Print the model output                             \nprint(credit_model)\n\n\nCall:\n randomForest(formula = default ~ ., data = credit_train) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n        OOB estimate of  error rate: 24.12%\nConfusion matrix:\n     no yes class.error\nno  521  49  0.08596491\nyes 144  86  0.62608696"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-out-of-bag-error",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-out-of-bag-error",
    "title": "ML with tree based models in r",
    "section": "Evaluate out-of-bag error",
    "text": "Evaluate out-of-bag error\nHere you will plot the OOB error as a function of the number of trees trained, and extract the final OOB error of the Random Forest model from the trained model object.\n\n# Grab OOB error matrix & take a look\nerr <- credit_model$err.rate\nhead(err)\n\n           OOB        no       yes\n[1,] 0.3310105 0.2400000 0.5402299\n[2,] 0.3519313 0.2283951 0.6338028\n[3,] 0.3164129 0.1912833 0.6067416\n[4,] 0.3130564 0.1886792 0.6142132\n[5,] 0.3039890 0.1776062 0.6172249\n[6,] 0.2957560 0.1713222 0.6036866\n\n# Look at final OOB error rate (last row in err matrix)\noob_err <- err[nrow(err), \"OOB\"]\nprint(oob_err)\n\n    OOB \n0.24125 \n\n# Plot the model trained in the previous exercise\nplot(credit_model)\n\n# Add a legend since it doesn't have one by default\nlegend(x = \"right\", \n       legend = colnames(err),\n       fill = 1:ncol(err))"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-model-performance-on-a-test-set",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-model-performance-on-a-test-set",
    "title": "ML with tree based models in r",
    "section": "Evaluate model performance on a test set",
    "text": "Evaluate model performance on a test set\nUse the caret::confusionMatrix() function to compute test set accuracy and generate a confusion matrix. Compare the test set accuracy to the OOB accuracy.\n\n# Generate predicted classes using the model object\nclass_prediction <- predict(object = credit_model,   # model object \n                            newdata = credit_test,  # test dataset\n                            type = \"class\") # return classification labels\n                            \n# Calculate the confusion matrix for the test set\ncm <- confusionMatrix(data = class_prediction,       # predicted classes\n                      reference = credit_test$default, positive = \"yes\")  # actual classes\nprint(cm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  121  39\n       yes   9  31\n                                          \n               Accuracy : 0.76            \n                 95% CI : (0.6947, 0.8174)\n    No Information Rate : 0.65            \n    P-Value [Acc > NIR] : 0.0005292       \n                                          \n                  Kappa : 0.4146          \n                                          \n Mcnemar's Test P-Value : 2.842e-05       \n                                          \n            Sensitivity : 0.4429          \n            Specificity : 0.9308          \n         Pos Pred Value : 0.7750          \n         Neg Pred Value : 0.7562          \n             Prevalence : 0.3500          \n         Detection Rate : 0.1550          \n   Detection Prevalence : 0.2000          \n      Balanced Accuracy : 0.6868          \n                                          \n       'Positive' Class : yes             \n                                          \n\n# Compare test set accuracy to OOB accuracy\npaste0(\"Test Accuracy: \", cm$overall[1])\n\n[1] \"Test Accuracy: 0.76\"\n\npaste0(\"OOB Accuracy: \", 1 - oob_err)\n\n[1] \"OOB Accuracy: 0.75875\""
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#advantage-of-oob-error",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#advantage-of-oob-error",
    "title": "ML with tree based models in r",
    "section": "Advantage of OOB error",
    "text": "Advantage of OOB error\nWhat is the main advantage of using OOB error instead of validation or test error? - If you evaluate your model using OOB error, then you don’t need to create a separate test set"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-test-set-auc",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-test-set-auc",
    "title": "ML with tree based models in r",
    "section": "Evaluate test set AUC",
    "text": "Evaluate test set AUC\nIn Chapter 3, we learned about the AUC metric for evaluating binary classification models. In this exercise, you will compute test set AUC for the Random Forest model.\n\n# Generate predictions on the test set\npred <- predict(object = credit_model,\n            newdata = credit_test,\n            type = \"prob\")\n\n# `pred` is a matrix\nclass(pred)\n\n[1] \"matrix\" \"array\"  \"votes\" \n\n# Look at the pred format\nhead(pred)\n\n     no   yes\n1 0.910 0.090\n2 0.892 0.108\n3 0.992 0.008\n4 0.952 0.048\n5 0.224 0.776\n6 0.846 0.154\n\n# Compute the AUC (`actual` must be a binary 1/0 numeric vector)\nauc(actual = ifelse(credit_test$default == \"yes\", 1, 0), \n    predicted = pred[,\"yes\"])                    \n\n[1] 0.8175824"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#tuning-a-random-forest-via-mtry",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#tuning-a-random-forest-via-mtry",
    "title": "ML with tree based models in r",
    "section": "Tuning a Random Forest via mtry",
    "text": "Tuning a Random Forest via mtry\nIn this exercise, you will use the randomForest::tuneRF() to tune mtry (by training several models). This function is a specific utility to tune the mtry parameter based on OOB error, which is helpful when you want a quick & easy way to tune your model. A more generic way of tuning Random Forest parameters will be presented in the following exercise.\n\n# Execute the tuning process\nset.seed(1)              \nres <- tuneRF(x = subset(credit_train, select = -default),\n              y = credit_train$default,\n              ntreeTry = 500)\n\nmtry = 4  OOB error = 24.12% \nSearching left ...\nmtry = 2    OOB error = 24.5% \n-0.01554404 0.05 \nSearching right ...\nmtry = 8    OOB error = 23.87% \n0.01036269 0.05 \n\n\n\n\n# Look at results\nprint(res)\n\n      mtry OOBError\n2.OOB    2  0.24500\n4.OOB    4  0.24125\n8.OOB    8  0.23875\n\n# Find the mtry value that minimizes OOB Error\nmtry_opt <- res[,\"mtry\"][which.min(res[,\"OOBError\"])]\nprint(mtry_opt)\n\n8.OOB \n    8 \n\n# If you just want to return the best RF model (rather than results)\n# you can set `doBest = TRUE` in `tuneRF()` to return the best RF model\n# instead of a set performance matrix."
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#tuning-a-random-forest-via-tree-depth",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#tuning-a-random-forest-via-tree-depth",
    "title": "ML with tree based models in r",
    "section": "Tuning a Random Forest via tree depth",
    "text": "Tuning a Random Forest via tree depth\nIn Chapter 2, we created a manual grid of hyperparameters using the expand.grid() function and wrote code that trained and evaluated the models of the grid in a loop. In this exercise, you will create a grid of mtry, nodesize and sampsize values. In this example, we will identify the “best model” based on OOB error. The best model is defined as the model from our grid which minimizes OOB error. Keep in mind that there are other ways to select a best model from a grid, such as choosing the best model based on validation AUC. However, for this exercise, we will use the built-in OOB error calculations instead of using a separate validation set.\n\n# Establish a list of possible values for mtry, nodesize and sampsize\nmtry <- seq(4, ncol(credit_train) * 0.8, 2)\nnodesize <- seq(3, 8, 2)\nsampsize <- nrow(credit_train) * c(0.7, 0.8)\n\n# Create a data frame containing all combinations \nhyper_grid <- expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize)\n\n# Create an empty vector to store OOB error values\noob_err <- c()\n\n# Write a loop over the rows of hyper_grid to train the grid of models\nfor (i in 1:nrow(hyper_grid)) {\n\n    # Train a Random Forest model\n    model <- randomForest(formula = default ~ ., \n                          data = credit_train,\n                          mtry = hyper_grid$mtry[i],\n                          nodesize = hyper_grid$nodesize[i],\n                          sampsize = hyper_grid$sampsize[i])\n                          \n    # Store OOB error for the model                      \n    oob_err[i] <- model$err.rate[nrow(model$err.rate), \"OOB\"]\n}\n\n# Identify optimal set of hyperparmeters based on OOB error\nopt_i <- which.min(oob_err)\nprint(hyper_grid[opt_i,])\n\n   mtry nodesize sampsize\n17    6        3      640"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#bagged-trees-vs.-boosted-trees",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#bagged-trees-vs.-boosted-trees",
    "title": "ML with tree based models in r",
    "section": "Bagged trees vs. boosted trees",
    "text": "Bagged trees vs. boosted trees\nWhat is the main difference between bagged trees and boosted trees?\n\nBoosted trees improve the model fit by considering past fits and bagged trees do not"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-gbm-model",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-gbm-model",
    "title": "ML with tree based models in r",
    "section": "Train a GBM model",
    "text": "Train a GBM model\nHere you will use the gbm() function to train a GBM classifier to predict loan default. You will train a 10,000-tree GBM on the credit_train dataset, which is pre-loaded into your workspace. Using such a large number of trees (10,000) is probably not optimal for a GBM model, but we will build more trees than we need and then select the optimal number of trees based on early performance-based stopping. The best GBM model will likely contain fewer trees than we started with. For binary classification, gbm() requires the response to be encoded as 0/1 (numeric), so we will have to convert from a “no/yes” factor to a 0/1 numeric response column.\nAlso, the the gbm() function requires the user to specify a distribution argument. For a binary classification problem, you should set distribution = “bernoulli”. The Bernoulli distribution models a binary response.\n\nlibrary(gbm)\n# Convert \"yes\" to 1, \"no\" to 0\ncredit_train$default <- ifelse(as.character(credit_train$default) == \"yes\", 1, 0) \n\n# Train a 10000-tree GBM model\nset.seed(1)\ncredit_model <- gbm(formula = default ~ ., \n                    distribution = \"bernoulli\", \n                    data = credit_train,\n                    n.trees =  10000)\n                    \n# Print the model object                    \nprint(credit_model)\n\ngbm(formula = default ~ ., distribution = \"bernoulli\", data = credit_train, \n    n.trees = 10000)\nA gradient boosted model with bernoulli loss function.\n10000 iterations were performed.\nThere were 16 predictors of which 16 had non-zero influence.\n\n# summary() prints variable importance\nsummary(credit_model)\n\n\n\n\n                                      var    rel.inf\namount                             amount 22.0897595\nage                                   age 17.9626175\ncredit_history             credit_history 10.6369658\npurpose                           purpose 10.2584546\nemployment_duration   employment_duration  8.8596192\nchecking_balance         checking_balance  6.4650840\nmonths_loan_duration months_loan_duration  5.8863990\nsavings_balance           savings_balance  3.7722735\njob                                   job  2.9418015\nother_credit                 other_credit  2.8613862\nhousing                           housing  2.5237773\nyears_at_residence     years_at_residence  2.3409228\npercent_of_income       percent_of_income  1.7687143\nphone                               phone  0.6373101\nexisting_loans_count existing_loans_count  0.5870700\ndependents                     dependents  0.4078447"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#prediction-using-a-gbm-model",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#prediction-using-a-gbm-model",
    "title": "ML with tree based models in r",
    "section": "Prediction using a GBM model",
    "text": "Prediction using a GBM model\nThe gbm package uses a predict() function to generate predictions from a model, similar to many other machine learning packages in R. When you see a function like predict() that works on many different types of input (a GBM model, a RF model, a GLM model, etc), that indicates that predict() is an “alias” for a GBM-specific version of that function. The GBM specific version of that function is predict.gbm(), but for convenience sake, we can just use predict() (either works).\nOne thing that’s particular to the predict.gbm() however, is that you need to specify the number of trees used in the prediction. There is no default, so you have to specify this manually. For now, we can use the same number of trees that we specified when training the model, which is 10,000 (though this may not be the optimal number to use).\nAnother argument that you can specify is type, which is only relevant to Bernoulli and Poisson distributed outcomes. When using Bernoulli loss, the returned value is on the log odds scale by default and for Poisson, it’s on the log scale. If instead you specify type = “response”, then gbm converts the predicted values back to the same scale as the outcome. This will convert the predicted values into probabilities for Bernoulli and expected counts for Poisson.\n\n# Since we converted the training response col, let's also convert the test response col\ncredit_test$default <- ifelse(credit_test$default == \"yes\", 1, 0) \n\n# Generate predictions on the test set\npreds1 <- predict(object = credit_model, \n                  newdata = credit_test,\n                  n.trees = 10000 )\n\n# Generate predictions on the test set (scale to response)\npreds2 <- predict(object = credit_model, \n                  newdata = credit_test,\n                  n.trees = 10000,\n                  type = \"response\")\n\n# Compare the range of the two sets of predictions\nrange(preds1)\n\n[1] -6.004812  4.646991\n\nrange(preds2)\n\n[1] 0.002460783 0.990500685"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-test-set-auc-1",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-test-set-auc-1",
    "title": "ML with tree based models in r",
    "section": "Evaluate test set AUC",
    "text": "Evaluate test set AUC\nCompute test set AUC of the GBM model for the two sets of predictions. We will notice that they are the same value. That’s because AUC is a rank-based metric, so changing the actual values does not change the value of the AUC.\nHowever, if we were to use a scale-aware metric like RMSE to evaluate performance, we would want to make sure we converted the predictions back to the original scale of the response.\n\n# Generate the test set AUCs using the two sets of preditions & compare\nauc(actual = credit_test$default, predicted = preds1)  #default\n\n[1] 0.7142857\n\nauc(actual = credit_test$default, predicted = preds2)  #rescaled\n\n[1] 0.7142857"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#early-stopping-in-gbms",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#early-stopping-in-gbms",
    "title": "ML with tree based models in r",
    "section": "Early stopping in GBMs",
    "text": "Early stopping in GBMs\nUse the gbm.perf() function to estimate the optimal number of boosting iterations (aka n.trees) for a GBM model object using both OOB and CV error. When you set out to train a large number of trees in a GBM (such as 10,000) and you use a validation method to determine an earlier (smaller) number of trees, then that’s called “early stopping”. The term “early stopping” is not unique to GBMs, but can describe auto-tuning the number of iterations in an iterative learning algorithm.\n\n# Optimal ntree estimate based on OOB\nntree_opt_oob <- gbm.perf(object = credit_model, \n                          method = \"OOB\", \n                          oobag.curve = TRUE)\n\n\n\n\n\n\n# Train a CV GBM model\nset.seed(1)\ncredit_model_cv <- gbm(formula = default ~ ., \n                       distribution = \"bernoulli\", \n                       data = credit_train,\n                       n.trees = 10000,\n                       cv.folds = 5)\n\n# Optimal ntree estimate based on CV\nntree_opt_cv <- gbm.perf(object = credit_model_cv , \n                         method = \"cv\")\n\n\n\n# Compare the estimates                         \nprint(paste0(\"Optimal n.trees (OOB Estimate): \", ntree_opt_oob))                         \n\n[1] \"Optimal n.trees (OOB Estimate): 76\"\n\nprint(paste0(\"Optimal n.trees (CV Estimate): \", ntree_opt_cv))\n\n[1] \"Optimal n.trees (CV Estimate): 127\""
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#oob-vs-cv-based-early-stopping",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#oob-vs-cv-based-early-stopping",
    "title": "ML with tree based models in r",
    "section": "OOB vs CV-based early stopping",
    "text": "OOB vs CV-based early stopping\nIn the previous exercise, we used OOB error and cross-validated error to estimate the optimal number of trees in the GBM. These are two different ways to estimate the optimal number of trees, so in this exercise we will compare the performance of the models on a test set. We can use the same model object to make both of these estimates since the predict.gbm() function allows you to use any subset of the total number of trees (in our case, the total number is 10,000).\n\n# Generate predictions on the test set using ntree_opt_oob number of trees\npreds1 <- predict(object = credit_model, \n                  newdata = credit_test,\n                  n.trees = ntree_opt_oob)\n                  \n# Generate predictions on the test set using ntree_opt_cv number of trees\ngbm_preds <- predict(object = credit_model, \n                  newdata = credit_test,\n                  n.trees = ntree_opt_cv)   \n\n# Generate the test set AUCs using the two sets of preditions & compare\nauc1 <- auc(actual = credit_test$default, predicted = preds1)  #OOB\nauc2 <- auc(actual = credit_test$default, predicted =gbm_preds)  #CV \n\n# Compare AUC \nprint(paste0(\"Test set AUC (OOB): \", auc1))                         \n\n[1] \"Test set AUC (OOB): 0.802527472527472\"\n\nprint(paste0(\"Test set AUC (CV): \", auc2))\n\n[1] \"Test set AUC (CV): 0.792527472527473\""
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#compare-all-models-based-on-auc",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#compare-all-models-based-on-auc",
    "title": "ML with tree based models in r",
    "section": "Compare all models based on AUC",
    "text": "Compare all models based on AUC\nIn this final exercise, we will perform a model comparison across all types of models that we’ve learned about so far: Decision Trees, Bagged Trees, Random Forest and Gradient Boosting Machine (GBM). The models were all trained on the same training set, credit_train, and predictions were made for the credit_test dataset.\nWe have pre-loaded four sets of test set predictions, generated using the models we trained in previous chapters (one for each model type). The numbers stored in the prediction vectors are the raw predicted values themselves – not the predicted class labels. Using the raw predicted values, we can calculate test set AUC for each model and compare the results.\n\n#credit_train$default <-  factor(credit_train$default, levels  = c(0, 1))\n#credit_test$default <-  factor(credit_test$default, levels = c(0, 1))\n\nmyFolds <- createFolds(credit_train$default, k = 5)\n# tuneGridRf <- data.frame(\n#   .mtry = c(2, 3, 7),\n#   .splitrule = \"variance\",\n#   .min.node.size = 5\n# )\n\nmyControl <- trainControl(\n  method = \"cv\",\n  number = 5,\n  summaryFunction = twoClassSummary,\n  classProbs = TRUE, # IMPORTANT!\n  verboseIter = FALSE,\n  index = myFolds\n)\n\ny =  factor(credit_train$default,levels = c(1,0),  labels = c(\"yes\", \"no\"))\nx <- credit_train %>% select(-default)\nmodel_rf <- train(x =  x ,\n                  y= y,\n                  method = \"ranger\",\n                  classification = TRUE,\n                  metric = \"ROC\",\n                  trControl = myControl)\n \nrf_preds <- predict(model_rf, newdata = credit_test, type = \"prob\")\n\nrf_preds <- rf_preds[, \"yes\"]\n\nmodel_bag <- train(x = x, \n                   y = y,\n                   method = \"treebag\",\n                   metric = \"ROC\",\n                   trControl = myControl)\n\n\nbag_preds <- predict(model_bag, newdata = credit_test, type = \"prob\")\n\nbag_preds  <-bag_preds[, \"yes\"]\n\n\nhyperparams_gbm <- expand.grid(n.trees = seq(100,500, by = 50), \n                           interaction.depth = 1:7, \n                           shrinkage = seq(0.1, 0.9, by = .1), \n                           n.minobsinnode = seq(10, 30, 10))\nmodel_gbm <- train(x = x,\n                   y = y,\n                   method = \"gbm\",\n                   #tuneGrid = hyperparams_gbm,\n                   metric = \"ROC\",\n                   trControl = myControl)\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2650            -nan     0.1000    0.0039\n     2        1.2427            -nan     0.1000    0.0095\n     3        1.2226            -nan     0.1000    0.0072\n     4        1.2078            -nan     0.1000    0.0015\n     5        1.1963            -nan     0.1000    0.0018\n     6        1.1816            -nan     0.1000    0.0039\n     7        1.1712            -nan     0.1000    0.0007\n     8        1.1601            -nan     0.1000    0.0007\n     9        1.1506            -nan     0.1000   -0.0010\n    10        1.1402            -nan     0.1000    0.0044\n    20        1.0561            -nan     0.1000    0.0007\n    40        0.9714            -nan     0.1000   -0.0006\n    60        0.9192            -nan     0.1000   -0.0048\n    80        0.8788            -nan     0.1000   -0.0041\n   100        0.8515            -nan     0.1000   -0.0014\n   120        0.8191            -nan     0.1000   -0.0056\n   140        0.7927            -nan     0.1000   -0.0041\n   150        0.7834            -nan     0.1000   -0.0037\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2528            -nan     0.1000    0.0081\n     2        1.2246            -nan     0.1000    0.0064\n     3        1.1934            -nan     0.1000    0.0106\n     4        1.1729            -nan     0.1000    0.0022\n     5        1.1475            -nan     0.1000    0.0070\n     6        1.1305            -nan     0.1000    0.0000\n     7        1.1180            -nan     0.1000    0.0005\n     8        1.0926            -nan     0.1000    0.0005\n     9        1.0726            -nan     0.1000    0.0045\n    10        1.0576            -nan     0.1000   -0.0013\n    20        0.9674            -nan     0.1000   -0.0046\n    40        0.8488            -nan     0.1000   -0.0153\n    60        0.7727            -nan     0.1000   -0.0039\n    80        0.6981            -nan     0.1000   -0.0045\n   100        0.6502            -nan     0.1000   -0.0015\n   120        0.6014            -nan     0.1000   -0.0036\n   140        0.5684            -nan     0.1000   -0.0033\n   150        0.5464            -nan     0.1000   -0.0009\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2438            -nan     0.1000    0.0155\n     2        1.2147            -nan     0.1000    0.0054\n     3        1.1900            -nan     0.1000    0.0060\n     4        1.1626            -nan     0.1000    0.0052\n     5        1.1412            -nan     0.1000    0.0017\n     6        1.1199            -nan     0.1000    0.0026\n     7        1.0992            -nan     0.1000    0.0009\n     8        1.0813            -nan     0.1000   -0.0046\n     9        1.0672            -nan     0.1000   -0.0023\n    10        1.0506            -nan     0.1000   -0.0080\n    20        0.9000            -nan     0.1000   -0.0032\n    40        0.7530            -nan     0.1000   -0.0073\n    60        0.6553            -nan     0.1000   -0.0054\n    80        0.5664            -nan     0.1000   -0.0073\n   100        0.5062            -nan     0.1000   -0.0059\n   120        0.4603            -nan     0.1000   -0.0057\n   140        0.4111            -nan     0.1000   -0.0043\n   150        0.3856            -nan     0.1000   -0.0035\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0493            -nan     0.1000    0.0012\n     2        1.0312            -nan     0.1000    0.0065\n     3        1.0190            -nan     0.1000   -0.0028\n     4        1.0054            -nan     0.1000    0.0000\n     5        0.9983            -nan     0.1000   -0.0054\n     6        0.9814            -nan     0.1000   -0.0030\n     7        0.9722            -nan     0.1000    0.0000\n     8        0.9649            -nan     0.1000    0.0024\n     9        0.9579            -nan     0.1000    0.0004\n    10        0.9509            -nan     0.1000    0.0011\n    20        0.9017            -nan     0.1000   -0.0067\n    40        0.8232            -nan     0.1000   -0.0027\n    60        0.7655            -nan     0.1000   -0.0019\n    80        0.7302            -nan     0.1000   -0.0026\n   100        0.6907            -nan     0.1000   -0.0018\n   120        0.6691            -nan     0.1000   -0.0074\n   140        0.6566            -nan     0.1000   -0.0030\n   150        0.6459            -nan     0.1000   -0.0042\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0321            -nan     0.1000    0.0031\n     2        1.0123            -nan     0.1000   -0.0019\n     3        0.9904            -nan     0.1000    0.0021\n     4        0.9813            -nan     0.1000   -0.0088\n     5        0.9736            -nan     0.1000   -0.0005\n     6        0.9543            -nan     0.1000    0.0044\n     7        0.9453            -nan     0.1000   -0.0038\n     8        0.9327            -nan     0.1000   -0.0033\n     9        0.9221            -nan     0.1000   -0.0018\n    10        0.9059            -nan     0.1000    0.0043\n    20        0.8193            -nan     0.1000   -0.0042\n    40        0.6906            -nan     0.1000   -0.0047\n    60        0.6227            -nan     0.1000   -0.0049\n    80        0.5567            -nan     0.1000   -0.0028\n   100        0.5066            -nan     0.1000   -0.0055\n   120        0.4600            -nan     0.1000   -0.0059\n   140        0.4173            -nan     0.1000   -0.0066\n   150        0.4008            -nan     0.1000   -0.0024\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0386            -nan     0.1000    0.0032\n     2        1.0199            -nan     0.1000    0.0012\n     3        1.0082            -nan     0.1000   -0.0025\n     4        0.9934            -nan     0.1000   -0.0032\n     5        0.9699            -nan     0.1000   -0.0042\n     6        0.9500            -nan     0.1000   -0.0011\n     7        0.9247            -nan     0.1000    0.0039\n     8        0.9062            -nan     0.1000   -0.0055\n     9        0.8931            -nan     0.1000   -0.0046\n    10        0.8790            -nan     0.1000   -0.0041\n    20        0.7603            -nan     0.1000   -0.0044\n    40        0.6094            -nan     0.1000   -0.0032\n    60        0.5139            -nan     0.1000   -0.0026\n    80        0.4287            -nan     0.1000   -0.0012\n   100        0.3686            -nan     0.1000   -0.0040\n   120        0.3319            -nan     0.1000   -0.0055\n   140        0.2831            -nan     0.1000   -0.0024\n   150        0.2612            -nan     0.1000   -0.0024\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1727            -nan     0.1000    0.0048\n     2        1.1566            -nan     0.1000    0.0042\n     3        1.1399            -nan     0.1000    0.0027\n     4        1.1174            -nan     0.1000    0.0048\n     5        1.0990            -nan     0.1000    0.0023\n     6        1.0846            -nan     0.1000    0.0023\n     7        1.0754            -nan     0.1000    0.0018\n     8        1.0645            -nan     0.1000    0.0035\n     9        1.0550            -nan     0.1000   -0.0008\n    10        1.0466            -nan     0.1000   -0.0008\n    20        0.9777            -nan     0.1000    0.0019\n    40        0.8611            -nan     0.1000   -0.0011\n    60        0.7978            -nan     0.1000   -0.0013\n    80        0.7429            -nan     0.1000   -0.0040\n   100        0.7077            -nan     0.1000   -0.0046\n   120        0.6889            -nan     0.1000   -0.0027\n   140        0.6654            -nan     0.1000   -0.0016\n   150        0.6559            -nan     0.1000   -0.0011\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1591            -nan     0.1000    0.0058\n     2        1.1253            -nan     0.1000    0.0114\n     3        1.0902            -nan     0.1000    0.0116\n     4        1.0662            -nan     0.1000    0.0025\n     5        1.0593            -nan     0.1000   -0.0112\n     6        1.0380            -nan     0.1000    0.0034\n     7        1.0167            -nan     0.1000    0.0040\n     8        0.9910            -nan     0.1000    0.0052\n     9        0.9677            -nan     0.1000    0.0079\n    10        0.9546            -nan     0.1000   -0.0062\n    20        0.8419            -nan     0.1000   -0.0025\n    40        0.6865            -nan     0.1000   -0.0037\n    60        0.6017            -nan     0.1000   -0.0013\n    80        0.5465            -nan     0.1000   -0.0042\n   100        0.4891            -nan     0.1000   -0.0037\n   120        0.4468            -nan     0.1000   -0.0030\n   140        0.4141            -nan     0.1000   -0.0066\n   150        0.3985            -nan     0.1000   -0.0030\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1740            -nan     0.1000   -0.0038\n     2        1.1288            -nan     0.1000    0.0121\n     3        1.0920            -nan     0.1000    0.0046\n     4        1.0637            -nan     0.1000    0.0088\n     5        1.0287            -nan     0.1000    0.0107\n     6        1.0025            -nan     0.1000   -0.0025\n     7        0.9746            -nan     0.1000    0.0011\n     8        0.9501            -nan     0.1000    0.0005\n     9        0.9284            -nan     0.1000   -0.0074\n    10        0.9116            -nan     0.1000   -0.0016\n    20        0.7753            -nan     0.1000   -0.0005\n    40        0.6060            -nan     0.1000   -0.0030\n    60        0.4993            -nan     0.1000    0.0012\n    80        0.4251            -nan     0.1000   -0.0042\n   100        0.3696            -nan     0.1000   -0.0020\n   120        0.3210            -nan     0.1000   -0.0020\n   140        0.2723            -nan     0.1000   -0.0012\n   150        0.2511            -nan     0.1000   -0.0026\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2101            -nan     0.1000   -0.0015\n     2        1.1790            -nan     0.1000    0.0183\n     3        1.1480            -nan     0.1000    0.0163\n     4        1.1148            -nan     0.1000    0.0101\n     5        1.0826            -nan     0.1000    0.0125\n     6        1.0723            -nan     0.1000    0.0024\n     7        1.0499            -nan     0.1000    0.0121\n     8        1.0395            -nan     0.1000   -0.0027\n     9        1.0187            -nan     0.1000    0.0074\n    10        1.0063            -nan     0.1000    0.0043\n    20        0.9174            -nan     0.1000   -0.0016\n    40        0.8156            -nan     0.1000   -0.0010\n    60        0.7405            -nan     0.1000   -0.0031\n    80        0.7003            -nan     0.1000   -0.0016\n   100        0.6655            -nan     0.1000   -0.0023\n   120        0.6434            -nan     0.1000   -0.0027\n   140        0.6147            -nan     0.1000   -0.0048\n   150        0.6067            -nan     0.1000   -0.0012\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1551            -nan     0.1000    0.0184\n     2        1.1113            -nan     0.1000    0.0181\n     3        1.0712            -nan     0.1000    0.0178\n     4        1.0338            -nan     0.1000    0.0092\n     5        1.0134            -nan     0.1000    0.0058\n     6        0.9854            -nan     0.1000    0.0107\n     7        0.9625            -nan     0.1000    0.0084\n     8        0.9432            -nan     0.1000    0.0041\n     9        0.9335            -nan     0.1000   -0.0023\n    10        0.9221            -nan     0.1000   -0.0025\n    20        0.8041            -nan     0.1000   -0.0017\n    40        0.6691            -nan     0.1000   -0.0066\n    60        0.5943            -nan     0.1000   -0.0049\n    80        0.5265            -nan     0.1000   -0.0049\n   100        0.4683            -nan     0.1000   -0.0041\n   120        0.4215            -nan     0.1000   -0.0018\n   140        0.3862            -nan     0.1000   -0.0031\n   150        0.3698            -nan     0.1000   -0.0018\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1719            -nan     0.1000    0.0194\n     2        1.1111            -nan     0.1000    0.0264\n     3        1.0633            -nan     0.1000    0.0174\n     4        1.0323            -nan     0.1000    0.0050\n     5        1.0085            -nan     0.1000    0.0025\n     6        0.9997            -nan     0.1000   -0.0087\n     7        0.9728            -nan     0.1000    0.0075\n     8        0.9311            -nan     0.1000    0.0128\n     9        0.9227            -nan     0.1000   -0.0048\n    10        0.9037            -nan     0.1000    0.0010\n    20        0.7400            -nan     0.1000    0.0013\n    40        0.5865            -nan     0.1000   -0.0055\n    60        0.4703            -nan     0.1000   -0.0030\n    80        0.3800            -nan     0.1000   -0.0036\n   100        0.3205            -nan     0.1000   -0.0015\n   120        0.2803            -nan     0.1000   -0.0023\n   140        0.2401            -nan     0.1000   -0.0015\n   150        0.2183            -nan     0.1000   -0.0013\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1683            -nan     0.1000    0.0149\n     2        1.1484            -nan     0.1000    0.0054\n     3        1.1290            -nan     0.1000    0.0081\n     4        1.1046            -nan     0.1000    0.0100\n     5        1.0993            -nan     0.1000   -0.0032\n     6        1.0877            -nan     0.1000    0.0007\n     7        1.0761            -nan     0.1000    0.0006\n     8        1.0694            -nan     0.1000    0.0002\n     9        1.0624            -nan     0.1000   -0.0023\n    10        1.0458            -nan     0.1000   -0.0009\n    20        0.9643            -nan     0.1000   -0.0015\n    40        0.8710            -nan     0.1000   -0.0019\n    60        0.8263            -nan     0.1000   -0.0067\n    80        0.7926            -nan     0.1000   -0.0050\n   100        0.7772            -nan     0.1000   -0.0060\n   120        0.7532            -nan     0.1000   -0.0031\n   140        0.7267            -nan     0.1000   -0.0032\n   150        0.7225            -nan     0.1000   -0.0028\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1726            -nan     0.1000    0.0081\n     2        1.1329            -nan     0.1000    0.0210\n     3        1.0909            -nan     0.1000    0.0075\n     4        1.0686            -nan     0.1000    0.0064\n     5        1.0454            -nan     0.1000    0.0060\n     6        1.0313            -nan     0.1000   -0.0003\n     7        1.0107            -nan     0.1000    0.0060\n     8        0.9922            -nan     0.1000    0.0039\n     9        0.9778            -nan     0.1000   -0.0013\n    10        0.9617            -nan     0.1000    0.0008\n    20        0.8664            -nan     0.1000   -0.0030\n    40        0.7598            -nan     0.1000   -0.0031\n    60        0.6822            -nan     0.1000   -0.0028\n    80        0.6271            -nan     0.1000    0.0005\n   100        0.5784            -nan     0.1000   -0.0030\n   120        0.5264            -nan     0.1000   -0.0046\n   140        0.4798            -nan     0.1000   -0.0042\n   150        0.4659            -nan     0.1000   -0.0067\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1689            -nan     0.1000    0.0036\n     2        1.1327            -nan     0.1000    0.0121\n     3        1.0929            -nan     0.1000    0.0143\n     4        1.0462            -nan     0.1000    0.0100\n     5        1.0181            -nan     0.1000    0.0078\n     6        0.9997            -nan     0.1000    0.0011\n     7        0.9750            -nan     0.1000    0.0056\n     8        0.9586            -nan     0.1000   -0.0003\n     9        0.9303            -nan     0.1000    0.0071\n    10        0.9179            -nan     0.1000   -0.0073\n    20        0.8060            -nan     0.1000   -0.0029\n    40        0.6746            -nan     0.1000   -0.0055\n    60        0.5787            -nan     0.1000   -0.0073\n    80        0.4995            -nan     0.1000   -0.0056\n   100        0.4327            -nan     0.1000   -0.0024\n   120        0.3789            -nan     0.1000   -0.0030\n   140        0.3350            -nan     0.1000   -0.0035\n   150        0.3100            -nan     0.1000   -0.0036\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1819            -nan     0.1000    0.0092\n     2        1.1663            -nan     0.1000    0.0076\n     3        1.1517            -nan     0.1000    0.0061\n     4        1.1427            -nan     0.1000    0.0039\n     5        1.1339            -nan     0.1000    0.0044\n     6        1.1280            -nan     0.1000    0.0022\n     7        1.1230            -nan     0.1000    0.0019\n     8        1.1151            -nan     0.1000    0.0023\n     9        1.1083            -nan     0.1000    0.0030\n    10        1.0985            -nan     0.1000    0.0035\n    20        1.0537            -nan     0.1000    0.0010\n    40        1.0014            -nan     0.1000   -0.0008\n    50        0.9848            -nan     0.1000   -0.0002\n\ngbm_preds <- predict(model_gbm, newdata = credit_test, type = \"prob\")\n\ngbm_preds  <- gbm_preds[, \"yes\"]\n\nmodel_dt <- train(x = x,\n                   y = y,\n                   method = \"rpart\",\n                   metric = \"ROC\",\n                   trControl = myControl)\n\n\ndt_preds <- predict(model_dt, newdata = credit_test, type = \"prob\")\ndt_preds  <- dt_preds[, \"yes\"]\n\n\n# Generate the test set AUCs using the two sets of predictions & compare\nactual <- credit_test$default\ndt_auc <- auc(actual = actual, predicted = dt_preds)\nbag_auc <- auc(actual = actual, predicted = bag_preds)\nrf_auc <- auc(actual = actual, predicted = rf_preds)\ngbm_auc <- auc(actual = actual, predicted = gbm_preds)\n# \n# # Print results\nsprintf(\"Decision Tree Test AUC: %.3f\", dt_auc)\n\n[1] \"Decision Tree Test AUC: 0.770\"\n\nsprintf(\"Bagged Trees Test AUC: %.3f\", bag_auc)\n\n[1] \"Bagged Trees Test AUC: 0.803\"\n\nsprintf(\"Random Forest Test AUC: %.3f\", rf_auc)\n\n[1] \"Random Forest Test AUC: 0.814\"\n\nsprintf(\"GBM Test AUC: %.3f\", gbm_auc)\n\n[1] \"GBM Test AUC: 0.804\""
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#plot-compare-roc-curves",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#plot-compare-roc-curves",
    "title": "ML with tree based models in r",
    "section": "Plot & compare ROC curves",
    "text": "Plot & compare ROC curves\nWe conclude this course by plotting the ROC curves for all the models (one from each chapter) on the same graph. The ROCR package provides the prediction() and performance() functions which generate the data required for plotting the ROC curve, given a set of predictions and actual (true) values.\nThe more “up and to the left” the ROC curve of a model is, the better the model. The AUC performance metric is literally the “Area Under the ROC Curve”, so the greater the area under this curve, the higher the AUC, and the better-performing the model is.\n\nlibrary(ROCR)\n# List of predictions\npreds_list <- list(dt_preds, bag_preds, rf_preds, gbm_preds)\n\n# List of actual values (same for all)\nm <- length(preds_list)\n\nactuals_list <- rep(list(credit_test$default), m)\n\n# Plot the ROC curves\npred <- prediction(preds_list, actuals_list)\nrocs <- performance(pred, \"tpr\", \"fpr\")\nplot(rocs, col = as.list(1:m), main = \"Test Set ROC Curves\")\nlegend(x = \"bottomright\", \n       legend = c(\"Decision Tree\", \"Bagged Trees\", \"Random Forest\", \"GBM\"),\n       fill = 1:m)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Projects",
    "section": "",
    "text": "Data Science projects I have been working on 😊\nWork in progress"
  },
  {
    "objectID": "myblog/kaggle/malaysia_tourist/malaysia_tourist.html",
    "href": "myblog/kaggle/malaysia_tourist/malaysia_tourist.html",
    "title": "Malaysian Tourist Sites",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(leaflet)\n\ntourist_site <-fread(\"dataset tempat perlancongan Malaysia.csv\")\n\ntourist_site[, site_label := paste0(Negeri, \", \" ,`Nama Tempat`)]"
  },
  {
    "objectID": "myblog/kaggle/malaysia_tourist/malaysia_tourist.html#leaflet-map",
    "href": "myblog/kaggle/malaysia_tourist/malaysia_tourist.html#leaflet-map",
    "title": "Malaysian Tourist Sites",
    "section": "Leaflet Map",
    "text": "Leaflet Map\n\nleaflet(tourist_site) %>%\n    addTiles() %>%\n    addMarkers(~Longitude, ~Latitude, label = ~site_label )"
  },
  {
    "objectID": "myblog/association_analysis/association_analysis.html",
    "href": "myblog/association_analysis/association_analysis.html",
    "title": "Association analysis",
    "section": "",
    "text": "Maybe you have heard of a grocery store that carried out analysis and found out that men who buy diapers between 5pm to 7pm were more likely to buy beer. The grocery store then moved the beer isle closer to the diaper isle and beer sales increased by 35%. This is called association analysis which was motivated by retail store data.\nIn this blog I will explore the basics of association analysis. The goal is to find out:\n\nItems frequently bought together in association analysis this is called support. Let say you have ten transactions and in those ten 3 transactions have maize floor, rice and bread the the support for maize floor, rice and bread is 3/10 = 0.3. This is just marginal probability. In other terms the percentage of transactions these items were bought together.\nIn this example the support is written as Support({bread, maize floor} –> {rice} ). In general this is written as Support of item one and item 2 is Support({item1} –> {item 2}). Item 1 and item 2 may contain one or more items.\nWe also want to find out if someone bought a set of items what other set of item(s) were they likely to buy. In association analysis this is called confidence. In our above example let say that you find the proportion of transactions that contained maize floor and bread are 0.4. Then the confidence is the proportion of those transactions with maize floor, bread and rice/proportion of transactions that contained maize floor and bread. Then the confidence is 0.3/0.4 which is 0.75. In other word 75% of those who bought maize floor and bread also bought rice.\n\nConfidence in this example is denoted as Confidence({bread, maize floor} –> {rice} ) and in general this is Confidence({item 1} –> {item 2} ).\n\nThe lift refers to how the chances of rice being purchased increased given that maize floor and bread are purchased. So the lift of rice is confidence of rice/support(rice). Support of rice is the number of transactions that contain rice.\n\nLift({Item 1} -> {Item 2 }) = (Confidence(Item1 -> Item2)) / (Support(Item2))\n\n\nTo make sense of all these I’m going to use a bakery to find association rules between items bought manually and then towards the end I will use r package arules which uses apriori algorithm to find association between items bought. The data set is available on kaggle as BreadBasket_DMS. We start by first having a glimpse of this data set.\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(data.table)\nlibrary(DT)\n\ndat <- setDT(read.csv( \"BreadBasket_DMS.csv\" ))\n\ndat <- dat[Item != \"NONE\"]\nhead(dat[sample(1:nrow(dat), 10)]) %>% datatable()\n\n\n\n\n\n\n\n\nFirst step is to transform the data set into wide format. Column headers will be items sold in the bakery and the rows will be populated with 1 and 0 indicating whether that item was bought for that transaction.\n\ndat2 <- dcast(Date+Time+Transaction~Item, data = dat, fun.aggregate = length)\n#dat2[, NONE := NULL]\n\nsample_cols <- sample(4:ncol(dat2), 5)\n\nitem_names <- names(dat2)[4:97]\n\ndat2[, (item_names) := lapply(.SD, function(x) ifelse(x==0, 0, 1)), .SDcols = item_names]\n\nhead(dat2[, c(1:3, sample_cols), with = F]) %>% datatable()\n\n\n\n\n\n\n\n\n\nOn average a transaction has 2 items. The median is also 2, this shows that atleast 50% of the transactions contained 2 or more items and atleast 25% of the transactions have 1 item.\n\nnumber_items <- rowSums(dat2[, 4:97, with =F])\n\ndat2[, number_items := number_items]\n\nhist(number_items, col = \"black\", main = \"Number of items bought\")\n\n\n\nsumStats <- dat2 %>%\n    summarise(Average = round(mean(number_items), 2), Stdev = round(sd(number_items), 2),\n              Median= median(number_items),\n              Minimum = min(number_items), Maximum = max(number_items),\n              First_qurtile = quantile(number_items,0.25,  na.rm = T),\n              Third_qurtile=quantile(number_items,0.75,  na.rm = T))\n\ndatatable(sumStats)\n\n\n\n\n\n\n\n\n\nTable below shows top ten most bought items and about 47.84% of the transactions contained coffee. Coffee was the most popular item in this bakery followed by bread.\n\nn_transacts_item_in <- colSums(dat2[, item_names, with = F])\n\ndata.frame(item = names(n_transacts_item_in),\n           number =n_transacts_item_in) %>%\n    mutate(Percentage = round(number/nrow(dat2)*100, 2)) %>%\n    arrange(desc(number)) %>% head(10) %>% datatable()\n\n\n\n\n\n\nSince we have transformed the data in the wide format and every transaction is in it’s row we can visualize how the baskets look like. This is done by extracting the column names for the transactions where the value is 1. For each transaction, 1 represent that item being in that transaction.\n\nitems_bought <- apply(dat2[, 4:97, with =F], 1, paste, collapse = \"\", sep = \"\")\n\nlist_items <- vector(mode = \"list\", length = length(items_bought))\nfor (i in 1:length(items_bought)) {\n    index <- unlist(gregexpr(\"1\", items_bought[i]))\n    items_transaction_i <- item_names[index]\n    items_transaction_i <- paste(items_transaction_i, sep = \" \", collapse = \" , \")\n    list_items[[i]] <- items_transaction_i\n}\n\nhead(unlist(list_items))\n\n[1] \"Bread\"                         \"Scandinavian\"                 \n[3] \"Cookies , Hot chocolate , Jam\" \"Muffin\"                       \n[5] \"Bread , Coffee , Pastry\"       \"Medialuna , Muffin , Pastry\"  \n\n\nData frame below shows how the baskets look like. Only 10 randomly selected rows are displayed. Items_bought column shows the baskets.\n\ndat2[, items_bought := unlist(list_items) ]\n\nhead(dat2[sample(1:nrow(dat2), 10),\n          .(Transaction, number_items,items_bought)]) %>%\n  datatable()\n\n\n\n\n\n\nA small example which I will work out manually to see what is the support for ({coffee, bread} –> {jam}). Generally I want to see how many transactions contained these 3 items. 0.12% of the transactions contained {bread, coffee, jam}\n\nmy_item_set <- Hmisc::Cs(Coffee , Jam , Bread)\n\nidx_sample <- grep( \"Transactio|^Coffee$|^Jam$|^Bread$\", names(dat2))\n\n\n\nitem_set_dat <- dat2[, idx_sample, with = F] \n\n#some transaction bought more thanone of \nitem_set_dat[, (my_item_set) := lapply(.SD, function(x) ifelse(x==0, 0, 1)), .SDcols = my_item_set]\n\nitem_set_dat[, total_items := rowSums(item_set_dat[, 2:4, with = F]) ]\n\nsupport_coffee_bread_jam <- table(item_set_dat$total_items)[\"3\"]/9531 \n\nsupport_coffee_bread_jam\n\n          3 \n0.001154129 \n\n\nTo calculate confidence({bread, coffee} –> {jam}) we should also calculate the support of ({bread, coffee}) which is the prorpotion of bread and coffee appearing together in the transactions which is about 8.9% of the transactions.\n\nitem_set_dat[, coffee_bread := rowSums(item_set_dat[, c(\"Bread\", \"Coffee\"), with = F]) ]\n\nhead(item_set_dat, 2)\n\n   Transaction Bread Coffee Jam total_items coffee_bread\n1:           1     1      0   0           1            1\n2:           2     0      0   0           0            0\n\nsupport_coffee_bread <- table(item_set_dat$coffee_bread)[\"2\"]/9531 \n\nsupport_coffee_bread\n\n         2 \n0.08939251 \n\n\n1.3% of the people who bought bread and coffee also bought jam. This is the confidence of({bread, coffee} –> {jam}) For statisticians this can be translated as conditional probability. In conditional probability notations P(Jam/bread, coffee) which is probability you will buy jam given that you have already bought bread and coffee. In association analysis we have {bread, coffee} >>{jam} bread and coffee implies jam. So the confidence measures the strength/probability of this implication.\n\nconfidence <- support_coffee_bread_jam/support_coffee_bread \n\nconfidence * 100\n\n      3 \n1.29108 \n\n\nUsing package arules we find the 10 rules with the highest confidence in descending order. Confidence({Toast} –>{Coffee}) had the highest confidence of 0.70440252. About 70.44% of the transactions that contained toast also contained coffee.\n\nlibrary(arules)\n\ntransactions <- as(split(dat$Item, dat$Transaction), \"transactions\")\n\nassoc_rules <- apriori(transactions,\n                 parameter = list(supp = 0.02, conf = 0.04, target = \"rules\"))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n       0.04    0.1    1 none FALSE            TRUE       5    0.02      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 189 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[94 item(s), 9465 transaction(s)] done [0.00s].\nsorting and recoding items ... [19 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 done [0.00s].\nwriting ... [38 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\nassoc_rules <- sort(assoc_rules, by='confidence', decreasing = TRUE)\n\n\ninspect(assoc_rules[1:30]) #%>% broom::tidy() %>% kable\n\n     lhs                rhs         support    confidence coverage   lift     \n[1]  {Toast}         => {Coffee}    0.02366614 0.70440252 0.03359746 1.4724315\n[2]  {Medialuna}     => {Coffee}    0.03518225 0.56923077 0.06180666 1.1898784\n[3]  {Pastry}        => {Coffee}    0.04754358 0.55214724 0.08610671 1.1541682\n[4]  {Juice}         => {Coffee}    0.02060222 0.53424658 0.03856313 1.1167500\n[5]  {Sandwich}      => {Coffee}    0.03824617 0.53235294 0.07184363 1.1127916\n[6]  {Cake}          => {Coffee}    0.05472795 0.52695829 0.10385631 1.1015151\n[7]  {Cookies}       => {Coffee}    0.02820919 0.51844660 0.05441099 1.0837229\n[8]  {Hot chocolate} => {Coffee}    0.02958267 0.50724638 0.05832013 1.0603107\n[9]  {}              => {Coffee}    0.47839408 0.47839408 1.00000000 1.0000000\n[10] {Tea}           => {Coffee}    0.04986793 0.34962963 0.14263074 0.7308402\n[11] {Pastry}        => {Bread}     0.02916006 0.33865031 0.08610671 1.0349774\n[12] {}              => {Bread}     0.32720549 0.32720549 1.00000000 1.0000000\n[13] {Bread}         => {Coffee}    0.09001585 0.27510494 0.32720549 0.5750592\n[14] {Cake}          => {Tea}       0.02377179 0.22889115 0.10385631 1.6047813\n[15] {Cake}          => {Bread}     0.02334918 0.22482197 0.10385631 0.6870972\n[16] {Tea}           => {Bread}     0.02810354 0.19703704 0.14263074 0.6021813\n[17] {Coffee}        => {Bread}     0.09001585 0.18816254 0.47839408 0.5750592\n[18] {Tea}           => {Cake}      0.02377179 0.16666667 0.14263074 1.6047813\n[19] {}              => {Tea}       0.14263074 0.14263074 1.00000000 1.0000000\n[20] {Coffee}        => {Cake}      0.05472795 0.11439929 0.47839408 1.1015151\n[21] {Coffee}        => {Tea}       0.04986793 0.10424028 0.47839408 0.7308402\n[22] {}              => {Cake}      0.10385631 0.10385631 1.00000000 1.0000000\n[23] {Coffee}        => {Pastry}    0.04754358 0.09938163 0.47839408 1.1541682\n[24] {Bread}         => {Pastry}    0.02916006 0.08911850 0.32720549 1.0349774\n[25] {}              => {Pastry}    0.08610671 0.08610671 1.00000000 1.0000000\n[26] {Bread}         => {Tea}       0.02810354 0.08588957 0.32720549 0.6021813\n[27] {Coffee}        => {Sandwich}  0.03824617 0.07994700 0.47839408 1.1127916\n[28] {Coffee}        => {Medialuna} 0.03518225 0.07354240 0.47839408 1.1898784\n[29] {}              => {Sandwich}  0.07184363 0.07184363 1.00000000 1.0000000\n[30] {Bread}         => {Cake}      0.02334918 0.07135938 0.32720549 0.6870972\n     count\n[1]   224 \n[2]   333 \n[3]   450 \n[4]   195 \n[5]   362 \n[6]   518 \n[7]   267 \n[8]   280 \n[9]  4528 \n[10]  472 \n[11]  276 \n[12] 3097 \n[13]  852 \n[14]  225 \n[15]  221 \n[16]  266 \n[17]  852 \n[18]  225 \n[19] 1350 \n[20]  518 \n[21]  472 \n[22]  983 \n[23]  450 \n[24]  276 \n[25]  815 \n[26]  266 \n[27]  362 \n[28]  333 \n[29]  680 \n[30]  221 \n\n\nI hope with this small example you can now understand how association analysis works."
  }
]