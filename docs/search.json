[
  {
    "objectID": "datacamp/network-analysis-in-r/network_analysis_r.html",
    "href": "datacamp/network-analysis-in-r/network_analysis_r.html",
    "title": "Network analysis in R",
    "section": "",
    "text": "Here you will learn how to create an igraph ‘object’ from data stored in an edgelist. The data are friendships in a group of students. You will also learn how to make a basic visualization of the network.\nEach row of the friends dataframe represents an edge in the network.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(igraph)\nlibrary(knitr)\nfriends <- fread(\"friends.csv\")\n\n\n# Inspect the first few rows of the dataframe 'friends'\nhead(friends) %>% kable\n\n\n\n\nname1\nname2\n\n\n\n\nJessie\nSidney\n\n\nJessie\nBritt\n\n\nSidney\nBritt\n\n\nSidney\nDonnie\n\n\nKarl\nBerry\n\n\nSidney\nRene\n\n\n\n\n# Convert friends dataframe to a matrix\nfriends.mat <- as.matrix(friends)\n\n# Convert friends matrix to an igraph object\ng <- graph.edgelist(friends.mat, directed = FALSE)\n\n\n# Make a very basic plot of the network\nplot(g)"
  },
  {
    "objectID": "datacamp/network-analysis-in-r/network_analysis_r.html#neighbors",
    "href": "datacamp/network-analysis-in-r/network_analysis_r.html#neighbors",
    "title": "Network analysis in R",
    "section": "Neighbors",
    "text": "Neighbors\nOften in network analysis it is important to explore the patterning of connections that exist between vertices. One way is to identify neighboring vertices of each vertex. You can then determine which neighboring vertices are shared even by unconnected vertices indicating how two vertices may have an indirect relationship through others. In this exercise you will learn how to identify neighbors and shared neighbors between pairs of vertices.\n\n# Identify all neighbors of vertex 12 regardless of direction\nneighbors(g, '12', mode = c('all'))\n\n+ 5/187 vertices, named, from 7758ad8:\n[1] 45  13  72  89  109\n\n# Identify other vertices that direct edges towards vertex 12\nneighbors(g, '12', mode = c('in'))\n\n+ 1/187 vertex, named, from 7758ad8:\n[1] 45\n\n# Identify any vertices that receive an edge from vertex 42 and direct an edge to vertex 124\nn1 <-neighbors(g, '42', mode = c('out'))\nn2 <- neighbors(g, '124', mode = c('in'))\nintersection(n1, n2)\n\n+ 1/187 vertex, named, from 7758ad8:\n[1] 7"
  },
  {
    "objectID": "datacamp/network-analysis-in-r/network_analysis_r.html#finding-longest-path-between-two-vertices",
    "href": "datacamp/network-analysis-in-r/network_analysis_r.html#finding-longest-path-between-two-vertices",
    "title": "Network analysis in R",
    "section": "Finding longest path between two vertices",
    "text": "Finding longest path between two vertices\n\nWhat is the longest possible path in a network referred to as?\nDiameter"
  },
  {
    "objectID": "datacamp/network-analysis-in-r/network_analysis_r.html#network-density-and-average-path-length",
    "href": "datacamp/network-analysis-in-r/network_analysis_r.html#network-density-and-average-path-length",
    "title": "Network analysis in R",
    "section": "Network density and average path length",
    "text": "Network density and average path length\nThe first graph level metric you will explore is the density of a graph. This is essentially the proportion of all potential edges between vertices that actually exist in the network graph. It is an indicator of how well connected the vertices of the graph are.\nAnother measure of how interconnected a network is average path length. This is calculated by determining the mean of the lengths of the shortest paths between all pairs of vertices in the network. The longest path length between any pair of vertices is called the diameter of the network graph. You will calculate the diameter and average path length of the original graph g.\n\n# Get density of a graph\ngd <- edge_density(g)\n\n# Get the diameter of the graph g\ndiameter(g, directed = FALSE)\n\n[1] 4\n\n# Get the average path length of the graph g\ng.apl <- mean_distance(g, directed = FALSE)\ng.apl\n\n[1] 1.994967\n\n\n\nIf a graph has 7 vertices there are 21 possible edges in the network. If 14 edges exist, what is the density of the network?\n0.67"
  },
  {
    "objectID": "datacamp/network-analysis-in-r/network_analysis_r.html#randomization-quiz",
    "href": "datacamp/network-analysis-in-r/network_analysis_r.html#randomization-quiz",
    "title": "Network analysis in R",
    "section": "Randomization quiz",
    "text": "Randomization quiz\n\nRandomization tests enable you to identify:\nWhether features of your original network are particularly unusual."
  },
  {
    "objectID": "datacamp/network-analysis-in-r/network_analysis_r.html#what-does-assortativity-measure",
    "href": "datacamp/network-analysis-in-r/network_analysis_r.html#what-does-assortativity-measure",
    "title": "Network analysis in R",
    "section": "What does assortativity measure",
    "text": "What does assortativity measure\n\nHow likely vertices are to connect to others that share some attribute in common."
  },
  {
    "objectID": "datacamp/regression_r/regression.html",
    "href": "datacamp/regression_r/regression.html",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "",
    "text": "We use the lm() function to fit linear models to data. In this case, we want to understand how the price of MarioKart games sold at auction varies as a function of not only the number of wheels included in the package, but also whether the item is new or used. Obviously, it is expected that you might have to pay a premium to buy these new. But how much is that premium? Can we estimate its value after controlling for the number of wheels?\nWe will fit a parallel slopes model using lm(). In addition to the data argument, lm() needs to know which variables you want to include in your regression model, and how you want to include them. It accomplishes this using a formula argument. A simple linear regression formula looks like y ~ x, where y is the name of the response variable, and x is the name of the explanatory variable. Here, we will simply extend this formula to include multiple explanatory variables. A parallel slopes model has the form y ~ x + z, where z is a categorical explanatory variable, and x is a numerical explanatory variable.\nThe output from lm() is a model object, which when printed, will show the fitted coefficients.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(openintro)\nlibrary(broom)\nlibrary(pander)\ndata( mariokart, package = \"openintro\")\nmario_kart <- mariokart\n\nmario_kart <- mario_kart %>% mutate(total_pr := ifelse(total_pr > 100, NA, total_pr))\n# Explore the data\nglimpse(mario_kart)\n\nRows: 143\nColumns: 12\n$ id          <dbl> 150377422259, 260483376854, 320432342985, 280405224677, 17…\n$ duration    <int> 3, 7, 3, 3, 1, 3, 1, 1, 3, 7, 1, 1, 1, 1, 7, 7, 3, 3, 1, 7…\n$ n_bids      <int> 20, 13, 16, 18, 20, 19, 13, 15, 29, 8, 15, 15, 13, 16, 6, …\n$ cond        <fct> new, used, new, new, new, new, used, new, used, used, new,…\n$ start_pr    <dbl> 0.99, 0.99, 0.99, 0.99, 0.01, 0.99, 0.01, 1.00, 0.99, 19.9…\n$ ship_pr     <dbl> 4.00, 3.99, 3.50, 0.00, 0.00, 4.00, 0.00, 2.99, 4.00, 4.00…\n$ total_pr    <dbl> 51.55, 37.04, 45.50, 44.00, 71.00, 45.00, 37.02, 53.99, 47…\n$ ship_sp     <fct> standard, firstClass, firstClass, standard, media, standar…\n$ seller_rate <int> 1580, 365, 998, 7, 820, 270144, 7284, 4858, 27, 201, 4858,…\n$ stock_photo <fct> yes, yes, no, yes, yes, yes, yes, yes, yes, no, yes, yes, …\n$ wheels      <int> 1, 1, 1, 1, 2, 0, 0, 2, 1, 1, 2, 2, 2, 2, 1, 0, 1, 1, 2, 2…\n$ title       <fct> \"~~ Wii MARIO KART &amp; WHEEL ~ NINTENDO Wii ~ BRAND NEW …\n\n# fit parallel slopes\n\nmod_mario <- lm(total_pr ~ wheels + cond, data = mario_kart)"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#reasoning-about-two-intercepts",
    "href": "datacamp/regression_r/regression.html#reasoning-about-two-intercepts",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Reasoning about two intercepts",
    "text": "Reasoning about two intercepts\nThe mario_kart data contains several other variables. The totalPr, startPr, and shipPr variables are numeric, while the cond and stockPhoto variables are categorical.\nWhich formula will result in a parallel slopes model?\n\ntotalPr ~ shipPr + stockPhoto"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#using-geom_line-and-augment",
    "href": "datacamp/regression_r/regression.html#using-geom_line-and-augment",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Using geom_line() and augment()",
    "text": "Using geom_line() and augment()\nParallel slopes models are so-named because we can visualize these models in the data space as not one line, but two parallel lines. To do this, we’ll draw two things:\na scatterplot showing the data, with color separating the points into groups a line for each value of the categorical variable Our plotting strategy is to compute the fitted values, plot these, and connect the points to form a line. The augment() function from the broom package provides an easy way to add the fitted values to our data frame, and the geom_line() function can then use that data frame to plot the points and connect them.\nNote that this approach has the added benefit of automatically coloring the lines appropriately to match the data.\nYou already know how to use ggplot() and geom_point() to make the scatterplot. The only twist is that now you’ll pass your augment()-ed model as the data argument in your ggplot() call. When you add your geom_line(), instead of letting the y aesthetic inherit its values from the ggplot() call, you can set it to the .fitted column of the augment()-ed model. This has the advantage of automatically coloring the lines for you.\n\n# Augment the model\naugmented_mod <- augment(mod_mario)\nglimpse(augmented_mod)\n\nRows: 141\nColumns: 10\n$ .rownames  <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"1…\n$ total_pr   <dbl> 51.55, 37.04, 45.50, 44.00, 71.00, 45.00, 37.02, 53.99, 47.…\n$ wheels     <int> 1, 1, 1, 1, 2, 0, 0, 2, 1, 1, 2, 2, 2, 2, 1, 0, 1, 1, 2, 0,…\n$ cond       <fct> new, used, new, new, new, new, used, new, used, used, new, …\n$ .fitted    <dbl> 49.60260, 44.01777, 49.60260, 49.60260, 56.83544, 42.36976,…\n$ .resid     <dbl> 1.9473995, -6.9777674, -4.1026005, -5.6026005, 14.1645592, …\n$ .hat       <dbl> 0.02103158, 0.01250410, 0.02103158, 0.02103158, 0.01915635,…\n$ .sigma     <dbl> 4.902339, 4.868399, 4.892414, 4.881308, 4.750591, 4.899816,…\n$ .cooksd    <dbl> 1.161354e-03, 8.712334e-03, 5.154337e-03, 9.612441e-03, 5.5…\n$ .std.resid <dbl> 0.40270893, -1.43671086, -0.84838977, -1.15857953, 2.926332…\n\n# scatterplot, with color\ndata_space <- ggplot(augmented_mod, aes(x = wheels, y = total_pr , color = cond )) + \n  geom_point()\n  \n# single call to geom_line()\ndata_space + \n  geom_line(aes(y = .fitted))"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#intercept-interpretation",
    "href": "datacamp/regression_r/regression.html#intercept-interpretation",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Intercept interpretation",
    "text": "Intercept interpretation\nRecall that the cond variable is either new or used. Here are the fitted coefficients from your model:\nCall: lm(formula = totalPr ~ wheels + cond, data = mario_kart)\nCoefficients: (Intercept) wheels condused\n42.370 7.233 -5.585\nChoose the correct interpretation of the coefficient on condused:\n\nThe expected price of a used MarioKart is $5.58 less than that of a new one with the same number of wheels.\nFor each additional wheel, the expected price of a MarioKart increases by $7.23 regardless of whether it is new or used.\n\nSyntax from math The babies data set contains observations about the birthweight and other characteristics of children born in the San Francisco Bay area from 1960–1967.\nWe would like to build a model for birthweight as a function of the mother’s age and whether this child was her first (parity == 0). Use the mathematical specification below to code the model in R.\n\\[birthweight = \\beta_0 + \\beta_1 * age  + \\beta_2 * parity + \\epsilon\\]\n\ndata( babies, package = \"openintro\")\n\nmod <- lm(bwt~ age+parity, data = babies)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n118.3\n2.788\n42.43\n3.957e-243\n\n\nage\n0.06315\n0.09577\n0.6594\n0.5097\n\n\nparity\n-1.652\n1.271\n-1.3\n0.1937"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#syntax-from-plot",
    "href": "datacamp/regression_r/regression.html#syntax-from-plot",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Syntax from plot",
    "text": "Syntax from plot\nThis time, we’d like to build a model for birthweight as a function of the length of gestation and the mother’s smoking status. Use the plot to inform your model specification.\n\nggplot(babies, aes(gestation, bwt, color = factor(smoke)))+\n    geom_point()\n\n\n\nmod <- lm(bwt~ gestation + smoke, data = babies)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.9317\n8.152\n-0.1143\n0.909\n\n\ngestation\n0.4429\n0.02902\n15.26\n3.156e-48\n\n\nsmoke\n-8.088\n0.9527\n-8.49\n5.963e-17\n\n\n\n\n\nR-squared vs. adjusted R-squared Two common measures of how well a model fits to data are \\[R^2\\] (the coefficient of determination) and the adjusted \\[R^2\\] . The former measures the percentage of the variability in the response variable that is explained by the model. To compute this, we define\n\\[R^2 = 1 - \\frac{sse}{sst} \\] where SSE and SST are the sum of the squared residuals, and the total sum of the squares, respectively. One issue with this measure is that the can only decrease as new variable are added to the model, while the SST depends only on the response variable and therefore is not affected by changes to the model. This means that you can increase \\[R^2\\] by adding any additional variable to your model—even random noise.\nThe adjusted \\[R^2\\] includes a term that penalizes a model for each additional explanatory variable (where is the number of explanatory variables). We can see both measures in the output of the summary() function on our model object.\n\n# R^2 and adjusted R^2\nsummary(mod_mario)\n\n\nCall:\nlm(formula = total_pr ~ wheels + cond, data = mario_kart)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.0078  -3.0754  -0.8254   2.9822  14.1646 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  42.3698     1.0651  39.780  < 2e-16 ***\nwheels        7.2328     0.5419  13.347  < 2e-16 ***\ncondused     -5.5848     0.9245  -6.041 1.35e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.887 on 138 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7165,    Adjusted R-squared:  0.7124 \nF-statistic: 174.4 on 2 and 138 DF,  p-value: < 2.2e-16\n\n# add random noise\nmario_kart_noisy <- mario_kart %>% \nmutate(noise = rnorm(n = nrow(mario_kart)))\n  \n# compute new model\nmod2_mario2 <- lm(total_pr ~ wheels + cond+noise, data = mario_kart_noisy)\n\n# new R^2 and adjusted R^2\nsummary(mod2_mario2)\n\n\nCall:\nlm(formula = total_pr ~ wheels + cond + noise, data = mario_kart_noisy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.6919  -3.2414  -0.7768   2.7511  12.8040 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  42.3422     1.0647  39.769  < 2e-16 ***\nwheels        7.1972     0.5425  13.266  < 2e-16 ***\ncondused     -5.3883     0.9414  -5.724  6.3e-08 ***\nnoise         0.5403     0.4968   1.088    0.279    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.884 on 137 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7189,    Adjusted R-squared:  0.7128 \nF-statistic: 116.8 on 3 and 137 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#prediction",
    "href": "datacamp/regression_r/regression.html#prediction",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Prediction",
    "text": "Prediction\nOnce we have fit a regression model, we can use it to make predictions for unseen observations or retrieve the fitted values. Here, we explore two methods for doing the latter.\nA traditional way to return the fitted values (i.e. the y ’s) is to run the predict() function on the model object. This will return a vector of the fitted values. Note that predict() will take an optional newdata argument that will allow you to make predictions for observations that are not in the original data.\nA newer alternative is the augment() function from the broom package, which returns a data.frame with the response varible (), the relevant explanatory variables (the ’s), the fitted value ( ) and some information about the residuals (). augment() will also take a newdata argument that allows you to make predictions.\n\n# return a vector\nlibrary(knitr)\n\npredict(mod_mario)\n\n       1        2        3        4        5        6        7        8 \n49.60260 44.01777 49.60260 49.60260 56.83544 42.36976 36.78493 56.83544 \n       9       10       11       12       13       14       15       16 \n44.01777 44.01777 56.83544 56.83544 56.83544 56.83544 44.01777 36.78493 \n      17       18       19       21       22       23       24       25 \n49.60260 49.60260 56.83544 36.78493 56.83544 56.83544 56.83544 44.01777 \n      26       27       28       29       30       31       32       33 \n56.83544 36.78493 36.78493 36.78493 49.60260 36.78493 36.78493 44.01777 \n      34       35       36       37       38       39       40       41 \n51.25061 44.01777 44.01777 36.78493 44.01777 56.83544 56.83544 49.60260 \n      42       43       44       45       46       47       48       49 \n44.01777 51.25061 56.83544 56.83544 44.01777 56.83544 36.78493 36.78493 \n      50       51       52       53       54       55       56       57 \n44.01777 56.83544 36.78493 44.01777 42.36976 36.78493 36.78493 44.01777 \n      58       59       60       61       62       63       64       66 \n44.01777 36.78493 36.78493 56.83544 36.78493 56.83544 36.78493 51.25061 \n      67       68       69       70       71       72       73       74 \n56.83544 44.01777 58.48345 51.25061 49.60260 44.01777 49.60260 56.83544 \n      75       76       77       78       79       80       81       82 \n56.83544 51.25061 44.01777 36.78493 36.78493 36.78493 44.01777 56.83544 \n      83       84       85       86       87       88       89       90 \n44.01777 65.71629 44.01777 56.83544 36.78493 49.60260 49.60260 36.78493 \n      91       92       93       94       95       96       97       98 \n44.01777 36.78493 51.25061 44.01777 36.78493 51.25061 42.36976 56.83544 \n      99      100      101      102      103      104      105      106 \n51.25061 44.01777 51.25061 56.83544 56.83544 56.83544 36.78493 49.60260 \n     107      108      109      110      111      112      113      114 \n51.25061 44.01777 56.83544 49.60260 36.78493 44.01777 51.25061 56.83544 \n     115      116      117      118      119      120      121      122 \n64.06828 44.01777 49.60260 44.01777 49.60260 51.25061 42.36976 44.01777 \n     123      124      125      126      127      128      129      130 \n56.83544 44.01777 49.60260 44.01777 51.25061 56.83544 56.83544 49.60260 \n     131      132      133      134      135      136      137      138 \n56.83544 36.78493 44.01777 44.01777 36.78493 56.83544 36.78493 44.01777 \n     139      140      141      142      143 \n36.78493 51.25061 49.60260 36.78493 56.83544 \n\n# return a data frame\n\naugment(mod_mario)%>% head() %>% kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.rownames\ntotal_pr\nwheels\ncond\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n1\n51.55\n1\nnew\n49.60260\n1.947399\n0.0210316\n4.902340\n0.0011614\n0.4027089\n\n\n2\n37.04\n1\nused\n44.01777\n-6.977767\n0.0125041\n4.868399\n0.0087123\n-1.4367109\n\n\n3\n45.50\n1\nnew\n49.60260\n-4.102601\n0.0210316\n4.892414\n0.0051543\n-0.8483898\n\n\n4\n44.00\n1\nnew\n49.60260\n-5.602601\n0.0210316\n4.881308\n0.0096124\n-1.1585795\n\n\n5\n71.00\n2\nnew\n56.83544\n14.164559\n0.0191563\n4.750591\n0.0557493\n2.9263328\n\n\n6\n45.00\n0\nnew\n42.36976\n2.630240\n0.0474932\n4.899816\n0.0050537\n0.5514192"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#thought-experiments",
    "href": "datacamp/regression_r/regression.html#thought-experiments",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Thought experiments",
    "text": "Thought experiments\nSuppose that after going apple picking you have 12 apples left over. You decide to conduct an experiment to investigate how quickly they will rot under certain conditions. You place six apples in a cool spot in your basement, and leave the other six on the window sill in the kitchen. Every week, you estimate the percentage of the surface area of the apple that is rotten or moldy.\nConsider the following models:\n\\[rot = \\beta_0 + \\beta_1 *t + \\beta_2 * temp + \\epsilon \\]\nand\n\\[rot = \\beta_0 + \\beta_1 *t + \\beta_2 * temp + \\beta_2 * temp *t +  \\epsilon \\]\n\nThe rate at which apples rot will vary based on the temperature."
  },
  {
    "objectID": "datacamp/regression_r/regression.html#fitting-a-model-with-interaction",
    "href": "datacamp/regression_r/regression.html#fitting-a-model-with-interaction",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Fitting a model with interaction",
    "text": "Fitting a model with interaction\nIncluding an interaction term in a model is easy—we just have to tell lm() that we want to include that new variable. An expression of the form\nlm(y ~ x + z + x:z, data = mydata)\nwill do the trick. The use of the colon (:) here means that the interaction between and will be a third term in the model.\n\n# include interaction\n\nmod <- lm(total_pr ~cond + duration + cond:duration, data = mario_kart)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n58.27\n1.366\n42.64\n5.832e-81\n\n\ncondused\n-17.12\n2.178\n-7.86\n1.014e-12\n\n\nduration\n-1.966\n0.4488\n-4.38\n2.342e-05\n\n\ncondused:duration\n2.325\n0.5484\n4.239\n4.102e-05"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#visualizing-interaction-models",
    "href": "datacamp/regression_r/regression.html#visualizing-interaction-models",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Visualizing interaction models",
    "text": "Visualizing interaction models\nInteraction allows the slope of the regression line in each group to vary. In this case, this means that the relationship between the final price and the length of the auction is moderated by the condition of each item.\nInteraction models are easy to visualize in the data space with ggplot2 because they have the same coefficients as if the models were fit independently to each group defined by the level of the categorical variable. In this case, new and used MarioKarts each get their own regression line. To see this, we can set an aesthetic (e.g. color) to the categorical variable, and then add a geom_smooth() layer to overlay the regression line for each color.\n\n# interaction plot\nggplot(mario_kart, aes(duration, total_pr, color = cond)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = 0)"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#consequences-of-simpsons-paradox",
    "href": "datacamp/regression_r/regression.html#consequences-of-simpsons-paradox",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Consequences of Simpson’s paradox",
    "text": "Consequences of Simpson’s paradox\nIn the simple linear regression model for average SAT score, (total) as a function of average teacher salary (salary), the fitted coefficient was -5.02 points per thousand dollars. This suggests that for every additional thousand dollars of salary for teachers in a particular state, the expected SAT score for a student from that state is about 5 points lower.\nIn the model that includes the percentage of students taking the SAT, the coefficient on salary becomes 1.84 points per thousand dollars. Choose the correct interpretation of this slope coefficient.\n\nFor every additional thousand dollars of salary for teachers in a particular state, the expected SAT score for a student from that state is about 2 points higher, after controlling for the percentage of students taking the SAT."
  },
  {
    "objectID": "datacamp/regression_r/regression.html#simpsons-paradox-in-action",
    "href": "datacamp/regression_r/regression.html#simpsons-paradox-in-action",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Simpson’s paradox in action",
    "text": "Simpson’s paradox in action\nA mild version of Simpson’s paradox can be observed in the MarioKart auction data. Consider the relationship between the final auction price and the length of the auction. It seems reasonable to assume that longer auctions would result in higher prices, since—other things being equal—a longer auction gives more bidders more time to see the auction and bid on the item.\nHowever, a simple linear regression model reveals the opposite: longer auctions are associated with lower final prices. The problem is that all other things are not equal. In this case, the new MarioKarts—which people pay a premium for—were mostly sold in one-day auctions, while a plurality of the used MarioKarts were sold in the standard seven-day auctions.\nOur simple linear regression model is misleading, in that it suggests a negative relationship between final auction price and duration. However, for the used MarioKarts, the relationship is positive.\n\nslr <- ggplot(mario_kart, aes(y = total_pr, x = duration)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n# model with one slope\nmod <- lm(total_pr ~ duration, data = mario_kart)\n\n# plot with two slopes\nslr + aes(color = cond)"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#fitting-a-mlr-model",
    "href": "datacamp/regression_r/regression.html#fitting-a-mlr-model",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Fitting a MLR model",
    "text": "Fitting a MLR model\nIn terms of the R code, fitting a multiple linear regression model is easy: simply add variables to the model formula you specify in the lm() command.\nIn a parallel slopes model, we had two explanatory variables: one was numeric and one was categorical. Here, we will allow both explanatory variables to be numeric.\n\n# Fit the model using duration and startPr\n\nmod <- lm(total_pr~ start_pr + duration, data = mario_kart)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n51.03\n1.179\n43.28\n3.666e-82\n\n\nstart_pr\n0.233\n0.04364\n5.339\n3.756e-07\n\n\nduration\n-1.508\n0.2555\n-5.902\n2.645e-08"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#tiling-the-plane",
    "href": "datacamp/regression_r/regression.html#tiling-the-plane",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Tiling the plane",
    "text": "Tiling the plane\nOne method for visualizing a multiple linear regression model is to create a heatmap of the fitted values in the plane defined by the two explanatory variables. This heatmap will illustrate how the model output changes over different combinations of the explanatory variables.\nThis is a multistep process:\nFirst, create a grid of the possible pairs of values of the explanatory variables. The grid should be over the actual range of the data present in each variable. We’ve done this for you and stored the result as a data frame called grid. Use augment() with the newdata argument to find the ’s corresponding to the values in grid. Add these to the data_space plot by using the fill aesthetic and geom_tile().\n\n# add predictions to grid\nprice_hats <- augment(mod, newdata = grid)\n\n# tile the plane\ndata_space + \n  geom_tile(data = price_hats, aes(fill = .fitted), alpha = 0.5)"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#models-in-3d",
    "href": "datacamp/regression_r/regression.html#models-in-3d",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Models in 3D",
    "text": "Models in 3D\nAn alternative way to visualize a multiple regression model with two numeric explanatory variables is as a plane in three dimensions. This is possible in R using the plotly package.\nWe have created three objects that you will need:\nx: a vector of unique values of duration y: a vector of unique values of startPr plane: a matrix of the fitted values across all combinations of x and y Much like ggplot(), the plot_ly() function will allow you to create a plot object with variables mapped to x, y, and z aesthetics. The add_markers() function is similar to geom_point() in that it allows you to add points to your 3D plot.\nNote that plot_ly uses the pipe (%>%) operator to chain commands together.\n\n# draw the 3D scatterplot\np <- plot_ly(data = mario_kart, z = ~totalPr, x = ~duration, y = ~startPr, opacity = 0.6) %>%\n  add_markers() \n  \n# draw the plane\np %>%\n  add_surface(x = ~x, y = ~y, z = ~plane, showscale = FALSE)"
  },
  {
    "objectID": "datacamp/scalable_data_R/scalable_data_r.html",
    "href": "datacamp/scalable_data_R/scalable_data_r.html",
    "title": "Scalable Data Processing in R",
    "section": "",
    "text": "If you are processing all elements of two data sets, and one data set is bigger, then the bigger data set will take longer to process. However, it’s important to realize that how much longer it takes is not always directly proportional to how much bigger it is. That is, if you have two data sets and one is two times the size of the other, it is not guaranteed that the larger one will take twice as long to process. It could take 1.5 times longer or even four times longer. It depends on which operations are used to process the data set.\nIn this exercise, you’ll use the microbenchmark package, which was covered in the Writing Efficient R Code course.\nNote: Numbers are specified using scientific notation\n\n# Load the microbenchmark package\nlibrary(microbenchmark)\n\n# Compare the timings for sorting different sizes of vector\nmb <- microbenchmark(\n  # Sort a random normal vector length 1e5\n  \"1e5\" = sort(rnorm(1e5)),\n  # Sort a random normal vector length 2.5e5\n  \"2.5e5\" = sort(rnorm(2.5e5)),\n  # Sort a random normal vector length 5e5\n  \"5e5\" = sort(rnorm(5e5)),\n  \"7.5e5\" = sort(rnorm(7.5e5)),\n  \"1e6\" = sort(rnorm(1e6)),\n  times = 10\n)\n\n# Plot the resulting benchmark object\nplot(mb)"
  },
  {
    "objectID": "datacamp/scalable_data_R/scalable_data_r.html#reading-a-big.matrix-object",
    "href": "datacamp/scalable_data_R/scalable_data_r.html#reading-a-big.matrix-object",
    "title": "Scalable Data Processing in R",
    "section": "Reading a big.matrix object",
    "text": "Reading a big.matrix object\nIn this exercise, you’ll create your first file-backed big.matrix object using the read.big.matrix() function. The function is meant to look similar to read.table() but, in addition, it needs to know what type of numeric values you want to read (“char”, “short”, “integer”, “double”), it needs the name of the file that will hold the matrix’s data (the backing file), and it needs the name of the file to hold information about the matrix (a descriptor file). The result will be a file on the disk holding the value read in along with a descriptor file which holds extra information (like the number of columns and rows) about the resulting big.matrix object.\n\n# Load the bigmemory package\nlibrary(bigmemory)\n\n# Create the big.matrix object: x\nx <- read.big.matrix(\"mortgage-sample.csv\", header = TRUE, \n                     type = \"integer\", \n                     backingfile = \"mortgage-sample.bin\", \n                     descriptorfile = \"mortgage-sample.desc\")\n    \n# Find the dimensions of x\ndim(x)\n\n[1] 70000    16"
  },
  {
    "objectID": "datacamp/scalable_data_R/scalable_data_r.html#attaching-a-big.matrix-object",
    "href": "datacamp/scalable_data_R/scalable_data_r.html#attaching-a-big.matrix-object",
    "title": "Scalable Data Processing in R",
    "section": "Attaching a big.matrix object",
    "text": "Attaching a big.matrix object\nNow that the big.matrix object is on the disk, we can use the information stored in the descriptor file to instantly make it available during an R session. This means that you don’t have to reimport the data set, which takes more time for larger files. You can simply point the bigmemory package at the existing structures on the disk and begin accessing data without the wait.\n\n# Attach mortgage-sample.desc\nmort <- attach.big.matrix(\"mortgage-sample.desc\")\n\n# Find the dimensions of mort\ndim(mort)\n\n[1] 70000    16\n\n# Look at the first 6 rows of mort\nhead(mort)\n\n     enterprise record_number msa perc_minority tract_income_ratio\n[1,]          1           566   1             1                  3\n[2,]          1           116   1             3                  2\n[3,]          1           239   1             2                  2\n[4,]          1            62   1             2                  3\n[5,]          1           106   1             2                  3\n[6,]          1           759   1             3                  3\n     borrower_income_ratio loan_purpose federal_guarantee borrower_race\n[1,]                     1            2                 4             3\n[2,]                     1            2                 4             5\n[3,]                     3            8                 4             5\n[4,]                     3            2                 4             5\n[5,]                     3            2                 4             9\n[6,]                     2            2                 4             9\n     co_borrower_race borrower_gender co_borrower_gender num_units\n[1,]                9               2                  4         1\n[2,]                9               1                  4         1\n[3,]                5               1                  2         1\n[4,]                9               2                  4         1\n[5,]                9               3                  4         1\n[6,]                9               1                  2         2\n     affordability year type\n[1,]             3 2010    1\n[2,]             3 2008    1\n[3,]             4 2014    0\n[4,]             4 2009    1\n[5,]             4 2013    1\n[6,]             4 2010    1"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html",
    "href": "datacamp/tidymodels/tidymodels.html",
    "title": "Modeling with tidymodels in R",
    "section": "",
    "text": "The rsample package is designed to create training and test datasets. Creating a test dataset is important for estimating how a trained model will likely perform on new data. It also guards against overfitting, where a model memorizes patterns that exist only in the training data and performs poorly on new data.\nIn this exercise, you will create training and test datasets from the home_sales data. This data contains information on homes sold in the Seattle, Washington area between 2015 and 2016.\nThe outcome variable in this data is selling_price.\nThe tidymodels package will be pre-loaded in every exercise in the course. The home_sales tibble has also been loaded for you.\n\n\n\nTidy model packages\n\n\n\nhome_sales <- readRDS(\"home_sales.rds\")\n\n# Create a data split object\nhome_split <- initial_split(home_sales, \n                            prop = 0.7, \n                            strata = selling_price)\n\n# Create the training data\nhome_training <- home_split %>%\n  training()\n\n# Create the test data\nhome_test <- home_split %>% \n  testing()\n\n# Check number of rows in each dataset\nnrow(home_training)\n\n[1] 1042\n\nnrow(home_test)\n\n[1] 450\n\n\nDistribution of outcome variable values Stratifying by the outcome variable when generating training and test datasets ensures that the outcome variable values have a similar range in both datasets.\nSince the original data is split at random, stratification avoids placing all the expensive homes in home_sales into the test dataset, for example. In this case, your model would most likely perform poorly because it was trained on less expensive homes.\nIn this exercise, you will calculate summary statistics for the selling_price variable in the training and test datasets. The home_training and home_test tibbles have been loaded from the previous exercise.\n\n# Distribution of selling_price in training data\nlibrary(knitr)\nsummary_func <- function(df){\n      df %>% summarize(min_sell_price = min(selling_price),\n            max_sell_price = max(selling_price),\n            mean_sell_price = mean(selling_price),\n            sd_sell_price = sd(selling_price)) %>%\n    kable()\n\n}\nhome_training %>% \n    summary_func()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n478986.2\n80915.09\n\n\n\n\nhome_test %>% \n  summary_func()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n479312.1\n81216.26"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#fitting-a-linear-regression-model",
    "href": "datacamp/tidymodels/tidymodels.html#fitting-a-linear-regression-model",
    "title": "Modeling with tidymodels in R",
    "section": "Fitting a linear regression model",
    "text": "Fitting a linear regression model\nThe parsnip package provides a unified syntax for the model fitting process in R.\nWith parsnip, it is easy to define models using the various packages, or engines, that exist in the R ecosystem.\nIn this exercise, you will define a parsnip linear regression object and train your model to predict selling_price using home_age and sqft_living as predictor variables from the home_sales data.\nThe home_training and home_test tibbles that you created in the previous lesson have been loaded into this session.\n\n# Specify a linear regression model, linear_model\nlinear_model <- linear_reg() %>% \n  # Set the model engine\n  set_engine('lm') %>% \n  # Set the model mode\n  set_mode('regression')\n\n# Train the model with the training data\nlm_fit <- linear_model %>% \n  fit(selling_price~ home_age + sqft_living,\n      data = home_training)\n\n# Print lm_fit to view model information\ntidy(lm_fit) %>%\n    kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n291856.6392\n7408.987519\n39.392243\n0\n\n\nhome_age\n-1484.9767\n174.432209\n-8.513203\n0\n\n\nsqft_living\n102.7973\n2.678621\n38.376944\n0"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#predicting-home-selling-prices",
    "href": "datacamp/tidymodels/tidymodels.html#predicting-home-selling-prices",
    "title": "Modeling with tidymodels in R",
    "section": "Predicting home selling prices",
    "text": "Predicting home selling prices\nAfter fitting a model using the training data, the next step is to use it to make predictions on the test dataset. The test dataset acts as a new source of data for the model and will allow you to evaluate how well it performs.\nBefore you can evaluate model performance, you must add your predictions to the test dataset.\nIn this exercise, you will use your trained model, lm_fit, to predict selling_price in the home_test dataset.\nYour trained model, lm_fit, as well as the test dataset, home_test have been loaded into your session.\n\n# Predict selling_price\nhome_predictions <- predict(lm_fit,\n                        new_data = home_test)\n\n# View predicted selling prices\n#home_predictions\n\n# Combine test data with predictions\nhome_test_results <- home_test %>% \n  select(selling_price, home_age, sqft_living) %>% \n  bind_cols(home_predictions)\n\n# View results\nhome_test_results %>% \n    head()\n\n# A tibble: 6 × 4\n  selling_price home_age sqft_living   .pred\n          <dbl>    <dbl>       <dbl>   <dbl>\n1        411000       18        1130 381288.\n2        635000        4        3350 630288.\n3        425000       11        1920 472893.\n4        525000       16        2100 483971.\n5        381000       25        1680 427432.\n6        540000       22        2220 487397."
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#model-performance-metrics",
    "href": "datacamp/tidymodels/tidymodels.html#model-performance-metrics",
    "title": "Modeling with tidymodels in R",
    "section": "Model performance metrics",
    "text": "Model performance metrics\nEvaluating model results is an important step in the modeling process. Model evaluation should be done on the test dataset in order to see how well a model will generalize to new datasets.\nIn the previous exercise, you trained a linear regression model to predict selling_price using home_age and sqft_living as predictor variables. You then created the home_test_results tibble using your trained model on the home_test data.\nIn this exercise, you will calculate the RMSE and R squared metrics using your results in home_test_results.\nThe home_test_results tibble has been loaded into your session.\n\n# Print home_test_results\n#home_test_results\n\n# Caculate the RMSE metric\nhome_test_results %>% \n  rmse(truth = selling_price, estimate =.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      48709.\n\n# Calculate the R squared metric\nhome_test_results %>% \n  rsq(truth = selling_price, estimate =.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.640"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#r-squared-plot",
    "href": "datacamp/tidymodels/tidymodels.html#r-squared-plot",
    "title": "Modeling with tidymodels in R",
    "section": "R squared plot",
    "text": "R squared plot\nIn the previous exercise, you got an R squared value of 0.651. The R squared metric ranges from 0 to 1, 0 being the worst and 1 the best.\nCalculating the R squared value is only the first step in studying your model’s predictions.\nMaking an R squared plot is extremely important because it will uncover potential problems with your model, such as non-linear patterns or regions where your model is either over or under-predicting the outcome variable.\nIn this exercise, you will create an R squared plot of your model’s performance.\nThe home_test_results tibble has been loaded into your session.\n\n# Create an R squared plot of model performance\nggplot(home_test_results, aes(x = selling_price, y = .pred)) +\n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred()  +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#complete-model-fitting-process-with-last_fit",
    "href": "datacamp/tidymodels/tidymodels.html#complete-model-fitting-process-with-last_fit",
    "title": "Modeling with tidymodels in R",
    "section": "Complete model fitting process with last_fit()",
    "text": "Complete model fitting process with last_fit()\nIn this exercise, you will train and evaluate the performance of a linear regression model that predicts selling_price using all the predictors available in the home_sales tibble.\nThis exercise will give you a chance to perform the entire model fitting process with tidymodels, from defining your model object to evaluating its performance on the test data.\nEarlier in the chapter, you created an rsample object called home_split by passing the home_sales tibble into initial_split(). The home_split object contains the instructions for randomly splitting home_sales into training and test sets.\nThe home_sales tibble, and home_split object have been loaded into this session.\n\n# Define a linear regression model\nlinear_model <- linear_reg() %>% \n  set_engine('lm') %>% \n  set_mode('regression')\n\n# Train linear_model with last_fit()\nlinear_fit <- linear_model %>% \n  last_fit(selling_price ~ ., split = home_split)\n\n# Collect predictions and view results\npredictions_df <- linear_fit %>% collect_predictions()\npredictions_df %>% head()\n\n# A tibble: 6 × 5\n  id                 .pred  .row selling_price .config             \n  <chr>              <dbl> <int>         <dbl> <chr>               \n1 train/test split 398796.     3        411000 Preprocessor1_Model1\n2 train/test split 693533.     4        635000 Preprocessor1_Model1\n3 train/test split 439551.     9        425000 Preprocessor1_Model1\n4 train/test split 505907.    13        525000 Preprocessor1_Model1\n5 train/test split 392617.    18        381000 Preprocessor1_Model1\n6 train/test split 513498.    20        540000 Preprocessor1_Model1\n\n# Make an R squared plot using predictions_df\nggplot(predictions_df, aes(x = selling_price, y = .pred)) + \n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred() +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#data-resampling",
    "href": "datacamp/tidymodels/tidymodels.html#data-resampling",
    "title": "Modeling with tidymodels in R",
    "section": "Data resampling",
    "text": "Data resampling\nThe first step in a machine learning project is to create training and test datasets for model fitting and evaluation. The test dataset provides an estimate of how your model will perform on new data and helps to guard against overfitting.\nYou will be working with the telecom_df dataset which contains information on customers of a telecommunications company. The outcome variable is canceled_service and it records whether a customer canceled their contract with the company. The predictor variables contain information about customers’ cell phone and internet usage as well as their contract type and monthly charges.\nThe telecom_df tibble has been loaded into your session.\n\ntelecom_df <- readRDS(\"telecom_df.rds\")\n# Create data split object\ntelecom_split <- initial_split(telecom_df, prop = 0.75,\n                     strata = canceled_service)\n\n# Create the training data\ntelecom_training <- telecom_split %>% \n  training()\n\n# Create the test data\ntelecom_test <- telecom_split %>% \n  testing()\n\n# Check the number of rows\nnrow(telecom_training)\n\n[1] 731\n\nnrow(telecom_test)\n\n[1] 244"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#fitting-a-logistic-regression-model",
    "href": "datacamp/tidymodels/tidymodels.html#fitting-a-logistic-regression-model",
    "title": "Modeling with tidymodels in R",
    "section": "Fitting a logistic regression model",
    "text": "Fitting a logistic regression model\nIn addition to regression models, the parsnip package also provides a general interface to classification models in R.\nIn this exercise, you will define a parsnip logistic regression object and train your model to predict canceled_service using avg_call_mins, avg_intl_mins, and monthly_charges as predictor variables from the telecom_df data.\nThe telecom_training and telecom_test tibbles that you created in the previous lesson have been loaded into this session.\n\n# Specify a logistic regression model\nlogistic_model <- logistic_reg() %>% \n  # Set the engine\n  set_engine('glm') %>% \n  # Set the mode\n  set_mode('classification')\n\n# Fit to training data\nlogistic_fit <- logistic_model %>% \n  fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n      data = telecom_training)\n\n# Print model fit object\nlogistic_fit %>% tidy()\n\n# A tibble: 4 × 5\n  term            estimate std.error statistic  p.value\n  <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)      1.87      0.581       3.22  1.28e- 3\n2 avg_call_mins   -0.0104    0.00130    -8.05  8.42e-16\n3 avg_intl_mins    0.0223    0.00311     7.18  6.94e-13\n4 monthly_charges  0.00231   0.00472     0.488 6.25e- 1"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#combining-test-dataset-results",
    "href": "datacamp/tidymodels/tidymodels.html#combining-test-dataset-results",
    "title": "Modeling with tidymodels in R",
    "section": "Combining test dataset results",
    "text": "Combining test dataset results\nEvaluating your model’s performance on the test dataset gives insights into how well your model predicts on new data sources. These insights will help you communicate your model’s value in solving problems or improving decision making.\nBefore you can calculate classification metrics such as sensitivity or specificity, you must create a results tibble with the required columns for yardstick metric functions.\nIn this exercise, you will use your trained model to predict the outcome variable in the telecom_test dataset and combine it with the true outcome values in the canceled_service column.\nYour trained model, logistic_fit, and test dataset, telecom_test, have been loaded from the previous exercise.\n\n# Predict outcome categories\nclass_preds <- predict(logistic_fit, new_data = telecom_test,\n                       type = 'class')\n\n# Obtain estimated probabilities for each outcome value\nprob_preds <- predict(logistic_fit, new_data = telecom_test, \n                      type = 'prob')\n\n# Combine test set results\ntelecom_results <- telecom_test %>% \n  select(canceled_service) %>% \n  bind_cols(class_preds, prob_preds)\n\n# View results tibble\ntelecom_results %>%\n    head()\n\n# A tibble: 6 × 4\n  canceled_service .pred_class .pred_yes .pred_no\n  <fct>            <fct>           <dbl>    <dbl>\n1 yes              no              0.366    0.634\n2 yes              no              0.143    0.857\n3 no               no              0.225    0.775\n4 yes              yes             0.556    0.444\n5 no               no              0.383    0.617\n6 yes              no              0.363    0.637"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#evaluating-performance-with-yardstick",
    "href": "datacamp/tidymodels/tidymodels.html#evaluating-performance-with-yardstick",
    "title": "Modeling with tidymodels in R",
    "section": "Evaluating performance with yardstick",
    "text": "Evaluating performance with yardstick\nIn the previous exercise, you calculated classification metrics from a sample confusion matrix. The yardstick package was designed to automate this process.\nFor classification models, yardstick functions require a tibble of model results as the first argument. This should include the actual outcome values, predicted outcome values, and estimated probabilities for each value of the outcome variable.\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate performance metrics.\nThe telecom_results tibble has been loaded into your session.\n\n# Calculate the confusion matrix\nconf_mat(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n          Truth\nPrediction yes  no\n       yes  32  22\n       no   50 140\n\n# Calculate the accuracy\naccuracy(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.705\n\n# Calculate the sensitivity\n\nsens(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.390\n\n# Calculate the specificity\n\nspec(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 spec    binary         0.864"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#creating-custom-metric-sets",
    "href": "datacamp/tidymodels/tidymodels.html#creating-custom-metric-sets",
    "title": "Modeling with tidymodels in R",
    "section": "Creating custom metric sets",
    "text": "Creating custom metric sets\nThe yardstick package also provides the ability to create custom sets of model metrics. In cases where the cost of obtaining false negative errors is different from the cost of false positive errors, it may be important to examine a specific set of performance metrics.\nInstead of calculating accuracy, sensitivity, and specificity separately, you can create your own metric function that calculates all three at the same time.\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate a custom set of performance metrics. You will also use a confusion matrix to calculate all available binary classification metrics in tidymodelsall at once.\nThe telecom_results tibble has been loaded into your session.\n\n# Create a custom metric function\ntelecom_metrics <- metric_set(accuracy, sens, spec)\n\n# Calculate metrics using model results tibble\ntelecom_metrics(telecom_results, \n                truth = canceled_service,\n                estimate = .pred_class)\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.705\n2 sens     binary         0.390\n3 spec     binary         0.864\n\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class) %>% \n  # Pass to the summary() function\n  summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary         0.705\n 2 kap                  binary         0.278\n 3 sens                 binary         0.390\n 4 spec                 binary         0.864\n 5 ppv                  binary         0.593\n 6 npv                  binary         0.737\n 7 mcc                  binary         0.290\n 8 j_index              binary         0.254\n 9 bal_accuracy         binary         0.627\n10 detection_prevalence binary         0.221\n11 precision            binary         0.593\n12 recall               binary         0.390\n13 f_meas               binary         0.471"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#plotting-the-confusion-matrix",
    "href": "datacamp/tidymodels/tidymodels.html#plotting-the-confusion-matrix",
    "title": "Modeling with tidymodels in R",
    "section": "Plotting the confusion matrix",
    "text": "Plotting the confusion matrix\nCalculating performance metrics with the yardstick package provides insight into how well a classification model is performing on the test dataset. Most yardstick functions return a single number that summarizes classification performance.\nMany times, it is helpful to create visualizations of the confusion matrix to more easily communicate your results.\nIn this exercise, you will make a heat map and mosaic plot of the confusion matrix from your logistic regression model on the telecom_df dataset.\nYour model results tibble, telecom_results, has been loaded into your session.\n\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a heat map\n  autoplot(type = \"heatmap\")\n\n\n\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a mosaic plot\n  autoplot(type = \"mosaic\")"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#roc-curves-and-area-under-the-roc-curve",
    "href": "datacamp/tidymodels/tidymodels.html#roc-curves-and-area-under-the-roc-curve",
    "title": "Modeling with tidymodels in R",
    "section": "ROC curves and area under the ROC curve",
    "text": "ROC curves and area under the ROC curve\nROC curves are used to visualize the performance of a classification model across a range of probability thresholds. An ROC curve with the majority of points near the upper left corner of the plot indicates that a classification model is able to correctly predict both the positive and negative outcomes correctly across a wide range of probability thresholds.\nThe area under this curve provides a letter grade summary of model performance.\nIn this exercise, you will create an ROC curve from your logistic regression model results and calculate the area under the ROC curve with yardstick.\nYour model results tibble, telecom_results has been loaded into your session.\n\n# Calculate metrics across thresholds\nthreshold_df <- telecom_results %>% \n  roc_curve(truth = canceled_service, .pred_yes)\n\n# View results\nthreshold_df %>%\n  head()\n\n# A tibble: 6 × 3\n  .threshold specificity sensitivity\n       <dbl>       <dbl>       <dbl>\n1  -Inf          0                 1\n2     0.0175     0                 1\n3     0.0398     0.00617           1\n4     0.0415     0.0123            1\n5     0.0485     0.0185            1\n6     0.0525     0.0247            1\n\n# Plot ROC curve\nthreshold_df %>% \n  autoplot()\n\n\n\n# Calculate ROC AUC\nroc_auc(telecom_results, truth = canceled_service, .pred_yes)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.747\n\n\nStreamlining the modeling process The last_fit() function is designed to streamline the modeling workflow in tidymodels. Instead of training your model on the training data and building a results tibble using the test data, last_fit() accomplishes this with one function.\nIn this exercise, you will train the same logistic regression model as you fit in the previous exercises, except with the last_fit() function.\nYour data split object, telecom_split, and model specification, logistic_model, have been loaded into your session.\n\n# Train model with last_fit()\ntelecom_last_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n           split = telecom_split)\n\n# View test set metrics\ntelecom_last_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.705 Preprocessor1_Model1\n2 roc_auc  binary         0.747 Preprocessor1_Model1"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#collecting-predictions-and-creating-custom-metrics",
    "href": "datacamp/tidymodels/tidymodels.html#collecting-predictions-and-creating-custom-metrics",
    "title": "Modeling with tidymodels in R",
    "section": "Collecting predictions and creating custom metrics",
    "text": "Collecting predictions and creating custom metrics\nUsing the last_fit() modeling workflow also saves time in collecting model predictions. Instead of manually creating a tibble of model results, there are helper functions that extract this information automatically.\nIn this exercise, you will use your trained model, telecom_last_fit, to create a tibble of model results on the test dataset as well as calculate custom performance metrics.\nYou trained model, telecom_last_fit, has been loaded into this session.\n\n# Collect predictions\nlast_fit_results <- telecom_last_fit %>% \n  collect_predictions()\n\n# View results\nlast_fit_results %>%\n  head()\n\n# A tibble: 6 × 7\n  id               .pred_yes .pred_no  .row .pred_class canceled_service .config\n  <chr>                <dbl>    <dbl> <int> <fct>       <fct>            <chr>  \n1 train/test split     0.366    0.634     2 no          yes              Prepro…\n2 train/test split     0.143    0.857     4 no          yes              Prepro…\n3 train/test split     0.225    0.775     6 no          no               Prepro…\n4 train/test split     0.556    0.444     9 yes         yes              Prepro…\n5 train/test split     0.383    0.617    13 no          no               Prepro…\n6 train/test split     0.363    0.637    17 no          yes              Prepro…\n\n# Custom metrics function\nlast_fit_metrics <- metric_set(accuracy, sens,\n                               spec, roc_auc)\n\n# Calculate metrics\nlast_fit_metrics(last_fit_results,\n                 truth = canceled_service,\n                 estimate = .pred_class,\n                 .pred_yes)\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.705\n2 sens     binary         0.390\n3 spec     binary         0.864\n4 roc_auc  binary         0.747"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#complete-modeling-workflow",
    "href": "datacamp/tidymodels/tidymodels.html#complete-modeling-workflow",
    "title": "Modeling with tidymodels in R",
    "section": "Complete modeling workflow",
    "text": "Complete modeling workflow\nIn this exercise, you will use the last_fit() function to train a logistic regression model and evaluate its performance on the test data by assessing the ROC curve and the area under the ROC curve.\nSimilar to previous exercises, you will predict canceled_service in the telecom_df data, but with an additional predictor variable to see if you can improve model performance.\nThe telecom_df tibble, telecom_split, and logistic_model objects from the previous exercises have been loaded into your workspace. The telecom_split object contains the instructions for randomly splitting the telecom_df tibble into training and test sets. The logistic_model object is a parsnip specification of a logistic regression model.\n\n# Train a logistic regression model\nlogistic_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins + avg_intl_mins + monthly_charges + months_with_company, \n           split = telecom_split)\n\n# Collect metrics\nlogistic_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.787 Preprocessor1_Model1\n2 roc_auc  binary         0.845 Preprocessor1_Model1\n\n# Collect model predictions\nlogistic_fit %>% \n  collect_predictions() %>% \n  # Plot ROC curve\n  roc_curve(truth = canceled_service, .pred_yes) %>% \n  autoplot()"
  },
  {
    "objectID": "datacamp/us_census/us_census.html",
    "href": "datacamp/us_census/us_census.html",
    "title": "Census data in r with tidycensus",
    "section": "",
    "text": "tidycensus is an R package designed to return data from the US Census Bureau ready for use within the Tidyverse.\nTo acquire data from the US Census Bureau using the tidycensus R package, you must first acquire and set a Census API key. After obtaining your key, you can install it for future use with the census_api_key() function in tidycensus.\nThis exercise uses a fake API key for purposes of illustration.\n\n# Load the tidycensus package into your R session\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(tigris)\nlibrary(here)\n\n# Define your Census API key and set it with census_api_key()\napi_key <- Sys.getenv(\"CENSUS_API_KEY\")\n\ncensus_api_key(api_key)\n\n##Setting a cache directory Spatial data from the US Census Bureau can get very big - sometimes hundreds of megabytes in size. By default, tigris functions download data from the US Census Bureau’s website - but this can get tiresome if downloading the same large datasets over and over. To resolve this, tigris includes an option to cache downloaded data on a user’s computer for future use, meaning that files only have to be downloaded from the Census website once. In this exercise, you’ll get acquainted with the caching functionality in tigris.\n\n# Set the cache directory\ntigris_cache_dir(here(\"tigris_cache\"))\n\n# Set the tigris_use_cache option\noptions(tigris_use_cache = TRUE)\n\n# Check to see that you've modified the option correctly\ngetOption(\"tigris_use_cache\")\n\n[1] TRUE"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#getting-census-data-with-tidycensus",
    "href": "datacamp/us_census/us_census.html#getting-census-data-with-tidycensus",
    "title": "Census data in r with tidycensus",
    "section": "Getting Census data with tidycensus",
    "text": "Getting Census data with tidycensus\nIn this exercise, you will load and inspect data from the 2010 US Census and 2012-2016 American Community Survey. The core functions of get_decennial() and get_acs() in tidycensus are used to obtain data from these sources; the 2010 Census and 2012-2016 ACS are the defaults for these functions, respectively.\nBy inspecting the data, you’ll get a sense of differences between decennial US Census data and data from the ACS, which is based on a sample and subject to a margin of error. Whereas get_decennial() returns a data value for each row, get_acs() returns estimate and moe columns representing the ACS estimate and margin of error.\n\n# Obtain and view state populations from the 2010 US Census\nstate_pop <- get_decennial(geography = \"state\", \n                           variables = \"P001001\")\n\nhead(state_pop)\n\n# A tibble: 6 × 4\n  GEOID NAME       variable    value\n  <chr> <chr>      <chr>       <dbl>\n1 01    Alabama    P001001   4779736\n2 02    Alaska     P001001    710231\n3 04    Arizona    P001001   6392017\n4 05    Arkansas   P001001   2915918\n5 06    California P001001  37253956\n6 22    Louisiana  P001001   4533372\n\n# Obtain and view state median household income from the 2012-2016 American Community Survey\nstate_income <- get_acs(geography = \"state\", \n                        variables = \"B19013_001\")\n\nhead(state_income)\n\n# A tibble: 6 × 5\n  GEOID NAME       variable   estimate   moe\n  <chr> <chr>      <chr>         <dbl> <dbl>\n1 01    Alabama    B19013_001    54943   377\n2 02    Alaska     B19013_001    80287  1113\n3 04    Arizona    B19013_001    65913   387\n4 05    Arkansas   B19013_001    52123   458\n5 06    California B19013_001    84097   236\n6 08    Colorado   B19013_001    80184   450"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#understanding-tidycensus-options",
    "href": "datacamp/us_census/us_census.html#understanding-tidycensus-options",
    "title": "Census data in r with tidycensus",
    "section": "Understanding tidycensus options",
    "text": "Understanding tidycensus options\nAs discussed in this lesson, Census data comprise thousands of variables available across dozens of geographies! Most of these geography-variable combinations are accessible with tidycensus; however, it helps to understand the package options.\nSome data, like Census tracts, are only available by state, and users might want to subset by county; tidycensus facilitates this with state and county parameters when appropriate. Additionally, tidycensus includes the Census variable ID in the variable column; however, a user might want to supply her own variable name, which can be accomplished with a named vector.\nYou’ll be using the Census variable B19013_001 here, which refers to median household income.\n\n# Get an ACS dataset for Census tracts in Texas by setting the state\ntx_income <- get_acs(geography = \"tract\",\n                     variables = \"B19013_001\",\n                     state = \"TX\")\n\n# Inspect the dataset\nhead(tx_income)\n\n# A tibble: 6 × 5\n  GEOID       NAME                                         varia…¹ estim…²   moe\n  <chr>       <chr>                                        <chr>     <dbl> <dbl>\n1 48001950100 Census Tract 9501, Anderson County, Texas    B19013…   61325  9171\n2 48001950401 Census Tract 9504.01, Anderson County, Texas B19013…   92813 45136\n3 48001950402 Census Tract 9504.02, Anderson County, Texas B19013…      NA    NA\n4 48001950500 Census Tract 9505, Anderson County, Texas    B19013…   41713  6650\n5 48001950600 Census Tract 9506, Anderson County, Texas    B19013…   32552 12274\n6 48001950700 Census Tract 9507, Anderson County, Texas    B19013…   35811  5573\n# … with abbreviated variable names ¹​variable, ²​estimate\n\n# Get an ACS dataset for Census tracts in Travis County, TX\ntravis_income <- get_acs(geography = \"tract\",\n                         variables = \"B19013_001\", \n                         state = \"TX\",\n                         county = \"Travis\")\n\n# Inspect the dataset\nhead(travis_income)\n\n# A tibble: 6 × 5\n  GEOID       NAME                                    variable   estimate   moe\n  <chr>       <chr>                                   <chr>         <dbl> <dbl>\n1 48453000101 Census Tract 1.01, Travis County, Texas B19013_001   121964 31935\n2 48453000102 Census Tract 1.02, Travis County, Texas B19013_001   201417 26672\n3 48453000203 Census Tract 2.03, Travis County, Texas B19013_001    81994 14344\n4 48453000204 Census Tract 2.04, Travis County, Texas B19013_001    93219 26118\n5 48453000205 Census Tract 2.05, Travis County, Texas B19013_001    75000 24198\n6 48453000206 Census Tract 2.06, Travis County, Texas B19013_001    88342 10549\n\n# Supply custom variable names\ntravis_income2 <- get_acs(geography = \"tract\", \n                          variables = c(hhincome = \"B19013_001\"), \n                          state = \"TX\",\n                          county = \"Travis\")\n\n# Inspect the dataset\nhead(travis_income2)\n\n# A tibble: 6 × 5\n  GEOID       NAME                                    variable estimate   moe\n  <chr>       <chr>                                   <chr>       <dbl> <dbl>\n1 48453000101 Census Tract 1.01, Travis County, Texas hhincome   121964 31935\n2 48453000102 Census Tract 1.02, Travis County, Texas hhincome   201417 26672\n3 48453000203 Census Tract 2.03, Travis County, Texas hhincome    81994 14344\n4 48453000204 Census Tract 2.04, Travis County, Texas hhincome    93219 26118\n5 48453000205 Census Tract 2.05, Travis County, Texas hhincome    75000 24198\n6 48453000206 Census Tract 2.06, Travis County, Texas hhincome    88342 10549"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#tidy-and-wide-data-in-tidycensus",
    "href": "datacamp/us_census/us_census.html#tidy-and-wide-data-in-tidycensus",
    "title": "Census data in r with tidycensus",
    "section": "Tidy and wide data in tidycensus",
    "text": "Tidy and wide data in tidycensus\nBy default, tidycensus functions return tidy data frames, in which each row represents a unique unit-variable combination. However, at times it is useful to have each Census variable in its own column for some methods of visualization and analysis. To accomplish this, you can set output = “wide” in your calls to get_acs() or get_decennial(), which will place estimates/values and margins of error in their own columns.\n\n# Return county data in wide format\nor_wide <- get_acs(geography = \"county\", \n                     state = \"OR\",\n                     variables = c(hhincome = \"B19013_001\", \n                            medage = \"B01002_001\"), \n                     output = \"wide\")\n\n# Compare output to the tidy format from previous exercises\nhead(or_wide)\n\n# A tibble: 6 × 6\n  GEOID NAME                     hhincomeE hhincomeM medageE medageM\n  <chr> <chr>                        <dbl>     <dbl>   <dbl>   <dbl>\n1 41001 Baker County, Oregon         46922      3271    47.7     0.9\n2 41003 Benton County, Oregon        68732      2689    33.3     0.3\n3 41005 Clackamas County, Oregon     88517      1424    41.6     0.2\n4 41007 Clatsop County, Oregon       61846      2651    44.5     0.4\n5 41009 Columbia County, Oregon      73909      3517    43.3     0.4\n6 41011 Coos County, Oregon          52548      3145    48.4     0.3\n\n# Create a scatterplot\nplot(or_wide$hhincomeE, or_wide$medageE)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#loading-variables-in-tidycensus",
    "href": "datacamp/us_census/us_census.html#loading-variables-in-tidycensus",
    "title": "Census data in r with tidycensus",
    "section": "Loading variables in tidycensus",
    "text": "Loading variables in tidycensus\nThere are hundreds of thousands of variables in the decennial Census and American Community Survey samples, which can make it difficult to know which variable codes to use! tidycensus aims to make this easier with the load_variables() function, which obtains a dataset of variables from a specified sample and loads it into R as a browsable data frame.\n\n# Load variables from the 2012-2016 ACS\nv16 <- load_variables(year = 2016,\n           dataset = \"acs5\",\n           cache = TRUE)\n\n# Get variables from the ACS Data Profile\nv16p <- load_variables(year = 2016,\n                       dataset = \"acs5/profile\",\n                       cache = TRUE)\n\n# Set year and dataset to get variables from the 2000 Census SF3\nv00 <- load_variables(year = 2000,\n                      dataset = \"sf3\",\n                      cache = TRUE)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#exploring-variables-with-tidyverse-tools",
    "href": "datacamp/us_census/us_census.html#exploring-variables-with-tidyverse-tools",
    "title": "Census data in r with tidycensus",
    "section": "Exploring variables with tidyverse tools",
    "text": "Exploring variables with tidyverse tools\nOnce loaded, your dataset of Census or ACS variables might contain thousands of rows. In RStudio, it is recommended to use the View() function to interactively search for these variables. Outside of RStudio, these datasets can be browsed using tidyverse filtering tools.\n\n# Filter for table B19001\nfilter(v16, str_detect(name, \"B19001\"))\n\n# A tibble: 170 × 4\n   name        label                               concept               geogr…¹\n   <chr>       <chr>                               <chr>                 <chr>  \n 1 B19001A_001 Estimate!!Total                     HOUSEHOLD INCOME IN … tract  \n 2 B19001A_002 Estimate!!Total!!Less than $10,000  HOUSEHOLD INCOME IN … tract  \n 3 B19001A_003 Estimate!!Total!!$10,000 to $14,999 HOUSEHOLD INCOME IN … tract  \n 4 B19001A_004 Estimate!!Total!!$15,000 to $19,999 HOUSEHOLD INCOME IN … tract  \n 5 B19001A_005 Estimate!!Total!!$20,000 to $24,999 HOUSEHOLD INCOME IN … tract  \n 6 B19001A_006 Estimate!!Total!!$25,000 to $29,999 HOUSEHOLD INCOME IN … tract  \n 7 B19001A_007 Estimate!!Total!!$30,000 to $34,999 HOUSEHOLD INCOME IN … tract  \n 8 B19001A_008 Estimate!!Total!!$35,000 to $39,999 HOUSEHOLD INCOME IN … tract  \n 9 B19001A_009 Estimate!!Total!!$40,000 to $44,999 HOUSEHOLD INCOME IN … tract  \n10 B19001A_010 Estimate!!Total!!$45,000 to $49,999 HOUSEHOLD INCOME IN … tract  \n# … with 160 more rows, and abbreviated variable name ¹​geography\n\n# Use public transportation to search for related variables\nfilter(v16p, str_detect(label, fixed(\"public transportation\", \n                                ignore_case = TRUE)))\n\n# A tibble: 2 × 3\n  name       label                                                       concept\n  <chr>      <chr>                                                       <chr>  \n1 DP03_0021  Estimate!!COMMUTING TO WORK!!Workers 16 years and over!!Pu… SELECT…\n2 DP03_0021P Percent!!COMMUTING TO WORK!!Workers 16 years and over!!Pub… SELECT…"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#comparing-geographies-with-ggplot2-visualizations",
    "href": "datacamp/us_census/us_census.html#comparing-geographies-with-ggplot2-visualizations",
    "title": "Census data in r with tidycensus",
    "section": "Comparing geographies with ggplot2 visualizations",
    "text": "Comparing geographies with ggplot2 visualizations\nWhen exploring Census or ACS data, you’ll often want to know how data varies among different geographic units. For example - which US states have higher - or lower - median household incomes? This can be accomplished through visualization using dot plots, which are particularly effective for showing ranks visually. In this exercise, you’ll use the popular ggplot2 data visualization package to accomplish this.\n\n# Access the 1-year ACS  with the survey parameter\nne_income <- get_acs(geography = \"state\",\n                     variables = \"B19013_001\", \n                     survey = \"acs1\", \n                     state = c(\"ME\", \"NH\", \"VT\", \"MA\", \n                               \"RI\", \"CT\", \"NY\"))\n\n# Create a dot plot\n\n  \n# Reorder the states in descending order of estimates\nggplot(ne_income, aes(x = estimate, y = reorder(NAME, estimate))) + \n  geom_point()"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#customizing-ggplot2-visualizations-of-acs-data",
    "href": "datacamp/us_census/us_census.html#customizing-ggplot2-visualizations-of-acs-data",
    "title": "Census data in r with tidycensus",
    "section": "Customizing ggplot2 visualizations of ACS data",
    "text": "Customizing ggplot2 visualizations of ACS data\nWhile the ggplot2 defaults are excellent for exploratory visualization of data, you’ll likely want to customize your charts before sharing them with others. In this exercise, you’ll customize your tidycensus dot plot by modifying the chart colors, tick labels, and axis labels. You’ll also learn how to format labels using the scales package, as label formatters can be imported using the :: syntax.\n\n# Set dot color and size\ng_color <- ggplot(ne_income, aes(x = estimate, y = reorder(NAME, estimate))) + \n  geom_point(color = \"navy\", size = 4)\n\n# Format the x-axis labels\ng_scale <- g_color + \n  scale_x_continuous(labels = scales::dollar) + \n  theme_minimal(base_size = 12) \n\n# Label your x-axis, y-axis, and title your chart\ng_label <- g_scale + \n  labs(x =\"2016 ACS estimate\", \n       y = \"\", \n       title = \"Median household income by state\")\n  \ng_label"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#download-and-view-a-table-of-data-from-the-acs",
    "href": "datacamp/us_census/us_census.html#download-and-view-a-table-of-data-from-the-acs",
    "title": "Census data in r with tidycensus",
    "section": "Download and view a table of data from the ACS",
    "text": "Download and view a table of data from the ACS\nVariables in the decennial Census and American Community Survey are organized into tables, within which they share a common prefix. Commonly, analysts will want to work with all variables in a given table, as these variables might represent different aspects of a common characteristic (such as race or income levels). To request data for an entire table in tidycensus, users can specify a table argument with the table prefix, and optionally cache a dataset of table codes to speed up table searching in future requests. In this exercise, you’ll acquire a table of variables representing different income bands, then filter out the denominator rows.\n\n# Download table \"B19001\"\nwa_income <- get_acs(geography = \"county\", \n                 state = \"WA\", \n                 table = \"B19001\")\n\n# Check out the first few rows of wa_income\nhead(wa_income)\n\n# A tibble: 6 × 5\n  GEOID NAME                     variable   estimate   moe\n  <chr> <chr>                    <chr>         <dbl> <dbl>\n1 53001 Adams County, Washington B19001_001     6158   123\n2 53001 Adams County, Washington B19001_002      474   171\n3 53001 Adams County, Washington B19001_003      255   107\n4 53001 Adams County, Washington B19001_004      204    90\n5 53001 Adams County, Washington B19001_005      393   134\n6 53001 Adams County, Washington B19001_006      358   159"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#get-a-summary-variable-and-calculate-percentages",
    "href": "datacamp/us_census/us_census.html#get-a-summary-variable-and-calculate-percentages",
    "title": "Census data in r with tidycensus",
    "section": "Get a summary variable and calculate percentages",
    "text": "Get a summary variable and calculate percentages\nMany variables in the Census and American Community Survey are represented as counts or estimated counts. While count data is useful for some applications, it is often good practice to normalize count data by its denominator to convert it to a proportion or percentage to make clearer comparisons. This is facilitated in tidycensus with the summary_var argument, which allows users to request that a variable is given its own column in a tidy Census dataset. This value can then be used as the denominator for subsequent calculations of percentages.\nSummary question: When the summary_var parameter is requested in get_acs(), what information is returned by the function?\n\n# Assign Census variables vector to race_vars \nrace_vars <- c(White = \"B03002_003\", Black = \"B03002_004\", Native = \"B03002_005\", \n               Asian = \"B03002_006\", HIPI = \"B03002_007\", Hispanic = \"B03002_012\")\n\n# Request a summary variable from the ACS\nca_race <- get_acs(geography = \"county\", \n                   state = \"CA\",\n                   variables = race_vars, \n                   summary_var = \"B03002_001\")\n\n# Calculate a new percentage column and check the result\nca_race_pct <- ca_race %>%\n  mutate(pct = 100 * (estimate / summary_est))\n\nhead(ca_race_pct)\n\n# A tibble: 6 × 8\n  GEOID NAME                       variable estim…¹   moe summa…² summa…³    pct\n  <chr> <chr>                      <chr>      <dbl> <dbl>   <dbl>   <dbl>  <dbl>\n1 06001 Alameda County, California White     499730   988 1673133      NA 29.9  \n2 06001 Alameda County, California Black     166017  1837 1673133      NA  9.92 \n3 06001 Alameda County, California Native      5248   318 1673133      NA  0.314\n4 06001 Alameda County, California Asian     524980  2437 1673133      NA 31.4  \n5 06001 Alameda County, California HIPI       12699   566 1673133      NA  0.759\n6 06001 Alameda County, California Hispanic  374542    NA 1673133      NA 22.4  \n# … with abbreviated variable names ¹​estimate, ²​summary_est, ³​summary_moe"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#finding-the-largest-group-by-county",
    "href": "datacamp/us_census/us_census.html#finding-the-largest-group-by-county",
    "title": "Census data in r with tidycensus",
    "section": "Finding the largest group by county",
    "text": "Finding the largest group by county\ntidyverse data wrangling tools in packages like dplyr and purrr are extremely powerful for exploring Census data. tidycensus is specifically designed with data exploration within the tidyverse in mind. For example, users might be interested in finding out the largest racial/ethnic group within each county for a given state. This can be accomplished using dplyr grouping capabilities, which allow users to identify the largest ACS group estimate and filter to retain the rows that match that group.\n\n# Group the dataset and filter the estimate\nca_largest <- ca_race %>%\n  group_by(GEOID) %>%\n  filter(estimate == max(estimate)) \n\nhead(ca_largest)\n\n# A tibble: 6 × 7\n# Groups:   GEOID [6]\n  GEOID NAME                         variable estimate   moe summary_est summa…¹\n  <chr> <chr>                        <chr>       <dbl> <dbl>       <dbl>   <dbl>\n1 06001 Alameda County, California   Asian      524980  2437     1673133      NA\n2 06003 Alpine County, California    White         730   153        1344     228\n3 06005 Amador County, California    White       30081   412       40095      NA\n4 06007 Butte County, California     White      153153   300      217884      NA\n5 06009 Calaveras County, California White       35925   129       45349      NA\n6 06011 Colusa County, California    Hispanic    13177    NA       21780      NA\n# … with abbreviated variable name ¹​summary_moe\n\n# Group the dataset and get a breakdown of the results\nca_largest %>% \n  group_by(variable) %>%\n  tally()\n\n# A tibble: 3 × 2\n  variable     n\n  <chr>    <int>\n1 Asian        2\n2 Hispanic    16\n3 White       40"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#recoding-variables-and-calculating-group-sums",
    "href": "datacamp/us_census/us_census.html#recoding-variables-and-calculating-group-sums",
    "title": "Census data in r with tidycensus",
    "section": "Recoding variables and calculating group sums",
    "text": "Recoding variables and calculating group sums\ndplyr, one of the core packages within the tidyverse, includes numerous functions for data wrangling. This functionality allows users to recode datasets, define groups within those datasets, and perform calculations over those groups. Such operations commonly take place within a pipe, denoted with the %>% operator.\nIn this exercise, you’ll work with ACS data in just such a tidyverse workflow. You’ll be identifying median household income variables in ACS table B19001 that are below $35,000; between $35,000 and $75,000; and above $75,000. You’ll then tabulate the number of households that fall into each group for counties in Washington.\n\n# Use a tidy workflow to wrangle ACS data\nwa_grouped <- wa_income %>%\n  filter(variable != \"B19001_001\") %>%\n  mutate(incgroup = case_when(\n    variable < \"B19001_008\" ~ \"below35k\", \n    variable < \"B19001_013\" ~ \"35kto75k\", \n    TRUE ~ \"above75k\"\n  )) %>%\n  group_by(NAME, incgroup) %>%\n  summarize(group_est = sum(estimate))\n\nwa_grouped\n\n# A tibble: 117 × 3\n# Groups:   NAME [39]\n   NAME                      incgroup group_est\n   <chr>                     <chr>        <dbl>\n 1 Adams County, Washington  35kto75k      2156\n 2 Adams County, Washington  above75k      2094\n 3 Adams County, Washington  below35k      1908\n 4 Asotin County, Washington 35kto75k      3215\n 5 Asotin County, Washington above75k      3537\n 6 Asotin County, Washington below35k      2535\n 7 Benton County, Washington 35kto75k     21285\n 8 Benton County, Washington above75k     37911\n 9 Benton County, Washington below35k     15094\n10 Chelan County, Washington 35kto75k      9163\n# … with 107 more rows"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#comparing-acs-estimates-for-multiple-years",
    "href": "datacamp/us_census/us_census.html#comparing-acs-estimates-for-multiple-years",
    "title": "Census data in r with tidycensus",
    "section": "Comparing ACS estimates for multiple years",
    "text": "Comparing ACS estimates for multiple years\nThe American Community Survey is updated every year, which allows researchers to use ACS datasets to study demographic changes over time.\nIn this exercise, you’ll learn how to use the tidyverse function map_df() to work with multi-year ACS data. map_df() helps analysts iterate through a sequence of values, compute a process for each of those values, then combine the results into a single data frame. You’ll be using map_df() in this way with ACS data, as you iterate through a vector of years, retrieve ACS data for each year, and combine the results. This will allow you to view how ACS estimates have changed over time.\n\n# Map through ACS1 estimates to see how they change through the years\nmi_cities <- map_df(2012:2016, function(x) {\n  get_acs(geography = \"place\", \n          variables = c(totalpop = \"B01003_001\"), \n          state = \"MI\", \n          survey = \"acs1\", \n          year = x) %>%\n    mutate(year = x)\n})\n\nmi_cities %>% arrange(NAME, year)\n\n# A tibble: 80 × 6\n   GEOID   NAME                     variable estimate   moe  year\n   <chr>   <chr>                    <chr>       <dbl> <dbl> <int>\n 1 2603000 Ann Arbor city, Michigan totalpop   116128    35  2012\n 2 2603000 Ann Arbor city, Michigan totalpop   117034    43  2013\n 3 2603000 Ann Arbor city, Michigan totalpop   117759    44  2014\n 4 2603000 Ann Arbor city, Michigan totalpop   117070    33  2015\n 5 2603000 Ann Arbor city, Michigan totalpop   120777    33  2016\n 6 2621000 Dearborn city, Michigan  totalpop    96470    28  2012\n 7 2621000 Dearborn city, Michigan  totalpop    95888    35  2013\n 8 2621000 Dearborn city, Michigan  totalpop    95546    48  2014\n 9 2621000 Dearborn city, Michigan  totalpop    95180    40  2015\n10 2621000 Dearborn city, Michigan  totalpop    94430    52  2016\n# … with 70 more rows"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#inspecting-margins-of-error",
    "href": "datacamp/us_census/us_census.html#inspecting-margins-of-error",
    "title": "Census data in r with tidycensus",
    "section": "Inspecting margins of error",
    "text": "Inspecting margins of error\nACS data are distinct from decennial Census data in that they represent estimates with an associated margin of error. ACS margins of error by default represent a 90 percent confidence level around an estimate, which means that we are 90 percent sure that the true value falls within a range of the reported estimate plus or minus the reported margin of error.\nIn this exercise, you’ll get some experience working with data that has high margins of error relative to their estimates. We’ll use the example of poverty for the population aged 75 and above for Census tracts in Vermont.\n\n# Get data on elderly poverty by Census tract in Vermont\nvt_eldpov <- get_acs(geography = \"tract\", \n                     variables = c(eldpovm = \"B17001_016\", \n                                   eldpovf = \"B17001_030\"), \n                     state = \"VT\")\n\nvt_eldpov\n\n# A tibble: 386 × 5\n   GEOID       NAME                                       variable estim…¹   moe\n   <chr>       <chr>                                      <chr>      <dbl> <dbl>\n 1 50001960100 Census Tract 9601, Addison County, Vermont eldpovm        2     5\n 2 50001960100 Census Tract 9601, Addison County, Vermont eldpovf        8     7\n 3 50001960200 Census Tract 9602, Addison County, Vermont eldpovm        4     6\n 4 50001960200 Census Tract 9602, Addison County, Vermont eldpovf        0    10\n 5 50001960300 Census Tract 9603, Addison County, Vermont eldpovm        0    10\n 6 50001960300 Census Tract 9603, Addison County, Vermont eldpovf        7     9\n 7 50001960400 Census Tract 9604, Addison County, Vermont eldpovm        7    10\n 8 50001960400 Census Tract 9604, Addison County, Vermont eldpovf       15    11\n 9 50001960500 Census Tract 9605, Addison County, Vermont eldpovm        6    10\n10 50001960500 Census Tract 9605, Addison County, Vermont eldpovf       14    16\n# … with 376 more rows, and abbreviated variable name ¹​estimate\n\n# Identify rows with greater margins of error than their estimates\nmoe_check <- filter(vt_eldpov, moe > estimate)\n\n# Check proportion of rows where the margin of error exceeds the estimate\nnrow(moe_check) / nrow(vt_eldpov)\n\n[1] 0.7927461"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#using-margin-of-error-functions-in-tidycensus",
    "href": "datacamp/us_census/us_census.html#using-margin-of-error-functions-in-tidycensus",
    "title": "Census data in r with tidycensus",
    "section": "Using margin of error functions in tidycensus",
    "text": "Using margin of error functions in tidycensus\nWhile the Census Bureau API and tidycensus return pre-computed margins of error for you, you may want to derive new estimates from downloaded ACS data and in turn understand the margins of error around these derived estimates. tidycensus includes four functions (listed below) to help you with these tasks, each of which incorporates the recommended formulas from the US Census Bureau.\nmoe_sum() moe_product() moe_ratio() moe_prop()\n\n# Calculate a margin of error for a sum\nmoe_sum(moe = c(55, 33, 44, 12, 4))\n\n[1] 78.80355\n\n# Calculate a margin of error for a product\nmoe_product(est1 = 55,\n    est2 = 33,\n    moe1 = 12,\n    moe2 = 9)\n\n[1] 633.9093\n\n# Calculate a margin of error for a ratio\nmoe_ratio(num = 1000,\n    denom = 950,\n    moe_num = 200,\n    moe_denom = 177)\n\n[1] 0.287724\n\n# Calculate a margin of error for a proportion\nmoe_prop(num = 374,\n    denom = 1200,\n    moe_num = 122,\n    moe_denom = 333)\n\n[1] 0.05344178"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#calculating-group-wise-margins-of-error",
    "href": "datacamp/us_census/us_census.html#calculating-group-wise-margins-of-error",
    "title": "Census data in r with tidycensus",
    "section": "Calculating group-wise margins of error",
    "text": "Calculating group-wise margins of error\nOne way to reduce margins of error in an ACS analysis is to combine estimates when appropriate. This can be accomplished using tidyverse group-wise data analysis tools. In this exercise, you’ll combine estimates for male and female elderly poverty in Vermont, and use the moe_sum() function as part of this group-wise analysis. While you may lose some detail with this type of approach, your estimates will be more reliable relative to their margins of error than before you combined them.\n\n# Group the dataset and calculate a derived margin of error\nvt_eldpov2 <- vt_eldpov %>%\n  group_by(GEOID) %>%\n  summarize(\n    estmf = sum(estimate), \n    moemf = moe_sum(moe = moe, estimate = estimate)\n  )\n\n# Filter rows where newly-derived margin of error exceeds newly-derived estimate\nmoe_check2 <- filter(vt_eldpov2, moemf > estmf)\n\n# Check proportion of rows where margin of error exceeds estimate\nnrow(moe_check2) / nrow(vt_eldpov2)\n\n[1] 0.626943"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#quick-visual-exploration-of-acs-margins-of-error",
    "href": "datacamp/us_census/us_census.html#quick-visual-exploration-of-acs-margins-of-error",
    "title": "Census data in r with tidycensus",
    "section": "Quick visual exploration of ACS margins of error",
    "text": "Quick visual exploration of ACS margins of error\nIn Chapter 1, you learned how to create a dot plot of ACS income estimates. In this chapter, you’ve also learned about the importance of taking margins of error into account in ACS analyses. While margins of error are likely minimal for state-level estimates, they may be more significant for sub-state estimates, like counties. In this exercise, you’ll learn how to visualize margins of error around estimates with ggplot2.\n\n# Request median household income data\nmaine_inc <- get_acs(geography = \"county\", \n                     variables = c(hhincome = \"B19013_001\"), \n                     state = \"ME\") \n\n# Generate horizontal error bars with dots\nggplot(maine_inc, aes(x = estimate, y = NAME)) + \n  geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) + \n  geom_point()"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#customizing-a-ggplot2-margin-of-error-plot",
    "href": "datacamp/us_census/us_census.html#customizing-a-ggplot2-margin-of-error-plot",
    "title": "Census data in r with tidycensus",
    "section": "Customizing a ggplot2 margin of error plot",
    "text": "Customizing a ggplot2 margin of error plot\nYou’ve hopefully identified some problems with the chart you created in the previous exercise. As the counties are not ordered, patterns in the data are difficult for a viewer to parse. Specifically, margin of error plots are much more effective when dots are ordered as the ordering allows viewers to understand the uncertainty in estimate values relative to other estimates. Additionally, the lack of plot formatting makes it difficult for chart viewers to understand the chart’s content. In this exercise, you’ll clean up your ggplot2 code to create a much more visually appealing margin of error chart.\n\n# Remove unnecessary content from the county's name\nmaine_inc2 <- maine_inc %>%\n  mutate(NAME = str_replace(NAME, \" County, Maine\", \"\"))\n\n# Build a margin of error plot incorporating your modifications\nggplot(maine_inc2, aes(x = estimate, y = reorder(NAME, estimate))) + \n  geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) + \n  geom_point(size = 3, color = \"darkgreen\") + \n  theme_grey(base_size = 14) + \n  labs(title = \"Median household income\", \n       subtitle = \"Counties in Maine\", \n       x = \"ACS estimate (bars represent margins of error)\", \n       y = \"\") + \n  scale_x_continuous(labels = scales::dollar)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#getting-census-boundary-files-with-tigris",
    "href": "datacamp/us_census/us_census.html#getting-census-boundary-files-with-tigris",
    "title": "Census data in r with tidycensus",
    "section": "Getting Census boundary files with tigris",
    "text": "Getting Census boundary files with tigris\nThe US Census Bureau’s TIGER/Line shapefiles include boundary files for the geography at which decennial Census and ACS data are aggregated. These geographies include legal entities that have legal standing in the U.S., such as states and counties, and statistical entities used for data tabulation such as Census tracts and block groups. In this exercise, you’ll use the tigris package to acquire such boundary files for counties in Colorado and Census tracts for Colorado’s Denver County, which covers the city of Denver.\n\n# Get a counties dataset for Colorado and plot it\nco_counties <- counties(state = \"CO\")\nplot(co_counties)\n\n\n\n# Get a Census tracts dataset for Denver County, Colorado and plot it\ndenver_tracts <- tracts(state = \"CO\", county = \"Denver\")\nplot(denver_tracts)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#getting-geographic-features-with-tigris",
    "href": "datacamp/us_census/us_census.html#getting-geographic-features-with-tigris",
    "title": "Census data in r with tidycensus",
    "section": "Getting geographic features with tigris",
    "text": "Getting geographic features with tigris\nIn addition to enumeration units, the TIGER/Line database produced by the Census Bureau includes geographic features. These features consist of several datasets for use in thematic mapping and spatial analysis, such as transportation infrastructure and water features. In this exercise, you’ll acquire and plot roads and water data with tigris.\n\n# Plot area water features for Lane County, Oregon\nlane_water <- area_water(state = \"OR\", county = \"Lane\")\nplot(lane_water)\n\n\n\n# Plot primary & secondary roads for the state of New Hampshire\nnh_roads <- primary_secondary_roads(state = \"NH\")\nplot(nh_roads)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#understanding-the-structure-of-tigris-objects",
    "href": "datacamp/us_census/us_census.html#understanding-the-structure-of-tigris-objects",
    "title": "Census data in r with tidycensus",
    "section": "Understanding the structure of tigris objects",
    "text": "Understanding the structure of tigris objects\nBy default, tigris returns objects of class SpatialDataFrame from the sp package. Objects of class Spatial represent components of spatial data in different slots, which include descriptions of the object’s geometry, attributes, and coordinate system. In this exercise, we’ll briefly examine the structure of objects returned by tigris functions.\n\n# Check the class of the data\nclass(co_counties)\n\n[1] \"sf\"         \"data.frame\"\n\n# Take a look at the information in the data slot\nhead(co_counties)\n\nSimple feature collection with 6 features and 17 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -108.3811 ymin: 36.99961 xmax: -102.0448 ymax: 41.0026\nGeodetic CRS:  NAD83\n    STATEFP COUNTYFP COUNTYNS GEOID     NAME        NAMELSAD LSAD CLASSFP MTFCC\n23       08      109 00198170 08109 Saguache Saguache County   06      H1 G4020\n107      08      115 00198173 08115 Sedgwick Sedgwick County   06      H1 G4020\n124      08      017 00198124 08017 Cheyenne Cheyenne County   06      H1 G4020\n163      08      027 00198129 08027   Custer   Custer County   06      H1 G4020\n200      08      067 00198148 08067 La Plata La Plata County   06      H1 G4020\n228      08      111 00198171 08111 San Juan San Juan County   06      H1 G4020\n    CSAFP CBSAFP METDIVFP FUNCSTAT      ALAND   AWATER    INTPTLAT     INTPTLON\n23   <NA>   <NA>     <NA>        A 8206547699  4454510 +38.0316514 -106.2346662\n107  <NA>   <NA>     <NA>        A 1419419015  3530746 +40.8715679 -102.3553579\n124  <NA>   <NA>     <NA>        A 4605713960  8166129 +38.8356456 -102.6017914\n163  <NA>   <NA>     <NA>        A 1913031975  3364150 +38.1019955 -105.3735123\n200  <NA>  20420     <NA>        A 4376255277 25642579 +37.2873673 -107.8397178\n228  <NA>   <NA>     <NA>        A 1003660672  2035929 +37.7810492 -107.6702567\n                          geometry\n23  MULTIPOLYGON (((-105.8093 3...\n107 MULTIPOLYGON (((-102.2091 4...\n124 MULTIPOLYGON (((-102.547 38...\n163 MULTIPOLYGON (((-105.7969 3...\n200 MULTIPOLYGON (((-107.7124 3...\n228 MULTIPOLYGON (((-107.9751 3...\n\n# Check the coordinate system of the data"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#tigerline-and-cartographic-boundary-files",
    "href": "datacamp/us_census/us_census.html#tigerline-and-cartographic-boundary-files",
    "title": "Census data in r with tidycensus",
    "section": "TIGER/Line and cartographic boundary files",
    "text": "TIGER/Line and cartographic boundary files\nIn addition to its TIGER/Line shapefiles, the US Census Bureau releases cartographic boundary shapefiles for enumeration units. TIGER/Line shapefiles correspond to legal boundaries of units, which can include water area and in turn, may not be preferable for thematic mapping. The Census Bureau’s cartographic boundary shapefiles are clipped to the US shoreline and are generalized, which can make them superior for mapping projects. In this exercise, you’ll compare the TIGER/Line and cartographic boundary representations of the US state of Michigan.\n\n# Get a counties dataset for Michigan\nmi_tiger <- counties(\"MI\")\n\n# Get the equivalent cartographic boundary shapefile\nmi_cb <- counties(\"MI\", cb = TRUE)\n\n# Overlay the two on a plot to make a comparison\nplot(mi_tiger)\nplot(mi_cb, add = TRUE, border = \"red\")"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#getting-data-as-simple-features-objects",
    "href": "datacamp/us_census/us_census.html#getting-data-as-simple-features-objects",
    "title": "Census data in r with tidycensus",
    "section": "Getting data as simple features objects",
    "text": "Getting data as simple features objects\nThe sf package, which stands for simple features, promises to revolutionize the way that vector spatial data are handled within R. sf objects represent spatial data much like regular data frames, with a list-column that contains the geometry of the geographic dataset. tigris can return spatial data as simple features objects either by declaring class = “sf” within a function call or by setting as a global option. In this exercise, you’ll get acquainted with simple features in tigris.\n\n# Get data from tigris as simple features\noptions(tigris_class = \"sf\")\n\n# Get countries from Colorado and view the first few rows\ncolorado_sf <- counties(\"CO\")\nhead(colorado_sf)\n\nSimple feature collection with 6 features and 17 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -108.3811 ymin: 36.99961 xmax: -102.0448 ymax: 41.0026\nGeodetic CRS:  NAD83\n    STATEFP COUNTYFP COUNTYNS GEOID     NAME        NAMELSAD LSAD CLASSFP MTFCC\n23       08      109 00198170 08109 Saguache Saguache County   06      H1 G4020\n107      08      115 00198173 08115 Sedgwick Sedgwick County   06      H1 G4020\n124      08      017 00198124 08017 Cheyenne Cheyenne County   06      H1 G4020\n163      08      027 00198129 08027   Custer   Custer County   06      H1 G4020\n200      08      067 00198148 08067 La Plata La Plata County   06      H1 G4020\n228      08      111 00198171 08111 San Juan San Juan County   06      H1 G4020\n    CSAFP CBSAFP METDIVFP FUNCSTAT      ALAND   AWATER    INTPTLAT     INTPTLON\n23   <NA>   <NA>     <NA>        A 8206547699  4454510 +38.0316514 -106.2346662\n107  <NA>   <NA>     <NA>        A 1419419015  3530746 +40.8715679 -102.3553579\n124  <NA>   <NA>     <NA>        A 4605713960  8166129 +38.8356456 -102.6017914\n163  <NA>   <NA>     <NA>        A 1913031975  3364150 +38.1019955 -105.3735123\n200  <NA>  20420     <NA>        A 4376255277 25642579 +37.2873673 -107.8397178\n228  <NA>   <NA>     <NA>        A 1003660672  2035929 +37.7810492 -107.6702567\n                          geometry\n23  MULTIPOLYGON (((-105.8093 3...\n107 MULTIPOLYGON (((-102.2091 4...\n124 MULTIPOLYGON (((-102.547 38...\n163 MULTIPOLYGON (((-105.7969 3...\n200 MULTIPOLYGON (((-107.7124 3...\n228 MULTIPOLYGON (((-107.9751 3...\n\n# Plot its geometry column\nplot(colorado_sf$geometry)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#working-with-historic-shapefiles",
    "href": "datacamp/us_census/us_census.html#working-with-historic-shapefiles",
    "title": "Census data in r with tidycensus",
    "section": "Working with historic shapefiles",
    "text": "Working with historic shapefiles\nTo ensure clean integration with the tidycensus package - which you’ll learn about in the next chapter - tigris defaults to returning shapefiles that correspond to the year of the most recently-released ACS data. However, you may want boundary files for other years. tigris allows R users to obtain shapefiles for 1990, 2000, and 2010 through 2017, which represent many boundary changes over time. In this exercise, you’ll use tigris to explore how Census tract boundaries have changed in Williamson County, Texas between 1990 and 2016.\n\n# Get a historic Census tract shapefile from 1990 for Williamson County, Texas\nwilliamson90 <- tracts(state = \"TX\", county = \"Williamson\", \n                       cb = TRUE, year = 1990)\n\n# Compare with a current dataset for 2016\nwilliamson16 <- tracts(state = \"TX\", county = \"Williamson\", \n                       cb = TRUE, year = 2016)\n\n# Plot the geometry to compare the results                       \npar(mfrow = c(1, 2))\nplot(williamson90$geometry)\nplot(williamson16$geometry)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#combining-datasets-of-the-same-tigris-type",
    "href": "datacamp/us_census/us_census.html#combining-datasets-of-the-same-tigris-type",
    "title": "Census data in r with tidycensus",
    "section": "Combining datasets of the same tigris type",
    "text": "Combining datasets of the same tigris type\nOften, datasets from the US Census Bureau are available by state, which means they are available by state from tigris as well. In many instances, you’ll want to combine datasets for multiple states. For example, an analysis of the Portland, Oregon metropolitan area would include areas in both Oregon and Washington north of the Columbia River; however, these areas are represented in different Census files. In this exercise, you’ll learn how to combine datasets with the rbind_tigris() function.\n\n# Get Census tract boundaries for Oregon and Washington\nor_tracts <- tracts(\"OR\", cb = TRUE)\nwa_tracts <- tracts(\"WA\", cb = TRUE)\n\n# Check the tigris attributes of each object\nattr(or_tracts, \"tigris\")\n\n[1] \"tract\"\n\nattr(wa_tracts, \"tigris\")\n\n[1] \"tract\"\n\n# Combine the datasets then plot the result\nor_wa_tracts <- rbind_tigris(or_tracts, wa_tracts)\nplot(or_wa_tracts$geometry)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#getting-data-for-multiple-states",
    "href": "datacamp/us_census/us_census.html#getting-data-for-multiple-states",
    "title": "Census data in r with tidycensus",
    "section": "Getting data for multiple states",
    "text": "Getting data for multiple states\nIn the previous exercise, you learned how to combine datasets with the rbind_tigris() function. If you need data for more than two states, however, this process can get tedious. In this exercise, you’ll learn how to generate a list of datasets for multiple states with the tidyverse map() function, and combine those datasets with rbind_tigris().\n\n# Generate a vector of state codes and assign to new_england\nnew_england <- c(\"ME\", \"NH\", \"VT\", \"MA\")\n\n# Iterate through the states and request tract data for state\nne_tracts <- map(new_england, function(x) {\n  tracts(state = x, cb = TRUE)\n}) %>%\n  rbind_tigris()\n\nplot(ne_tracts$geometry)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#joining-data-from-an-external-data-frame",
    "href": "datacamp/us_census/us_census.html#joining-data-from-an-external-data-frame",
    "title": "Census data in r with tidycensus",
    "section": "Joining data from an external data frame",
    "text": "Joining data from an external data frame\nWhen working with geographic data in R, you’ll commonly want to join attribute information from an external dataset to it for mapping and spatial analysis. The sf package enables the use of the tidyverse *_join() functions for simple features objects for this purpose. In this exercise, you’ll learn how to join data to a spatial dataset of legislative boundaries for the Texas House of Representatives that you’ve obtained using tigris.\n\n# Get boundaries for Texas and set the house parameter\ntx_house <- state_legislative_districts(state = \"TX\", house = \"lower\", cb = TRUE)\n\n# Merge data on legislators to their corresponding boundaries\ntx_joined <- left_join(tx_house, tx_members, by = c(\"NAME\" = \"District\"))\n\nhead(tx_joined)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#plotting-simple-features-with-geom_sf",
    "href": "datacamp/us_census/us_census.html#plotting-simple-features-with-geom_sf",
    "title": "Census data in r with tidycensus",
    "section": "Plotting simple features with geom_sf()",
    "text": "Plotting simple features with geom_sf()\nThe newest version of ggplot2 includes a geom_sf() function to plot simple features objects natively. This allows you to make maps using familiar ggplot2 syntax! In this exercise, you’ll walk through the process of creating a map with ggplot2 step-by-step.\n\n# Plot the legislative district boundaries\nggplot(tx_joined) + \n  geom_sf()\n\n# Set fill aesthetic to map areas represented by Republicans and Democrats\nggplot(tx_joined, aes(fill = Party)) + \n  geom_sf()\n\n# Set values so that Republican areas are red and Democratic areas are blue\nggplot(tx_joined, aes(fill = Party)) + \n  geom_sf() + \n  scale_fill_manual(values = c(\"R\" = \"red\", \"D\" = \"blue\"))"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#customizing-geom_sf-plots",
    "href": "datacamp/us_census/us_census.html#customizing-geom_sf-plots",
    "title": "Census data in r with tidycensus",
    "section": "Customizing geom_sf() plots",
    "text": "Customizing geom_sf() plots\nAs you’ve learned in previous chapters, it is a good idea to clean up and format your ggplot2 visualizations before sharing with others. In this exercise, you’ll make some modifications to your map of Texas House districts such as removing the gridlines and adding an informative title.\n\n# Draw a ggplot without gridlines and with an informative title\nggplot(tx_joined, aes(fill = Party)) + \n  geom_sf() + \n  coord_sf(crs = 3083, datum = NA) + \n  scale_fill_manual(values = c(\"R\" = \"red\", \"D\" = \"blue\")) + \n  theme_minimal(base_size = 16) + \n  labs(title = \"State House Districts in Texas\")"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#getting-simple-feature-geometry",
    "href": "datacamp/us_census/us_census.html#getting-simple-feature-geometry",
    "title": "Census data in r with tidycensus",
    "section": "Getting simple feature geometry",
    "text": "Getting simple feature geometry\ntidycensus can obtain simple feature geometry for many geographies by adding the argument geometry = TRUE. In this exercise, you’ll obtain a dataset of median housing values for owner-occupied units by Census tract in Orange County, California with simple feature geometry included.\n\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(sf)\n\n# Get dataset with geometry set to TRUE\norange_value <- get_acs(geography = \"tract\", state = \"CA\", \n                    county = \"Orange\", \n                    variables = \"B25077_001\", \n                    geometry = TRUE)\n\n# Plot the estimate to view a map of the data\nplot(orange_value[\"estimate\"])"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#joining-data-from-tigris-and-tidycensus",
    "href": "datacamp/us_census/us_census.html#joining-data-from-tigris-and-tidycensus",
    "title": "Census data in r with tidycensus",
    "section": "Joining data from tigris and tidycensus",
    "text": "Joining data from tigris and tidycensus\nGeometry is currently supported in tidycensus for geographies in the core Census hierarchy - state, county, Census tract, block group, and block - as well as zip code tabulation areas. However, you may be interested in mapping data for other geographies. In this case, you can download the equivalent boundary file from the Census Bureau using the tigris package and join your demographic data to it for mapping.\n\n# Get an income dataset for Idaho by school district\nidaho_income <- get_acs(geography = \"school district (unified)\", \n                        variables = \"B19013_001\", \n                        state = \"ID\")\n\n# Get a school district dataset for Idaho\nidaho_school <- school_districts(state = \"ID\", type = \"unified\", class = \"sf\")\n\n# Join the income dataset to the boundaries dataset\nid_school_joined <- left_join(idaho_school, idaho_income, by = \"GEOID\")\n\nplot(id_school_joined[\"estimate\"])"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#shifting-alaska-and-hawaii-geometry",
    "href": "datacamp/us_census/us_census.html#shifting-alaska-and-hawaii-geometry",
    "title": "Census data in r with tidycensus",
    "section": "Shifting Alaska and Hawaii geometry",
    "text": "Shifting Alaska and Hawaii geometry\nAnalysts will commonly want to map data for the entire United States by state or county; however, this can be difficult by default as Alaska and Hawaii are distant from the continental United States. A common solution is to rescale and shift Alaska and Hawaii for mapping purposes, which is supported in tidycensus. You’ll learn how to do this in this exercise.\n\n# Get a dataset of median home values from the 1-year ACS\nstate_value <- get_acs(geography = \"state\", \n                       variables = \"B25077_001\", \n                       survey = \"acs1\", \n                       geometry = TRUE, \n                       shift_geo = TRUE)\n\n# Plot the dataset to view the shifted geometry\nplot(state_value[\"estimate\"])"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#making-a-choropleth-map",
    "href": "datacamp/us_census/us_census.html#making-a-choropleth-map",
    "title": "Census data in r with tidycensus",
    "section": "Making a choropleth map",
    "text": "Making a choropleth map\nChoropleth maps, which visualize statistical variation through the shading of areas, are among the most popular ways to map demographic data. Census or ACS data acquired with tidycensus can be mapped in this way in ggplot2 with geom_sf using the estimate column as a fill aesthetic. In this exercise, you’ll make a choropleth map with ggplot2 of median owner-occupied home values by Census tract for Marin County, California.\n\n# Create a choropleth map with ggplot\nggplot(marin_value, aes(fill = estimate)) + \n  geom_sf()"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#modifying-map-colors",
    "href": "datacamp/us_census/us_census.html#modifying-map-colors",
    "title": "Census data in r with tidycensus",
    "section": "Modifying map colors",
    "text": "Modifying map colors\nggplot2 version 3.0 integrated the viridis color palettes, which are perceptually uniform and legible to colorblind individuals and in black and white. For these reasons, the viridis palettes have become very popular for data visualization, including for choropleth mapping. In this exercise, you’ll learn how to use the viridis palettes for choropleth mapping in ggplot2.\n\n# Set continuous viridis palettes for your map\nggplot(marin_value, aes(fill = estimate, color = estimate)) + \n  geom_sf() + \n  scale_fill_viridis_c() +  \n  scale_color_viridis_c()"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#customizing-the-map-output",
    "href": "datacamp/us_census/us_census.html#customizing-the-map-output",
    "title": "Census data in r with tidycensus",
    "section": "Customizing the map output",
    "text": "Customizing the map output\nNow that you’ve chosen an appropriate color palette for your choropleth map of median home values by Census tract in Marin County, you’ll want to customize the output. In this exercise, you’ll clean up some map elements and add some descriptive information to provide context to your map.\n\n# Set the color guide to FALSE and add a subtitle and caption to your map\nggplot(marin_value, aes(fill = estimate, color = estimate)) + \n  geom_sf() + \n  scale_fill_viridis_c(labels = scales::dollar) +  \n  scale_color_viridis_c(guide = FALSE) + \n  theme_minimal() + \n  coord_sf(crs = 26911, datum = NA) + \n  labs(title = \"Median owner-occupied housing value by Census tract\", \n       subtitle = \"Marin County, California\", \n       caption = \"Data source: 2012-2016 ACS.\\nData acquired with the R tidycensus package.\", \n       fill = \"ACS estimate\")"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#graduated-symbol-maps",
    "href": "datacamp/us_census/us_census.html#graduated-symbol-maps",
    "title": "Census data in r with tidycensus",
    "section": "Graduated symbol maps",
    "text": "Graduated symbol maps\nThere are many other effective ways to map statistical data besides choropleth maps. One such example is the graduated symbol map, which represents statistical variation by differently-sized symbols. In this exercise, you’ll learn how to use the st_centroid() tool in the sf package to create points at the center of each state to be used as inputs to a graduated symbol map in ggplot2.\n\nlibrary(sf)\n\n# Generate point centers\ncenters <- st_centroid(state_value)\n\n# Set size parameter and the size range\nggplot() + \n  geom_sf(data = state_value, fill = \"white\") + \n  geom_sf(data = centers, aes(size = estimate), shape = 21, \n          fill = \"lightblue\", alpha = 0.7, show.legend = \"point\") + \n  scale_size_continuous(range = c(1, 20))"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#faceted-maps-with-ggplot2",
    "href": "datacamp/us_census/us_census.html#faceted-maps-with-ggplot2",
    "title": "Census data in r with tidycensus",
    "section": "Faceted maps with ggplot2",
    "text": "Faceted maps with ggplot2\nOne of the most powerful features of ggplot2 is its support for faceted plotting, in which multiple plots are generated for unique values of a column in the data. Faceted maps can be produced with geom_sf() in this way as well if tidycensus data are in tidy format. In this exercise, you’ll produce faceted maps that show the racial and ethnic geography of Washington, DC from the 2010 decennial Census.\n\n# Check the first few rows of the loaded dataset dc_race\nhead(dc_race)\n\n# Remove the gridlines and generate faceted maps\nggplot(dc_race, aes(fill = percent, color = percent)) + \n  geom_sf() + \n  coord_sf(datum = NA) + \n  facet_wrap(~variable)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#interactive-visualization-with-mapview",
    "href": "datacamp/us_census/us_census.html#interactive-visualization-with-mapview",
    "title": "Census data in r with tidycensus",
    "section": "Interactive visualization with mapview",
    "text": "Interactive visualization with mapview\nThe mapview R package allows R users to interactively map spatial datasets in one line of R code. This makes it an essential tool for exploratory spatial data analysis in R. In this exercise, you’ll learn how to quickly map tidycensus data interactively using mapview and your Orange County, California median housing values dataset.\n\n# Add a legend to your map\nm <- mapview(orange_value, \n         zcol = \"estimate\", \n         legend = TRUE)\nm@map"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#generating-random-dots-with-sf",
    "href": "datacamp/us_census/us_census.html#generating-random-dots-with-sf",
    "title": "Census data in r with tidycensus",
    "section": "Generating random dots with sf",
    "text": "Generating random dots with sf\nDot-density maps are created by randomly placing dots within areas where each dot is proportional to a certain number of observations. In this exercise, you’ll learn how to create dots in this way with the sf package using the st_sample() function. You will generate dots that are proportional to about 100 people in the decennial Census, and then you will group the dots to speed up plotting with ggplot2.\n\n# Generate dots, create a group column, and group by group column\ndc_dots <- map(c(\"White\", \"Black\", \"Hispanic\", \"Asian\"), function(group) {\n  dc_race %>%\n    filter(variable == group) %>%\n    st_sample(., size = .$value / 100) %>%\n    st_sf() %>%\n    group_by(group = group) \n}) %>%\n  reduce(rbind) %>%\n  group_by(group) %>%\n  summarize()"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#obtaining-data-for-cartography-with-tigris",
    "href": "datacamp/us_census/us_census.html#obtaining-data-for-cartography-with-tigris",
    "title": "Census data in r with tidycensus",
    "section": "Obtaining data for cartography with tigris",
    "text": "Obtaining data for cartography with tigris\nBefore making your dot-density map of Washington, DC with ggplot2, it will be useful to acquire some ancillary cartographic data with the tigris package that will help map viewers understand what you’ve visualized. These datasets will include major roads in DC; area water features; and the boundary of the District of Columbia, which you’ll use as a background in your map.\n\n# Filter the DC roads object for major roads only\ndc_roads <- roads(\"DC\", \"District of Columbia\") %>%\n  filter(RTTYP %in% c(\"I\", \"S\", \"U\"))\n\n# Get an area water dataset for DC\ndc_water <- area_water(\"DC\", \"District of Columbia\")\n\n# Get the boundary of DC\ndc_boundary <- counties(\"DC\", cb = TRUE)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#making-a-dot-density-map-with-ggplot2",
    "href": "datacamp/us_census/us_census.html#making-a-dot-density-map-with-ggplot2",
    "title": "Census data in r with tidycensus",
    "section": "Making a dot-density map with ggplot2",
    "text": "Making a dot-density map with ggplot2\nIn your final exercise of this course, you are going to put together the datasets you’ve acquired and generated into a dot-density map with ggplot2. You’ll plot your generated dots and ancillary datasets with geom_sf(), and add some informative map elements to create a cartographic product.\n\n# Plot your datasets and give your map an informative caption\nggplot() + \n  geom_sf(data = dc_boundary, color = NA, fill = \"white\") + \n  geom_sf(data = dc_dots, aes(color = group, fill = group), size = 0.1) + \n  geom_sf(data = dc_roads, color = \"lightblue\", fill = \"lightblue\") + \n  geom_sf(data = dc_water, color = \"grey\") + \n  coord_sf(crs = 26918, datum = NA) + \n  scale_color_brewer(palette = \"Set1\", guide = FALSE) +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(title = \"The racial geography of Washington, DC\", \n       subtitle = \"2010 decennial U.S. Census\", \n       fill = \"\", \n       caption = \"1 dot = approximately 100 people.\\nData acquired with the R tidycensus and tigris packages.\")"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html",
    "href": "datacamp/sampling_R/sampling_R.html",
    "title": "Sampling in R",
    "section": "",
    "text": "Sampling is an important technique in your statistical arsenal. It isn’t always appropriate though—you need to know when to use it and when to work with the whole dataset.\n\nWhich of the following is not a good scenario to use sampling?\nwhen data set is small"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html#simple-sampling-with-dplyr",
    "href": "datacamp/sampling_R/sampling_R.html#simple-sampling-with-dplyr",
    "title": "Sampling in R",
    "section": "Simple sampling with dplyr",
    "text": "Simple sampling with dplyr\nThroughout this chapter you’ll be exploring song data from Spotify. Each row of the dataset represents a song, and there are 41656 rows. Columns include the name of the song, the artists who performed it, the release year, and attributes of the song like its duration, tempo, and danceability. We’ll start by looking at the durations.\nYour first task is to sample the song dataset and compare a calculation on the whole population and on a sample.\nspotify_population is available and dplyr is loaded.\n\nlibrary(tidyverse)\nlibrary(fst)\nlibrary(knitr)\nspotify_population <- read_fst(\"data/spotify_2000_2020.fst\")\n# View the whole population dataset\n\n# Sample 1000 rows from spotify_population\nspotify_sample <- slice_sample(spotify_population, n = 10)\n\n\n# See the result\nkable(spotify_sample)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nacousticness\nartists\ndanceability\nduration_ms\nduration_minutes\nenergy\nexplicit\nid\ninstrumentalness\nkey\nliveness\nloudness\nmode\nname\npopularity\nrelease_date\nspeechiness\ntempo\nvalence\nyear\n\n\n\n\n0.00665\n[‘Hurt’]\n0.507\n281373\n4.689550\n0.684\n0\n0N48Iqm5s1DVkiBSF2e2et\n2.85e-05\n8\n0.1150\n-6.230\n0\nFalls Apart\n43\n2006-01-01\n0.0506\n146.021\n0.371\n2006\n\n\n0.00312\n[‘Kevin Gates’]\n0.728\n247320\n4.122000\n0.550\n1\n5KJ7TOWSkcRCbgnRmGatnA\n0.00e+00\n2\n0.0871\n-6.049\n1\nIDGAF\n45\n2013-04-09\n0.3790\n139.863\n0.305\n2013\n\n\n0.05350\n[‘J Dilla’]\n0.230\n119387\n1.989783\n0.770\n1\n4jVqbLx0MvlIaj3h2D872X\n1.56e-04\n1\n0.6440\n-6.349\n1\nDon’t Cry\n53\n2006-02-07\n0.3370\n173.442\n0.588\n2006\n\n\n0.82500\n[‘Los Panchos’]\n0.844\n180627\n3.010450\n0.561\n0\n1HlX3tDP7eJBs5CJx4XIIk\n9.19e-04\n6\n0.1030\n-7.677\n0\nBésame Mucho\n44\n2008-01-26\n0.0378\n121.755\n0.936\n2008\n\n\n0.41400\n[‘Zac Brown Band’]\n0.400\n227560\n3.792667\n0.465\n0\n2q40yq9UxqYGKuVZnynPxV\n2.24e-05\n1\n0.0739\n-6.530\n1\nCold Hearted\n53\n2010-09-20\n0.0264\n166.010\n0.316\n2010\n\n\n0.69500\n[‘Tom Odell’]\n0.445\n244360\n4.072667\n0.537\n1\n3JvKfv6T31zO0ini8iNItO\n1.60e-05\n4\n0.0944\n-8.532\n0\nAnother Love\n75\n2013-06-24\n0.0400\n122.764\n0.131\n2013\n\n\n0.00282\n[‘WALK THE MOON’]\n0.629\n232733\n3.878883\n0.721\n0\n4LgIcna8LRrR3DOomihYMY\n1.40e-06\n11\n0.1280\n-6.292\n1\nShiver Shiver\n46\n2012-06-19\n0.0326\n100.208\n0.854\n2012\n\n\n0.04240\n[‘Blur’]\n0.529\n233373\n3.889550\n0.455\n0\n79PrPZu9zWyc1qwUwXchVl\n3.64e-03\n0\n0.3640\n-8.603\n1\nOut of Time\n54\n2003\n0.0274\n138.678\n0.318\n2003\n\n\n0.13000\n[‘Taylor Swift’]\n0.459\n261800\n4.363333\n0.459\n0\n6HWYtS215rxaaMjvpyG18W\n2.30e-06\n6\n0.1130\n-4.126\n1\nYou’re Not Sorry\n46\n2008-11-11\n0.0275\n134.018\n0.281\n2008\n\n\n0.02020\n[‘Black Stone Cherry’]\n0.394\n230707\n3.845117\n0.834\n0\n5lIqY3Yb3bPj5z1tiUIiCJ\n0.00e+00\n1\n0.3090\n-3.296\n1\nLonely Train\n52\n2006-07-17\n0.0767\n123.600\n0.438\n2006"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html#simple-sampling-with-dplyr-1",
    "href": "datacamp/sampling_R/sampling_R.html#simple-sampling-with-dplyr-1",
    "title": "Sampling in R",
    "section": "Simple sampling with dplyr",
    "text": "Simple sampling with dplyr\nThroughout this chapter you’ll be exploring song data from Spotify. Each row of the dataset represents a song, and there are 41656 rows. Columns include the name of the song, the artists who performed it, the release year, and attributes of the song like its duration, tempo, and danceability. We’ll start by looking at the durations.\nYour first task is to sample the song dataset and compare a calculation on the whole population and on a sample.\nspotify_population is available and dplyr is loaded.\n\n# Calculate the mean duration in mins from spotify_population\nmean_dur_pop <- summarize(spotify_population, mean(duration_minutes))\n\n\n# Calculate the mean duration in mins from spotify_sample\nmean_dur_samp <- summarize(spotify_sample, mean(duration_minutes))\n\n\n# See the results\nmean_dur_pop\n\n  mean(duration_minutes)\n1               3.852152\n\nmean_dur_samp\n\n  mean(duration_minutes)\n1                 3.7654"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html#simple-sampling-with-base-r",
    "href": "datacamp/sampling_R/sampling_R.html#simple-sampling-with-base-r",
    "title": "Sampling in R",
    "section": "Simple sampling with base-R",
    "text": "Simple sampling with base-R\nWhile dplyr provides great tools for sampling data frames, if you want to work with vectors you can use base-R.\nLet’s turn it up to eleven and look at the loudness property of each song.\nspotify_population is available.\n\n# From previous step\nloudness_pop <- spotify_population$loudness\nloudness_samp <- sample(loudness_pop, size = 100)\n\n# Calculate the standard deviation of loudness_pop\nsd_loudness_pop <- sd(loudness_pop)\n\n# Calculate the standard deviation of loudness_samp\nsd_loudness_samp <- sd(loudness_samp)\n\n# See the results\nsd_loudness_pop\n\n[1] 4.524076\n\nsd_loudness_samp\n\n[1] 3.908033"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html#are-findings-from-the-sample-generalizable",
    "href": "datacamp/sampling_R/sampling_R.html#are-findings-from-the-sample-generalizable",
    "title": "Sampling in R",
    "section": "Are findings from the sample generalizable?",
    "text": "Are findings from the sample generalizable?\nYou just saw how convenience sampling—collecting data via the easiest method can result in samples that aren’t representative of the whole population. Equivalently, this means findings from the sample are not generalizable to the whole population. Visualizing the distributions of the population and the sample can help determine whether or not the sample is representative of the population.\nThe Spotify dataset contains a column named acousticness, which is a confidence measure from zero to one of whether the track is acoustic, that is, it was made with instruments that aren’t plugged in. Here, you’ll look at acousticness in the total population of songs, and in a sample of those songs.\nspotify_population and spotify_mysterious_sample are available; dplyr and ggplot2 are loaded.\n\nggplot(spotify_population, aes(acousticness))+\n    geom_histogram(binwidth = 0.01)\n\n\n\n\n\nggplot(spotify_population, aes(duration_minutes))+\n    geom_histogram(binwidth = 0.5)"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html#generating-random-numbers",
    "href": "datacamp/sampling_R/sampling_R.html#generating-random-numbers",
    "title": "Sampling in R",
    "section": "Generating random numbers",
    "text": "Generating random numbers\nYou’ve seen sample() and it’s dplyr cousin, slice_sample() for generating pseudo-random numbers from a set of values. A related task is to generate random numbers that follow a statistical distribution, like the uniform distribution or the normal distribution.\nEach random number generation function has a name beginning with “r”. It’s first argument is the number of numbers to generate, but other arguments are distribution-specific. Free hint: Try args(runif) and args(rnorm) to see what arguments you need to pass to those functions.\nn_numbers is available and set to 5000; ggplot2 is loaded.\n\nn_numbers <- 5000\n# Generate random numbers from ...\nrandoms <- data.frame(\n  # a uniform distribution from -3 to 3\n  uniform =runif(n_numbers, -3, 3),\n  # a normal distribution with mean 5 and sd 2\n  normal = rnorm(n_numbers, mean = 5, sd = 2)\n)\n\n\n# Plot a histogram of uniform values, binwidth 0.25\nggplot(randoms, aes(uniform)) +\n    geom_histogram(binwidth = 0.25)\n\n\n\n# Plot a histogram of normal values, binwidth 0.5\nggplot(randoms, aes(normal)) +\n    geom_histogram(binwidth = 0.5)"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html#understanding-random-seeds",
    "href": "datacamp/sampling_R/sampling_R.html#understanding-random-seeds",
    "title": "Sampling in R",
    "section": "Understanding random seeds",
    "text": "Understanding random seeds\nWhile random numbers are important for many analyses, they create a problem: the results you get can vary slightly. This can cause awkward conversations with your boss when your script for calculating the sales forecast gives different answers each time.\nSetting the seed to R’s random number generator helps avoid such problems by making the random number generation reproducible. - The values of x are different to those of y.\n\nset.seed(123)\nx <- rnorm(5)\ny <- rnorm(5)\nx\n\n[1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774\n\ny\n\n[1]  1.7150650  0.4609162 -1.2650612 -0.6868529 -0.4456620\n\n\n\nx and y have identical values.\n\n\nset.seed(123)\nx <- rnorm(5)\nset.seed(123)\ny <- rnorm(5)\nx\n\n[1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774\n\ny\n\n[1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774\n\n\n\nx and y have identical values.\n\n\nset.seed(123)\nx <- c(rnorm(5), rnorm(5))\nset.seed(123)\ny <- rnorm(10)\nx\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n\ny\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197"
  },
  {
    "objectID": "datacamp/tensorflow_R/tensorflow_R.html",
    "href": "datacamp/tensorflow_R/tensorflow_R.html",
    "title": "Introduction to TensorFlow in R",
    "section": "",
    "text": "library(tensorflow)\nmsg = tf$constant('Hello, TensorFlow!')\ntf$print(msg)"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "",
    "text": "You will use the MNIST dataset in several exercises through the course. Let’s do some data exploration to gain a better understanding. Remember that the MNIST dataset contains a set of records that represent handwritten digits using 28x28 features, which are stored into a 784-dimensional vector.\nmnistInput  Each record of the MNIST dataset corresponds to a handwritten digit and each feature represents one pixel of the digit image. In this exercise, a sample of 200 records of the MNIST dataset named mnist_sample is loaded for you.\n\nload(\"mnist-sample-200.RData\")\n# Have a look at the MNIST dataset names\n#names(mnist_sample)\n\n# Show the first records\n#str(mnist_sample)\n\n# Labels of the first 6 digits\nhead(mnist_sample$label)\n\n[1] 5 0 7 0 9 3"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#digits-features",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#digits-features",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Digits features",
    "text": "Digits features\nLet’s continue exploring the dataset. Firstly, it would be helpful to know how many different digits are present by computing a histogram of the labels. Next, the basic statistics (min, mean, median, maximum) of the features for all digits can be calculated. Finally, you will compute the basic statistics for only those digits with label 0. The MNIST sample data is loaded for you as mnist_sample.\n\n# Plot the histogram of the digit labels\nhist(mnist_sample$label)\n\n\n\n# Compute the basic statistics of all records\n#summary(mnist_sample)\n\n# Compute the basic statistics of digits with label 0\n#summary(mnist_sample[, mnist_sample$label == 0])"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#euclidean-distance",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#euclidean-distance",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Euclidean distance",
    "text": "Euclidean distance\nEuclidean distance is the basis of many measures of similarity and is the most important distance metric. You can compute the Euclidean distance in R using the dist() function. In this exercise, you will compute the Euclidean distance between the first 10 records of the MNIST sample data.\nThe mnist_sample object is loaded for you.\n\n# Show the labels of the first 10 records\nmnist_sample$label[1:10]\n\n [1] 5 0 7 0 9 3 4 1 2 6\n\n# Compute the Euclidean distance of the first 10 records\ndistances <- dist(mnist_sample[1:10, -1])\n\n# Show the distances values\ndistances\n\n          1        2        3        4        5        6        7        8\n2  2185.551                                                               \n3  2656.407 2869.979                                                      \n4  2547.027 2341.249 2936.772                                             \n5  2406.509 2959.108 1976.406 2870.928                                    \n6  2343.982 2759.681 2452.568 2739.470 2125.723                           \n7  2464.388 2784.158 2573.667 2870.918 2174.322 2653.703                  \n8  2149.872 2668.903 1999.892 2585.980 2067.044 2273.248 2407.962         \n9  2959.129 3209.677 2935.262 3413.821 2870.746 3114.508 2980.663 2833.447\n10 2728.657 3009.771 2574.752 2832.521 2395.589 2655.864 2464.301 2550.126\n          9\n2          \n3          \n4          \n5          \n6          \n7          \n8          \n9          \n10 2695.166\n\n# Plot the numeric matrix of the distances in a heatmap\nheatmap(as.matrix(distances), \n        Rowv = NA, symm = TRUE, \n        labRow = mnist_sample$label[1:10], \n        labCol = mnist_sample$label[1:10])"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#minkowsky-distance",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#minkowsky-distance",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Minkowsky distance",
    "text": "Minkowsky distance\nThere are other well-known distance metrics besides the Euclidean distance, like the Minkowski distance. This metric can be considered a generalisation of both the Euclidean and Manhattan distance. In R, you can calculate the Minkowsky distance of order p by using dist(…, method = “minkowski”, p).\nThe MNIST sample data is loaded for you as mnist_sample\n\n# Minkowski distance or order 3\ndistances_3 <- dist(mnist_sample[1:10, -1], method = \"minkowski\", p = 3)\n\ndistances_3 \n\n           1         2         3         4         5         6         7\n2  1002.6468                                                            \n3  1169.6470 1228.8295                                                  \n4  1127.4919 1044.9182 1249.6133                                        \n5  1091.3114 1260.3549  941.1654 1231.7432                              \n6  1063.7026 1194.1212 1104.2581 1189.9558  996.2687                    \n7  1098.4279 1198.8891 1131.4498 1227.7888 1005.7588 1165.4475          \n8  1006.9070 1169.4720  950.6812 1143.3503  980.6450 1056.1814 1083.2255\n9  1270.0240 1337.2068 1257.4052 1401.2461 1248.0777 1319.2768 1271.7095\n10 1186.9620 1268.1539 1134.0371 1219.1388 1084.5416 1166.9129 1096.3586\n           8         9\n2                     \n3                     \n4                     \n5                     \n6                     \n7                     \n8                     \n9  1236.9178          \n10 1133.2929 1180.7970\n\nheatmap(as.matrix(distances_3 ), \n        Rowv = NA, symm = TRUE, \n        labRow = mnist_sample$label[1:10], \n        labCol = mnist_sample$label[1:10])\n\n\n\n# Minkowski distance of order 2\ndistances_2 <- dist(mnist_sample[1:10, -1], method = \"minkowski\", p = 2)\ndistances_2\n\n          1        2        3        4        5        6        7        8\n2  2185.551                                                               \n3  2656.407 2869.979                                                      \n4  2547.027 2341.249 2936.772                                             \n5  2406.509 2959.108 1976.406 2870.928                                    \n6  2343.982 2759.681 2452.568 2739.470 2125.723                           \n7  2464.388 2784.158 2573.667 2870.918 2174.322 2653.703                  \n8  2149.872 2668.903 1999.892 2585.980 2067.044 2273.248 2407.962         \n9  2959.129 3209.677 2935.262 3413.821 2870.746 3114.508 2980.663 2833.447\n10 2728.657 3009.771 2574.752 2832.521 2395.589 2655.864 2464.301 2550.126\n          9\n2          \n3          \n4          \n5          \n6          \n7          \n8          \n9          \n10 2695.166\n\nheatmap(as.matrix(distances_2), \n        Rowv = NA, symm = TRUE, \n        labRow = mnist_sample$label[1:10], \n        labCol = mnist_sample$label[1:10])\n\n\n\n\n\nVery Good! As you can see, when using Minkowski distance of order 2 the most similar digits are in positions 3 and 5 of the heatmap grid which corresponds to digits 7 and 9."
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#kl-divergence",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#kl-divergence",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "KL divergence",
    "text": "KL divergence\nThere are more distance metrics that can be used to compute how similar two feature vectors are. For instance, the philentropy package has the function distance(), which implements 46 different distance metrics. For more information, use ?distance in the console. In this exercise, you will compute the KL divergence and check if the results differ from the previous metrics. Since the KL divergence is a measure of the difference between probability distributions you need to rescale the input data by dividing each input feature by the total pixel intensities of that digit. The philentropy package and mnist_sample data have been loaded.\n\nlibrary(philentropy)\nlibrary(tidyverse)\n# Get the first 10 records\nmnist_10 <- mnist_sample[1:10, -1]\n\n# Add 1 to avoid NaN when rescaling\nmnist_10_prep <- mnist_10 + 1 \n\n# Compute the sums per row\nsums <- rowSums(mnist_10_prep)\n\n# Compute KL divergence\ndistances <- distance(mnist_10_prep/sums, method = \"kullback-leibler\")\nheatmap(as.matrix(distances), \n        Rowv = NA, symm = TRUE, \n        labRow = mnist_sample$label[1:10], \n        labCol = mnist_sample$label[1:10])"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#generating-pca-from-mnist-sample",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#generating-pca-from-mnist-sample",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Generating PCA from MNIST sample",
    "text": "Generating PCA from MNIST sample\nYou are going to compute a PCA with the previous mnist_sample dataset. The goal is to have a good representation of each digit in a lower dimensional space. PCA will give you a set of variables, named principal components, that are a linear combination of the input variables. These principal components are ordered in terms of the variance they capture from the original data. So, if you plot the first two principal components you can see the digits in a 2-dimensional space. A sample of 200 records of the MNIST dataset named mnist_sample is loaded for you.\n\n# Get the principal components from PCA\npca_output <- prcomp(mnist_sample[, -1])\n\n# Observe a summary of the output\n#summary(pca_output)\n\n# Store the first two coordinates and the label in a data frame\npca_plot <- data.frame(pca_x = pca_output$x[, \"PC1\"], pca_y = pca_output$x[, \"PC2\"], \n                       label = as.factor(mnist_sample$label))\n\n# Plot the first two principal components using the true labels as color and shape\nggplot(pca_plot, aes(x = pca_x, y = pca_y, color = label)) + \n    ggtitle(\"PCA of MNIST sample\") + \n    geom_text(aes(label = label)) + \n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#t-sne-output-from-mnist-sample",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#t-sne-output-from-mnist-sample",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "t-SNE output from MNIST sample",
    "text": "t-SNE output from MNIST sample\nYou have seen that PCA has some limitations in correctly classifying digits, mainly due to its linear nature. In this exercise, you are going to use the output from the t-SNE algorithm on the MNIST sample data, named tsne_output and visualize the obtained results. In the next chapter, you will focus on the t-SNE algorithm and learn more about how to use it! The MNIST sample dataset mnist_sample as well as the tsne_output are available in your workspace.\n\n# Explore the tsne_output structure\nlibrary(Rtsne)\nlibrary(tidyverse)\ntsne_output <- Rtsne(mnist_sample[, -1])\n#str(tsne_output)\n\n# Have a look at the first records from the t-SNE output\n#head(tsne_output)\n\n# Store the first two coordinates and the label in a data.frame\ntsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], \n                        label = as.factor(mnist_sample$label))\n\n# Plot the t-SNE embedding using the true labels as color and shape\nggplot(tsne_plot, aes(x =tsne_x, y = tsne_y, color = label)) + \n    ggtitle(\"T-Sne output\") + \n    geom_text(aes(label = label)) + \n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-t-sne",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-t-sne",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Computing t-SNE",
    "text": "Computing t-SNE\nAs we have seen, the t-SNE embedding can be computed in R using the Rtsne() function from the Rtsne package in CRAN. Performing a PCA is a common step before running the t-SNE algorithm, but we can skip this step by setting the parameter PCA to FALSE. The dimensionality of the embedding generated by t-SNE can be indicated with the dims parameter. In this exercise, we will generate a three-dimensional embedding from the mnist_sample dataset without doing the PCA step and then, we will plot the first two dimensions. The MNIST sample dataset mnist_sample, as well as the Rtsne and ggplot2 packages, are already loaded.\n\n# Compute t-SNE without doing the PCA step\ntsne_output <- Rtsne(mnist_sample[,-1], PCA = FALSE, dim = 3)\n\n# Show the obtained embedding coordinates\nhead(tsne_output$Y)\n\n           [,1]          [,2]      [,3]\n[1,]  -5.422495  7.447943e-01 -6.083799\n[2,] -10.436057 -2.944640e+00 -5.919307\n[3,]   5.875347  6.348422e+00 -4.179561\n[4,] -13.276586 -8.857034e-05 -7.953639\n[5,]   6.315575  2.209903e+00 -2.553357\n[6,]  -3.942594  8.102608e+00 -6.429995\n\n# Store the first two coordinates and plot them \ntsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], \n                        digit = as.factor(mnist_sample$label))\n\n# Plot the coordinates\nggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = digit)) + \n    ggtitle(\"t-SNE of MNIST sample\") + \n    geom_text(aes(label = digit)) + \n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#understanding-t-sne-output",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#understanding-t-sne-output",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Understanding t-SNE output",
    "text": "Understanding t-SNE output\nThe most important t-SNE output are those related to the K-L divergence of the points in the original high dimensions and in the new lower dimensional space. Remember that the goal of t-SNE is to minimize the K-L divergence between the original space and the new one. In the returned object, the itercosts structure indicates the total cost from the K-L divergence of all the objects in each 50th iteration and the cost structure indicates the K-L divergence of each record in the final iteration. The Rtsne package and the tsne_output object have been loaded for you.\n\n# Inspect the output object's structure\nstr(tsne_output)\n\nList of 14\n $ N                  : int 200\n $ Y                  : num [1:200, 1:3] -5.42 -10.44 5.88 -13.28 6.32 ...\n $ costs              : num [1:200] 0.002384 0.001437 0.000736 0.002261 0.002459 ...\n $ itercosts          : num [1:20] 53.8 53.4 52.2 53.5 52.6 ...\n $ origD              : int 50\n $ perplexity         : num 30\n $ theta              : num 0.5\n $ max_iter           : num 1000\n $ stop_lying_iter    : int 250\n $ mom_switch_iter    : int 250\n $ momentum           : num 0.5\n $ final_momentum     : num 0.8\n $ eta                : num 200\n $ exaggeration_factor: num 12\n - attr(*, \"class\")= chr [1:2] \"Rtsne\" \"list\"\n\n# Show total costs after each 50th iteration\ntsne_output$itercosts\n\n [1] 53.7817918 53.4454502 52.1718550 53.5171454 52.6175938  0.9930376\n [7]  0.5949278  0.5329407  0.5259669  0.5169865  0.5106936  0.5061686\n[13]  0.5039689  0.5014811  0.5005688  0.4984711  0.4984594  0.4983841\n[19]  0.4987725  0.4974953\n\n# Plot the evolution of the KL divergence at each 50th iteration\nplot(tsne_output$itercosts, type = \"l\")\n\n\n\n# Inspect the output object's structure\nstr(tsne_output)\n\nList of 14\n $ N                  : int 200\n $ Y                  : num [1:200, 1:3] -5.42 -10.44 5.88 -13.28 6.32 ...\n $ costs              : num [1:200] 0.002384 0.001437 0.000736 0.002261 0.002459 ...\n $ itercosts          : num [1:20] 53.8 53.4 52.2 53.5 52.6 ...\n $ origD              : int 50\n $ perplexity         : num 30\n $ theta              : num 0.5\n $ max_iter           : num 1000\n $ stop_lying_iter    : int 250\n $ mom_switch_iter    : int 250\n $ momentum           : num 0.5\n $ final_momentum     : num 0.8\n $ eta                : num 200\n $ exaggeration_factor: num 12\n - attr(*, \"class\")= chr [1:2] \"Rtsne\" \"list\"\n\n# Show the K-L divergence of each record after the final iteration\ntsne_output$costs\n\n  [1]  0.0023844058  0.0014373774  0.0007356665  0.0022605242  0.0024587724\n  [6]  0.0055495101  0.0037524675  0.0012784164  0.0010902023  0.0029394660\n [11]  0.0022622056  0.0033955237  0.0030057479  0.0020440955  0.0040694677\n [16]  0.0011789599  0.0032039925  0.0030509381  0.0009212586  0.0021077062\n [21]  0.0017768242  0.0022350148  0.0032285732  0.0023182562  0.0036043732\n [26]  0.0018751147  0.0019905807  0.0037266527  0.0026170002  0.0008345732\n [31]  0.0043394695  0.0026265029  0.0046496576  0.0022914613  0.0024275018\n [36]  0.0036293822  0.0040672651  0.0026602230  0.0010785096  0.0030803185\n [41]  0.0036225071  0.0056562060  0.0029444718  0.0015566000  0.0020306467\n [46]  0.0037686295  0.0022377087  0.0061390019  0.0009516935  0.0037664574\n [51]  0.0022295887  0.0036092669  0.0014710552  0.0047759706  0.0022813989\n [56]  0.0068269455  0.0010659808  0.0042171260  0.0005930941  0.0012967703\n [61]  0.0040460825  0.0014584978  0.0014170265  0.0011157559  0.0026663249\n [66]  0.0006784082  0.0009352408  0.0009931230  0.0025511860  0.0007132606\n [71]  0.0030704173  0.0043991609  0.0006731115  0.0039172030  0.0009420319\n [76]  0.0025232269  0.0007068376  0.0028841192  0.0013489248  0.0010720987\n [81]  0.0025286890  0.0018135418  0.0030853159  0.0011667276  0.0065434094\n [86]  0.0016131304  0.0014196083  0.0032659328  0.0012147827  0.0025622840\n [91]  0.0038276989  0.0007314898  0.0032469951  0.0009378792  0.0070743032\n [96]  0.0029567718  0.0014572411  0.0007318395  0.0015007185  0.0014316284\n[101]  0.0015254401  0.0022723764  0.0015970270  0.0017374709  0.0012390277\n[106]  0.0023088992  0.0015844734  0.0036419925  0.0027985492  0.0016896556\n[111]  0.0014987136  0.0023533432  0.0015622461  0.0016558792  0.0032102343\n[116]  0.0026972636  0.0023602635  0.0030351603 -0.0001086606  0.0052256268\n[121]  0.0026226500  0.0036352615  0.0006190549  0.0015166402  0.0039291254\n[126]  0.0034453451  0.0039643445  0.0017804225  0.0028792256  0.0022712485\n[131]  0.0099898970  0.0030212613  0.0032262660  0.0020213949  0.0010562675\n[136]  0.0032321342  0.0039153757  0.0009302122  0.0012694927  0.0030620416\n[141]  0.0015087431  0.0026524340  0.0031113262  0.0011137531  0.0009529135\n[146]  0.0057971534  0.0020185609  0.0014165240  0.0011601128  0.0018690167\n[151]  0.0002739555  0.0020308004  0.0024475588  0.0023012673  0.0054896148\n[156]  0.0027341046  0.0054814411  0.0039287537  0.0003136276  0.0008936736\n[161]  0.0013837895  0.0010290410  0.0026527543  0.0013963689  0.0016641197\n[166]  0.0020441859  0.0018436149  0.0020429520  0.0026891737  0.0016339650\n[171]  0.0006784326  0.0012342956  0.0030615117  0.0026752971  0.0043732823\n[176]  0.0042678727  0.0021164188  0.0028531074  0.0061959601  0.0025752812\n[181]  0.0034217815  0.0016683349  0.0029223021  0.0012631594  0.0017579291\n[186]  0.0014594366  0.0007318894  0.0024549379  0.0038915971  0.0017056476\n[191]  0.0043928583  0.0027573638  0.0023826315  0.0034827462  0.0026894793\n[196]  0.0009209024  0.0018289489  0.0027080912  0.0029780971  0.0014506093\n\n# Plot the K-L divergence of each record after the final iteration\nplot(tsne_output$costs, type = \"l\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#reproducing-results",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#reproducing-results",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Reproducing results",
    "text": "Reproducing results\nt-SNE is a stochastic algorithm, meaning there is some randomness inherent to the process. To ensure reproducible results it is necessary to fix a seed before every new execution. This way, you can tune the algorithm hyper-parameters and isolate the effect of the randomness. In this exercise, the goal is to generate two embeddings and check that they are identical. The mnist_sample dataset is available in your workspace.\n\n# Generate a three-dimensional t-SNE embedding without PCA\ntsne_output <- Rtsne(mnist_sample[, -1], PCA = FALSE, dim = 3)\n\n# Generate a new t-SNE embedding with the same hyper-parameter values\ntsne_output_new <- Rtsne(mnist_sample[, -1], PCA = FALSE, dim = 3)\n\n# Check if the two outputs are identical\nidentical(tsne_output, tsne_output_new)\n\n[1] FALSE\n\n# Generate a three-dimensional t-SNE embedding without PCA\nset.seed(1234)\ntsne_output <- Rtsne(mnist_sample[, -1], PCA = FALSE, dims = 3)\n\n# Generate a new t-SNE embedding with the same hyper-parameter values\nset.seed(1234)\ntsne_output_new <- Rtsne(mnist_sample[, -1], PCA = FALSE, dims = 3)\n\n# Check if the two outputs are identical\nidentical(tsne_output, tsne_output_new)\n\n[1] TRUE"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#optimal-number-of-iterations",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#optimal-number-of-iterations",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Optimal number of iterations",
    "text": "Optimal number of iterations\nA common hyper-parameter to optimize in t-SNE is the optimal number of iterations. As you have seen before it is important to always use the same seed before you can compare different executions. To optimize the number of iterations, you can increase the max_iter parameter of Rtsne() and observe the returned itercosts to find the minimum K-L divergence. The mnist_sample dataset and the Rtsne package have been loaded for you.\n\n# Set seed to ensure reproducible results\nset.seed(1234)\n\n# Execute a t-SNE with 2000 iterations\ntsne_output <- Rtsne(mnist_sample[, -1], max_iter = 2000,PCA = TRUE, dims = 2)\n\n# Observe the output costs \ntsne_output$itercosts\n\n [1] 53.2661477 51.4609512 52.6459945 52.0146489 51.9524842  0.9740208\n [7]  0.7721006  0.7459989  0.7159438  0.7086349  0.7035461  0.6974368\n[13]  0.6951862  0.6965771  0.6956987  0.6958303  0.6860614  0.6845716\n[19]  0.6844257  0.6842720  0.6820150  0.6813309  0.6812841  0.6833337\n[25]  0.6834150  0.6823819  0.6822590  0.6829398  0.6819058  0.6823040\n[31]  0.6824528  0.6831008  0.6819460  0.6829194  0.6828687  0.6829107\n[37]  0.6821132  0.6813898  0.6818957  0.6809017\n\n# Get the 50th iteration with the minimum K-L cost\nwhich.min(tsne_output$itercosts)\n\n[1] 40"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#perplexity-of-mnist-sample",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#perplexity-of-mnist-sample",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Perplexity of MNIST sample",
    "text": "Perplexity of MNIST sample\nThe perplexity parameter indicates the balance between the local and global aspect of the input data. The parameter is an estimate of the number of close neighbors of each original point. Typical values of this parameter fall in the range of 5 to 50. We will generate three different t-SNE executions with the same number of iterations and perplexity values of 5, 20, and 50 and observe the differences in the K-L divergence costs. The optimal number of iterations we found in the last exercise (1200) will be used here. The mnist_sample dataset and the Rtsne package have been loaded for you.\n\n# Set seed to ensure reproducible results\npar(mfrow = c(3, 1))\nset.seed(1234)\n\nperp <- c(5, 20, 50)\nmodels <- list()\nfor (i in 1:length(perp)) {\n        \n        # Execute a t-SNE with perplexity 5\n        perplexity  = perp[i]\n        tsne_output <- Rtsne(mnist_sample[, -1], perplexity = perplexity, max_iter = 1300)\n        # Observe the returned K-L divergence costs at every 50th iteration\n        models[[i]] <- tsne_output\n        plot(tsne_output$itercosts,\n             main = paste(\"Perplexity\", perplexity),\n             type = \"l\", ylab = \"itercosts\")\n}\n\n\n\nnames(models) <- paste0(\"perplexity\",perp)"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#perplexity-of-bigger-mnist-dataset",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#perplexity-of-bigger-mnist-dataset",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Perplexity of bigger MNIST dataset",
    "text": "Perplexity of bigger MNIST dataset\nNow, let’s investigate the effect of the perplexity values with a bigger MNIST dataset of 10.000 records. It would take a lot of time to execute t-SNE for this many records on the DataCamp platform. This is why the pre-loaded output of two t-SNE embeddings with perplexity values of 5 and 50, named tsne_output_5 and tsne_output_50 are available in the workspace. We will look at the K-L costs and plot them using the digit label from the mnist_10k dataset, which is also available in the environment. The Rtsne and ggplot2 packages have been loaded.\n\nI used mnist smaller data set\n\n\n# Observe the K-L divergence costs with perplexity 5 and 50\ntsne_output_5 <- models$perplexity5\ntsne_output_50  <- models$perplexity50\n# Generate the data frame to visualize the embedding\ntsne_plot_5 <- data.frame(tsne_x = tsne_output_5$Y[, 1], tsne_y = tsne_output_5$Y[, 2], digit = as.factor(mnist_sample$label))\n\ntsne_plot_50 <- data.frame(tsne_x = tsne_output_50$Y[, 1], tsne_y = tsne_output_50$Y[, 2], digit = as.factor(mnist_sample$label))\n\n# Plot the obtained embeddings\nggplot(tsne_plot_5, aes(x = tsne_x, y = tsne_y, color = digit)) + \n    ggtitle(\"MNIST t-SNE with 1300 iter and Perplexity=5\") +\n    geom_text(aes(label = digit)) + \n    theme(legend.position=\"none\")\n\n\n\nggplot(tsne_plot_50, aes(x = tsne_x, y = tsne_y, color = digit)) + \n    ggtitle(\"MNIST t-SNE with 1300 iter and Perplexity=50\") + \n    geom_text(aes(label = digit)) + \n    theme(legend.position=\"none\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#plotting-spatial-distribution-of-true-classes",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#plotting-spatial-distribution-of-true-classes",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Plotting spatial distribution of true classes",
    "text": "Plotting spatial distribution of true classes\nAs seen in the video, you can use the obtained representation of t-SNE in a lower dimension space to classify new digits based on the Euclidean distance to known clusters of digits. For this task, let’s start with plotting the spatial distribution of the digit labels in the embedding space. You are going to use the output of a t-SNE execution of 10K MNIST records named tsne and the true labels can be found in a dataset named mnist_10k. In this exercise, you will use the first 5K records of tsne and mnist_10k datasets and the goal is to visualize the obtained t-SNE embedding. The ggplot2 package has been loaded for you.\n\nlibrary(data.table)\nmnist_10k <- readRDS(\"mnist_10k.rds\") %>% setDT()\ntsne <- Rtsne(mnist_10k[, -1], perplexity = 50, max_iter = 1500)\n# Prepare the data.frame\ntsne_plot <- data.frame(tsne_x = tsne$Y[1:5000, 1], \n                        tsne_y = tsne$Y[1:5000, 2], \n                        digit = as.factor(mnist_10k[1:5000, ]$label))\n\n# Plot the obtained embedding\nggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = digit)) + \n    ggtitle(\"MNIST embedding of the first 5K digits\") + \n    geom_text(aes(label = digit)) + \n    theme(legend.position=\"none\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-the-centroids-of-each-class",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-the-centroids-of-each-class",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Computing the centroids of each class",
    "text": "Computing the centroids of each class\nSince the previous visual representation of the digit in a low dimensional space makes sense, you want to compute the centroid of each class in this lower dimensional space. This centroid can be used as a prototype of the digit and you can classify new digits based on their Euclidean distance to these ones. The MNIST data mnist_10k and t-SNE output tsne are available in the workspace. The data.table package has been loaded for you.\n\n# Get the first 5K records and set the column names\ndt_prototypes <- as.data.table(tsne$Y[1:5000,])\nsetnames(dt_prototypes, c(\"X\",\"Y\"))\n\n# Paste the label column as factor\ndt_prototypes[, label := as.factor(mnist_10k[1:5000,]$label)]\n\n# Compute the centroids per label\ndt_prototypes[, mean_X := mean(X), by = label]\ndt_prototypes[, mean_Y := mean(Y), by = label]\n\n# Get the unique records per label\ndt_prototypes <- unique(dt_prototypes, by = \"label\")\ndt_prototypes\n\n             X          Y label     mean_X     mean_Y\n 1:  34.057637  16.347760     7  29.468898  13.368839\n 2:  15.930599   3.488559     4  22.629900  -4.840852\n 3:   7.778437  23.683282     1   1.100101  33.723745\n 4:   9.362583 -26.758608     6   3.611878 -27.625343\n 5: -10.993721  -6.656823     5  -5.993067  -8.892714\n 6:  -5.713590   7.008925     8  -8.308147   5.869606\n 7: -33.804912   2.023328     3 -25.422405  -1.426823\n 8: -29.043160  17.139066     2 -19.258460  21.030112\n 9: -19.477592 -28.485313     0 -18.196709 -30.347110\n10:  27.006914   2.946410     9  20.031918  -3.660056"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-similarities-of-digits-1-and-0",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-similarities-of-digits-1-and-0",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Computing similarities of digits 1 and 0",
    "text": "Computing similarities of digits 1 and 0\nOne way to measure the label similarity for each digit is by computing the Euclidean distance in the lower dimensional space obtained from the t-SNE algorithm. You need to use the previously calculated centroids stored in dt_prototypes and compute the Euclidean distance to the centroid of digit 1 for the last 5000 records from tsne and mnist_10k datasets that are labeled either as 1 or 0. Note that the last 5000 records of tsne were not used before. The MNIST data mnist_10k and t-SNE output tsne are available in the workspace. The data.table package has been loaded for you.\n\n# Store the last 5000 records in distances and set column names\ndistances <- as.data.table(tsne$Y[5001:10000,])\nsetnames(distances, c(\"X\", \"Y\"))\n# Paste the true label\ndistances[, label := mnist_10k[5001:10000,]$label]\ndistances[, mean_X := mean(X), by = label]\ndistances[, mean_Y := mean(Y), by = label]\n\n\n# Filter only those labels that are 1 or 0 \ndistances_filtered <- distances[label == 1 | label == 0]\n\n# Compute Euclidean distance to prototype of digit 1\ndistances_filtered[, dist_1 := sqrt( (X - dt_prototypes[label == 1,]$mean_X)^2 + \n                             (Y - dt_prototypes[label == 1,]$mean_Y)^2)]"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#plotting-similarities-of-digits-1-and-0",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#plotting-similarities-of-digits-1-and-0",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Plotting similarities of digits 1 and 0",
    "text": "Plotting similarities of digits 1 and 0\nIn distances, the distances of 1108 records to the centroid of digit 1 are stored in dist_1. Those records correspond to digits you already know are 1’s or 0’s. You can have a look at the basic statistics of the distances from records that you know are 0 and 1 (label column) to the centroid of class 1 using summary(). Also, if you plot a histogram of those distances and fill them with the label you can check if you are doing a good job identifying the two classes with this t-SNE classifier. The data.table and ggplot2 packages, as well as the distances object, have been loaded for you.\n\n# Compute the basic statistics of distances from records of class 1\nsummary(distances_filtered[label == 1]$dist_1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.003   6.542   8.499   8.978  11.493  55.041 \n\n# Compute the basic statistics of distances from records of class 0\nsummary(distances_filtered[label == 0]$dist_1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  49.63   62.94   66.27   66.95   70.98   76.45 \n\n# Plot the histogram of distances of each class\nggplot(distances_filtered, \n       aes(x = dist_1, fill = as.factor(label))) +\n    geom_histogram(binwidth = 5, alpha = .5, \n                   position = \"identity\", show.legend = FALSE) + \n    ggtitle(\"Distribution of Euclidean distance 1 vs 0\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#exploring-credit-card-fraud-dataset",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#exploring-credit-card-fraud-dataset",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Exploring credit card fraud dataset",
    "text": "Exploring credit card fraud dataset\nIn this exercise, you will do some data exploration on a sample of the credit card fraud detection dataset from Kaggle. For any problem, starting with some data exploration is a good practice and helps us better understand the characteristics of the data.\nThe credit card fraud dataset is already loaded in the environment as a data table with the name creditcard. As you saw in the video, it consists of 30 numerical variables. The Class column indicates if the transaction is fraudulent. The ggplot2 package has been loaded for you.\n\nload(\"creditcard.RData\") \n\nsetDT(creditcard)\n# Look at the data dimensions\ndim(creditcard)\n\n[1] 28923    31\n\n# Explore the column names\n#names(creditcard)\n\n# Explore the structure\n#str(creditcard)\n\n# Generate a summary\nsummary(creditcard)\n\n      Time              V1                  V2                  V3          \n Min.   :    26   Min.   :-56.40751   Min.   :-72.71573   Min.   :-31.1037  \n 1st Qu.: 54230   1st Qu.: -0.96058   1st Qu.: -0.58847   1st Qu.: -0.9353  \n Median : 84512   Median : -0.02400   Median :  0.08293   Median :  0.1659  \n Mean   : 94493   Mean   : -0.08501   Mean   :  0.05955   Mean   : -0.1021  \n 3rd Qu.:139052   3rd Qu.:  1.30262   3rd Qu.:  0.84003   3rd Qu.:  1.0119  \n Max.   :172788   Max.   :  2.41150   Max.   : 22.05773   Max.   :  3.8771  \n       V4                  V5                  V6           \n Min.   :-5.071241   Min.   :-31.35675   Min.   :-26.16051  \n 1st Qu.:-0.824978   1st Qu.: -0.70869   1st Qu.: -0.78792  \n Median : 0.007618   Median : -0.06071   Median : -0.28396  \n Mean   : 0.073391   Mean   : -0.04367   Mean   : -0.02722  \n 3rd Qu.: 0.789293   3rd Qu.:  0.61625   3rd Qu.:  0.37911  \n Max.   :16.491217   Max.   : 34.80167   Max.   : 20.37952  \n       V7                  V8                  V9           \n Min.   :-43.55724   Min.   :-50.42009   Min.   :-13.43407  \n 1st Qu.: -0.57404   1st Qu.: -0.21025   1st Qu.: -0.66974  \n Median :  0.02951   Median :  0.01960   Median : -0.06343  \n Mean   : -0.08873   Mean   : -0.00589   Mean   : -0.04295  \n 3rd Qu.:  0.57364   3rd Qu.:  0.33457   3rd Qu.:  0.58734  \n Max.   : 29.20587   Max.   : 20.00721   Max.   :  8.95567  \n      V10                 V11                V12                V13           \n Min.   :-24.58826   Min.   :-4.11026   Min.   :-18.6837   Min.   :-3.844974  \n 1st Qu.: -0.54827   1st Qu.:-0.75404   1st Qu.: -0.4365   1st Qu.:-0.661168  \n Median : -0.09843   Median :-0.01036   Median :  0.1223   Median :-0.009685  \n Mean   : -0.08468   Mean   : 0.06093   Mean   : -0.0943   Mean   :-0.002110  \n 3rd Qu.:  0.44762   3rd Qu.: 0.77394   3rd Qu.:  0.6172   3rd Qu.: 0.664794  \n Max.   : 15.33174   Max.   :12.01891   Max.   :  4.8465   Max.   : 4.569009  \n      V14                 V15                 V16                 V17          \n Min.   :-19.21432   Min.   :-4.498945   Min.   :-14.12985   Min.   :-25.1628  \n 1st Qu.: -0.44507   1st Qu.:-0.595272   1st Qu.: -0.48770   1st Qu.: -0.4951  \n Median :  0.04865   Median : 0.045992   Median :  0.05736   Median : -0.0742  \n Mean   : -0.09653   Mean   :-0.007251   Mean   : -0.06186   Mean   : -0.1046  \n 3rd Qu.:  0.48765   3rd Qu.: 0.646584   3rd Qu.:  0.52147   3rd Qu.:  0.3956  \n Max.   :  7.75460   Max.   : 5.784514   Max.   :  5.99826   Max.   :  7.2150  \n      V18                V19                 V20            \n Min.   :-9.49875   Min.   :-4.395283   Min.   :-20.097918  \n 1st Qu.:-0.51916   1st Qu.:-0.462158   1st Qu.: -0.211663  \n Median :-0.01595   Median : 0.010494   Median : -0.059160  \n Mean   :-0.04344   Mean   : 0.009424   Mean   :  0.006943  \n 3rd Qu.: 0.48634   3rd Qu.: 0.471172   3rd Qu.:  0.141272  \n Max.   : 3.88618   Max.   : 5.228342   Max.   : 24.133894  \n      V21                  V22                 V23           \n Min.   :-22.889347   Min.   :-8.887017   Min.   :-36.66600  \n 1st Qu.: -0.230393   1st Qu.:-0.550210   1st Qu.: -0.16093  \n Median : -0.028097   Median :-0.000187   Median : -0.00756  \n Mean   :  0.004995   Mean   :-0.006271   Mean   :  0.00418  \n 3rd Qu.:  0.190465   3rd Qu.: 0.516596   3rd Qu.:  0.15509  \n Max.   : 27.202839   Max.   : 8.361985   Max.   : 13.65946  \n      V24                 V25                 V26           \n Min.   :-2.822684   Min.   :-6.712624   Min.   :-1.658162  \n 1st Qu.:-0.354367   1st Qu.:-0.319410   1st Qu.:-0.328496  \n Median : 0.038722   Median : 0.011815   Median :-0.054131  \n Mean   : 0.000741   Mean   :-0.002847   Mean   :-0.002546  \n 3rd Qu.: 0.440797   3rd Qu.: 0.351797   3rd Qu.: 0.237782  \n Max.   : 3.962197   Max.   : 5.376595   Max.   : 3.119295  \n      V27                 V28               Amount            Class          \n Min.   :-8.358317   Min.   :-8.46461   Min.   :    0.00   Length:28923      \n 1st Qu.:-0.071275   1st Qu.:-0.05424   1st Qu.:    5.49   Class :character  \n Median : 0.002727   Median : 0.01148   Median :   22.19   Mode  :character  \n Mean   :-0.000501   Mean   : 0.00087   Mean   :   87.90                     \n 3rd Qu.: 0.095974   3rd Qu.: 0.08238   3rd Qu.:   78.73                     \n Max.   : 7.994762   Max.   :33.84781   Max.   :11898.09                     \n\n# Plot a histogram of the transaction time\nggplot(creditcard, aes(x = Time)) + \n    geom_histogram()"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#generating-training-and-test-sets",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#generating-training-and-test-sets",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Generating training and test sets",
    "text": "Generating training and test sets\nBefore we can apply the t-SNE algorithm to perform a dimensionality reduction, we need to split the original data into a training and test set. Next, we will perform an under-sampling of the majority class and generate a balanced training set. Generating a balanced dataset is a good practice when we are using tree-based models. In this exercise you already have the creditcard dataset loaded in the environment. The ggplot2 and data.table packages are already loaded.\n\n# Extract positive and negative instances of fraud\ncreditcard_pos <- creditcard[Class == 1]\ncreditcard_neg <- creditcard[Class == 0]\n\n# Fix the seed\nset.seed(1234)\n\n# Create a new negative balanced dataset by undersampling\ncreditcard_neg_bal <- creditcard_neg[sample(1:nrow(creditcard_neg), nrow(creditcard_pos))]\n\n# Generate a balanced train set\ncreditcard_train <- rbind(creditcard_pos, creditcard_neg_bal)"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-original-features",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-original-features",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Training a random forest with original features",
    "text": "Training a random forest with original features\nIn this exercise, we are going to train a random forest model using the original features from the credit card dataset. The goal is to detect new fraud instances in the future and we are doing that by learning the patterns of fraud instances in the balanced training set. Remember that a random forest can be trained with the following piece of code: randomForest(x = features, y = label, ntree = 100) The only pre-processing that has been done to the original features was to scale the Time and Amount variables. You have the balanced training dataset available in the environment as creditcard_train. The randomForest package has been loaded.\n\n# Fix the seed\nset.seed(1234)\nlibrary(randomForest)\n# Separate x and y sets\ntrain_x <- creditcard_train[,-31]\ntrain_y <- creditcard_train$Class %>% as.factor()\n\n# Train a random forests\nrf_model <- randomForest(x = train_x, y = train_y, ntree = 100)\n\n# Plot the error evolution and variable importance\nplot(rf_model, main = \"Error evolution vs number of trees\")\n# Fix the seed\nset.seed(1234)\n\n# Separate x and y sets\ntrain_x <- creditcard_train[,-31]\ntrain_y <- creditcard_train$Class %>%as.factor()\n\n# Train a random forests\nrf_model <- randomForest(x = train_x, y = train_y, ntree = 100)\n\n# Plot the error evolution and variable importance\nplot(rf_model, main = \"Error evolution vs number of trees\")\nlegend(\"topright\", colnames(rf_model$err.rate),col=1:3,cex=0.8,fill=1:3)\n\n\n\nvarImpPlot(rf_model, main = \"Variable importance\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-and-visualising-the-t-sne-embedding",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-and-visualising-the-t-sne-embedding",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Computing and visualising the t-SNE embedding",
    "text": "Computing and visualising the t-SNE embedding\nIn this exercise, we are going to generate a t-SNE embedding using only the balanced training set creditcard_train. The idea is to train a random forest using the two coordinates of the generated embedding instead of the original 30 dimensions. Due to computational restrictions, we are going to compute the embedding of the training data only, but note that in order to generate predictions from the test set we should compute the embedding of the test set together with the train set. Then, we will visualize the obtained embedding highlighting the two classes in order to clarify if we can differentiate between fraud and non-fraud transactions. The creditcard_train data, as well as the Rtsne and ggplot2 packages, have been loaded.\n\n# Set the seed\n#set.seed(1234)\n\n# Generate the t-SNE embedding \ncreditcard_train[, Time := scale(Time)]\nnms <- names(creditcard_train)\npred_nms <- nms[nms != \"Class\"]\nrange01 <- function(x){(x-min(x))/(max(x)-min(x))}\ncreditcard_train[, (pred_nms) := lapply(.SD ,range01), .SDcols = pred_nms]\n\ntsne_output <- Rtsne(as.matrix(creditcard_train[, -31]), check_duplicates = FALSE, PCA = FALSE)\n\n# Generate a data frame to plot the result\ntsne_plot <- data.frame(tsne_x = tsne_output$Y[,1],\n                        tsne_y = tsne_output$Y[,2],\n                        Class = creditcard_train$Class)\n\n# Plot the embedding usign ggplot and the label\nggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = factor(Class))) + \n  ggtitle(\"t-SNE of credit card fraud train set\") + \n  geom_text(aes(label = Class)) + theme(legend.position = \"none\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-embedding-features",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-embedding-features",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Training a random forest with embedding features",
    "text": "Training a random forest with embedding features\nIn this exercise, we are going to train a random forest model using the embedding features from the previous t-SNE embedding. So, in this case, we are going to use a two-dimensional dataset that has been generated from the original input features. In the rest of the chapter, we are going to verify if we have a worse, similar, or better performance for this model in comparison to the random forest trained with the original features. In the environment two objects named train_tsne_x and train_tsne_y that contain the features and the Class variable are available. The randomForest package has been loaded as well.\n\n# Fix the seed\nset.seed(1234)\ntrain_tsne_x <- tsne_output$Y\n# Train a random forest\nrf_model_tsne <- randomForest(x = train_tsne_x, y = train_y, ntree = 100)\n\n# Plot the error evolution\n\nplot(rf_model_tsne)\n\n\n\n# Plot the variable importance\nvarImpPlot(rf_model_tsne)"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#predicting-data-using-original-features",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#predicting-data-using-original-features",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Predicting data using original features",
    "text": "Predicting data using original features\nIn this exercise, we are using the random forest trained with the original features and generate predictions using the test set. These predictions will be plotted to see the distribution and will be evaluated using the ROCR package by considering the area under the curve.\nThe random forest model, named rf_model, and the test set, named creditcard_test, are available in the environment. The randomForest and ROCR packages have been loaded for you\n\n# Predict on the test set using the random forest \ncreditcard_test <- creditcard\npred_rf <- predict(rf_model, creditcard_test, type = \"prob\")\n\n# Plot a probability distibution of the target class\nhist(pred_rf[,2])\n\n\n\nlibrary(ROCR)\n# Compute the area under the curve\npred <-  prediction(pred_rf[,2], creditcard_test$Class)\nperf <- performance(pred, measure = \"auc\") \nperf@y.values\n\n[[1]]\n[1] 0.9995958"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#predicting-data-using-embedding-random-forest",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#predicting-data-using-embedding-random-forest",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Predicting data using embedding random forest",
    "text": "Predicting data using embedding random forest\nNow, we are going to do the same analysis, but instead of using the random forest trained with the original features, we will make predictions using the random forest trained with the t-SNE embedding coordinates. The random forest model is pre-loaded in an object named rf_model_tsne and the t-SNE embedding features from the original test set are stored in the object test_x. Finally, the test set labels are stored in creditcard_test. The randomForest and ROCR packages have been loaded for you.\n\ncreditcard_test[, (pred_nms) := lapply(.SD ,range01), .SDcols = pred_nms]\n\ntsne_output <- Rtsne(as.matrix(creditcard_test[, -31]), check_duplicates = FALSE, PCA = FALSE)\n\ntest_x <- tsne_output$Y\n# Predict on the test set using the random forest generated with t-SNE features\npred_rf <- predict(rf_model_tsne, test_x, type = \"prob\")\n\n# Plot a probability distibution of the target class\nhist(pred_rf[, 2])\n\n\n\n# Compute the area under the curve\npred <- prediction(pred_rf[, 2] , creditcard_test$Class)\nperf <- performance(pred, measure = \"auc\") \nperf@y.values\n\n[[1]]\n[1] 0.3765589"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#exploring-neural-network-layer-output",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#exploring-neural-network-layer-output",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Exploring neural network layer output",
    "text": "Exploring neural network layer output\nIn this exercise, we will have a look at the data that is being generated in a specific layer of a neural network. In particular, this data corresponds to the third layer, composed of 128 neurons, of a neural network trained with the balanced credit card fraud dataset generated before. The goal of the exercise is to perform an exploratory data analysis.\n\n# Observe the dimensions\n#dim(layer_128_train)\n\n# Show the first six records of the last ten columns\n#head(layer_128_train[, 119:128])\n\n# Generate a summary of all columns\n#summary(layer_128_train)"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#using-t-sne-to-visualise-a-neural-network-layer",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#using-t-sne-to-visualise-a-neural-network-layer",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Using t-SNE to visualise a neural network layer",
    "text": "Using t-SNE to visualise a neural network layer\nNow, we would like to visualize the patterns obtained from the neural network model, in particular from the last layer of the neural network. As we mentioned before this last layer has 128 neurons and we have pre-loaded the weights of these neurons in an object named layer_128_train. The goal is to compute a t-SNE embedding using the output of the neurons from this last layer and visualize the embedding colored according to the class. The Rtsne and ggplot2 packages as well as the layer_128_train and creditcard_train have been loaded for you\n\n# Set the seed\nset.seed(1234)\n\n# Generate the t-SNE\n#tsne_output <- Rtsne(as.matrix(layer_128_train), check_duplicates = FALSE, max_iter = 400, perplexity = 50)\n\n# Prepare data.frame\n#tsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], \n#                        Class = creditcard_train$Class)\n\n# Plot the data \n# ggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = Class)) + \n#   geom_point() + \n#   ggtitle(\"Credit card embedding of Last Neural Network Layer\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#using-t-sne-to-visualise-a-neural-network-layer-1",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#using-t-sne-to-visualise-a-neural-network-layer-1",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Using t-SNE to visualise a neural network layer",
    "text": "Using t-SNE to visualise a neural network layer\nNow, we would like to visualize the patterns obtained from the neural network model, in particular from the last layer of the neural network. As we mentioned before this last layer has 128 neurons and we have pre-loaded the weights of these neurons in an object named layer_128_train. The goal is to compute a t-SNE embedding using the output of the neurons from this last layer and visualize the embedding colored according to the class. The Rtsne and ggplot2 packages as well as the layer_128_train and creditcard_train have been loaded for you\n\n# Set the seed\n# set.seed(1234)\n# \n# # Generate the t-SNE\n# tsne_output <- Rtsne(as.matrix(layer_128_train), check_duplicates = FALSE, max_iter = 400, perplexity = 50)\n# \n# # Prepare data.frame\n# tsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], \n#                         Class = creditcard_train$Class)\n# \n# # Plot the data \n# ggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = Class)) + \n#   geom_point() + \n#   ggtitle(\"Credit card embedding of Last Neural Network Layer\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#exploring-fashion-mnist",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#exploring-fashion-mnist",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Exploring fashion MNIST",
    "text": "Exploring fashion MNIST\nThe Fashion MNIST dataset contains grayscale images of 10 clothing categories. The first thing to do when you are analyzing a new dataset is to perform an exploratory data analysis in order to understand the data. A sample of the fashion MNIST dataset fashion_mnist, with only 500 records, is pre-loaded for you.\n\nlibrary(data.table)\n#load(\"fashion_mnist_500.RData\")\nload(\"fashion_mnist.rda\")\nset.seed(100)\n\nind <- sample(1:nrow(fashion_mnist), 1000)\n\nfashion_mnist <- fashion_mnist[ind, ]\n# Show the dimensions\ndim(fashion_mnist)\n\n[1] 1000  785\n\n# Create a summary of the last six columns \nsummary(fashion_mnist[, 780:785])\n\n    pixel779         pixel780         pixel781        pixel782      \n Min.   :  0.00   Min.   :  0.00   Min.   :  0.0   Min.   :  0.000  \n 1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:  0.0   1st Qu.:  0.000  \n Median :  0.00   Median :  0.00   Median :  0.0   Median :  0.000  \n Mean   : 22.16   Mean   : 18.64   Mean   : 10.6   Mean   :  3.781  \n 3rd Qu.:  1.00   3rd Qu.:  0.00   3rd Qu.:  0.0   3rd Qu.:  0.000  \n Max.   :236.00   Max.   :255.00   Max.   :231.0   Max.   :188.000  \n    pixel783          pixel784     \n Min.   :  0.000   Min.   : 0.000  \n 1st Qu.:  0.000   1st Qu.: 0.000  \n Median :  0.000   Median : 0.000  \n Mean   :  0.934   Mean   : 0.043  \n 3rd Qu.:  0.000   3rd Qu.: 0.000  \n Max.   :147.000   Max.   :39.000  \n\n# Table with the class distribution\ntable(fashion_mnist$label)\n\n\n  0   1   2   3   4   5   6   7   8   9 \n100  94 115  90  97  98  97 105 102 102"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#visualizing-fashion-mnist",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#visualizing-fashion-mnist",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Visualizing fashion MNIST",
    "text": "Visualizing fashion MNIST\nIn this exercise, we are going to visualize an example image of the fashion MNIST dataset. Basically, we are going to plot the 28x28 pixels values. To do this we use:\nA custom ggplot theme named plot_theme. A data structure named xy_axis where the pixels values are stored. A character vector named class_names with the names of each class. The fashion_mnist dataset with 500 examples is available in the workspace. The `ggplot2 package is loaded. Note that you can access the definition of the custom theme by typing plot_theme in the console.\n\nlibrary(tidyverse)\nplot_theme <- list(\n    raster = geom_raster(hjust = 0, vjust = 0),\n    gradient_fill = scale_fill_gradient(low = \"white\",\n                                        high = \"black\", guide = FALSE),\n    theme = theme(axis.line = element_blank(),\n                  axis.text = element_blank(),\n                  axis.ticks = element_blank(),\n                  axis.title = element_blank(),\n                  panel.background = element_blank(),\n                  panel.border = element_blank(),\n                  panel.grid.major = element_blank(),\n                  panel.grid.minor = element_blank(),\n                  plot.background = element_blank()))\n\n\nclass_names <-  c(\"T-shirt/top\", \"Trouser\", \"Pullover\", \n                  \"Dress\", \"Coat\", \"Sandal\", \"Shirt\",\n                  \"Sneaker\", \"Bag\", \"Ankle\", \"boot\")\n\n\nxy_axis <- data.frame(x = expand.grid(1:28, 28:1)[,1],\n                      y = expand.grid(1:28, 28:1)[,2])\n\n# Get the data from the last image\nplot_data <- cbind(xy_axis, fill = as.data.frame(t(fashion_mnist[500, -1]))[,1])\n\n# Observe the first records\nhead(plot_data)\n\n  x  y fill\n1 1 28    0\n2 2 28    0\n3 3 28    0\n4 4 28    0\n5 5 28    0\n6 6 28    0\n\n# Plot the image using ggplot()\nggplot(plot_data, aes(x, y, fill = fill)) + \n  ggtitle(class_names[as.integer(fashion_mnist[500, 1])]) + \n  plot_theme"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#reducing-data-with-glrm",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#reducing-data-with-glrm",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Reducing data with GLRM",
    "text": "Reducing data with GLRM\nWe are going to reduce the dimensionality of the fashion MNIST sample data using the GLRM implementation of h2o. In order to do this, in the next steps we are going to: Start a connection to a h2o cluster by invoking the method h2o.init(). Store the fashion_mnist data into the h2o cluster with as.h2o(). Launch a GLRM model with K=2 (rank-2 model) using the h2o.glrm() function. As we have discussed in the video session, it is important to check the convergence of the objective function. Note that here we are also fixing the seed to ensure the same results. The h2o package and fashion_mnist data are pre-loaded in the environment.\n\nlibrary(h2o)\n# Start a connection with the h2o cluster\nh2o.init()\n\n Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         5 hours 6 minutes \n    H2O cluster timezone:       Africa/Nairobi \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.40.0.1 \n    H2O cluster version age:    17 days \n    H2O cluster name:           H2O_started_from_R_mburu_cvk380 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   3.83 GB \n    H2O cluster total cores:    8 \n    H2O cluster allowed cores:  8 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.2.2 Patched (2022-11-10 r83330) \n\n# Store the data into h2o cluster\nfashion_mnist.hex <- as.h2o(fashion_mnist, \"fashion_mnist.hex\")\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Launch a GLRM model over fashion_mnist data\nmodel_glrm <- h2o.glrm(training_frame = fashion_mnist.hex,\n                       cols = 2:ncol(fashion_mnist), \n                       k = 2,\n                       seed = 123,\n                       max_iterations = 2100)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |======================================================================| 100%\n\n# Plotting the convergence\nplot(model_glrm)"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#improving-model-convergence",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#improving-model-convergence",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Improving model convergence",
    "text": "Improving model convergence\nIn the previous exercise, we didn’t get good convergence values for the GLRM model. Improving convergence values can sometimes be achieved by applying a transformation to the input data. In this exercise, we are going to normalize the input data before we start building the GLRM model. This can be achieved by setting the transform parameter of h2o.glrm() equal to “NORMALIZE”. The h2o package and fashion_mnist dataset are pre-loaded.\n\n# Start a connection with the h2o cluster\n#h2o.init()\n\n# Store the data into h2o cluster\n#fashion_mnist.hex <- as.h2o(fashion_mnist, \"fashion_mnist.hex\")\n\n# Launch a GLRM model with normalized fashion_mnist data  \nmodel_glrm <- h2o.glrm(training_frame = fashion_mnist.hex, \n                       transform = \"NORMALIZE\",\n                       cols = 2:ncol(fashion_mnist), \n                       k = 2, \n                       seed = 123,\n                       max_iterations = 2100)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |======================================================================| 100%\n\n# Plotting the convergence\nplot(model_glrm)"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#visualizing-the-output-of-glrm",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#visualizing-the-output-of-glrm",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Visualizing the output of GLRM",
    "text": "Visualizing the output of GLRM\nA GLRM model generates the X and Y matrixes. In this exercise, we are going to visualize the obtained low-dimensional representation of the input records in the new K-dimensional space. The output of the X matrix from the previous GLRM model has been loaded with the name X_matrix. This matrix has been obtained by calling:\n\nX_matrix <- as.data.table(h2o.getFrame(model_glrm@model$representation_name))\n# Dimension of X_matrix\ndim(X_matrix)\n\n[1] 1000    2\n\n# First records of X_matrix\nhead(X_matrix)\n\n        Arch1       Arch2\n1: -1.4562898  0.28620951\n2:  0.7132191  1.18922464\n3:  0.5600450 -1.29628758\n4:  0.9997013 -0.51894405\n5:  1.3377989 -0.05616662\n6:  1.0687898  0.07447071\n\n# Plot the records in the new two dimensional space\nggplot(as.data.table(X_matrix), aes(x= Arch1, y = Arch2, color =  fashion_mnist$label)) + \n    ggtitle(\"Fashion Mnist GLRM Archetypes\") + \n    geom_text(aes(label =  fashion_mnist$label)) + \n    theme(legend.position=\"none\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#visualizing-the-prototypes",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#visualizing-the-prototypes",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Visualizing the prototypes",
    "text": "Visualizing the prototypes\nNow, we are going to compute the centroids of the coordinates for each of the two archetypes for each label. We did something similar before for the t-SNE embedding. The goal is to have a representation or prototype of each label in this new two-dimensional space.\nThe ggplot2 and data.table packages are pre-loaded, as well as the X_matrix object and the fashion_mnist dataset.\n\n# Store the label of each record and compute the centroids\nX_matrix[, label := as.numeric(fashion_mnist$label)]\nX_matrix[, mean_x := mean(Arch1), by = label]\nX_matrix[, mean_y := mean(Arch2), by = label]\n\n# Get one record per label and create a vector with class names\nX_mean <- unique(X_matrix, by = \"label\")\n\nlabel_names <- c(\"T-shirt/top\", \"Trouser\", \"Pullover\",\n                 \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \n                 \"Sneaker\", \"Bag\", \"Ankle boot\")\n\n# Plot the centroids\nX_mean[, label := factor(label, levels = 0:9, labels = label_names)]\nggplot(X_mean, aes(x = mean_x, y = mean_y, color = label_names)) + \n    ggtitle(\"Fashion Mnist GLRM class centroids\") + \n    geom_text(aes(label = label_names)) +\n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#imputing-missing-data",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#imputing-missing-data",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Imputing missing data",
    "text": "Imputing missing data\nIn this exercise, we will use GLRM to impute missing data. We are going to build a GLRM model from a dataset named fashion_mnist_miss, where 20% of values are missing. The goal is to fill these values by making a prediction using h2o.predict() with the GLRM model. In this exercise an h2o instance is already running, so it is not necessary to call h2o.init(). The h2o package and fashion_mnist_miss have been loaded\n\nfashion_mnist_miss <- h2o.insertMissingValues(fashion_mnist.hex, \n                                              fraction = 0.2, seed = 1234)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Store the input data in h2o\nfashion_mnist_miss.hex <- as.h2o(fashion_mnist_miss, \"fashion_mnist_miss.hex\")\n\n# Build a GLRM model\nmodel_glrm <- h2o.glrm(training_frame = fashion_mnist_miss.hex,\n                       k = 2,\n                       transform = \"NORMALIZE\",\n                       max_iterations = 100)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |======================================================================| 100%\n\n# Impute missing values\nfashion_pred <- predict(model_glrm, fashion_mnist_miss.hex)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Observe the statistics of the first 5 pixels\nsummary(fashion_pred[, 1:5])\n\n reconstr_label     reconstr_pixel2      reconstr_pixel3     \n Min.   :-0.47688   Min.   :-0.0014943   Min.   :-0.0048400  \n 1st Qu.:-0.26582   1st Qu.:-0.0008671   1st Qu.:-0.0023056  \n Median :-0.10121   Median :-0.0003521   Median :-0.0003533  \n Mean   :-0.08643   Mean   :-0.0003016   Mean   :-0.0002199  \n 3rd Qu.: 0.10129   3rd Qu.: 0.0001902   3rd Qu.: 0.0018372  \n Max.   : 0.33802   Max.   : 0.0015396   Max.   : 0.0057419  \n reconstr_pixel4      reconstr_pixel5     \n Min.   :-0.0057791   Min.   :-0.0034649  \n 1st Qu.:-0.0032436   1st Qu.:-0.0018371  \n Median :-0.0014309   Median : 0.0003392  \n Mean   :-0.0012507   Mean   : 0.0001963  \n 3rd Qu.: 0.0007319   3rd Qu.: 0.0019750  \n Max.   : 0.0056037   Max.   : 0.0044852"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-original-data",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-original-data",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Training a random forest with original data",
    "text": "Training a random forest with original data\nIn this exercise, we are going to train a random forest using the original fashion MNIST dataset with 500 examples. This dataset is preloaded in the environment with the name fashion_mnist. We are going to train a random forest with 20 trees and we will look at the time it takes to compute the model and the out-of-bag error in the 20th tree. The randomForest package is loaded.\n\n# Get the starting timestamp\nlibrary(randomForest)\n\ntime_start <- proc.time()\n\n# Train the random forest\nfashion_mnist[, label := factor(label)]\nrf_model <- randomForest(label~., ntree = 20,\n                         data = fashion_mnist)\n\n# Get the end timestamp\ntime_end <- timetaken(time_start)\n\n# Show the error and the time\nrf_model$err.rate[20]\n\n[1] 0.26\n\ntime_end\n\n[1] \"0.776s elapsed (0.772s cpu)\""
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-compressed-data",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-compressed-data",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Training a random forest with compressed data",
    "text": "Training a random forest with compressed data\nNow, we are going to train a random forest using a compressed representation of the previous 500 input records, using only 8 dimensions!\nIn this exercise, you a dataset named train_x that contains the compressed training data and another one named train_y that contains the labels are pre-loaded. We are going to calculate computation time and accuracy, similar to what was done in the previous exercise. Since the dimensionality of this dataset is much smaller, we can train a random forest using 500 trees in less time. The randomForest package is already loaded.\n\nmodel_glrm <- h2o.glrm(training_frame = fashion_mnist.hex, \n                       transform = \"NORMALIZE\",\n                       cols = 2:ncol(fashion_mnist), \n                       k = 8, \n                       seed = 123,\n                       max_iterations = 1000)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |======================================================================| 100%\n\ntrain_x <- as.data.table(h2o.getFrame(model_glrm@model$representation_name))\ntrain_y <- fashion_mnist$label %>% as.factor()\n\n\nlibrary(randomForest)\n# Get the starting timestamp\ntime_start <- proc.time()\n\n# Train the random forest\nrf_model <- randomForest(x = train_x, y = train_y, ntree = 500)\n\n# Get the end timestamp\ntime_end <- timetaken(time_start)\n\n\n# Show the error and the time\nrf_model$err.rate[500]\n\n[1] 0.252\n\ntime_end\n\n[1] \"0.293s elapsed (0.294s cpu)\""
  },
  {
    "objectID": "notes/iml/intro.html",
    "href": "notes/iml/intro.html",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "",
    "text": "Code is from here\nMachine learning models usually perform really well for predictions, but are not interpretable. The iml package provides tools for analysing any black box machine learning model:\nThis document shows you how to use the iml package to analyse machine learning models.\nIf you want to learn more about the technical details of all the methods, read chapters from: https://christophm.github.io/interpretable-ml-book/agnostic.html"
  },
  {
    "objectID": "notes/iml/intro.html#data-boston-housing",
    "href": "notes/iml/intro.html#data-boston-housing",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Data: Boston Housing",
    "text": "Data: Boston Housing\nWe’ll use the MASS::Boston dataset to demonstrate the abilities of the iml package. This dataset contains median house values from Boston neighbourhoods.\n\ndata(\"Boston\", package = \"MASS\")\nhead(Boston)\n\n#>      crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n#> 1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n#> 2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n#> 3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n#> 4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n#> 5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n#> 6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n#>   medv\n#> 1 24.0\n#> 2 21.6\n#> 3 34.7\n#> 4 33.4\n#> 5 36.2\n#> 6 28.7"
  },
  {
    "objectID": "notes/iml/intro.html#fitting-the-machine-learning-model",
    "href": "notes/iml/intro.html#fitting-the-machine-learning-model",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Fitting the machine learning model",
    "text": "Fitting the machine learning model\nFirst we train a randomForest to predict the Boston median housing value:\n\nset.seed(42)\nlibrary(\"iml\")\nlibrary(\"randomForest\")\ndata(\"Boston\", package = \"MASS\")\nrf <- randomForest(medv ~ ., data = Boston, ntree = 50)"
  },
  {
    "objectID": "notes/iml/intro.html#using-the-iml-predictor-container",
    "href": "notes/iml/intro.html#using-the-iml-predictor-container",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Using the iml Predictor() container",
    "text": "Using the iml Predictor() container\nWe create a Predictor object, that holds the model and the data. The iml package uses R6 classes: New objects can be created by calling Predictor$new().\n\nX <- Boston[which(names(Boston) != \"medv\")]\npredictor <- Predictor$new(rf, data = X, y = Boston$medv)"
  },
  {
    "objectID": "notes/iml/intro.html#feature-importance",
    "href": "notes/iml/intro.html#feature-importance",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Feature importance",
    "text": "Feature importance\nWe can measure how important each feature was for the predictions with FeatureImp. The feature importance measure works by shuffling each feature and measuring how much the performance drops. For this regression task we choose to measure the loss in performance with the mean absolute error (‘mae’), another choice would be the mean squared error (‘mse’).\nOnce we create a new object of FeatureImp, the importance is automatically computed. We can call the plot() function of the object or look at the results in a data.frame.\n\nimp <- FeatureImp$new(predictor, loss = \"mae\")\nlibrary(\"ggplot2\")\nplot(imp)\n\n\n\n\n\n\n\nimp$results\n\n#>    feature importance.05 importance importance.95 permutation.error\n#> 1    lstat      4.394480   4.459282      4.740041          4.394662\n#> 2       rm      3.349928   3.620192      3.715445          3.567732\n#> 3      nox      1.772047   1.796058      1.833039          1.770032\n#> 4     crim      1.661024   1.703701      1.738660          1.679013\n#> 5      dis      1.670726   1.690742      1.694655          1.666242\n#> 6  ptratio      1.423169   1.426636      1.443533          1.405963\n#> 7    indus      1.399432   1.416183      1.434735          1.395661\n#> 8      age      1.346887   1.387961      1.423804          1.367848\n#> 9      tax      1.352382   1.376506      1.384641          1.356559\n#> 10   black      1.227135   1.234735      1.235719          1.216842\n#> 11     rad      1.097253   1.109357      1.131675          1.093281\n#> 12      zn      1.035018   1.042360      1.043935          1.027256\n#> 13    chas      1.032211   1.035745      1.047224          1.020736"
  },
  {
    "objectID": "notes/iml/intro.html#feature-effects",
    "href": "notes/iml/intro.html#feature-effects",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Feature effects",
    "text": "Feature effects\nBesides knowing which features were important, we are interested in how the features influence the predicted outcome. The FeatureEffect class implements accumulated local effect plots, partial dependence plots and individual conditional expectation curves. The following plot shows the accumulated local effects (ALE) for the feature ‘lstat’. ALE shows how the prediction changes locally, when the feature is varied. The marks on the x-axis indicates the distribution of the ‘lstat’ feature, showing how relevant a region is for interpretation (little or no points mean that we should not over-interpret this region).\n\nale <- FeatureEffect$new(predictor, feature = \"lstat\")\nale$plot()\n\n\n\n\n\n\n\n\nIf we want to compute the partial dependence curves on another feature, we can simply reset the feature:\n\nale$set.feature(\"rm\")\nale$plot()"
  },
  {
    "objectID": "notes/iml/intro.html#measure-interactions",
    "href": "notes/iml/intro.html#measure-interactions",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Measure interactions",
    "text": "Measure interactions\nWe can also measure how strongly features interact with each other. The interaction measure regards how much of the variance of \\(f(x)\\) is explained by the interaction. The measure is between 0 (no interaction) and 1 (= 100% of variance of \\(f(x)\\) due to interactions). For each feature, we measure how much they interact with any other feature:\n\ninteract <- Interaction$new(predictor)\n\n#> \n#> Attaching package: 'withr'\n\n\n#> The following objects are masked from 'package:rlang':\n#> \n#>     local_options, with_options\n\n\n#> The following object is masked from 'package:tools':\n#> \n#>     makevars_user\n\nplot(interact)\n\n\n\n\n\n\n\n\nWe can also specify a feature and measure all it’s 2-way interactions with all other features:\n\ninteract <- Interaction$new(predictor, feature = \"crim\")\nplot(interact)\n\n\n\n\n\n\n\n\nYou can also plot the feature effects for all features at once:\n\neffs <- FeatureEffects$new(predictor)\nplot(effs)"
  },
  {
    "objectID": "notes/iml/intro.html#surrogate-model",
    "href": "notes/iml/intro.html#surrogate-model",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Surrogate model",
    "text": "Surrogate model\nAnother way to make the models more interpretable is to replace the black box with a simpler model - a decision tree. We take the predictions of the black box model (in our case the random forest) and train a decision tree on the original features and the predicted outcome. The plot shows the terminal nodes of the fitted tree. The maxdepth parameter controls how deep the tree can grow and therefore how interpretable it is.\n\ntree <- TreeSurrogate$new(predictor, maxdepth = 2)\n\n#> Loading required package: partykit\n\n\n#> Loading required package: libcoin\n\n\n#> Loading required package: mvtnorm\n\nplot(tree)\n\n\n\n\n\n\n\n\nWe can use the tree to make predictions:\n\nhead(tree$predict(Boston))\n\n#> Warning in self$predictor$data$match_cols(data.frame(newdata)): Dropping\n#> additional columns: medv\n\n\n#>     .y.hat\n#> 1 27.09989\n#> 2 27.09989\n#> 3 27.09989\n#> 4 27.09989\n#> 5 27.09989\n#> 6 27.09989"
  },
  {
    "objectID": "notes/iml/intro.html#explain-single-predictions-with-a-local-model",
    "href": "notes/iml/intro.html#explain-single-predictions-with-a-local-model",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Explain single predictions with a local model",
    "text": "Explain single predictions with a local model\nGlobal surrogate model can improve the understanding of the global model behaviour. We can also fit a model locally to understand an individual prediction better. The local model fitted by LocalModel is a linear regression model and the data points are weighted by how close they are to the data point for wich we want to explain the prediction.\n\nlime.explain <- LocalModel$new(predictor, x.interest = X[1, ])\n\n#> Loading required package: glmnet\n\n\n#> Loading required package: Matrix\n\n\n#> Loaded glmnet 4.1-6\n\n\n#> Loading required package: gower\n\nlime.explain$results\n\n#>               beta x.recoded    effect x.original feature feature.value\n#> rm       4.4836483     6.575 29.479987      6.575      rm      rm=6.575\n#> ptratio -0.5244767    15.300 -8.024493       15.3 ptratio  ptratio=15.3\n#> lstat   -0.4348698     4.980 -2.165652       4.98   lstat    lstat=4.98\n\nplot(lime.explain)\n\n\n\n\n\n\n\n\n\nlime.explain$explain(X[2, ])\nplot(lime.explain)"
  },
  {
    "objectID": "notes/iml/intro.html#explain-single-predictions-with-game-theory",
    "href": "notes/iml/intro.html#explain-single-predictions-with-game-theory",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Explain single predictions with game theory",
    "text": "Explain single predictions with game theory\nAn alternative for explaining individual predictions is a method from coalitional game theory named Shapley value. Assume that for one data point, the feature values play a game together, in which they get the prediction as a payout. The Shapley value tells us how to fairly distribute the payout among the feature values.\n\nshapley <- Shapley$new(predictor, x.interest = X[1, ])\nshapley$plot()\n\n\n\n\n\n\n\n\nWe can reuse the object to explain other data points:\n\nshapley$explain(x.interest = X[2, ])\nshapley$plot()\n\n\n\n\n\n\n\n\nThe results in data.frame form can be extracted like this:\n\nresults <- shapley$results\nhead(results)\n\n#>   feature          phi    phi.var feature.value\n#> 1    crim -0.030772464 0.88726982  crim=0.02731\n#> 2      zn -0.009163333 0.01266404          zn=0\n#> 3   indus -0.412309000 0.65608174    indus=7.07\n#> 4    chas -0.065604772 0.04865024        chas=0\n#> 5     nox  0.067133048 0.37635155     nox=0.469\n#> 6      rm -0.354769984 7.26420349      rm=6.421"
  },
  {
    "objectID": "notes/hdx_data/hdx_data.html",
    "href": "notes/hdx_data/hdx_data.html",
    "title": "World Bank kenya data",
    "section": "",
    "text": "library(rhdx)\nlibrary(tidyverse)\nlibrary(data.table)\nset_rhdx_config(hdx_site = \"prod\")\nkenya_growth <- search_datasets(\"Kenya - Economy and Growth\", rows = 1) %>%\n    nth(1) %>%\n  get_resource(1) %>%\n  read_resource(filename = \"world-bank-economy-and-growth-indicators-for-kenya.csv\", hxl = TRUE)"
  },
  {
    "objectID": "datacamp.html",
    "href": "datacamp.html",
    "title": "Personal Blog",
    "section": "",
    "text": "The reduction in weekly working hours in Europe\n\n\nLooking at the development between 1996 and 2006\n\n\n\n\n\n\n\n\n\nInsert your name here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Data Science in Python\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nFunctions in R\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nCoding the forward propagation algorithm\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nModeling with tidymodels in R\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nCensus data in r with tidycensus\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling in R\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nHandling Missing Data with Imputations in R\n\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnomaly Detection in R\n\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2022\n\n\nMburu\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to advanced dimensionality reduction\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2022\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWriting Efficient R Code\n\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunicating with Data in the Tidyverse\n\n\n\n\n\n\n\n\n\n\n\n\nAug 11, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple and Logistic Regression in R\n\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Linear and Logistic Regression in R\n\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScalable Data Processing in R\n\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork analysis in R\n\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML with tree based models in r\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to TensorFlow in R\n\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nMburu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html",
    "href": "kaggle/heart_disease/heart_disease.html",
    "title": "Heart Disease Explainable ML",
    "section": "",
    "text": "correlation network plot ml In health research we are not only interested with prediction level but explanability of the fitted algorithm what people in health research call risk factors. We can use heart disease data set to figure out how we can utilize some of the mc.\nI have stolen your idea. For someone working in health research your notebook is a gold mine. In health especially cohort studies, clinical trials people are largely still using generalized linear models because at least from them you can get odds/risk/rate ratios, p values, AIC and so on. I have implemented the same with R(see here). I have also searched for other materials online see here. Thank you.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(caret)\nlibrary(DT)\nlibrary(iml)\nlibrary(patchwork)\nlibrary(gridExtra)\nheart <- fread(\"heart.csv\")\n\nheart %>% head %>%\n    datatable(options = list(scrollX = TRUE))"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#some-cleaning",
    "href": "kaggle/heart_disease/heart_disease.html#some-cleaning",
    "title": "Heart Disease Explainable ML",
    "section": "Some Cleaning",
    "text": "Some Cleaning\nYou can can check the variable levels from here\n\nheart[, sex := factor(sex, \n                      levels = 0:1, \n                      labels = c(\"female\", \"male\"))]\n\n\nheart[, cp := factor(cp,\n                     levels = 0:3,\n                     labels = c(\"angina\", \"atypical angina\",\n                                \"non-anginal pain\", \"asymptomatic\") )]\n\nheart[, fbs := factor(fbs, levels = 0:1,\n                      labels = c(\"fasting blood sugar <= 120 mg/dl)\",\n                                 \"fasting blood sugar > 120 mg/dl)\"))]\n\n\nlnls_restg <- c(\"normal\", \n                 \"having ST-T wave abnormality \\n (T wave inversions and/or \\n ST elevation or depression\n                of >0.05 mV)\", \n                 \"showing probable or definite \\n left ventricular hypertrophy \\n by Estes' criteria\")\n \nheart[, restecg := factor(restecg,\n                          levels = 0:2,\n                          labels = lnls_restg)]\n\n\n\nheart[, exang := factor(exang, \n                        levels = 0:1,\n                        labels = c(\"yes\", \"no\"))]\n\n\n \nheart[, slope := factor(slope,\n                         levels = 0:2,\n                         labels = c(\"upsloping\", \"flat\", \"downsloping\"))]\n \nheart[thal == 0, thal := 1 ]\nheart[, thal := factor(thal,\n                        levels =  1:3,\n                        labels = c(\"normal\", \"fixed defect\", \"reversable defect\"))]\n\nheart[, ca := factor(ca)]\n\nheart[, target := factor(target,\n                         levels = 0:1,\n                         labels = c(\"No_heart_disease\", \"Heart_disease\"))]"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#visualize-categorical-variables",
    "href": "kaggle/heart_disease/heart_disease.html#visualize-categorical-variables",
    "title": "Heart Disease Explainable ML",
    "section": "Visualize categorical variables",
    "text": "Visualize categorical variables\n\nnms <- names(heart)\ncateg_nms <- nms[sapply(heart, is.factor)]\n\n\ncateg_nms <- categ_nms[categ_nms != \"target\"]\nplots_categ <- list()\nfor (i in categ_nms) {\n    \n   plots_categ[[i]] = heart[, .(freq = .N), by = c(i,\"target\")] %>%\n        .[, perc := round(freq/sum(freq) * 100, 2), by = i] %>%\n        ggplot(aes_string(i, \"perc\", fill = \"target\")) +\n        geom_bar(stat = \"identity\", width = 0.5)+\n       geom_text(aes_string(i, \"perc\", label = \"perc\"),\n                 size = 3 , position =  position_stack(vjust = 0.5))+\n       theme(legend.position = \"bottom\")+\n       scale_fill_brewer(palette  = \"Dark2\")\n\n}\n\ngrid.arrange(grobs = plots_categ, ncol = 2)"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#visualize-numeric-variable",
    "href": "kaggle/heart_disease/heart_disease.html#visualize-numeric-variable",
    "title": "Heart Disease Explainable ML",
    "section": "Visualize numeric variable",
    "text": "Visualize numeric variable\n\nnum_nms <- nms[sapply(heart, is.numeric)]\n\n# zero_one <- function(x){\n#   minx = min(x, na.rm = T)\n#   maxx = max(x, na.rm = T)\n#   \n#   z = (x - minx)/(maxx - minx)\n# }\n# \n# heart[, (num_nms) := lapply(.SD, zero_one), .SDcols = num_nms]\n\nnum_nms <- num_nms[num_nms != \"target\"]\n\nfor (i in num_nms) {\n    \n        p = ggplot(heart, aes_string(\"target\", i)) +\n            geom_boxplot()\n   print(p)\n}"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#fit-logistic-regression",
    "href": "kaggle/heart_disease/heart_disease.html#fit-logistic-regression",
    "title": "Heart Disease Explainable ML",
    "section": "Fit logistic regression",
    "text": "Fit logistic regression\n\nglm_heart <- glm(target ~., data = heart,\n                 family = binomial())\n\nlibrary(broom)\n\ntidy(glm_heart) %>% datatable() %>%\n  formatRound(columns = c(\"estimate\", \"std.error\", \"statistic\", \"p.value\"), digits = 4)"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#chisq-test",
    "href": "kaggle/heart_disease/heart_disease.html#chisq-test",
    "title": "Heart Disease Explainable ML",
    "section": "Chisq test",
    "text": "Chisq test\n\ndrop1(glm_heart, test = \"Chisq\") %>% tidy %>%\n    datatable()  %>%\n  formatRound(columns = c(\"df\", \"Deviance\", \"AIC\", \"LRT\", \"p.value\"), digits = 4)"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#ml-model",
    "href": "kaggle/heart_disease/heart_disease.html#ml-model",
    "title": "Heart Disease Explainable ML",
    "section": "ML model",
    "text": "ML model\n\ntrain_ctrl <- trainControl(method = \"cv\",\n                        number = 5,\n                        summaryFunction = twoClassSummary,\n                        classProbs = TRUE,\n                        allowParallel=T,\n                        #index = cv_fold,\n                        verboseIter = FALSE,\n                        returnResamp = \"all\", \n                        savePredictions = \"final\", \n                        search = \"grid\")\n\n\n\n\n\nranger_grid <- expand.grid(splitrule = \"extratrees\",\n                        mtry = c(2, 5, 10),\n                        min.node.size = c(2, 5, 7))\n\n\nheart_randomforest <- train(target~ .,\n      data = heart,\n      trControl = train_ctrl,\n      tuneGrid = ranger_grid,\n      method = \"ranger\")\nheart_randomforest \n\nRandom Forest \n\n303 samples\n 13 predictor\n  2 classes: 'No_heart_disease', 'Heart_disease' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 243, 242, 243, 242, 242 \nResampling results across tuning parameters:\n\n  mtry  min.node.size  ROC        Sens       Spec     \n   2    2              0.9104137  0.8187831  0.8484848\n   2    5              0.9121292  0.7896825  0.8424242\n   2    7              0.9114558  0.8113757  0.8424242\n   5    2              0.9067981  0.8116402  0.8303030\n   5    5              0.9077201  0.8187831  0.8303030\n   5    7              0.9077040  0.8116402  0.8242424\n  10    2              0.9018118  0.8190476  0.8181818\n  10    5              0.9037117  0.8261905  0.8181818\n  10    7              0.9034632  0.8116402  0.8303030\n\nTuning parameter 'splitrule' was held constant at a value of extratrees\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were mtry = 2, splitrule = extratrees\n and min.node.size = 5."
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#var-importance",
    "href": "kaggle/heart_disease/heart_disease.html#var-importance",
    "title": "Heart Disease Explainable ML",
    "section": "Var Importance",
    "text": "Var Importance\n\npred <- function(heart_randomforest, heart)  {\n  results <- predict(heart_randomforest, newdata = heart, type = \"prob\")\n  return(results[[2L]])\n}"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#partial-dependence",
    "href": "kaggle/heart_disease/heart_disease.html#partial-dependence",
    "title": "Heart Disease Explainable ML",
    "section": "Partial Dependence",
    "text": "Partial Dependence\n\nX_pred <- heart[, .SD, .SDcols = !\"target\"] %>%\n  as.data.frame()\nmodel <- Predictor$new(model = heart_randomforest, \n                      data =X_pred,\n                      predict.function = pred,\n                      y = heart$target)\neffect <- FeatureEffects$new(model)"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#section",
    "href": "kaggle/heart_disease/heart_disease.html#section",
    "title": "Heart Disease Explainable ML",
    "section": "",
    "text": "effect$plot(features = c( \"trestbps\"))\n\n\n\n#effect"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#section-1",
    "href": "kaggle/heart_disease/heart_disease.html#section-1",
    "title": "Heart Disease Explainable ML",
    "section": "",
    "text": "imp <- FeatureImp$new(model, loss =\"ce\" )\n#imp\n\nvar_important <-imp$results %>% data.table()\n\nsetorder(var_important, -importance)\nplot(imp)\n\n\n\n\n\ninteract <- Interaction$new(model, feature = \"thal\")\nplot(interact)\n\n\n\n\n\nlibrary(tictoc)\ntic()\n\nshap_list <- vector(\"list\", nrow(X_pred)) \n\nfor (i in 1:nrow(X_pred)) {\n  shap <- Shapley$new(model,  x.interest = X_pred[i, ], sample.size = 30)\n  shap_import <-shap$results %>% data.table()\n  shap_import <- shap_import[class == \"Heart_disease\"]\n  shap_list[[i]] <- shap_import[, record_id := i]\n\n  }\ntoc()\n\n56.451 sec elapsed\n\nshap_values <- rbindlist(shap_list, fill = T)\n\n\nlibrary(ggforce)\n\nshap_values[, feature := factor(feature, levels = rev(var_important$feature) )]\nminx <- shap_values[, min(phi.var)]\nmaxx <- shap_values[, max(phi.var)]\nggplot(shap_values, aes(feature, phi,  color = phi.var))+\n  #geom_point()+\n    ggbeeswarm::geom_quasirandom(groupOnX = FALSE, varwidth = TRUE, size = 0.9, alpha = 0.25) +\n  geom_hline(yintercept = 0) +\n  scale_color_gradient(low=\"#2187E3\", high=\"#F32858\", \n                       breaks=c(minx,maxx), labels=c(\"Low\",\"High\"), limits=c(minx,maxx))+ \n  theme_bw() + \n    theme(axis.line.y = element_blank(), \n          axis.ticks.y = element_blank(), # remove axis line\n          legend.position=\"bottom\") +\n  coord_flip()"
  },
  {
    "objectID": "kaggle/heart_failure/heart_failure.html",
    "href": "kaggle/heart_failure/heart_failure.html",
    "title": "Heart Failure Prediction",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(xgboost)\nlibrary(knitr)\nlibrary(broom)\nlibrary(caret)\nlibrary(e1071)\nlibrary(kableExtra)\nlibrary(ggthemes)\nlibrary(DT)\nlibrary(here)\n\n\nheart_failure<- fread(\"data/heart_failure_clinical_records_dataset.csv\")\nnms <- names(heart_failure)  \nnms_new <- nms %>% tolower()\nsetnames(heart_failure, nms, nms_new)\n\ndatatable(heart_failure, \n          options = list(scrollX = T) )\n\n\n\n\n\n\n\nheart_failure[, death_char := factor(death_event,\n                                     levels = c(0, 1), \n                                     labels = c(\"Alive\", \"Death\"))]\n\n\ndeaths_tab <- heart_failure[, .(freq = .N),\n       by = death_char] %>% \n    .[, perc := round(100 * freq/sum(freq), 2)] \n\ndeaths_tab[, bar_text := paste0(\"N = \",freq, \", \", perc, \"%\")]\n#datatable(deaths_tab)\n\n\nggplot(deaths_tab, aes(death_char, perc))+\n    geom_bar(stat = \"identity\", width = 0.5)+\n    geom_text(aes(death_char, perc, label = bar_text),\n              position = position_dodge(width = 0.5),\n              vjust = 0.05)\n\n\n\n\n\nfind_factors <- function(x){\n    y = unique(x)\n    len_x = length(y)\n    val = len_x < 4\n    return(val)\n}\nnms_dt <- sapply(heart_failure, find_factors)\nnms_factors_all <- nms_dt[nms_dt == T] %>% names()\nnms_factors <- nms_factors_all[!nms_factors_all %in% c(\"death_event\")]\n\ndt_factors <- heart_failure[, ..nms_factors]\ndatatable(dt_factors[1:10], \n          options = list(scrollX = T) )\n\n\n\n\n\n\n\ndt_factors_m <- melt(dt_factors, \n                     id.vars = \"death_char\",\n                     variable.factor = F)\n\ndatatable(dt_factors_m[1:10], \n          options = list(scrollX = T) )\n\n\n\n\n\n\n\nsummary_factors <- dt_factors_m[, .(freq = .N), \n             by = c( \"variable\", \"death_char\", \"value\")] %>%\n    .[, perc := round(100 * freq/sum(freq), 2),\n      by = c( \"variable\",\"value\")] \n\n\nsummary_factors[, value := as.factor(value)]\nggplot(summary_factors, aes(value, perc, fill = death_char))+\n    geom_bar(stat = \"identity\")+\n    geom_text(aes(value, perc, label = perc),\n              position = position_stack(vjust = .5))+\n    facet_wrap(~variable)\n\n\n\n\n\nnms_numeric <- nms_dt[nms_dt == FALSE] %>% names()\n\nheart_failure[, (nms_numeric) := lapply(.SD, \n                                        function(x)scales::rescale(x) ),\n              .SDcols = nms_numeric]\n\n\nnms_numeric2 <- c(\"death_char\", nms_numeric)\ndt_num <- heart_failure[, ..nms_numeric2]\ndt_num_m <- melt(dt_num, id.vars = \"death_char\")\n\n\nggplot(dt_num_m, aes(death_char, value))+\n  geom_violin()+\n  facet_wrap(~variable)\n\n\n\n\n\nnms <- c(\"death_event\", \"time\")\ncv_fold <- createFolds(heart_failure$death_char, k = 10)\n\nheart_failure[, (nms) := NULL]\nxgb_ctrl <- trainControl(method = \"cv\",\n                        number = 10,\n                        summaryFunction = twoClassSummary,\n                        classProbs = TRUE,\n                        allowParallel=T,\n                        index = cv_fold,\n                        verboseIter = FALSE,\n                        savePredictions = TRUE,\n                        search = \"grid\")\n\nxgb_grid <- expand.grid(nrounds = c(10, 50, 100),\n                        eta = seq(0.06, .2, length.out = 3),\n                        max_depth = c(50, 80),\n                        gamma = c(0,.01, 0.1),\n                        colsample_bytree = c(0.6, 0.7,0.8),\n                        min_child_weight = 1,\n                        subsample =  .7\n                        \n    )\n\nxgb_model <-train(death_char~.,\n                 data=heart_failure,\n                 method=\"xgbTree\",\n                 trControl= xgb_ctrl,\n                 tuneGrid=xgb_grid,\n                 verbose=T,\n                 metric=\"ROC\",\n                 nthread =4\n                     \n    )\n\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\nxgb_model\n\neXtreme Gradient Boosting \n\n299 samples\n 11 predictor\n  2 classes: 'Alive', 'Death' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 30, 29, 29, 29, 30, 30, ... \nResampling results across tuning parameters:\n\n  eta   max_depth  gamma  colsample_bytree  nrounds  ROC        Sens     \n  0.06  50         0.00   0.6                10      0.6991674  0.9376359\n  0.06  50         0.00   0.6                50      0.7065753  0.8938450\n  0.06  50         0.00   0.6               100      0.6985187  0.8686963\n  0.06  50         0.00   0.7                10      0.7140390  0.9031886\n  0.06  50         0.00   0.7                50      0.7029280  0.8911337\n  0.06  50         0.00   0.7               100      0.6977720  0.8599291\n  0.06  50         0.00   0.8                10      0.6911803  0.9053444\n  0.06  50         0.00   0.8                50      0.6904661  0.8823695\n  0.06  50         0.00   0.8               100      0.6941211  0.8615775\n  0.06  50         0.01   0.6                10      0.6903192  0.9070017\n  0.06  50         0.01   0.6                50      0.6981350  0.8971326\n  0.06  50         0.01   0.6               100      0.6952965  0.8686903\n  0.06  50         0.01   0.7                10      0.6759658  0.9053924\n  0.06  50         0.01   0.7                50      0.7000085  0.8922506\n  0.06  50         0.01   0.7               100      0.7003534  0.8648652\n  0.06  50         0.01   0.8                10      0.6895849  0.8966132\n  0.06  50         0.01   0.8                50      0.6989295  0.8905723\n  0.06  50         0.01   0.8               100      0.7008458  0.8599231\n  0.06  50         0.10   0.6                10      0.6971920  0.9250315\n  0.06  50         0.10   0.6                50      0.7015474  0.8954963\n  0.06  50         0.10   0.6               100      0.7028462  0.8785384\n  0.06  50         0.10   0.7                10      0.6915385  0.9004384\n  0.06  50         0.10   0.7                50      0.7072150  0.8971236\n  0.06  50         0.10   0.7               100      0.7021999  0.8686933\n  0.06  50         0.10   0.8                10      0.7000491  0.9163094\n  0.06  50         0.10   0.8                50      0.7038118  0.8872996\n  0.06  50         0.10   0.8               100      0.6991175  0.8599201\n  0.06  80         0.00   0.6                10      0.6922588  0.9245241\n  0.06  80         0.00   0.6                50      0.6882695  0.8845703\n  0.06  80         0.00   0.6               100      0.6956193  0.8676365\n  0.06  80         0.00   0.7                10      0.6982782  0.9146400\n  0.06  80         0.00   0.7                50      0.7007888  0.8922206\n  0.06  80         0.00   0.7               100      0.7036573  0.8670540\n  0.06  80         0.00   0.8                10      0.6974558  0.9288867\n  0.06  80         0.00   0.8                50      0.7036682  0.8851048\n  0.06  80         0.00   0.8               100      0.7015020  0.8577433\n  0.06  80         0.01   0.6                10      0.6753779  0.8911638\n  0.06  80         0.01   0.6                50      0.7034268  0.8993335\n  0.06  80         0.01   0.6               100      0.6991734  0.8692098\n  0.06  80         0.01   0.7                10      0.6766924  0.8769531\n  0.06  80         0.01   0.7                50      0.6990327  0.8851168\n  0.06  80         0.01   0.7               100      0.6928515  0.8659701\n  0.06  80         0.01   0.8                10      0.6825694  0.9124692\n  0.06  80         0.01   0.8                50      0.6929638  0.8823695\n  0.06  80         0.01   0.8               100      0.6952230  0.8572119\n  0.06  80         0.10   0.6                10      0.7008830  0.9304720\n  0.06  80         0.10   0.6                50      0.7015833  0.9053354\n  0.06  80         0.10   0.6               100      0.6979281  0.8741518\n  0.06  80         0.10   0.7                10      0.6984532  0.8966162\n  0.06  80         0.10   0.7                50      0.7014838  0.8938420\n  0.06  80         0.10   0.7               100      0.7000350  0.8588392\n  0.06  80         0.10   0.8                10      0.6989098  0.9037080\n  0.06  80         0.10   0.8                50      0.6992521  0.8785534\n  0.06  80         0.10   0.8               100      0.6981307  0.8610401\n  0.13  50         0.00   0.6                10      0.6717370  0.9157479\n  0.13  50         0.00   0.6                50      0.6909021  0.8632048\n  0.13  50         0.00   0.6               100      0.6888321  0.8352969\n  0.13  50         0.00   0.7                10      0.6842663  0.9157239\n  0.13  50         0.00   0.7                50      0.6804251  0.8522669\n  0.13  50         0.00   0.7               100      0.6814364  0.8271062\n  0.13  50         0.00   0.8                10      0.6783430  0.8944364\n  0.13  50         0.00   0.8                50      0.6901888  0.8539122\n  0.13  50         0.00   0.8               100      0.6920261  0.8347715\n  0.13  50         0.01   0.6                10      0.6995521  0.9179337\n  0.13  50         0.01   0.6                50      0.6890757  0.8692308\n  0.13  50         0.01   0.6               100      0.6903451  0.8358734\n  0.13  50         0.01   0.7                10      0.6791652  0.8856692\n  0.13  50         0.01   0.7                50      0.6969954  0.8621209\n  0.13  50         0.01   0.7               100      0.6899256  0.8293010\n  0.13  50         0.01   0.8                10      0.6890758  0.9059028\n  0.13  50         0.01   0.8                50      0.6906272  0.8593827\n  0.13  50         0.01   0.8               100      0.6901043  0.8407554\n  0.13  50         0.10   0.6                10      0.6868214  0.9092025\n  0.13  50         0.10   0.6                50      0.6885170  0.8577373\n  0.13  50         0.10   0.6               100      0.6913488  0.8331081\n  0.13  50         0.10   0.7                10      0.6952042  0.8939110\n  0.13  50         0.10   0.7                50      0.6968606  0.8489882\n  0.13  50         0.10   0.7               100      0.6925225  0.8216297\n  0.13  50         0.10   0.8                10      0.6726708  0.8944364\n  0.13  50         0.10   0.8                50      0.6845301  0.8555606\n  0.13  50         0.10   0.8               100      0.6841057  0.8331472\n  0.13  80         0.00   0.6                10      0.6937412  0.8987720\n  0.13  80         0.00   0.6                50      0.7009886  0.8675974\n  0.13  80         0.00   0.6               100      0.6945929  0.8320363\n  0.13  80         0.00   0.7                10      0.6769830  0.8714646\n  0.13  80         0.00   0.7                50      0.6952223  0.8566565\n  0.13  80         0.00   0.7               100      0.6896150  0.8336756\n  0.13  80         0.00   0.8                10      0.6934445  0.8856632\n  0.13  80         0.00   0.8                50      0.6984557  0.8566505\n  0.13  80         0.00   0.8               100      0.6880624  0.8276797\n  0.13  80         0.01   0.6                10      0.6671728  0.8998769\n  0.13  80         0.01   0.6                50      0.6873090  0.8665135\n  0.13  80         0.01   0.6               100      0.6882712  0.8396956\n  0.13  80         0.01   0.7                10      0.6915172  0.8834775\n  0.13  80         0.01   0.7                50      0.6880404  0.8544737\n  0.13  80         0.01   0.7               100      0.6888640  0.8342401\n  0.13  80         0.01   0.8                10      0.6954172  0.9020777\n  0.13  80         0.01   0.8                50      0.6939822  0.8462649\n  0.13  80         0.01   0.8               100      0.6879673  0.8238215\n  0.13  80         0.10   0.6                10      0.6717149  0.8993064\n  0.13  80         0.10   0.6                50      0.6936011  0.8604846\n  0.13  80         0.10   0.6               100      0.6929754  0.8336576\n  0.13  80         0.10   0.7                10      0.6706358  0.8758062\n  0.13  80         0.10   0.7                50      0.7001394  0.8577494\n  0.13  80         0.10   0.7               100      0.6943577  0.8298175\n  0.13  80         0.10   0.8                10      0.6833212  0.8900108\n  0.13  80         0.10   0.8                50      0.6884718  0.8506125\n  0.13  80         0.10   0.8               100      0.6886013  0.8232721\n  0.20  50         0.00   0.6                10      0.6814029  0.9173873\n  0.20  50         0.00   0.6                50      0.6892818  0.8342191\n  0.20  50         0.00   0.6               100      0.6822590  0.8068606\n  0.20  50         0.00   0.7                10      0.6757506  0.8561280\n  0.20  50         0.00   0.7                50      0.6904492  0.8429802\n  0.20  50         0.00   0.7               100      0.6807745  0.8090524\n  0.20  50         0.00   0.8                10      0.6805655  0.8648682\n  0.20  50         0.00   0.8                50      0.6918282  0.8254669\n  0.20  50         0.00   0.8               100      0.6836651  0.8145199\n  0.20  50         0.01   0.6                10      0.6700121  0.8960668\n  0.20  50         0.01   0.6                50      0.6887541  0.8440311\n  0.20  50         0.01   0.6               100      0.6834611  0.8205068\n  0.20  50         0.01   0.7                10      0.6776024  0.8840119\n  0.20  50         0.01   0.7                50      0.6860949  0.8292800\n  0.20  50         0.01   0.7               100      0.6851903  0.8139675\n  0.20  50         0.01   0.8                10      0.7000549  0.8845944\n  0.20  50         0.01   0.8                50      0.6928333  0.8435177\n  0.20  50         0.01   0.8               100      0.6888423  0.8161593\n  0.20  50         0.10   0.6                10      0.6979041  0.8998619\n  0.20  50         0.10   0.6                50      0.6910097  0.8451330\n  0.20  50         0.10   0.6               100      0.6873215  0.8265448\n  0.20  50         0.10   0.7                10      0.6886938  0.8845283\n  0.20  50         0.10   0.7                50      0.6853145  0.8363958\n  0.20  50         0.10   0.7               100      0.6810830  0.8106888\n  0.20  50         0.10   0.8                10      0.7032968  0.8791179\n  0.20  50         0.10   0.8                50      0.6955192  0.8407945\n  0.20  50         0.10   0.8               100      0.6926511  0.8194469\n  0.20  80         0.00   0.6                10      0.6682048  0.8725755\n  0.20  80         0.00   0.6                50      0.6835662  0.8293160\n  0.20  80         0.00   0.6               100      0.6835065  0.8101393\n  0.20  80         0.00   0.7                10      0.6779035  0.8889149\n  0.20  80         0.00   0.7                50      0.6871511  0.8402480\n  0.20  80         0.00   0.7               100      0.6864257  0.8227196\n  0.20  80         0.00   0.8                10      0.6763919  0.8758452\n  0.20  80         0.00   0.8                50      0.6777788  0.8309164\n  0.20  80         0.00   0.8               100      0.6750242  0.8095929\n  0.20  80         0.01   0.6                10      0.6696939  0.8900318\n  0.20  80         0.01   0.6                50      0.6839999  0.8282021\n  0.20  80         0.01   0.6               100      0.6831399  0.8188795\n  0.20  80         0.01   0.7                10      0.6915076  0.8916862\n  0.20  80         0.01   0.7                50      0.6923763  0.8429712\n  0.20  80         0.01   0.7               100      0.6910804  0.8227346\n  0.20  80         0.01   0.8                10      0.6794582  0.8566895\n  0.20  80         0.01   0.8                50      0.6884565  0.8363958\n  0.20  80         0.01   0.8               100      0.6782361  0.8112292\n  0.20  80         0.10   0.6                10      0.6741507  0.8960638\n  0.20  80         0.10   0.6                50      0.6895128  0.8397136\n  0.20  80         0.10   0.6               100      0.6772756  0.8222062\n  0.20  80         0.10   0.7                10      0.6764553  0.8752657\n  0.20  80         0.10   0.7                50      0.6925546  0.8374917\n  0.20  80         0.10   0.7               100      0.6844138  0.8079655\n  0.20  80         0.10   0.8                10      0.6972115  0.8818171\n  0.20  80         0.10   0.8                50      0.6870099  0.8216237\n  0.20  80         0.10   0.8               100      0.6839784  0.8156308\n  Spec     \n  0.2108260\n  0.3276798\n  0.3590083\n  0.3024058\n  0.3207698\n  0.3786688\n  0.2551056\n  0.3289762\n  0.3728949\n  0.2769580\n  0.3057471\n  0.3531542\n  0.2664261\n  0.3196872\n  0.3601043\n  0.2998797\n  0.3243250\n  0.3763833\n  0.2570703\n  0.3230553\n  0.3439455\n  0.2713713\n  0.3197140\n  0.3729484\n  0.2734162\n  0.3473536\n  0.3786955\n  0.2395883\n  0.3161187\n  0.3531943\n  0.2526464\n  0.3231222\n  0.3636594\n  0.2570970\n  0.3299652\n  0.3751938\n  0.2880914\n  0.3172547\n  0.3554397\n  0.2824379\n  0.3150628\n  0.3521117\n  0.2595429\n  0.3174419\n  0.3682839\n  0.2236568\n  0.2849505\n  0.3450147\n  0.2534483\n  0.3161321\n  0.3716653\n  0.2605453\n  0.3139669\n  0.3683240\n  0.2444801\n  0.3498129\n  0.4030206\n  0.2584202\n  0.3613606\n  0.4076851\n  0.2929698\n  0.3820904\n  0.4064020\n  0.2433841\n  0.3509356\n  0.3960572\n  0.3128442\n  0.3577653\n  0.3971264\n  0.2824779\n  0.3775194\n  0.4005480\n  0.2885191\n  0.3589949\n  0.4053863\n  0.3067629\n  0.3716520\n  0.3994520\n  0.2827586\n  0.3682572\n  0.3972334\n  0.2919006\n  0.3636060\n  0.4064555\n  0.3299385\n  0.3752473\n  0.4018845\n  0.3186447\n  0.3764769\n  0.4065223\n  0.2835472\n  0.3658915\n  0.3936915\n  0.3323443\n  0.3659449\n  0.4065624\n  0.3069099\n  0.3671478\n  0.4064154\n  0.2549185\n  0.3438786\n  0.4017241\n  0.3207565\n  0.3785886\n  0.4226143\n  0.2780005\n  0.3670142\n  0.4121893\n  0.2444667\n  0.4052259\n  0.4319433\n  0.3404972\n  0.3856856\n  0.4168805\n  0.3369286\n  0.4168271\n  0.4192462\n  0.2744721\n  0.3809810\n  0.4133654\n  0.3079925\n  0.4075916\n  0.4296712\n  0.3230152\n  0.4006549\n  0.4214916\n  0.3020850\n  0.3820770\n  0.4191794\n  0.3287490\n  0.3950548\n  0.4238840\n  0.3392809\n  0.3937584\n  0.4273323\n  0.2812751\n  0.4030072\n  0.4308474\n  0.2884924\n  0.4019113\n  0.4239107\n  0.3136862\n  0.3844694\n  0.4169206\n  0.3080994\n  0.3914461\n  0.4191927\n  0.2998663\n  0.3798717\n  0.4227747\n  0.3451216\n  0.3890003\n  0.4155573\n  0.2892542\n  0.3833734\n  0.3983427\n  0.3300321\n  0.4005346\n  0.4284683\n  0.3240577\n  0.4064288\n  0.4203823\n\nTuning parameter 'min_child_weight' was held constant at a value of 1\n\nTuning parameter 'subsample' was held constant at a value of 0.7\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were nrounds = 10, max_depth = 50, eta\n = 0.06, gamma = 0, colsample_bytree = 0.7, min_child_weight = 1 and\n subsample = 0.7."
  },
  {
    "objectID": "kaggle/malaysia_tourist/malaysia_tourist.html",
    "href": "kaggle/malaysia_tourist/malaysia_tourist.html",
    "title": "Malaysian Tourist Sites",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(leaflet)\n\ntourist_site <-fread(\"dataset tempat perlancongan Malaysia.csv\")\n\ntourist_site[, site_label := paste0(Negeri, \", \" ,`Nama Tempat`)]"
  },
  {
    "objectID": "kaggle/malaysia_tourist/malaysia_tourist.html#leaflet-map",
    "href": "kaggle/malaysia_tourist/malaysia_tourist.html#leaflet-map",
    "title": "Malaysian Tourist Sites",
    "section": "Leaflet Map",
    "text": "Leaflet Map\n\nleaflet(tourist_site) %>%\n    addTiles() %>%\n    addMarkers(~Longitude, ~Latitude, label = ~site_label )"
  },
  {
    "objectID": "kaggle/microsoft_malware_prediction/malware_prediction.html",
    "href": "kaggle/microsoft_malware_prediction/malware_prediction.html",
    "title": "Malware prediction",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(ggthemes)\nlibrary(caret)"
  },
  {
    "objectID": "kaggle/microsoft_malware_prediction/malware_prediction.html#section-1",
    "href": "kaggle/microsoft_malware_prediction/malware_prediction.html#section-1",
    "title": "Malware prediction",
    "section": "",
    "text": "set.seed(100)\nsample_sub <- sample(nrow(micro_train), 1000000)\nmicro_train_sub <- micro_train[sample_sub,]\n\nnrow_train <- nrow(micro_train_sub)\n\nsample_train <- sample(nrow_train, as.integer(nrow_train * 0.7))\n\nmalware_train <- micro_train_sub[sample_train]\n\nmalware_test <- micro_train_sub[-sample_train]\n\n\nna_tally <- round(colSums(is.na(micro_train_sub))/nrow(micro_train_sub) * 100, 2)\n\nna_tally <- na_tally[na_tally != 0]\n\nnms_na <- names(na_tally)\n\nna_dt <- data.table(var = nms_na, perc = na_tally)\n\nggplot(na_dt, aes(reorder(var, perc), perc)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip()+\n    theme(axis.text.x = element_text(size = 10))\n\n\nna_over25 <- na_dt[perc>25, var]\n\nmicro_train_sub[, (na_over25) := NULL]\n\nna_under25dt <- na_dt[perc <= 25]\nval_del <- na_dt[between(perc,6, 25), var]\n\nggplot(na_under25dt, aes(reorder(var, perc), perc)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip()+\n    theme(axis.text.y = element_text(size = 10))"
  },
  {
    "objectID": "kaggle/mnist_digits/mnist.html",
    "href": "kaggle/mnist_digits/mnist.html",
    "title": "MNIST Digits",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(keras)\nlibrary(caret)\nlibrary(DT)\nlibrary(caretEnsemble)\nlibrary(tictoc)\n\ntrain_data <- fread(\"data/train.csv\")\n\ntest_data <- fread(\"data/test.csv\")\n\n\n\n\n\nggplot(train_data, aes(x = factor(label))) +\n    geom_bar()\n\n\n\n\n\n#  image coordinates\nxy_axis <- data.frame(x = expand.grid(1:28, 28:1)[,1],\n                      y = expand.grid(1:28, 28:1)[,2])\n\n\n# get 12 images\nset.seed(100)\nsample_10 <- train_data[sample(1:.N, 12), -1] %>% as.matrix()\n\ndatatable(sample_10, \n          options = list(scrollX = TRUE))\n\nsample_10 <- t(sample_10)\n\nplot_data <- cbind(xy_axis, sample_10 )\n\nsetDT(plot_data, keep.rownames = \"pixel\")\n\n# Observe the first records\nhead(plot_data) %>% datatable(options = list(scrollX = TRUE))\n\n\n\n\n\nplot_data_m <- melt(plot_data, id.vars = c(\"pixel\", \"x\", \"y\"))\n\n# Plot the image using ggplot()\nggplot(plot_data_m, aes(x, y, fill = value)) +\n    geom_raster()+\n     facet_wrap(~variable)+\n    scale_fill_gradient(low = \"white\",\n                        high = \"black\", guide = FALSE)+\n    theme(axis.line = element_blank(),\n                  axis.text = element_blank(),\n                  axis.ticks = element_blank(),\n                  axis.title = element_blank(),\n                  panel.background = element_blank(),\n                  panel.border = element_blank(),\n                  panel.grid.major = element_blank(),\n                  panel.grid.minor = element_blank(),\n                  plot.background = element_blank())\n\n\n\n\n\nDecided to have a self test set\n\n\nnmst <- names(train_data)\nnmst <- nmst[nmst != \"label\"]\nminmax <- function(x) {\n  top =  (x - min(x))\n  bottom = (max(x) - min(x))\n  if(bottom == 0){ \n    return(0)\n  }else{\n      return(top/bottom)\n    }\n}\n#train_data[, (nmst) := lapply(.SD,  function(x) x/255), .SDcols = nmst]\ntrain_data[, (nmst) := lapply(.SD,  minmax), .SDcols = nmst]\nset.seed(100)\nN1 = nrow(train_data)\nsample_one <- sample(N1, 5000)\ntrain_data <- train_data[sample_one]\nN = nrow(train_data)\nsample_train <- sample(N, size = round(0.75 *N ))\ntest_own <- train_data[-sample_train]\ntrain_data2 <- train_data[sample_train, ]\ntrain_y <-to_categorical(train_data2$label, 10)\n\ntrain_x <- train_data2[, -1]\n#convert to matrix\ntrain_x <- train_x %>%\n    as.matrix()\n\n#train_x <- train_x/255\n\n\n\n\n\nmodel <- keras_model_sequential() \nmodel %>% \n    layer_dense(units = 784, activation = 'relu', input_shape = 784) %>%\n    layer_dropout(rate = 0.3) %>%\n    layer_dense(units = 784, activation = 'relu') %>%\n    layer_dropout(rate = 0.3) %>%\n    layer_dense(units = 392, activation = 'relu') %>%\n    layer_dropout(rate = 0.2) %>%\n    layer_dense(units = 200, activation = 'tanh') %>%\n    #layer_dropout(rate = 0.) %>%\n    layer_dense(units = 10, activation = 'softmax')\n\n\n\n\n\nmodel %>% compile(\n    loss = 'categorical_crossentropy',\n    optimizer = 'adam',\n    metrics = c('accuracy'))\n\n\n\n\n\nhist <- model %>% fit(train_x, train_y, \n                      epochs = 9, batch_size = 1000,\n                      validation_split = .2)\n\n\nplot(hist,type = \"b\")\n\n\n\n\n\ntest_own_x <- test_own[, -1] %>% as.matrix()\n\ntest_own_pred <- model %>% predict_classes(test_own_x) %>% factor()\n\nconfusionMatrix(data = test_own_pred, reference = factor(test_own$label))\n\n\n\n\ntest_x <- as.matrix(test_data)/255\n\ntest_pred <- model %>% predict_classes(test_x)\n#head(test_pred)\n\ndf_pred1 <- data.frame(ImageId = 1:length(test_pred),\n                       Label = test_pred)\n\nwrite.csv(df_pred1, file = \"sample_submission.csv\", row.names = F)"
  },
  {
    "objectID": "kaggle/mnist_digits/mnist.html#tsne",
    "href": "kaggle/mnist_digits/mnist.html#tsne",
    "title": "MNIST Digits",
    "section": "TSNE",
    "text": "TSNE\n\nlibrary(Rtsne)\n\ntsne_output <- Rtsne(train_x, check_duplicates = FALSE)\n\n# Generate a data frame to plot the result\ntsne_train <- data.table(tsne_x = tsne_output$Y[,1],\n                        tsne_y = tsne_output$Y[,2],\n                        label =  train_data2$label)\n\n# Plot the embedding usign ggplot and the label\nggplot(tsne_train,\n       aes(x = tsne_x, y = tsne_y, color = factor(label))) + \n  ggtitle(\"t-SNE of MNIST data set\") + \n  geom_text(aes(label = label)) +\n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "kaggle/mnist_digits/mnist.html#plot-tsne-group-means",
    "href": "kaggle/mnist_digits/mnist.html#plot-tsne-group-means",
    "title": "MNIST Digits",
    "section": "Plot tsne group means",
    "text": "Plot tsne group means\n\ntsne_mean <- tsne_train[, \n                        .(mean_x = mean(tsne_x), mean_y = mean(tsne_y)),\n                        by = label]\n\n\nggplot(tsne_mean,\n       aes(x = mean_x, y = mean_y, color = factor(label))) + \n  ggtitle(\"t-SNE of MNIST data set group means\") + \n  geom_text(aes(label = label)) +\n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "kaggle/mnist_digits/mnist.html#kmeans-to-see-if-tsne-and-kmeans-agree",
    "href": "kaggle/mnist_digits/mnist.html#kmeans-to-see-if-tsne-and-kmeans-agree",
    "title": "MNIST Digits",
    "section": "Kmeans to see if tsne and kmeans agree",
    "text": "Kmeans to see if tsne and kmeans agree\n\nset.seed(123)\nk_means_mnist <- kmeans(train_x, 10)\n\ntsne_train[, cluster := k_means_mnist$cluster]\nggplot(tsne_train,\n       aes(x = tsne_x, y = tsne_y, color = factor(cluster))) + \n  geom_point()+\n  ggtitle(\"t-SNE of MNIST data set\") + \n    theme(legend.position = \"none\")\n\ntsne_train[, cluster := NULL]\n\n\nlibrary(kernlab)\ncol_sum <- colSums(train_data2[, .SD, .SDcols = !\"label\"])\n\nzero_var_cols <- col_sum[col_sum == 0] %>% names()\ntrain_data2 <- train_data2[, .SD, .SDcols = !zero_var_cols]\ndf_nms <-  data.frame(vars = names(train_data2))\nwrite.csv(df_nms, file = \"df_nms.csv\", row.names = F)\nmnist_matrix <- train_data2[, .SD, .SDcols = !\"label\"] %>% na.omit %>% as.matrix()\n\n\nspec_models <- list()\ntot_withinss <- c()\nfor(i in 1:10){\n    \n    spec_fit <- specc(mnist_matrix, centers=i+1)\n    tot_withinss[i] <-withinss(spec_fit) %>% median()\n    spec_models[[i]] <- spec_fit\n}\n\nplot( tot_withinss)\n\n\nspec_fit_final <- spec_models[[4]]"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html",
    "title": "New york Airbnb",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(DT)\nlibrary(ggthemes)\nlibrary(leaflet)\nlibrary(ggmap)\nlibrary(plotly)\nlibrary(lubridate)"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#first-few-variables",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#first-few-variables",
    "title": "New york Airbnb",
    "section": "First few variables",
    "text": "First few variables\n\nairbnb_nyc <- fread(\"AB_NYC_2019.csv\")\n\nhead(airbnb_nyc) %>% \n    datatable(options = list(scrollX = TRUE))"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#neighbourhood-disitribution",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#neighbourhood-disitribution",
    "title": "New york Airbnb",
    "section": "Neighbourhood Disitribution",
    "text": "Neighbourhood Disitribution\n\np1 <- airbnb_nyc[, .(freq = .N), by =  neighbourhood_group] %>%\n    .[, perc := round(freq/sum(freq) *100, 2)] %>%\n    ggplot(aes(neighbourhood_group, perc))+\n    geom_bar(stat = \"identity\", width = 0.5, fill =\"slategray2\"  ) +\n    geom_text(aes(neighbourhood_group, perc, label = paste0(perc, \"%\")),\n              position = position_dodge(width = 0.5),\n              vjust = 0.07)+\n    theme_fivethirtyeight() \np1"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#popular-room-types-in-neighbourhoods-disitribution",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#popular-room-types-in-neighbourhoods-disitribution",
    "title": "New york Airbnb",
    "section": "Popular room types in neighbourhoods Disitribution",
    "text": "Popular room types in neighbourhoods Disitribution\n\nairbnb_nyc[, .(freq =.N), by = .(neighbourhood_group, room_type)] %>%\n  .[, perc := round(freq/sum(freq) *100, 2), by = neighbourhood_group] %>%\n  ggplot(aes(neighbourhood_group, perc, fill = room_type))+\n  geom_bar(stat = \"identity\", width = 0.5 ) +\n    geom_text(aes(neighbourhood_group, perc, label = paste0(perc, \"%\")),\n              position = position_stack(vjust = 0.5),\n              vjust = 0.07)+\n  scale_fill_viridis_d(name = \"\")+\n    theme_fivethirtyeight()"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#summary-stats-for-price-based-on-location",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#summary-stats-for-price-based-on-location",
    "title": "New york Airbnb",
    "section": "Summary Stats for price based on location",
    "text": "Summary Stats for price based on location\n\nairbnb_nyc[, price := as.double(price)]\n\nsummary_function <- function(by_col){\n    \n    summary_stats <- airbnb_nyc[!is.na(price)&price !=0, \n                            .(Mean = round(mean(price), 2), \n                              Median = median(price),\n                              First_quartile = quantile(price, .25),\n                              Third_quartile = quantile(price, .75),\n                              Min = min(price),\n                              Max = max(price)),\n                            by = by_col]\n    return(summary_stats)\n}\n\n\ndatatable(summary_function(by_col = \"neighbourhood_group\"))"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#summary-stats-price-based-on-room-type",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#summary-stats-price-based-on-room-type",
    "title": "New york Airbnb",
    "section": "Summary Stats price based on room type",
    "text": "Summary Stats price based on room type\n\ndatatable(summary_function(by_col = \"room_type\"))"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#median-price-roomtype-in-different-neighbourhoods",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#median-price-roomtype-in-different-neighbourhoods",
    "title": "New york Airbnb",
    "section": "Median price roomtype in different neighbourhoods",
    "text": "Median price roomtype in different neighbourhoods\n\nairbnb_nyc[, .(Median = median(price)), \n           by = .(neighbourhood_group, room_type)] %>%\n  dcast(neighbourhood_group ~room_type, value.var = \"Median\") %>%\n  datatable()"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#get-map-using-ggmap",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#get-map-using-ggmap",
    "title": "New york Airbnb",
    "section": "Get map using ggmap",
    "text": "Get map using ggmap\n\nnewyork_map <- get_map(c(left = min(airbnb_nyc$longitude) - .0001,\n                         bottom = min(airbnb_nyc$latitude) - .0001,\n                         right = max(airbnb_nyc$longitude) + .0001,\n                         top = max(airbnb_nyc$latitude) + .0001),\n                       maptype = \"watercolor\", source = \"stamen\")"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#mapping-function",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#mapping-function",
    "title": "New york Airbnb",
    "section": "Mapping function",
    "text": "Mapping function\n\nmap_plot <- function(df, color_col, continues_color_col = TRUE){\n  \n  if(continues_color_col) {\n    \n    scale_fill <- scale_color_viridis_c()\n    \n    } else{\n      \n      scale_fill <- scale_color_viridis_d()\n      \n    }\n    \n    \n  ggmap(newyork_map) +\n    geom_point(data =df, \n               aes_string(\"longitude\", \"latitude\",\n                          color = color_col), size = 1)+\n    theme(legend.position = \"bottom\")+\n                           \n    scale_fill\n  \n  \n}"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#newyork-price-map",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#newyork-price-map",
    "title": "New york Airbnb",
    "section": "Newyork price map",
    "text": "Newyork price map\n\nper95 <- airbnb_nyc[, quantile(price, 0.95)]\nmap_plot(df = airbnb_nyc[price <=per95  ], \n         color_col = \"price\")"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#categorise-price-variable",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#categorise-price-variable",
    "title": "New york Airbnb",
    "section": "Categorise price variable",
    "text": "Categorise price variable\n\nbreaks <-  quantile(airbnb_nyc$price, seq(0, 1, by = .1))\n\nairbnb_nyc[, price_factor := cut(price, breaks = breaks,\n                                 include.lowest = TRUE)]\n\nmap_plot(df = airbnb_nyc, \n         color_col = \"price_factor\",\n         continues_color_col = FALSE)"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#reviews-per-month",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#reviews-per-month",
    "title": "New york Airbnb",
    "section": "Reviews per month",
    "text": "Reviews per month\n\nWe can use this as a proxy of host receiving a lot of guests\nwe could use this for instance to check if some neighborhoods are more popular\n\n\nper95_rev <- airbnb_nyc[!is.na(last_review) & !is.na(reviews_per_month),\n                        quantile(reviews_per_month, 0.95)]\n\nmap_plot(df = airbnb_nyc[reviews_per_month < per95_rev ], \n         color_col = \"reviews_per_month\")"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#dates",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#dates",
    "title": "New york Airbnb",
    "section": "Dates",
    "text": "Dates\n\nairbnb_nyc[, last_review := ymd(last_review)]\nairbnb_nyc[, summary(last_review)]\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"2011-03-28\" \"2018-07-08\" \"2019-05-19\" \"2018-10-04\" \"2019-06-23\" \"2019-07-08\" \n        NA's \n     \"10052\" \n\n\n\nWork in progress !"
  },
  {
    "objectID": "kaggle/who_stats/who_stats.html",
    "href": "kaggle/who_stats/who_stats.html",
    "title": "World Health 2020 STATS",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(DT)\nlibrary(ggthemes)\nlibrary(leaflet)\nlibrary(ggmap)\nlibrary(plotly)\nlibrary(lubridate)\nadolescentBirthRate <- fread(\"data/adolescentBirthRate.csv\")\n\n\nold_nms <- names(adolescentBirthRate)\nold_nms\n\n[1] \"Location\"      \"Period\"        \"Indicator\"     \"First Tooltip\"\n\nold_nms <- old_nms %>% tolower()\nold_nms <- gsub(\"\\\\s\", \"_\", old_nms)\nold_nms\n\n[1] \"location\"      \"period\"        \"indicator\"     \"first_tooltip\"\n\nnames(adolescentBirthRate) <- old_nms\n\nhead(adolescentBirthRate, 10) %>% datatable()\n\n\n\n\n\n\n\nea_country <- c(\"Kenya\", \"Uganda\",  \"Tanzania\")\n\nea_data <- adolescentBirthRate[location %in% ea_country ]\n\np = ggplot(ea_data, aes(period, first_tooltip, group = location, color = location) ) +\n    geom_line(sizee = 1)+\n    theme_hc()+\n    labs(title = \"\", x = \"Year\", y = \"%\")+\n    scale_color_viridis_d(name=\"\")\np"
  },
  {
    "objectID": "kaggle/climate_change/scripts/kenyan_temp.html",
    "href": "kaggle/climate_change/scripts/kenyan_temp.html",
    "title": "The trend of Earth surface temperatures in Kenyan towns",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(here)\nlibrary(knitr)\nlibrary(foreach)\nlibrary(ggthemes)\n\n\n#global_land_temp <- fread(here(\"data/GlobalLandTemperaturesByCity.csv\"))\nload(here(\"data/GlobalLandTemperaturesByCity.rda\"))\nglobal_land_temp <- global_land_temp[!is.na(AverageTemperature)]\nglobal_land_temp[, lat := str_extract(Latitude, \"N|S\")]\n\nglobal_land_temp[, lon := str_extract(Longitude, \"E|W\")]\n\nlat_cols <- c(\"Latitude\", \"Longitude\")\nglobal_land_temp[ ,(lat_cols) := lapply(.SD, gsub, pattern = \"N|W|E|S\", replacement = \"\"), \n                                           .SDcols = lat_cols]\nglobal_land_temp[, Latitude := as.numeric(Latitude)]\nglobal_land_temp[, Latitude := ifelse(lat == \"S\", - Latitude, Latitude)]\nglobal_land_temp[, Longitude := as.numeric(Longitude)]\nglobal_land_temp[, Longitude := ifelse(lon == \"W\", - Longitude, Longitude)]\n\n\ncities_dates <- global_land_temp[, .(min_date = min(dt), \n                              max_date = max(dt), freq = .N), by  = City][\n                                min_date < \"1904-01-01\" & \n                                  max_date > \"2013-08-01\" & freq > 1400\n                              ]\n\nkenya <- global_land_temp[Country == \"Kenya\"]\n\n\nkisumu <- global_land_temp[City == \"Kisumu\"]\n\n\ntowns = unique(kenya$City)\n#towns <- sample(towns, 5)\n#kenya <- global_land_temp[City %in% towns ]\nkenyaTowns = split(kenya, kenya$City)\n\ncities <- foreach(i = 1:length(kenyaTowns)) %do% {\n  this = kenyaTowns[[i]]\n  this = this[!is.na(AverageTemperature) & dt >= \"1900-01-01\"]\n  this[, AverageTemperature := ts(AverageTemperature, start = c(1900, 1), end = c(2013, 8), frequency = 12)]\n  this[, AverageTemperature := stats::filter(AverageTemperature, rep(1, 120)/120, method = \"con\", sides = 2)]\n}\n\n\nsummaryStats <- list()\n\nfor(i in 1:length(kenyaTowns)){\n  this <- setDT(kenyaTowns[[i]])\n  this <- this %>% group_by(City, dt) %>%\n    summarise(max = max(AverageTemperature, na.rm = T)) %>% arrange(desc(max))\n  summaryStats[[i]] <- this[1:2, ]\n\n}\nrbindlist(summaryStats) %>% kable\n\n\n\n\nCity\ndt\nmax\n\n\n\n\nEldoret\n2005-02-01\n24.536\n\n\nEldoret\n1997-03-01\n24.052\n\n\nKisumu\n2005-02-01\n24.636\n\n\nKisumu\n1997-03-01\n24.301\n\n\nMombasa\n2003-03-01\n28.974\n\n\nMombasa\n1987-03-01\n28.903\n\n\nNairobi\n1987-03-01\n19.446\n\n\nNairobi\n2005-02-01\n19.431\n\n\nNakuru\n1987-03-01\n19.446\n\n\nNakuru\n2005-02-01\n19.431\n\n\nRuiru\n1987-03-01\n25.064\n\n\nRuiru\n2005-03-01\n24.554\n\n\n\n\n\n\nlibrary(plotly)\ntitl = paste(towns, \"simple Moving average surface temperature from 1900 to 2015\")\ny = expression(\"Average temperature (  \"  * degree~C * \" )\")\n#my_plots <-htmltools::tagList()\nmy_plots <- list()\nfor (i in 1:length(cities)) {\n  this = cities[[i]]\n  this[, dt2 := dt]\n  this[, Date := as.character(dt2)]\n  temp_max <- max(this$AverageTemperature, na.rm = T)\n  temp_min <- min(this$AverageTemperature, na.rm = T)\n  mybreaks = seq(temp_min, temp_max, by = .2) %>% round(2)\n  p = ggplot(this, aes(x = dt2, y = AverageTemperature, label = Date))+\n    geom_line(colour = \"blue\")+ theme_hc()+\n  labs(x = \"Year\", y = y, title = titl[i])+\n    scale_y_continuous(breaks = mybreaks)+\n  #ylim(min(this$AverageTemperature)-2, max(this$AverageTemperature)+2)+\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\")\n  my_plots[[i]] = p # ggplotly(p) %>% as_widget()\n  #p\n}\nmy_plots\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n[[6]]"
  },
  {
    "objectID": "kenya_population/ea_poverty.html",
    "href": "kenya_population/ea_poverty.html",
    "title": "East Africa Poverty Indicators",
    "section": "",
    "text": "kenya poverty data Uganda poverty data Tanzania poverty data Tanzania poverty data\n\n\n\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sf)\nlibrary(DT)\nlibrary(tmap)\nlibrary(ggthemes)"
  },
  {
    "objectID": "kenya_population/ea_poverty.html#read-data",
    "href": "kenya_population/ea_poverty.html#read-data",
    "title": "East Africa Poverty Indicators",
    "section": "Read data",
    "text": "Read data\n\nRead data and row bind\n\n\npoverty_data <- fread(\"poverty_data/9c15861e-aeec-486a-8e4c-8bd7c9a40275_Data.csv\", na.strings = c(\"NA\", \"..\", \" \"))\n\n\npoverty_data[sample(nrow(poverty_data), 10)] %>% datatable(options = list(scrollX = TRUE))"
  },
  {
    "objectID": "kenya_population/ea_poverty.html#minor-cleaning",
    "href": "kenya_population/ea_poverty.html#minor-cleaning",
    "title": "East Africa Poverty Indicators",
    "section": "Minor Cleaning",
    "text": "Minor Cleaning\n\nConvert to long\nConvert column names into lower\nReplace space with underscore\n\n\nnms_old <- names(poverty_data)[1:4]\npoverty_data <- melt(poverty_data,\n                                id.vars = nms_old, \n                           variable.factor = F,\n                           value.factor = F,\n                           variable.name = \"year\")\npoverty_data[, value:= as.numeric(value)]\n\npoverty_data[, year:= gsub(\"\\\\[.*\", \"\", year)]\npoverty_data[, year := str_trim(year)]\npoverty_data[, year := as.numeric(year)]\npoverty_data <-  poverty_data[!is.na(value)]\n\n\nnms_new <- nms_old %>% tolower() \nnms_new <- gsub(\"\\\\s\", \"_\", nms_new)\nnms_new\n\n[1] \"country_name\" \"country_code\" \"series_name\"  \"series_code\" \n\nsetnames(poverty_data, nms_old, nms_new)\npoverty_data[, value:= as.numeric(value)]\npoverty_data <-  poverty_data[!is.na(value)]\n\npoverty_data[sample(nrow(poverty_data), 10)] %>% datatable(options = list(scrollX = TRUE))"
  },
  {
    "objectID": "kenya_population/ea_poverty.html#plots",
    "href": "kenya_population/ea_poverty.html#plots",
    "title": "East Africa Poverty Indicators",
    "section": "Plots",
    "text": "Plots\n\npoverty_data[, year := as.numeric(year)]\nea_country <- c(\"Kenya\", \"Uganda\", \"Ghana\", \"Tanzania\")\n\npoverty_data[, unique(series_name)]\n\n [1] \"Population, total\"                                                                                                                                          \n [2] \"Gini index (World Bank estimate)\"                                                                                                                           \n [3] \"Income share held by fourth 20%\"                                                                                                                            \n [4] \"Income share held by highest 10%\"                                                                                                                           \n [5] \"Income share held by highest 20%\"                                                                                                                           \n [6] \"Income share held by lowest 10%\"                                                                                                                            \n [7] \"Income share held by lowest 20%\"                                                                                                                            \n [8] \"Income share held by second 20%\"                                                                                                                            \n [9] \"Income share held by third 20%\"                                                                                                                             \n[10] \"Number of poor at $1.90 a day (2011 PPP) (millions)\"                                                                                                        \n[11] \"Number of poor at $3.20 a day (2011 PPP) (millions)\"                                                                                                        \n[12] \"Number of poor at $5.50 a day (2011 PPP) (millions)\"                                                                                                        \n[13] \"Poverty gap at $1.90 a day (2011 PPP) (%)\"                                                                                                                  \n[14] \"Poverty gap at $3.20 a day (2011 PPP) (% of population)\"                                                                                                    \n[15] \"Poverty gap at $5.50 a day (2011 PPP) (% of population)\"                                                                                                    \n[16] \"Poverty headcount ratio at $1.90 a day (2011 PPP) (% of population)\"                                                                                        \n[17] \"Poverty headcount ratio at $3.20 a day (2011 PPP) (% of population)\"                                                                                        \n[18] \"Poverty headcount ratio at $5.50 a day (2011 PPP) (% of population)\"                                                                                        \n[19] \"Annualized growth in per capita real survey mean consumption or income, bottom 40% (%)\"                                                                     \n[20] \"Annualized growth in per capita real survey mean consumption or income, top 10% (%)\"                                                                        \n[21] \"Annualized growth in per capita real survey mean consumption or income, top 60% (%)\"                                                                        \n[22] \"Annualized growth in per capita real survey mean consumption or income, total population (%)\"                                                               \n[23] \"Annualized growth in per capita real survey median income or consumption expenditure (%)\"                                                                   \n[24] \"Median daily per capita income or consumption expenditure (2011 PPP)\"                                                                                       \n[25] \"Multidimensional poverty, Drinking water (% of population deprived)\"                                                                                        \n[26] \"Multidimensional poverty, Educational attainment (% of population deprived)\"                                                                                \n[27] \"Multidimensional poverty, Educational enrollment (% of population deprived)\"                                                                                \n[28] \"Multidimensional poverty, Electricity (% of population deprived)\"                                                                                           \n[29] \"Multidimensional poverty, Headcount ratio (% of population)\"                                                                                                \n[30] \"Multidimensional poverty, Monetary poverty (% of population deprived)\"                                                                                      \n[31] \"Multidimensional poverty, Sanitation (% of population deprived)\"                                                                                            \n[32] \"Poverty gap at national poverty lines (%)\"                                                                                                                  \n[33] \"Poverty gap at national poverty lines (%), including noncomparable values\"                                                                                  \n[34] \"Poverty headcount ratio at $1.90 a day, age 0-14  (2011 PPP) (% of population age 0-14)\"                                                                    \n[35] \"Poverty headcount ratio at $1.90 a day, age 15-64 (2011 PPP) (% of population age 15-64)\"                                                                   \n[36] \"Poverty headcount ratio at $1.90 a day, age 65+ (2011 PPP) (% of population age 65+)\"                                                                       \n[37] \"Poverty headcount ratio at $1.90 a day, Female (2011 PPP) (% of female population)\"                                                                         \n[38] \"Poverty headcount ratio at $1.90 a day, Male  (2011 PPP) (% of male population)\"                                                                            \n[39] \"Poverty headcount ratio at $1.90 a day, rural (2011 PPP) (% of rural population)\"                                                                           \n[40] \"Poverty headcount ratio at $1.90 a day, urban (2011 PPP) (% of urban population)\"                                                                           \n[41] \"Poverty headcount ratio at $1.90 a day, with primary education (2011 PPP) (% of population age 16+ with primary education)\"                                 \n[42] \"Poverty headcount ratio at $1.90 a day, with secondary education (2011 PPP) (% of population age 16+ with secondary education)\"                             \n[43] \"Poverty headcount ratio at $1.90 a day, without education (2011 PPP) (% of population age 16+ without education)\"                                           \n[44] \"Poverty headcount ratio at $1.90 a day,  with Tertiary/post-secondary education (2011 PPP) (% of population age 16+ with Tertiary/post-secondary education)\"\n[45] \"Poverty headcount ratio at national poverty lines (% of population)\"                                                                                        \n[46] \"Poverty headcount ratio at national poverty lines (% of population), including noncomparable values\"                                                        \n[47] \"Rural poverty gap at national poverty lines (%)\"                                                                                                            \n[48] \"Rural poverty gap at national poverty lines (%), including noncomparable values\"                                                                            \n[49] \"Rural poverty headcount ratio at national poverty lines (% of rural population)\"                                                                            \n[50] \"Rural poverty headcount ratio at national poverty lines (% of rural population), including noncomparable values\"                                            \n[51] \"Survey mean consumption or income per capita, bottom 40% (2011 PPP $ per day)\"                                                                              \n[52] \"Survey mean consumption or income per capita, top 10% (2011 PPP $ per day)\"                                                                                 \n[53] \"Survey mean consumption or income per capita, top 60% (2011 PPP $ per day)\"                                                                                 \n[54] \"Survey mean consumption or income per capita, total population (2011 PPP $ per day)\"                                                                        \n[55] \"Urban poverty gap at national poverty lines (%)\"                                                                                                            \n[56] \"Urban poverty gap at national poverty lines (%), including noncomparable values\"                                                                            \n[57] \"Urban poverty headcount ratio at national poverty lines (% of urban population)\"                                                                            \n[58] \"Urban poverty headcount ratio at national poverty lines (% of urban population), including noncomparable values\"                                            \n[59] \"\"                                                                                                                                                           \n\nindicator <- c(\"Poverty gap at $1.90 a day (2011 PPP) (%)\", \n               \"Income share held by highest 10%\",\n               \"Income share held by lowest 10%\", \n               \"Income share held by highest 20%\",\n               \"Income share held by lowest 20%\",\n               \"Multidimensional poverty, Drinking water (% of population deprived)\",\n               \"Multidimensional poverty, Educational attainment (% of population deprived)\",\n               \"Poverty gap at $3.20 a day (2011 PPP) (% of population)\")\n\npoverty_data <- poverty_data[country_name %in% ea_country & series_name %in% indicator]\npoverty_data_split <- split(poverty_data, f = poverty_data$series_name)\n\nn <- length(poverty_data_split)\nmy_plots <-htmltools::tagList()\nfor (i in 1:n) {\n    df = poverty_data_split[[i]]\n    my_title = df[, unique(series_name)]\n    mn = df[, min(year)]\n    mx = df[, max(year)]\n    breaks = seq(mn, mx,by = 3)\n    p = ggplot(df, aes(year, value, group = country_name, color = country_name) ) +\n        geom_line()+\n        theme_hc()+\n        labs(title = my_title, x = \"year\", y = \"%\")+\n      scale_color_viridis_d(name=\"\")+\n      scale_x_continuous(breaks = breaks)\n    \n    my_plots[[i]] = ggplotly(p)\n    \n}\n\nmy_plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neconomy_and_growth <- fread(\"API_3_DS2_en_csv_v2_1743760.csv\") \n\nnms_old <- economy_and_growth[1,]  %>% as.character()\nnms_old\n\n [1] \"Country Name\"   \"Country Code\"   \"Indicator Name\" \"Indicator Code\"\n [5] \"1960\"           \"1961\"           \"1962\"           \"1963\"          \n [9] \"1964\"           \"1965\"           \"1966\"           \"1967\"          \n[13] \"1968\"           \"1969\"           \"1970\"           \"1971\"          \n[17] \"1972\"           \"1973\"           \"1974\"           \"1975\"          \n[21] \"1976\"           \"1977\"           \"1978\"           \"1979\"          \n[25] \"1980\"           \"1981\"           \"1982\"           \"1983\"          \n[29] \"1984\"           \"1985\"           \"1986\"           \"1987\"          \n[33] \"1988\"           \"1989\"           \"1990\"           \"1991\"          \n[37] \"1992\"           \"1993\"           \"1994\"           \"1995\"          \n[41] \"1996\"           \"1997\"           \"1998\"           \"1999\"          \n[45] \"2000\"           \"2001\"           \"2002\"           \"2003\"          \n[49] \"2004\"           \"2005\"           \"2006\"           \"2007\"          \n[53] \"2008\"           \"2009\"           \"2010\"           \"2011\"          \n[57] \"2012\"           \"2013\"           \"2014\"           \"2015\"          \n[61] \"2016\"           \"2017\"           \"2018\"           \"2019\"          \n[65] \"2020\"           \"NA\"            \n\neconomy_and_growth <- economy_and_growth[-1,]\nnames(economy_and_growth) <- nms_old\nnms_new <- nms_old %>% tolower() \nnms_new <- gsub(\"\\\\s\", \"_\", nms_new)\nnms_new\n\n [1] \"country_name\"   \"country_code\"   \"indicator_name\" \"indicator_code\"\n [5] \"1960\"           \"1961\"           \"1962\"           \"1963\"          \n [9] \"1964\"           \"1965\"           \"1966\"           \"1967\"          \n[13] \"1968\"           \"1969\"           \"1970\"           \"1971\"          \n[17] \"1972\"           \"1973\"           \"1974\"           \"1975\"          \n[21] \"1976\"           \"1977\"           \"1978\"           \"1979\"          \n[25] \"1980\"           \"1981\"           \"1982\"           \"1983\"          \n[29] \"1984\"           \"1985\"           \"1986\"           \"1987\"          \n[33] \"1988\"           \"1989\"           \"1990\"           \"1991\"          \n[37] \"1992\"           \"1993\"           \"1994\"           \"1995\"          \n[41] \"1996\"           \"1997\"           \"1998\"           \"1999\"          \n[45] \"2000\"           \"2001\"           \"2002\"           \"2003\"          \n[49] \"2004\"           \"2005\"           \"2006\"           \"2007\"          \n[53] \"2008\"           \"2009\"           \"2010\"           \"2011\"          \n[57] \"2012\"           \"2013\"           \"2014\"           \"2015\"          \n[61] \"2016\"           \"2017\"           \"2018\"           \"2019\"          \n[65] \"2020\"           \"na\"            \n\nsetnames(economy_and_growth, nms_old, nms_new)\n\nid_vars <- c(\"country_name\", \"country_code\",\n             \"indicator_name\", \"indicator_code\")\n\neconomy_and_growth <- melt(economy_and_growth,\n                                id.vars = id_vars, \n                           variable.factor = F,\n                           value.factor = F,\n                           variable.name = \"year\")\neconomy_and_growth[, value:= as.numeric(value)]\n\neconomy_and_growth[, year := as.numeric(year)]\neconomy_and_growth <-  economy_and_growth[!is.na(value)|!is.na(year)]\neconomy_and_growth[sample(nrow(economy_and_growth), 10)] %>% datatable(options = list(scrollX = TRUE))\n\n\n\n\n\n\n\ngni_gdp_savings_vec <- c(\"Gross savings (% of GNI)\", \n                         \"Gross savings (% of GDP)\",\n                         \"Total debt service (% of exports of goods, services and primary income)\",\n                         \"Total debt service (% of GNI)\",\n                         \"Trade (% of GDP)\",\n                         \"Current account balance (% of GDP)\",\n                         \"Exports of goods and services (annual % growth)\",\n                         \"Manufacturing, value added (annual % growth)\",\n                         \"Price level ratio of PPP conversion factor (GDP) to market exchange rate\")\nea_country <- c(\"Kenya\", \"Uganda\",  \"Tanzania\")\neconomy_and_growth[, unique(country_name)]\n\n  [1] \"Aruba\"                                               \n  [2] \"Afghanistan\"                                         \n  [3] \"Angola\"                                              \n  [4] \"Albania\"                                             \n  [5] \"Andorra\"                                             \n  [6] \"Arab World\"                                          \n  [7] \"United Arab Emirates\"                                \n  [8] \"Argentina\"                                           \n  [9] \"Armenia\"                                             \n [10] \"American Samoa\"                                      \n [11] \"Antigua and Barbuda\"                                 \n [12] \"Australia\"                                           \n [13] \"Austria\"                                             \n [14] \"Azerbaijan\"                                          \n [15] \"Burundi\"                                             \n [16] \"Belgium\"                                             \n [17] \"Benin\"                                               \n [18] \"Burkina Faso\"                                        \n [19] \"Bangladesh\"                                          \n [20] \"Bulgaria\"                                            \n [21] \"Bahrain\"                                             \n [22] \"Bahamas, The\"                                        \n [23] \"Bosnia and Herzegovina\"                              \n [24] \"Belarus\"                                             \n [25] \"Belize\"                                              \n [26] \"Bermuda\"                                             \n [27] \"Bolivia\"                                             \n [28] \"Brazil\"                                              \n [29] \"Barbados\"                                            \n [30] \"Brunei Darussalam\"                                   \n [31] \"Bhutan\"                                              \n [32] \"Botswana\"                                            \n [33] \"Central African Republic\"                            \n [34] \"Canada\"                                              \n [35] \"Central Europe and the Baltics\"                      \n [36] \"Switzerland\"                                         \n [37] \"Channel Islands\"                                     \n [38] \"Chile\"                                               \n [39] \"China\"                                               \n [40] \"Cote d'Ivoire\"                                       \n [41] \"Cameroon\"                                            \n [42] \"Congo, Dem. Rep.\"                                    \n [43] \"Congo, Rep.\"                                         \n [44] \"Colombia\"                                            \n [45] \"Comoros\"                                             \n [46] \"Cabo Verde\"                                          \n [47] \"Costa Rica\"                                          \n [48] \"Caribbean small states\"                              \n [49] \"Cuba\"                                                \n [50] \"Curacao\"                                             \n [51] \"Cayman Islands\"                                      \n [52] \"Cyprus\"                                              \n [53] \"Czech Republic\"                                      \n [54] \"Germany\"                                             \n [55] \"Djibouti\"                                            \n [56] \"Dominica\"                                            \n [57] \"Denmark\"                                             \n [58] \"Dominican Republic\"                                  \n [59] \"Algeria\"                                             \n [60] \"East Asia & Pacific (excluding high income)\"         \n [61] \"Early-demographic dividend\"                          \n [62] \"East Asia & Pacific\"                                 \n [63] \"Europe & Central Asia (excluding high income)\"       \n [64] \"Europe & Central Asia\"                               \n [65] \"Ecuador\"                                             \n [66] \"Egypt, Arab Rep.\"                                    \n [67] \"Euro area\"                                           \n [68] \"Eritrea\"                                             \n [69] \"Spain\"                                               \n [70] \"Estonia\"                                             \n [71] \"Ethiopia\"                                            \n [72] \"European Union\"                                      \n [73] \"Fragile and conflict affected situations\"            \n [74] \"Finland\"                                             \n [75] \"Fiji\"                                                \n [76] \"France\"                                              \n [77] \"Faroe Islands\"                                       \n [78] \"Micronesia, Fed. Sts.\"                               \n [79] \"Gabon\"                                               \n [80] \"United Kingdom\"                                      \n [81] \"Georgia\"                                             \n [82] \"Ghana\"                                               \n [83] \"Gibraltar\"                                           \n [84] \"Guinea\"                                              \n [85] \"Gambia, The\"                                         \n [86] \"Guinea-Bissau\"                                       \n [87] \"Equatorial Guinea\"                                   \n [88] \"Greece\"                                              \n [89] \"Grenada\"                                             \n [90] \"Greenland\"                                           \n [91] \"Guatemala\"                                           \n [92] \"Guam\"                                                \n [93] \"Guyana\"                                              \n [94] \"High income\"                                         \n [95] \"Hong Kong SAR, China\"                                \n [96] \"Honduras\"                                            \n [97] \"Heavily indebted poor countries (HIPC)\"              \n [98] \"Croatia\"                                             \n [99] \"Haiti\"                                               \n[100] \"Hungary\"                                             \n[101] \"IBRD only\"                                           \n[102] \"IDA & IBRD total\"                                    \n[103] \"IDA total\"                                           \n[104] \"IDA blend\"                                           \n[105] \"Indonesia\"                                           \n[106] \"IDA only\"                                            \n[107] \"Isle of Man\"                                         \n[108] \"India\"                                               \n[109] \"Not classified\"                                      \n[110] \"Ireland\"                                             \n[111] \"Iran, Islamic Rep.\"                                  \n[112] \"Iraq\"                                                \n[113] \"Iceland\"                                             \n[114] \"Israel\"                                              \n[115] \"Italy\"                                               \n[116] \"Jamaica\"                                             \n[117] \"Jordan\"                                              \n[118] \"Japan\"                                               \n[119] \"Kazakhstan\"                                          \n[120] \"Kenya\"                                               \n[121] \"Kyrgyz Republic\"                                     \n[122] \"Cambodia\"                                            \n[123] \"Kiribati\"                                            \n[124] \"St. Kitts and Nevis\"                                 \n[125] \"Korea, Rep.\"                                         \n[126] \"Kuwait\"                                              \n[127] \"Latin America & Caribbean (excluding high income)\"   \n[128] \"Lao PDR\"                                             \n[129] \"Lebanon\"                                             \n[130] \"Liberia\"                                             \n[131] \"Libya\"                                               \n[132] \"St. Lucia\"                                           \n[133] \"Latin America & Caribbean\"                           \n[134] \"Least developed countries: UN classification\"        \n[135] \"Low income\"                                          \n[136] \"Liechtenstein\"                                       \n[137] \"Sri Lanka\"                                           \n[138] \"Lower middle income\"                                 \n[139] \"Low & middle income\"                                 \n[140] \"Lesotho\"                                             \n[141] \"Late-demographic dividend\"                           \n[142] \"Lithuania\"                                           \n[143] \"Luxembourg\"                                          \n[144] \"Latvia\"                                              \n[145] \"Macao SAR, China\"                                    \n[146] \"St. Martin (French part)\"                            \n[147] \"Morocco\"                                             \n[148] \"Monaco\"                                              \n[149] \"Moldova\"                                             \n[150] \"Madagascar\"                                          \n[151] \"Maldives\"                                            \n[152] \"Middle East & North Africa\"                          \n[153] \"Mexico\"                                              \n[154] \"Marshall Islands\"                                    \n[155] \"Middle income\"                                       \n[156] \"North Macedonia\"                                     \n[157] \"Mali\"                                                \n[158] \"Malta\"                                               \n[159] \"Myanmar\"                                             \n[160] \"Middle East & North Africa (excluding high income)\"  \n[161] \"Montenegro\"                                          \n[162] \"Mongolia\"                                            \n[163] \"Northern Mariana Islands\"                            \n[164] \"Mozambique\"                                          \n[165] \"Mauritania\"                                          \n[166] \"Mauritius\"                                           \n[167] \"Malawi\"                                              \n[168] \"Malaysia\"                                            \n[169] \"North America\"                                       \n[170] \"Namibia\"                                             \n[171] \"New Caledonia\"                                       \n[172] \"Niger\"                                               \n[173] \"Nigeria\"                                             \n[174] \"Nicaragua\"                                           \n[175] \"Netherlands\"                                         \n[176] \"Norway\"                                              \n[177] \"Nepal\"                                               \n[178] \"Nauru\"                                               \n[179] \"New Zealand\"                                         \n[180] \"OECD members\"                                        \n[181] \"Oman\"                                                \n[182] \"Other small states\"                                  \n[183] \"Pakistan\"                                            \n[184] \"Panama\"                                              \n[185] \"Peru\"                                                \n[186] \"Philippines\"                                         \n[187] \"Palau\"                                               \n[188] \"Papua New Guinea\"                                    \n[189] \"Poland\"                                              \n[190] \"Pre-demographic dividend\"                            \n[191] \"Puerto Rico\"                                         \n[192] \"Korea, Dem. People’s Rep.\"                           \n[193] \"Portugal\"                                            \n[194] \"Paraguay\"                                            \n[195] \"West Bank and Gaza\"                                  \n[196] \"Pacific island small states\"                         \n[197] \"Post-demographic dividend\"                           \n[198] \"French Polynesia\"                                    \n[199] \"Qatar\"                                               \n[200] \"Romania\"                                             \n[201] \"Russian Federation\"                                  \n[202] \"Rwanda\"                                              \n[203] \"South Asia\"                                          \n[204] \"Saudi Arabia\"                                        \n[205] \"Sudan\"                                               \n[206] \"Senegal\"                                             \n[207] \"Singapore\"                                           \n[208] \"Solomon Islands\"                                     \n[209] \"Sierra Leone\"                                        \n[210] \"El Salvador\"                                         \n[211] \"San Marino\"                                          \n[212] \"Somalia\"                                             \n[213] \"Serbia\"                                              \n[214] \"Sub-Saharan Africa (excluding high income)\"          \n[215] \"South Sudan\"                                         \n[216] \"Sub-Saharan Africa\"                                  \n[217] \"Small states\"                                        \n[218] \"Sao Tome and Principe\"                               \n[219] \"Suriname\"                                            \n[220] \"Slovak Republic\"                                     \n[221] \"Slovenia\"                                            \n[222] \"Sweden\"                                              \n[223] \"Eswatini\"                                            \n[224] \"Sint Maarten (Dutch part)\"                           \n[225] \"Seychelles\"                                          \n[226] \"Syrian Arab Republic\"                                \n[227] \"Turks and Caicos Islands\"                            \n[228] \"Chad\"                                                \n[229] \"East Asia & Pacific (IDA & IBRD countries)\"          \n[230] \"Europe & Central Asia (IDA & IBRD countries)\"        \n[231] \"Togo\"                                                \n[232] \"Thailand\"                                            \n[233] \"Tajikistan\"                                          \n[234] \"Turkmenistan\"                                        \n[235] \"Latin America & the Caribbean (IDA & IBRD countries)\"\n[236] \"Timor-Leste\"                                         \n[237] \"Middle East & North Africa (IDA & IBRD countries)\"   \n[238] \"Tonga\"                                               \n[239] \"South Asia (IDA & IBRD)\"                             \n[240] \"Sub-Saharan Africa (IDA & IBRD countries)\"           \n[241] \"Trinidad and Tobago\"                                 \n[242] \"Tunisia\"                                             \n[243] \"Turkey\"                                              \n[244] \"Tuvalu\"                                              \n[245] \"Tanzania\"                                            \n[246] \"Uganda\"                                              \n[247] \"Ukraine\"                                             \n[248] \"Upper middle income\"                                 \n[249] \"Uruguay\"                                             \n[250] \"United States\"                                       \n[251] \"Uzbekistan\"                                          \n[252] \"St. Vincent and the Grenadines\"                      \n[253] \"Venezuela, RB\"                                       \n[254] \"British Virgin Islands\"                              \n[255] \"Virgin Islands (U.S.)\"                               \n[256] \"Vietnam\"                                             \n[257] \"Vanuatu\"                                             \n[258] \"World\"                                               \n[259] \"Samoa\"                                               \n[260] \"Kosovo\"                                              \n[261] \"Yemen, Rep.\"                                         \n[262] \"South Africa\"                                        \n[263] \"Zambia\"                                              \n[264] \"Zimbabwe\"                                            \n\ngni_gdp_savings_df <- economy_and_growth[indicator_name %in% gni_gdp_savings_vec & country_name %in% ea_country ]\n\n\ngni_gdp_savings_df_split <- split(gni_gdp_savings_df, f = gni_gdp_savings_df$indicator_name)\nn <- length(gni_gdp_savings_df_split)\n#my_plots_econ <-htmltools::tagList()\nmy_plots_econ <-list()\nfor (i in 1:n) {\n    df = gni_gdp_savings_df_split[[i]]\n    my_title = df[, unique(indicator_name)]\n    mn = df[, min(year)]\n    mx = df[, max(year)]\n    breaks = seq(mn, mx,by = 5)\n    p = ggplot(df, aes(year, value, group = country_name, color = country_name) ) +\n        geom_line(size = .3)+\n        theme_hc()+\n        labs(title = my_title, x = \"year\", y = \"%\")+\n      scale_color_colorblind(name=\"\")+\n      scale_x_continuous(breaks = breaks)\n    \n    my_plots_econ[[i]] = ggplotly(p)\n    \n}\n\nmy_plots_econ\n\n[[1]]\n\n[[2]]\n\n[[3]]\n\n[[4]]\n\n[[5]]\n\n[[6]]\n\n[[7]]\n\n[[8]]\n\n[[9]]"
  },
  {
    "objectID": "kenya_population/kenya_debt_and_dollar_price.html",
    "href": "kenya_population/kenya_debt_and_dollar_price.html",
    "title": "USD-KES Hisotrical Data",
    "section": "",
    "text": "library(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(DT)\nlibrary(ggthemes)\nlibrary(patchwork)\nusd_kes_hist <- fread(\"USD_KES Historical Data.csv\")\n\n\nusd_kes_hist[, Date :=  as.Date(Date, format = \"%b %d,%Y\")]\n\n\nx <- as.Date(\"2002-12-01\")\nx_end <- as.Date(\"2003-01-01\")\n\ny <-90\ny_end <- 77.500\n\nx_uhuru <- as.Date(\"2013-03-09\")\nx_end_uhuru <- as.Date(\"2013-04-09\")\n\ny_uhuru <-105\ny_end_uhuru <- 85.600\n\ndollar <- ggplot(usd_kes_hist,aes(Date, Price))+\n    geom_line()+\n    scale_x_date(date_breaks = \"3 year\", date_labels = \"%b-%y\")+\n    labs(y = \"1 USD to KES\", title = \"Historical Prices USD to KES\")+\n  \n    annotate(\n    geom = \"curve\", x = x, y = y, xend = x_end, yend = y_end, \n    curvature = .3, arrow = arrow(length = unit(3, \"mm\"))\n  ) +\n  annotate(geom = \"text\", x = x, y = y,\n           label = \"Pres Kibaki \\n takes Office\", hjust = \"left\",\n           angle = 30)+\n    annotate(\n    geom = \"curve\", x = x_uhuru, y = y_uhuru, \n    xend = x_end_uhuru, yend = y_end_uhuru, \n    curvature = .1, arrow = arrow(length = unit(3, \"mm\"))\n  ) +\n  annotate(geom = \"text\", x = x_uhuru, y = y_uhuru+2,\n           label = \"Pres Uhuru \\n takes Office\", hjust = \"left\",\n           angle = 30)\ndollar\n\n\n\n\n\nkenya_debt <- read_csv( \"kenya_debt/Public Debt (Ksh Million).csv\") %>% setDT()\nlibrary(lubridate)\nkenya_debt[, Date := paste(Year, Month, \"01\", sep = \"-\")]\nkenya_debt[, Date := ymd(Date)]\nkenya_debt[, perc_external := round(`External Debt`/Total* 100, 1)]\nkenya_debt[, Total := Total/1000000]\n\n\nx <- as.Date(\"2002-12-01\")\nx_end <- as.Date(\"2003-01-01\")\n\ny <-2.000000\ny_end <- .6152281\n\nx_uhuru <- as.Date(\"2013-03-09\")\nx_end_uhuru <- as.Date(\"2013-04-09\")\n\ny_uhuru <-4.000000\ny_end_uhuru <- 1.8824059\n\n\ndebt <- ggplot(kenya_debt, aes(Date, Total)) +\n    geom_line()+\n  \n    scale_x_date(date_breaks = \"3 year\", date_labels = \"%b-%y\")+\n  \n    labs(title = \"Kenya Debt from 1999 to 2020 June\",\n         y = \"Total Debt in Trillions(KES)\")+\n  \n    annotate(\n    geom = \"curve\", x = x, y = y, xend = x_end, yend = y_end, \n    curvature = .3, arrow = arrow(length = unit(2, \"mm\"))\n  ) +\n  annotate(geom = \"text\", x = x, y = y,\n           label = \"Pres Kibaki \\n takes Office\", hjust = \"left\",\n           angle = 30)+\n  \n    annotate(\n    geom = \"curve\", x = x_uhuru, y = y_uhuru, \n    xend = x_end_uhuru, yend = y_end_uhuru, \n    curvature = -.1, arrow = arrow(length = unit(3, \"mm\"))\n  ) +\n  \n  annotate(geom = \"text\", x = x_uhuru, y = y_uhuru+.5,\n           label = \"Pres Uhuru \\n takes Office\", hjust = \"right\",\n           angle = 30)\n \n\ndebt\n\n\n\n\n\nmy_breaks <- seq(15, 70, 5)\nexternal <- ggplot(kenya_debt, aes(Date, perc_external)) +\n    geom_line()+\n  \n    scale_x_date(date_breaks = \"2 year\", date_labels = \"%Y\")+\n  \n    labs(title = \"Kenya External Debt % from 1999 to 2020 June\",\n         y = \"External Debt (%)\")+\n    theme_hc()+\n    scale_y_continuous(breaks = my_breaks )\n\nexternal\n\n\n\n\n\ndebt_data <- fread(\"poverty_data/IDS-DRSCountries_WLD_Data.csv\")\n\n\nnms_old <-debt_data[1,]  %>% as.character()\nnms_old\n\n [1] \"Country Name\"          \"Country Code\"          \"Counterpart-Area Name\"\n [4] \"Counterpart-Area Code\" \"Series Name\"           \"Series Code\"          \n [7] \"1970\"                  \"1971\"                  \"1972\"                 \n[10] \"1973\"                  \"1974\"                  \"1975\"                 \n[13] \"1976\"                  \"1977\"                  \"1978\"                 \n[16] \"1979\"                  \"1980\"                  \"1981\"                 \n[19] \"1982\"                  \"1983\"                  \"1984\"                 \n[22] \"1985\"                  \"1986\"                  \"1987\"                 \n[25] \"1988\"                  \"1989\"                  \"1990\"                 \n[28] \"1991\"                  \"1992\"                  \"1993\"                 \n[31] \"1994\"                  \"1995\"                  \"1996\"                 \n[34] \"1997\"                  \"1998\"                  \"1999\"                 \n[37] \"2000\"                  \"2001\"                  \"2002\"                 \n[40] \"2003\"                  \"2004\"                  \"2005\"                 \n[43] \"2006\"                  \"2007\"                  \"2008\"                 \n[46] \"2009\"                  \"2010\"                  \"2011\"                 \n[49] \"2012\"                  \"2013\"                  \"2014\"                 \n[52] \"2015\"                  \"2016\"                  \"2017\"                 \n[55] \"2018\"                  \"2019\"                  \"2020\"                 \n[58] \"2021\"                  \"2022\"                  \"2023\"                 \n[61] \"2024\"                  \"2025\"                  \"2026\"                 \n[64] \"2027\"                 \n\ndebt_data <-debt_data[-1,]\nnames(debt_data) <- nms_old\nnms_new <- nms_old %>% tolower() \nnms_new <- gsub(\"\\\\s|-\", \"_\", nms_new)\nnms_new\n\n [1] \"country_name\"          \"country_code\"          \"counterpart_area_name\"\n [4] \"counterpart_area_code\" \"series_name\"           \"series_code\"          \n [7] \"1970\"                  \"1971\"                  \"1972\"                 \n[10] \"1973\"                  \"1974\"                  \"1975\"                 \n[13] \"1976\"                  \"1977\"                  \"1978\"                 \n[16] \"1979\"                  \"1980\"                  \"1981\"                 \n[19] \"1982\"                  \"1983\"                  \"1984\"                 \n[22] \"1985\"                  \"1986\"                  \"1987\"                 \n[25] \"1988\"                  \"1989\"                  \"1990\"                 \n[28] \"1991\"                  \"1992\"                  \"1993\"                 \n[31] \"1994\"                  \"1995\"                  \"1996\"                 \n[34] \"1997\"                  \"1998\"                  \"1999\"                 \n[37] \"2000\"                  \"2001\"                  \"2002\"                 \n[40] \"2003\"                  \"2004\"                  \"2005\"                 \n[43] \"2006\"                  \"2007\"                  \"2008\"                 \n[46] \"2009\"                  \"2010\"                  \"2011\"                 \n[49] \"2012\"                  \"2013\"                  \"2014\"                 \n[52] \"2015\"                  \"2016\"                  \"2017\"                 \n[55] \"2018\"                  \"2019\"                  \"2020\"                 \n[58] \"2021\"                  \"2022\"                  \"2023\"                 \n[61] \"2024\"                  \"2025\"                  \"2026\"                 \n[64] \"2027\"                 \n\nsetnames(debt_data, nms_old, nms_new)\nid_vars <- c(\"country_name\", \"country_code\", \"counterpart_area_name\", \n             \"counterpart_area_code\", \"series_name\", \"series_code\")\n\ndebt_data <- melt(debt_data,\n                                id.vars = id_vars, \n                           variable.factor = F,\n                           value.factor = F,\n                           variable.name = \"year\")\n\ndebt_data[, year := str_trim(year)]\ndebt_data[, year := as.numeric(year)]\ndebt_data <-  debt_data[!is.na(value)]\n\n\nindicator_name <- c(\"Currency composition of PPG debt, U.S. dollars (%)\",\n                    \"Interest payments on external debt (% of exports of goods, services and primary income)\",\n                    \"Interest payments on external debt (% of GNI)\",\n                     \"Short-term debt (% of total external debt)\",\n                     \"Multilateral debt (% of total external debt)\" )\n\n#debt_data[, unique(country_name)]\n#debt_data[, unique(series_name)]\n#\"Uganda\", \"Tanzania\"\nea_country <- c(\"Kenya\", \"Lower middle income\" )\n\n\ndebt_data <- debt_data[country_name %in% ea_country & series_name %in% indicator_name]\ndebt_data_split <- split(debt_data, f = debt_data$series_name)\n\nn <- length(debt_data_split)\nmy_plots <-htmltools::tagList()\nmy_plots <- list()\nfor (i in 1:n) {\n    df = debt_data_split[[i]]\n    my_title = df[, unique(series_name)]\n    mn = df[, min(year)]\n    mx = df[, max(year)]\n    breaks = seq(mn, mx,by = 3)\n    p = ggplot(df, aes(year, value, group = country_name, color = country_name) ) +\n        geom_line()+\n        theme_hc()+\n        labs(title = my_title, x = \"year\", y = \"%\")+\n      scale_color_viridis_d(name=\"\")+\n      scale_x_continuous(breaks = breaks)\n    \n   # my_plots[[i]] = ggplotly(p)\n    my_plots[[i]] = p\n    \n}\n\nmy_plots\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\nworld_debt_data  <- fread(\"poverty_data/API_DT.TDS.DECT.EX.ZS_DS2_en_csv_v2_1865914.csv\", \n                          skip = 4, header = T)\n\nnms_old <- names(world_debt_data)\n\nnms_new <- nms_old %>% tolower() \nnms_new <- gsub(\"\\\\s|-\", \"_\", nms_new)\nnms_new\n\n [1] \"country_name\"   \"country_code\"   \"indicator_name\" \"indicator_code\"\n [5] \"1960\"           \"1961\"           \"1962\"           \"1963\"          \n [9] \"1964\"           \"1965\"           \"1966\"           \"1967\"          \n[13] \"1968\"           \"1969\"           \"1970\"           \"1971\"          \n[17] \"1972\"           \"1973\"           \"1974\"           \"1975\"          \n[21] \"1976\"           \"1977\"           \"1978\"           \"1979\"          \n[25] \"1980\"           \"1981\"           \"1982\"           \"1983\"          \n[29] \"1984\"           \"1985\"           \"1986\"           \"1987\"          \n[33] \"1988\"           \"1989\"           \"1990\"           \"1991\"          \n[37] \"1992\"           \"1993\"           \"1994\"           \"1995\"          \n[41] \"1996\"           \"1997\"           \"1998\"           \"1999\"          \n[45] \"2000\"           \"2001\"           \"2002\"           \"2003\"          \n[49] \"2004\"           \"2005\"           \"2006\"           \"2007\"          \n[53] \"2008\"           \"2009\"           \"2010\"           \"2011\"          \n[57] \"2012\"           \"2013\"           \"2014\"           \"2015\"          \n[61] \"2016\"           \"2017\"           \"2018\"           \"2019\"          \n[65] \"2020\"           \"v66\"           \n\nsetnames(world_debt_data, nms_old, nms_new)\n\nid_vars_debt <- nms_new[1:4]\nworld_debt_data <- melt(world_debt_data,\n                           id.vars = id_vars_debt,\n                           variable.factor = F,\n                           value.factor = F,\n                           variable.name = \"year\")\nworld_debt_data[, year := as.numeric(year)]\nworld_debt_data[, value := as.numeric(value)]\nworld_debt_data <- world_debt_data[!is.na(year)]\nworld_debt_data <- world_debt_data[!is.na(value)]\nhead(world_debt_data[country_name == \"Kenya\"], 10) %>%\n  datatable(options = list(scrollX= T))\n\n\n\n\n\n\n\nea_country <- c(\"Kenya\", \"Lower middle income\" )\n\n\nworld_debt_data <- world_debt_data[country_name %in% ea_country]\nworld_debt_data_split <- split(world_debt_data, f = world_debt_data$indicator_name)\n\nn <- length(world_debt_data_split)\nmy_plots <-htmltools::tagList()\nmy_plots <- list()\nfor (i in 1:n) {\n    df = world_debt_data_split[[i]]\n    my_title = df[, unique(indicator_name)]\n    mn = df[, min(year)]\n    mx = df[, max(year)]\n    breaks = seq(mn, mx,by = 3)\n    p = ggplot(df, aes(year, value, group = country_name, color = country_name) ) +\n        geom_line()+\n        theme_hc()+\n        labs(title = my_title, x = \"year\", y = \"%\")+\n      scale_color_viridis_d(name=\"\")+\n      scale_x_continuous(breaks = breaks)\n    \n   # my_plots[[i]] = ggplotly(p)\n    my_plots[[i]] = p\n    \n}\n\nmy_plots\n\n[[1]]"
  },
  {
    "objectID": "kenya_population/kenya_maps.html",
    "href": "kenya_population/kenya_maps.html",
    "title": "Kenya Census Data",
    "section": "",
    "text": "library(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sf)\nlibrary(DT)\nlibrary(tmap)\nlibrary(rKenyaCensus)\n\n\n# kenya_5yr_births <- readGoogleSheet(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vSf-yNXNk68cIOH0Rb6alaDk9SxKEMm3h6kb2p8jxT8oMdfj4OqUDbs2Ln9OOGGCI9V7SNiZJCDWm4H/pubhtml\")\n# \n# kenya_5yr_births <- cleanGoogleTable(kenya_5yr_births , table = 1, skip = 1 ) %>%\n#   setDT()\ndata(V4_T2.40)\nkenya_5yr_births = V4_T2.40\nsetDT(kenya_5yr_births)\n\n\nold_nms_births <- names(kenya_5yr_births)\nnew_nms_births <- gsub(\"\\\\s\", \"_\", old_nms_births) %>%\n    tolower()\n\nsetnames(kenya_5yr_births, old_nms_births, new_nms_births)\n\n\nnumerics_nms <- c(\"total\", \"notified\", \"not_notified\",\n                  \"don’t_know\", \"not_stated\", \"percent_notified\")\n\nkenya_5yr_births[, (numerics_nms) := lapply(.SD, function(x) gsub(\",\", \"\", x)), .SDcols = numerics_nms]\nkenya_5yr_births[, (numerics_nms) := lapply(.SD, as.numeric), .SDcols = numerics_nms]\n\n\nkenya_counties <- st_read(\"County\")\n\nReading layer `County' from data source \n  `/media/mburu/mburu_ext/home/personal_projects/github_io_blog/kenya_population/County' \n  using driver `ESRI Shapefile'\nSimple feature collection with 47 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 33.91182 ymin: -4.702271 xmax: 41.90626 ymax: 5.430648\nGeodetic CRS:  WGS 84\n\nggplot(kenya_counties)+\n  geom_sf() +\n  theme_void()\n\n\n\n\n\nsetnames(kenya_counties, \"COUNTY\", \"county\")\n\n\nkenya_5yr_births[, county := tolower(gsub(\"\\\\s\", \"-\", county))]\nkenya_5yr_births[county == \"elgeyo-marakwet\", county := \"keiyo-marakwet\" ]\nkenya_5yr_births[county == \"nairobi-city\" , county := \"nairobi\"]\nkenya_5yr_births[county == \"tharaka-nithi\"  , county := \"tharaka\" ]\nkenya_5yr_births[county == \"homabay\"  , county := \"homa-bay\" ]\nsetDT(kenya_counties)\nkenya_counties[, county := tolower(gsub(\"\\\\s\", \"-\", county))]\n\nkenya_births_counties <- merge(kenya_counties, kenya_5yr_births, by = \"county\" )\nsetnames(kenya_births_counties, \"percent_notified\", \"per_cent_in_health_facility\")\nkenya_births_counties[, county := paste(county, \" \", per_cent_in_health_facility,\"%\", sep = \"\" )]\n\nkenya_births_counties <- st_set_geometry(kenya_births_counties, \"geometry\")\n\n\nmap1 <- tm_shape(kenya_births_counties)+\n  tm_borders(col = \"gold\")+\n  tm_polygons(col = \"per_cent_in_health_facility\")+\n  tm_layout(title = \"% Health facility births\",\n            title.position = c(0.3, \"top\"))\n\n\ntmap_leaflet(map1)\n\n\n\n\n\n\n\nkenyan_pop <- fread(\"distribution-of-population-by-age-and-sex-kenya-2019-census-volume-iii.csv\")\n\n\nnumerics_nms <- c(\"Male\", \"Female\", \"Intersex\")\nkenyan_pop[, (numerics_nms) := lapply(.SD, function(x) gsub(\",\", \"\", x)), .SDcols = numerics_nms]\nkenyan_pop[, (numerics_nms) := lapply(.SD, as.numeric), .SDcols = numerics_nms]\nkenyan_pop[, (numerics_nms) := lapply(.SD, function(x) ifelse(is.na(x), 0, x)), .SDcols = numerics_nms]\nkenyan_pop[, Age := gsub(\"Sep\", \"09\", Age)]\nkenyan_pop[, Age := gsub(\"Oct\", \"10\", Age)]\nkenyan_pop[,Total:= Reduce(`+`, .SD),.SDcols= numerics_nms]\n\n\nage_cat <- kenyan_pop[!grepl(\"-\", Age)]\n\nage_cat[is.na(age_cat)] <- NA\n#age_cat[,Total:= Reduce(`+`, .SD),.SDcols= numerics_nms]\n\n\nage_cat_m <- melt(age_cat, id.vars = c(\"Age\", \"Total\"), variable.name = \"Sex\")\nage_cat_m[, Perc := round(value/Total*100, 2)]\n\n\n\n\n\nlibrary(ggthemes)\nage_cat_m <- age_cat_m[Age != \"Total\"]\nage_cat_m[, Age := as.numeric(Age)]\npop_plot <- ggplot(age_cat_m, aes(Age, Perc, group = Sex))+\n  geom_line(aes(color = Sex)) +\n  labs(y = \"Percentage of Population\")+\n  scale_x_continuous(breaks = seq(0, 100, by = 10))+\n  scale_color_viridis_d()+\n  theme_hc()\n\nggplotly(pop_plot)\n\n\n\n\n\n\nage_cat[is.na(Age), Age := 100]\n\nage_cat[, Age := as.numeric(Age)]\nage_cat[, age_factor :=  cut(Age,\n                             breaks = c(0, 5, 15, 20, 30, 40, 50, 65, 100 ), \n                             include.lowest = T,\n                             labels = c(\"0-5\", \"6-15\", \"16-20\", \n                                        \"20-30\", \"31-40\", \"41,50]\", \n                                        \"51-65\", \"> 65\"))]\n\n\n\nage_cat_sum <- age_cat[, .(sum_total = sum(Total)), by = age_factor] %>%\n  .[, Perc := round(sum_total/sum(sum_total)*100,2)]"
  },
  {
    "objectID": "kenya_population/inflation_kenya.html",
    "href": "kenya_population/inflation_kenya.html",
    "title": "Inflation Kenya",
    "section": "",
    "text": "library(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sf)\nlibrary(DT)\nlibrary(tmap)\nlibrary(ggthemes)\nlibrary(here)"
  },
  {
    "objectID": "kenya_population/household_assets_2019census.html",
    "href": "kenya_population/household_assets_2019census.html",
    "title": "Kenya Household Assets",
    "section": "",
    "text": "Kenya selected households assets 2019 census\nI figured that it will be important for me to do this to show why it is impossible for online learning to be adopted in Kenya.\nI used 2019 census data set which can be found here and the counties shape file can be found here\n\n#Packages used\n\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sf)\nlibrary(DT)\nlibrary(tmap)\nlibrary(ggthemes)\nlibrary(ggiraph)\n\n\n\nRead data set\n\nhousehold_assets <- fread(\"percentage-distribution-of-conventional-households-by-ownership-of-selected-household-assets-201.csv\")\n\n#\nkenya_shapefile <- st_read(\"County\") %>% setDT()\n\nReading layer `County' from data source \n  `/media/mburu/mburu_ext/home/personal_projects/github_io_blog/kenya_population/County' \n  using driver `ESRI Shapefile'\nSimple feature collection with 47 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 33.91182 ymin: -4.702271 xmax: 41.90626 ymax: 5.430648\nGeodetic CRS:  WGS 84\n\nkenya_shapefile[, county := tolower(COUNTY)]\n\n# rename column\nsetnames(household_assets, \n         c(\"County / Sub-County\", \"Conventional Households\"  ), \n         c(\"county\", \"households\"))\n\n\n\nSome minor cleaning\n\n# remove commas \nhousehold_assets[, households := as.numeric(gsub(\",|AR K    \", \"\", households))]\n\nhousehold_assets[,  county := tolower(county)]\n\nsub_county <- household_assets %>% \n    group_by(county) %>%\n    filter(households == max(households)) %>% setDT()\n\n\nsub_county_melt <- melt(sub_county, \n                        id.vars = c(\"county\", \"households\"))\n\n\n\n% of households with various assets 2019 census\n\nThis is overall data set for the whole country computer devices is ownership about 8.8% this just means that about 91% of the students can’t access online learning. This is is just a naive estimation the number could be higher.\n\n\nkenya_dat <- sub_county_melt[county == \"kenya\"]\n\np <- ggplot(kenya_dat, aes(variable, value, tooltip = paste(variable, \" : \", value))) +\n    geom_bar_interactive(stat = \"identity\", width = 0.5, fill =\"slategray2\"  ) +\n    geom_text_interactive(aes(variable, value, label = paste0(value, \"%\")),\n              position = position_dodge(width = 0.5),\n              vjust = 0.001, size = 3)+\n    labs(x = \"Household assets\", y = \"%\",\n         title = \"% of households with various assets 2019 census\",\n         caption = \"twitter\\n@mmburu_w\")+\n    theme_fivethirtyeight()+\n  theme(\n    axis.text = element_text(size = 7, angle = 45, vjust = 1, hjust =1),\n    plot.title = element_text_interactive(size =11)\n  )\np1 <- girafe(ggobj = p, width_svg = 7, height_svg = 4.5, \n  options = list(\n    opts_sizing(rescale = T) )\n  )\n\np1\n\n\n\n\n\n\n\n\nMerge shapefile with asset data sets\n\nsub_county_melt[county == \"elgeyo/marakwet\", county := \"keiyo-marakwet\"]\nsub_county_melt[county == \"tharaka-nithi\", county := \"tharaka\"]\nsub_county_melt[county ==  \"taita/taveta\", county := \"taita taveta\"]\nsub_county_melt[county ==  \"nairobi city\", county := \"nairobi\"]\n\ncounty_shapes <- merge(kenya_shapefile, sub_county_melt, by = \"county\") \n\nsetnames(county_shapes, \"value\", \"Percentage\")\n\n\n\nComputer devices data\n\ncomputer <- county_shapes[variable  %in% c(\"Desk Top\\nComputer/\\nLaptop/ Tablet\")]\n\n# this converts to sf object\ncomputer <- st_set_geometry(computer, \"geometry\")\n\n\n\nPercentage of households with computer devices per county\n\nThat is if a household owns a tablet, laptop or a desktop\n\n\n#ttm()\ntm_shape(computer)+\n    tm_borders(col = \"grey\")+\n    tm_fill(col = \"Percentage\")+\n    tm_layout(title = \"% of households with laptop/tablet/desk top \\n 2019 census\",\n              title.size = 1, title.position = c(0.3, 0.95))\n\n\n\n#ttm()\n\n\n\n% of households that can access internet\n\nThis looks like is internet access through mobile phones\n\n\ninternet <- county_shapes[variable == \"Internet\"]\n\ninternet <- st_set_geometry(internet, \"geometry\")\n\n#ttm()\ntm_shape(internet)+\n    tm_borders(col = \"grey\")+\n    tm_fill(col = \"Percentage\")+\n    tm_layout(title = \"% of households with internet acess\",\n              title.size = 1, title.position = c(0.3, 0.95))"
  },
  {
    "objectID": "kenya_population/kenya_inflation.html",
    "href": "kenya_population/kenya_inflation.html",
    "title": "Kenya Inflation",
    "section": "",
    "text": "inflation_rates <- fread(here(\"kenya_debt/Inflation Rates.csv\"))\nsource(\".Rprofile\")\ninflation_rates <- nms_clean(inflation_rates)\ninflation_rates[, date := ymd(paste(year, month, \"01\", sep = \"-\"))]\nsetnames(inflation_rates, \n         \"12_month_inflation\", \n         \"twelve_month_inflation\")\n\n\nggplot(inflation_rates, aes(date, twelve_month_inflation))+\n  geom_line()\n\n\n\n\n\nexports <- fread(here(\"kenya_debt/Principal Exports Volume, Value and Unit Prices (Ksh Million).csv\"))\nexports_m <- melt(exports,\n                  id.vars = c(\"Year\", \"Month\"),\n                  variable.factor = F)\n\nexports_m[, type := fcase(str_detect(variable, \"^Volume\"), \"Volume\",\n                          str_detect(variable, \"^Value\"), \"Income\",\n                           str_detect(variable, \"^Average\"), \"price_per_tonne\",\n                          default = \"na\")]\nexports_m[, variable := tolower(variable)]\nexports_m[, crop := fcase(str_detect(variable, \"coffee\"), \"Coffee\",\n                          str_detect(variable, \"tea\"), \"Tea\",\n                           str_detect(variable, \"horticulture\"), \"Horticulture\",\n                          default = \"na\")]\nexports_mw <- dcast(Year+ Month + crop ~ type, \n                    value.var = \"value\", \n                    data = exports_m,\n                    fun.aggregate = NULL)\n\n\nexports_mw[, date := ymd(paste(Year, Month, \"01\", sep = \"-\"))]\nggplot(exports_mw, aes(date, Income, color = crop))+\n  geom_line()\n\n\n\nggplot(exports_mw, aes(date, price_per_tonne, color = crop))+\n  geom_line()\n\n\n\nggplot(exports_mw, aes(date, Volume, color = crop))+\n  geom_line()"
  },
  {
    "objectID": "water_pumps_tz/water_pumps.html",
    "href": "water_pumps_tz/water_pumps.html",
    "title": "Tanzanian Water Pumps",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(knitr)\nlibrary(broom)\nlibrary(caret)\nlibrary(kableExtra)\nlibrary(ggthemes)\nlibrary(DT)"
  },
  {
    "objectID": "water_pumps_tz/water_pumps.html#section",
    "href": "water_pumps_tz/water_pumps.html#section",
    "title": "Tanzanian Water Pumps",
    "section": "",
    "text": "train_y <- fread(\"0bf8bc6e-30d0-4c50-956a-603fc693d966.csv\")\ntrain_x <-  fread(\"4910797b-ee55-40a7-8668-10efd5c1b960.csv\")\n\ntest <- fread(\"702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv\")"
  },
  {
    "objectID": "water_pumps_tz/water_pumps.html#section-1",
    "href": "water_pumps_tz/water_pumps.html#section-1",
    "title": "Tanzanian Water Pumps",
    "section": "",
    "text": "train_data <- merge(train_y, train_x, by = \"id\")\ntrain_data[, set := \"train\"]\n\ntest[, set := \"test\"]\n\npump_data <- rbindlist(list(train_data, test), fill = T)\n\n\ntrain_data[, .N, by = status_group] %>%\n    .[, Perc :=  round(N/sum(N) * 100, 2)] %>%\n    datatable()\n\n\n\n\n\n\n\npump_data[amount_tsh != 0, .(Mean = mean(amount_tsh),\n               Median = median(amount_tsh),\n               Min = min(amount_tsh),\n               Max = max(amount_tsh),\n               First_qurtile = quantile(amount_tsh, .25),\n               Third_qurtile = quantile(amount_tsh, .75)),\n           by = status_group] %>%\n  datatable()\n\n\n\n\n\n\n\ncol_class <- sapply(train_data, class)\nchar_cols <- names(col_class[col_class == \"character\"])\n\nchar_cols <- char_cols[char_cols != \"date_recorded\"]\nchar_cols\n\n [1] \"status_group\"          \"funder\"                \"installer\"            \n [4] \"wpt_name\"              \"basin\"                 \"subvillage\"           \n [7] \"region\"                \"lga\"                   \"ward\"                 \n[10] \"recorded_by\"           \"scheme_management\"     \"scheme_name\"          \n[13] \"extraction_type\"       \"extraction_type_group\" \"extraction_type_class\"\n[16] \"management\"            \"management_group\"      \"payment\"              \n[19] \"payment_type\"          \"water_quality\"         \"quality_group\"        \n[22] \"quantity\"              \"quantity_group\"        \"source\"               \n[25] \"source_type\"           \"source_class\"          \"waterpoint_type\"      \n[28] \"waterpoint_type_group\" \"set\""
  },
  {
    "objectID": "water_pumps_tz/water_pumps.html#a-lazy-way-of-collapsing-columns-please-do-not-do-it",
    "href": "water_pumps_tz/water_pumps.html#a-lazy-way-of-collapsing-columns-please-do-not-do-it",
    "title": "Tanzanian Water Pumps",
    "section": "A lazy way of collapsing columns, Please DO NOT do it,",
    "text": "A lazy way of collapsing columns, Please DO NOT do it,\n\nIt’s best to go through all columns one by one to see how well lumping together will be beneficial\n\n\nfactor_cols <- pump_data[, ..char_cols]\nfactor_cols[, (char_cols) := lapply(.SD, str_to_lower), .SDcols = char_cols]\nfactor_cols[, (char_cols) := lapply(.SD,  fct_lump_n, n = 20), .SDcols = char_cols]\n#factor_cols[, (char_cols) := lapply(.SD,  fct_lump_n, n = 5), .SDcols = char_cols]\nfactor_cols[factor_cols == \"\"] = NA\nfactor_cols[factor_cols == \"0\"] = NA\nfactor_cols[, (char_cols) :=lapply(.SD, fct_explicit_na, na_level = \"missing\"), .SDcols = char_cols]\n\n\nchars_dat <- melt(factor_cols, id.vars = \"status_group\")\n\nchars_dat[, .(freq = .N), by = .(variable, value)] %>%\n  .[, perc := round(freq/sum(freq) * 100), by = .(variable)] %>%\n    datatable()\n\n\n\n\n\n\n\nfactor_cols[set == \"train\", .N, by = .(funder,status_group)] %>%\n    .[, perc := round(N/sum(N) * 100, 2), by = .(funder)] %>%\n    \n     ggplot(aes(funder, perc, fill = status_group)) +\n     geom_bar(stat = \"identity\") +\n     # geom_text(aes(funder, perc, label = paste0(perc, \"%\"),\n     #               vjust = .05, hjust = .5),\n     #           size = 3, position = position_stack(vjust = 0.5))+\n     theme_hc()+\n    labs(title = \"Percentage loans_\")+\n     scale_fill_economist(name = \"\")+\n    theme(legend.position = \"bottom\",\n          axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n\n\n\n\nfactor_cols[set == \"train\", .N, by = .(water_quality,status_group)] %>%\n    .[, perc := round(N/sum(N) * 100, 2), by = .(water_quality)] %>%\n    \n     ggplot(aes(water_quality, perc, fill = status_group)) +\n     geom_bar(stat = \"identity\") +\n     geom_text(aes(water_quality, perc, label = paste(perc, \"%\"),\n                   vjust = .05, hjust = .5),\n               size = 3, position = position_stack(vjust = 0.5))+\n     theme_hc()+\n    labs(title = \"\")+\n     scale_fill_economist(name = \"\")+\n    theme(legend.position = \"bottom\")\n\n\n\n\n\nfactor_cols[set == \"train\", .N, by = .(installer,status_group)] %>%\n    .[, perc := round(N/sum(N) * 100, 2), by = .(installer)] %>%\n    \n     ggplot(aes(installer, perc, fill = status_group)) +\n     geom_bar(stat = \"identity\") +\n     theme_hc()+\n    labs(title = \"\")+\n     scale_fill_economist(name = \"\")+\n    theme(legend.position = \"bottom\",\n          axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n\n\n\n\npump_data[construction_year == 0, construction_year := NA]\nsummary(pump_data$construction_year)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   1960    1988    2000    1997    2008    2013   25969 \n\npump_data[, construction_year2  := cut(construction_year, breaks = c(1959, 1988, 2000, 2008, 2013))]\npump_data[, construction_year2 := fct_explicit_na(construction_year2, na_level = \"unknown\")]\nsummary(pump_data$construction_year2)\n\n(1959,1988] (1988,2000] (2000,2008] (2008,2013]     unknown \n      12650       12585       13389        9657       25969 \n\n\n\n# train_set <- pump_data[set == \"train\",\n#                        .(status_group, go_funded, water_quality,\n#                          quantity,  construction_year2,\n#                          management_group, go_installer,\n#                          waterpoint_type_group1, longitude, latitude, basin,\n#                          management_group1)]\n\n#is there biase in recording\ndel_cols <- c( \"recorded_by\")\ndata_clean <- cbind(pump_data[, .(construction_year2, latitude, longitude)], factor_cols)\n\ntrain_set <- data_clean[set == \"train\"]\ntrain_set[, set := NULL]\nchar_cols2 <- char_cols[!char_cols %in% c(\"status_group\" )]\ntrain_set_dmmy <- dummies::dummy.data.frame(train_set, names = char_cols2) %>% setDT()\ntrain_set_dmmy[, status_group := factor(status_group,\n                                   levels = c(\"functional\", \"functional needs repair\", \"non functional\"),\n                                   labels  = c(\"functional\", \"functional_needs_repair\", \"non_functional\"))]\n\n\nset.seed(100)\n#train_set <- train_set[status_group %in% c(\"functional\", \"functional_needs_repair\") ]\ntrain_ind <- sample(1:nrow(train_set_dmmy), round(0.7 * nrow(train_set)))\ntrain_set_dmmy[, status_group := factor(status_group)]\n\ntrain_set1 <- train_set_dmmy[train_ind,]\nset.seed(100)\ncv_fold <- createFolds(train_set1$status_group, k = 3)\n\n\nlibrary(caretEnsemble)\n\ntrain_ctrl <- trainControl(method = \"cv\",\n                        number = 3,\n                        summaryFunction = multiClassSummary,\n                        classProbs = TRUE,\n                        allowParallel=T,\n                        index = cv_fold,\n                        verboseIter = TRUE,\n                        returnResamp = \"all\", \n                        savePredictions = \"final\", \n                        search = \"grid\")\n\n\nxgb_grid <-  expand.grid(nrounds = c(50,100),\n                        eta = 0.06,\n                        max_depth =c(10, 50),\n                        gamma = 6,\n                        colsample_bytree = 0.8,\n                        min_child_weight =0.8,\n                        subsample =  .8)\n\n\n\nranger_grid <- expand.grid(splitrule = \"extratrees\",\n                        mtry = c(20, 50, 100),\n                        min.node.size = 1)\n\n\nglmnet_grid <- expand.grid(alpha = c(0, 1),\n                           lambda = seq(0.0001, 1, length = 3))\n\n\nset.seed(100)\n\nlibrary(tictoc)\n\ntic()\n\nmodel_list <- caretList(\n   status_group~.,\n    data=train_set1,\n    trControl=train_ctrl,\n    tuneList = list(caretModelSpec(method=\"xgbTree\", tuneGrid= xgb_grid),\n                    caretModelSpec(method=\"ranger\", tuneGrid= ranger_grid),\n                    caretModelSpec(method=\"glmnet\", tuneGrid= glmnet_grid)\n                    \n                    )\n)\n\n+ Fold1: eta=0.06, max_depth=10, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n[10:39:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:39:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n- Fold1: eta=0.06, max_depth=10, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n+ Fold1: eta=0.06, max_depth=50, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n[10:40:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:40:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n- Fold1: eta=0.06, max_depth=50, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n+ Fold2: eta=0.06, max_depth=10, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n[10:40:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:40:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n- Fold2: eta=0.06, max_depth=10, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n+ Fold2: eta=0.06, max_depth=50, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n[10:41:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:41:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n- Fold2: eta=0.06, max_depth=50, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n+ Fold3: eta=0.06, max_depth=10, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n[10:41:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:41:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n- Fold3: eta=0.06, max_depth=10, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n+ Fold3: eta=0.06, max_depth=50, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n[10:43:00] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:43:00] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n- Fold3: eta=0.06, max_depth=50, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \nAggregating results\nSelecting tuning parameters\nFitting nrounds = 100, max_depth = 50, eta = 0.06, gamma = 6, colsample_bytree = 0.8, min_child_weight = 0.8, subsample = 0.8 on full training set\n+ Fold1: splitrule=extratrees, mtry= 20, min.node.size=1 \n- Fold1: splitrule=extratrees, mtry= 20, min.node.size=1 \n+ Fold1: splitrule=extratrees, mtry= 50, min.node.size=1 \n- Fold1: splitrule=extratrees, mtry= 50, min.node.size=1 \n+ Fold1: splitrule=extratrees, mtry=100, min.node.size=1 \nGrowing trees.. Progress: 76%. Estimated remaining time: 9 seconds.\n- Fold1: splitrule=extratrees, mtry=100, min.node.size=1 \n+ Fold2: splitrule=extratrees, mtry= 20, min.node.size=1 \n- Fold2: splitrule=extratrees, mtry= 20, min.node.size=1 \n+ Fold2: splitrule=extratrees, mtry= 50, min.node.size=1 \n- Fold2: splitrule=extratrees, mtry= 50, min.node.size=1 \n+ Fold2: splitrule=extratrees, mtry=100, min.node.size=1 \nGrowing trees.. Progress: 75%. Estimated remaining time: 10 seconds.\n- Fold2: splitrule=extratrees, mtry=100, min.node.size=1 \n+ Fold3: splitrule=extratrees, mtry= 20, min.node.size=1 \n- Fold3: splitrule=extratrees, mtry= 20, min.node.size=1 \n+ Fold3: splitrule=extratrees, mtry= 50, min.node.size=1 \n- Fold3: splitrule=extratrees, mtry= 50, min.node.size=1 \n+ Fold3: splitrule=extratrees, mtry=100, min.node.size=1 \nGrowing trees.. Progress: 69%. Estimated remaining time: 13 seconds.\n- Fold3: splitrule=extratrees, mtry=100, min.node.size=1 \nAggregating results\nSelecting tuning parameters\nFitting mtry = 20, splitrule = extratrees, min.node.size = 1 on full training set\n+ Fold1: alpha=0, lambda=1 \n- Fold1: alpha=0, lambda=1 \n+ Fold1: alpha=1, lambda=1 \n- Fold1: alpha=1, lambda=1 \n+ Fold2: alpha=0, lambda=1 \n- Fold2: alpha=0, lambda=1 \n+ Fold2: alpha=1, lambda=1 \n- Fold2: alpha=1, lambda=1 \n+ Fold3: alpha=0, lambda=1 \n- Fold3: alpha=0, lambda=1 \n+ Fold3: alpha=1, lambda=1 \n- Fold3: alpha=1, lambda=1 \nAggregating results\nSelecting tuning parameters\nFitting alpha = 0, lambda = 1e-04 on full training set\n\ntoc()\n\n930.916 sec elapsed\n\n\n\nmodel_list\n\n$xgbTree\neXtreme Gradient Boosting \n\n41580 samples\n  317 predictor\n    3 classes: 'functional', 'functional_needs_repair', 'non_functional' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 13860, 13860, 13860 \nResampling results across tuning parameters:\n\n  max_depth  nrounds  logLoss    AUC        prAUC      Accuracy   Kappa    \n  10          50      0.6184224  0.8460033  0.6801849  0.7550024  0.5167732\n  10         100      0.5926097  0.8526962  0.6886836  0.7600890  0.5293239\n  50          50      0.6072174  0.8521591  0.6874586  0.7623617  0.5339285\n  50         100      0.5776355  0.8593970  0.6983684  0.7678451  0.5481911\n  Mean_F1    Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value\n  0.5739352  0.5559239         0.8305834         0.7524923          \n  0.5829963  0.5638717         0.8353955         0.7565636          \n  0.5850994  0.5658591         0.8369418         0.7603295          \n  0.5990653  0.5775931         0.8426834         0.7566597          \n  Mean_Neg_Pred_Value  Mean_Precision  Mean_Recall  Mean_Detection_Rate\n  0.8719837            0.7524923       0.5559239    0.2516675          \n  0.8717203            0.7565636       0.5638717    0.2533630          \n  0.8732480            0.7603295       0.5658591    0.2541206          \n  0.8729287            0.7566597       0.5775931    0.2559484          \n  Mean_Balanced_Accuracy\n  0.6932536             \n  0.6996336             \n  0.7014004             \n  0.7101382             \n\nTuning parameter 'eta' was held constant at a value of 0.06\nTuning\n\nTuning parameter 'min_child_weight' was held constant at a value of 0.8\n\nTuning parameter 'subsample' was held constant at a value of 0.8\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were nrounds = 100, max_depth = 50, eta\n = 0.06, gamma = 6, colsample_bytree = 0.8, min_child_weight = 0.8\n and subsample = 0.8.\n\n$ranger\nRandom Forest \n\n41580 samples\n  317 predictor\n    3 classes: 'functional', 'functional_needs_repair', 'non_functional' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 13860, 13860, 13860 \nResampling results across tuning parameters:\n\n  mtry  logLoss    AUC        prAUC      Accuracy   Kappa      Mean_F1  \n   20   0.5486824  0.8714188  0.7137107  0.7795334  0.5792095  0.6407851\n   50   0.6116078  0.8671270  0.6760604  0.7760823  0.5782371  0.6481112\n  100   0.7202941  0.8628325  0.6346966  0.7723665  0.5735920  0.6466763\n  Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value  Mean_Neg_Pred_Value\n  0.6147133         0.8551205         0.7255057            0.8752240          \n  0.6261864         0.8567513         0.6981690            0.8704719          \n  0.6275167         0.8560894         0.6854621            0.8673085          \n  Mean_Precision  Mean_Recall  Mean_Detection_Rate  Mean_Balanced_Accuracy\n  0.7255057       0.6147133    0.2598445            0.7349169             \n  0.6981690       0.6261864    0.2586941            0.7414688             \n  0.6854621       0.6275167    0.2574555            0.7418031             \n\nTuning parameter 'splitrule' was held constant at a value of extratrees\n\nTuning parameter 'min.node.size' was held constant at a value of 1\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were mtry = 20, splitrule = extratrees\n and min.node.size = 1.\n\n$glmnet\nglmnet \n\n41580 samples\n  317 predictor\n    3 classes: 'functional', 'functional_needs_repair', 'non_functional' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 13860, 13860, 13860 \nResampling results across tuning parameters:\n\n  alpha  lambda   logLoss    AUC        prAUC      Accuracy   Kappa    \n  0      0.00010  0.6218298  0.8319479  0.6497870  0.7439514  0.5036845\n  0      0.50005  0.6746615  0.8169808  0.6330926  0.7311568  0.4643535\n  0      1.00000  0.7068795  0.8096260  0.6250271  0.7228114  0.4433703\n  1      0.00010  0.6251197  0.8319851  0.6495486  0.7437350  0.5066840\n  1      0.50005  0.8893577  0.5000000  0.0000000  0.5424242  0.0000000\n  1      1.00000  0.8893577  0.5000000  0.0000000  0.5424242  0.0000000\n  Mean_F1    Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value\n  0.5646638  0.5517857         0.8294169         0.6773631          \n        NaN  0.5068225         0.8139668               NaN          \n        NaN  0.4978057         0.8061242               NaN          \n  0.5783964  0.5619689         0.8310915         0.6650360          \n        NaN  0.3333333         0.6666667               NaN          \n        NaN  0.3333333         0.6666667               NaN          \n  Mean_Neg_Pred_Value  Mean_Precision  Mean_Recall  Mean_Detection_Rate\n  0.8538487            0.6773631       0.5517857    0.2479838          \n  0.8556091                  NaN       0.5068225    0.2437189          \n  0.8557185                  NaN       0.4978057    0.2409371          \n  0.8526489            0.6650360       0.5619689    0.2479117          \n        NaN                  NaN       0.3333333    0.1808081          \n        NaN                  NaN       0.3333333    0.1808081          \n  Mean_Balanced_Accuracy\n  0.6906013             \n  0.6603946             \n  0.6519649             \n  0.6965302             \n  0.5000000             \n  0.5000000             \n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were alpha = 0 and lambda = 1e-04.\n\nattr(,\"class\")\n[1] \"caretList\"\n\n\n\ntest_own <- train_set_dmmy[-train_ind]\n\n\naccuracy <- c()\nfor (i in 1:length(model_list)) {\n    \n    rf <- model_list[[i]]\n    \n    pred <- predict(rf,  newdata = test_own)\n    \n    \n   #auc[i] <-  caret::(pred, test_own$status_group)\n   accuracy[i] <-  round(Metrics::accuracy(pred, test_own$status_group) * 100, 2)\n   \n   \n    \n}\n\n\npred_df <- data.frame(models = names(model_list), accuracy)\n\nDT::datatable(pred_df)\n\n\n\n\n\n\n\nresamples_models <- resamples(model_list)\n\ndotplot(resamples_models, metric = \"AUC\")\n\n\n\n\n\n# Alternatively, you can put in dense matrix, i.e. basic R-matrix\n# library(lightgbm)\n# \n# train_x <- as.matrix(train_set1[, !status_group])\n# train_y <- train_set1$status_group\n# \n# \n# params = list('task'= 'train',\n#     'boosting_type'= 'gbdt',\n#     'objective'= 'multiclass',\n#     'num_class'=3,\n#     'metric'= 'multi_logloss',\n#     'learning_rate'= 0.002296,\n#     'max_depth'= 7,\n#     'num_leaves'= 17,\n#     'feature_fraction'= 0.4,\n#     'bagging_fraction'= 0.6,\n#     'bagging_freq'= 17)\n# \n# train_lgb = lgb.Dataset(data = train_x , label = train_y, params = params)\n# \n# print(\"Training lightgbm with Matrix\")\n# \n# bst <- lightgbm(\n#     data = train_lgb\n#     , num_leaves = 4L\n#     , learning_rate = 1.0\n#     , nrounds = 2L\n#     , objective = \"multiclass\"\n# )"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html",
    "href": "posts/breast_cancer_prediction/cancer_data.html",
    "title": "Predict whether the cancer is benign or malignant",
    "section": "",
    "text": "In this tutorial I’m going to predict whether a breast cancer tumor is benign or malignant. Using Wiscosin breast cancer data set available on Kaggle. The 30 predictors are divided into three parts first is Mean ( variables 3-13), Standard Error(13-23) and Worst(23-32) and each contain 10 parameter (radius, texture,area, perimeter, smoothness,compactness,concavity,concave points,symmetry and fractal dimension of the tumor). When predicting cancer breast tumor types there two types of cost;\n\nThe cost of telling someone who has malignant tumor that they have benign these are the false negatives in this case someone might not seek medical help which is can cause death.\nTelling someone that they have malignant type of tumor but they don’t which is usually false positives. In this case you subject someone to unnecessary stress\n\nSo it’s highly desirable that our model has good accuracy $ f_1 score$ and high recall.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(xgboost)\nlibrary(knitr)\nlibrary(broom)\nlibrary(caret)\nlibrary(e1071)\nlibrary(kableExtra)\nlibrary(ggthemes)\nlibrary(DT)\nlibrary(glmnet)\n\noptions(scipen = 1, digits = 4)\ncancer <- fread(\"data.csv\")\nsource(\"functions.R\")\n\ncancer[, V33 := NULL]\n\n\nhead(cancer)  %>%\n  data_table()"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#percentage-of-women-with-malignant-tumor",
    "href": "posts/breast_cancer_prediction/cancer_data.html#percentage-of-women-with-malignant-tumor",
    "title": "Predict whether the cancer is benign or malignant",
    "section": "Percentage of women with malignant tumor",
    "text": "Percentage of women with malignant tumor\nThe percentage of women with malignant tumor is 37.26%(212 out 569) while the rest 62.74%(357) had benign tumors.\n\ncancer[, .(freq = .N),\n       by = diagnosis] %>% \n    .[, perc := round(100 * freq/sum(freq), 2)] %>%\n  \nggplot(aes(x=diagnosis, y=perc, fill = diagnosis)) + \n    geom_bar(stat = \"identity\", width  = 0.5)+ theme_hc() +\n    geom_text(aes(x=diagnosis, y=perc, label = paste(perc, \"%\")),\n              position =  position_dodge(width = 0.5),\n              vjust = 0.05, hjust = 0.5, size = 5)+\n    scale_fill_hc(name = \"\")+\n    labs(x = \"Cancer Type\",\n         y = \"Percentage\",\n         title = \"Percentage of women with benign or malignant breast bancer\")+\n    theme(legend.position = \"none\",\n          axis.title = element_text(size =12))"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#boxplots",
    "href": "posts/breast_cancer_prediction/cancer_data.html#boxplots",
    "title": "Predict whether the cancer is benign or malignant",
    "section": "Boxplots",
    "text": "Boxplots\nFrom the boxplots we can identify variables where we expect there is a significance difference between the two groups of cancer tumors. When using a boxplot if two distributions do not averlap or more than 75% of two boxplot do not overlap then we expect that there is a significance difference in the mean/median between the two groups. Some of the variables where the distribution of two cancer tumors are significantly different are radius_mean, texture_mean etc. The visible differences between malignant tumors and benign tumors can be seen in means of all cells and worst means where worst means is the average of all the worst cells. The distribution of malignant tumors have higher scores than the benign tumors in this cases.\n\ncancerm <- melt(cancer[, -1, with = F], id.vars = \"diagnosis\")\n\nggplot(cancerm, aes(x = diagnosis, y = value))+\n    geom_boxplot() + facet_wrap(~variable, scales = \"free_y\")"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#features-scaling",
    "href": "posts/breast_cancer_prediction/cancer_data.html#features-scaling",
    "title": "Predict whether the cancer is benign or malignant",
    "section": "Features Scaling",
    "text": "Features Scaling\nWe find that some variables are highly correlated. We can use principle component analysis for dimension reduction. Since variables are correlated it’s evident that we can use a smaller set of features to build our models.\n\ncancer[, id := NULL]\npredictors <- names(cancer)[3:31]\ncancer[, (predictors) := lapply(.SD, function(x) scale(x)), .SDcols = predictors ]\ncancer[, diagnosis := as.factor(diagnosis)]"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#correlation-matrix",
    "href": "posts/breast_cancer_prediction/cancer_data.html#correlation-matrix",
    "title": "Predict whether the cancer is benign or malignant",
    "section": "Correlation matrix",
    "text": "Correlation matrix\n\ncor(cancer[, -(1:2), with = F]) %>%\n  data_table()"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#principle-component-analysis",
    "href": "posts/breast_cancer_prediction/cancer_data.html#principle-component-analysis",
    "title": "Predict whether the cancer is benign or malignant",
    "section": "Principle Component Analysis",
    "text": "Principle Component Analysis\nUsing the elbow rule we can use the first 5 principle components. Using 15 principle components we will have achieved al most 100% of the variance from the original data set.\n\npca <- prcomp(cancer[, predictors, with = F], scale. = F)"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#variance-explained",
    "href": "posts/breast_cancer_prediction/cancer_data.html#variance-explained",
    "title": "Predict whether the cancer is benign or malignant",
    "section": "Variance Explained",
    "text": "Variance Explained\nSince PCA forms new characteristics the variance explained plot shows the amount of variation of the original features captured by each principle component. The new features are simply linear combinations of the old features.\n\nstdpca <- pca$sdev\n\nvarpca <- stdpca^2\n\nprop_var <- varpca/sum(varpca)\nprop_var * 100\n\n [1] 43.706363 18.472237  9.716239  6.816736  5.676223  4.161723  2.292352\n [8]  1.643434  1.363238  1.191515  1.011032  0.897368  0.832105  0.539193\n[15]  0.323823  0.269517  0.198317  0.178851  0.153573  0.107095  0.102579\n[22]  0.093821  0.082603  0.058725  0.053331  0.027514  0.022985  0.005110\n[29]  0.002394\n\nsum(prop_var[1:15])\n\n[1] 0.9864"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#scree-plot",
    "href": "posts/breast_cancer_prediction/cancer_data.html#scree-plot",
    "title": "Predict whether the cancer is benign or malignant",
    "section": "Scree plot",
    "text": "Scree plot\nScree plot shows the variance explained by each principle component which reduces as the number of principle components increase.\n\nplot(prop_var, xlab = \"Principal Component\",\n     ylab = \"Proportion of Variance Explained\",\n     type = \"b\", xlim = c(0, 30))"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#cumulative-variance-explained",
    "href": "posts/breast_cancer_prediction/cancer_data.html#cumulative-variance-explained",
    "title": "Predict whether the cancer is benign or malignant",
    "section": "Cumulative Variance Explained",
    "text": "Cumulative Variance Explained\nThe cumulative of variance plot helps to choose the number of features based on the amount of variation from original data set you want captured. In this case, I wanted to use number of principle components that capture almost 100% of the variation. After trying with different number of principle components I found out that the accuracy of the models did not increase after the 15th principle components.\n\ncum_var <- cumsum(prop_var)\nplot(cum_var, xlab = \"Principal Component\",\n     ylab = \"Cumulative Proportion of Variance Explained\",\n     type = \"b\", xlim = c(0, 30))"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#construct-new-data-set",
    "href": "posts/breast_cancer_prediction/cancer_data.html#construct-new-data-set",
    "title": "Predict whether the cancer is benign or malignant",
    "section": "Construct new data set",
    "text": "Construct new data set\nWe use the first 15 principle components as our new predictors, then we randomly split data into training and test set in 7:3 ratio.\n\nset.seed(100)\ntrain_sample <- sample(1:nrow(cancer), round(0.7*nrow(cancer)))\n\npcadat <- data.table( label = cancer$diagnosis, pca$x[,1:15]) \npcadat[, label := factor(label, levels = c(\"M\", \"B\"))]\ntrain <- pcadat[train_sample,]\ntest <- pcadat[-train_sample,]"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#logistic-regression",
    "href": "posts/breast_cancer_prediction/cancer_data.html#logistic-regression",
    "title": "Predict whether the cancer is benign or malignant",
    "section": "Logistic regression",
    "text": "Logistic regression\nThis is one of generalized linear models which deals with binary data. There is a generalization of this model which is called multinomial regression where you can fit multi class data. The equation for logistic regression model is:\n\\[log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1*X_1 + ... \\beta_n * X_n\\] and using mle the cost function can be derived as: \\[J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m} y^i log(h_\\theta(x^i)) + (1-y^i) log(1 - h_\\theta(x^i))\\] Given that \\[y = 0\\] \\[y = 1\\] . Finding \\[\\beta\\] s we minimizing the cost function.\n\nfit_glm <- glm(label ~., data = train, family = binomial)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#regularization-in-logistic-regression",
    "href": "posts/breast_cancer_prediction/cancer_data.html#regularization-in-logistic-regression",
    "title": "Predict whether the cancer is benign or malignant",
    "section": "Regularization in logistic regression",
    "text": "Regularization in logistic regression\nThe warning “glm.fit: fitted probabilities numerically 0 or 1 occurred” shows that there is a perfect separation/over fitting. In this case you can load glmnet library and fit a regularized logistic regression. These can be achieved by adding a regularization term to the cost function.The L1 regularization(Lasso) adds a penalty equal to the sum of the absolute values of the coefficients.\n\\[J(\\theta) = -\\frac{1}{m}\\sum_{i=0}^{m} y^i log(h_\\theta(x^i)) + (1-y^i) log(1 - h_\\theta(x^i)) + \\frac {\\lambda}{2m}\\sum_{j=1}^{n} |\\theta^i|\\]\n\ntrainx <- train[,-1]\n\ny_train <- factor(train$label, levels = c(\"B\", \"M\"), labels = 0:1)\n#y <- as.numeric(as.character(y))\n\ny_test <- factor(test$label, levels = c(\"B\", \"M\"), labels = 0:1) %>% as.character() %>% as.numeric()\n#ytest <- as.numeric(as.character(ytest))\n\ntestx <- data.matrix(test[, -1]) \n\nTo find the optimal values \\(\\lambda\\) we use cross validation. We choose \\(\\lambda\\) which gives the highest cross validation accuracy.\n\ncv_fold <- createFolds(train$label, k = 10)\n\nmyControl <- trainControl(\n  method = \"cv\", \n  number = 10,\n  summaryFunction = twoClassSummary,\n  savePredictions = \"all\",\n  classProbs = TRUE,\n  verboseIter = FALSE,\n  index = cv_fold,\n  allowParallel = TRUE\n  \n)\n\ntuneGrid <-  expand.grid(\n    alpha = 0:1,\n    lambda = seq(0.001, 1, length.out = 10))\n    \nglmnet_model <- train(\n  label ~.,\n  data = train,\n  method = \"glmnet\",\n  metric = \"ROC\",\n  trControl = myControl,\n  tuneGrid = tuneGrid\n)\n\ns\n\nplot(glmnet_model) \n\n\n\n#lamda_min <- cv_glm$lambda.min\n\n\nresample_glmnet <- thresholder(glmnet_model, \n                              threshold = seq(.2, 1, by = 0.05), \n                              final = TRUE, \n                              statistics = \"all\")\n\nggplot(resample_glmnet , aes(x = prob_threshold, y = F1)) + \n  geom_point() + \n  geom_point(aes(y = Sensitivity), col = \"blue\")\n\n\n\n\n\nlibrary(caTools)\n\npred_glm <- predict(glmnet_model, test, type = \"prob\")\n\ncolAUC(pred_glm , test$label, plotROC = TRUE)\n\n\n\n\n             M      B\nM vs. B 0.9683 0.9683\n\npred_glm1 <- ifelse(pred_glm[, \"M\"] > 0.4, \"M\", \"B\")\n#pred_glm1 <- predict(glmnet_model, test, type = \"raw\")\n\n\npred_glm1 <- factor(pred_glm1, levels = levels(test$label))\n\n\nconfusionMatrix(pred_glm1, test$label,positive = \"M\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   M   B\n         M  53   7\n         B   7 104\n                                        \n               Accuracy : 0.918         \n                 95% CI : (0.866, 0.955)\n    No Information Rate : 0.649         \n    P-Value [Acc > NIR] : <2e-16        \n                                        \n                  Kappa : 0.82          \n                                        \n Mcnemar's Test P-Value : 1             \n                                        \n            Sensitivity : 0.883         \n            Specificity : 0.937         \n         Pos Pred Value : 0.883         \n         Neg Pred Value : 0.937         \n             Prevalence : 0.351         \n         Detection Rate : 0.310         \n   Detection Prevalence : 0.351         \n      Balanced Accuracy : 0.910         \n                                        \n       'Positive' Class : M"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#svm",
    "href": "posts/breast_cancer_prediction/cancer_data.html#svm",
    "title": "Predict whether the cancer is benign or malignant",
    "section": "SVM",
    "text": "SVM\nSupport Vector Machines is a type of supervised learning algorithm that is used for classification and regression. Most of the times however, it’s used for classification.\nTo understand how SVM works consider the following example of linearly separable data. It’s clear that we can separate the two classes using a straight line(decision boundary). Which is normally referred to a separating hyperplane.\n\n\n\n\n\nThe question is, since there exists many lines that can separate the red and the black classes which is the best one. This introduces us to the maximal margin classification, In short SVM finds the hyperplane/line that gives the biggest margin/gap between the two classes. In this case SVM will choose the solid line as the hyperplane while the margins are the dotted lines. The circled points that lie directly on the margin, or on the wrong side of the margin for their class, are known as support vectors. This shows that SVM uses this points to come up with a the decision boundary, the other points are not used. In this case since it’s a two dimensional space the equation of the separating line will be \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2\\]. Then when equations evaluates to more than 0 then 1 is predicted \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2 > 0, y = 1\\] and when it evaluates to less than zero then predicted class is -1 \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2 < 0, \\; y = -1\\] This becomes maximisation problem \\[width \\; of \\;the \\; margin = M \\] \\[\\sum_{j=1}^{n}\\beta_j = 1\\]\n\\[y_i(\\beta_0 + \\beta_1X_1 + \\beta_2X_2) >= M\\]\n\n\n\n\n\nThis is a best case scenario but in most cases the classes are noisy. Consider the plot below no matter which line you choose some points are bound to be on the wrong side of the desicion boundary. Thus maximal margin classification would not work.\n\n\n\n\n\nSVM then introduces what is called a soft margin. In naive explanation you can think of this as a margin that allows some points to be on the wrong side. By introducing an error term we allow for some slack. Thus in a two case the maximisation becomes \\[y_i(\\beta_0 + \\beta_1X_1 + \\beta_2X_2) >= M(1- \\epsilon)\\]\n\\[\\sum_{i=0}^{n} \\epsilon_i <= C\\] C is a tuning parameter which determines the width of the margin while \\[\\epsilon_i  \\;'s\\] are slack variables. that allow individual observations to fall on the wrong side of the margin. In some cases the decision boundary maybe non linear. In case your are dealing with logistic regression you will be forced to introduce polynomial terms which might result in a very large feature space. SVM then introduces what are called kernels"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#tuning-svm",
    "href": "posts/breast_cancer_prediction/cancer_data.html#tuning-svm",
    "title": "Predict whether the cancer is benign or malignant",
    "section": "Tuning SVM",
    "text": "Tuning SVM\n\nsvm_tune <-  expand.grid(\n    C =c(1 ,5 ,  10, 100, 150),\n    sigma = seq(0, .01, length.out = 5))\n    \nsvm_model <- train(\n  label ~.,\n  data = train,\n   metric=\"ROC\",\n  method = \"svmRadial\",\n  trControl = myControl,\n  tuneGrid = svm_tune,\n  verbose = FALSE\n)\n\n\nresample_svm <- thresholder(svm_model, \n                              threshold = seq(.0, 1, by = 0.05), \n                              final = TRUE, \n                              statistics = \"all\")\n\nggplot(resample_svm , aes(x = prob_threshold, y = F1, col = \"F1\")) + \n  geom_point() + \n  geom_point(aes(y = Sensitivity,  col = \"Sensitivity\"))+\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1))\n\n\n\n#mean(pred_svm == ytest)\n\n\npred_svm <-predict(svm_model, newdata = test, type = \"prob\")\n\npred_svm <- ifelse(pred_svm[, \"M\"] > 0.40, \"M\", \"B\")\n\npred_svm <- factor(pred_svm, levels = levels(test$label))\n\nconfusionMatrix(test$label, pred_svm, positive = \"M\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   M   B\n         M  58   2\n         B   2 109\n                                        \n               Accuracy : 0.977         \n                 95% CI : (0.941, 0.994)\n    No Information Rate : 0.649         \n    P-Value [Acc > NIR] : <2e-16        \n                                        \n                  Kappa : 0.949         \n                                        \n Mcnemar's Test P-Value : 1             \n                                        \n            Sensitivity : 0.967         \n            Specificity : 0.982         \n         Pos Pred Value : 0.967         \n         Neg Pred Value : 0.982         \n             Prevalence : 0.351         \n         Detection Rate : 0.339         \n   Detection Prevalence : 0.351         \n      Balanced Accuracy : 0.974         \n                                        \n       'Positive' Class : M"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#xgboost",
    "href": "posts/breast_cancer_prediction/cancer_data.html#xgboost",
    "title": "Predict whether the cancer is benign or malignant",
    "section": "Xgboost",
    "text": "Xgboost\nXGBoost is a type of an ensemble learner. Ensemble learning is where multiple machine learning algorithms are used at the same time for prediction. A good example will be Random Forests. In random Forest multiple decision trees are used together for prediction. There are two main types of ensemble learners, bagging and boosting. Random forest use the bagging approach. Trees are built from random subsets(rows and columns) of training set and then the final prediction is the weighted sum of all decision trees functions. Boosting methods are similar but in boosting samples are selected sequentially. For instance the first sample is selected and a decision tree is fitted, The model then picks the examples that were hard to learn and using this examples and a few others selected randomly from the training set the second model is fitted, Using the first model and the second model prediction is made, the model is evaluated and hard examples are picked and together with another randomly selected new examples from training set another model is trained. This is the process for boosting algorithms which continues for a specified number of n.\nIn gradient boosting the first model is fitted to the original training set. Let say your fitting a simple regression model for ease of explanation. Then your first model will be $ y = f(x) + $. When you find that the error is too large one of the things you might try to do is add more features, use another algorithm, tune your algorithm, look for more training data etc. But what if the error is not white noise and it has some relationship with output \\(y\\) . Then we can fit a second model. $ = f_1(x) + _1$. then this process can continue lets say until n times. Then the final model will be\n$ n = f*{n}(x) + _{n-1}$.\nThen the final step is to add this models together with some weighting criteria $ weights = ’s$ which gives us the final function used for prediction.\n\\(y = \\alpha * f(x) + \\alpha_1 * f_1(x) + \\alpha_2 * f_2(x)...+ \\alpha_n * f_n + \\epsilon\\)\n\n# \"subsample\" is the fraction of the training samples (randomly selected) that will be used to train each tree.\n# \"colsample_by_tree\" is the fraction of features (randomly selected) that will be used to train each tree.\n# \"colsample_bylevel\" is the fraction of features (randomly selected) that will be used in each node to train each tree.\n#eta learning rate\n\n\n\nxgb_ctrl <- trainControl(method = \"cv\",\n                        number = 5,\n                        summaryFunction = twoClassSummary,\n                        classProbs = TRUE,\n                        allowParallel=T,\n                        index = cv_fold,\n                        verboseIter = FALSE,\n                        savePredictions = TRUE,\n                        search = \"grid\")\n\nxgb_grid <- expand.grid(nrounds = c(10, 50, 100),\n                        eta = seq(0.06, .2, length.out = 3),\n                        max_depth = c(50, 80),\n                        gamma = c(0,.01, 0.1),\n                        colsample_bytree = c(0.6, 0.7,0.8),\n                        min_child_weight = 1,\n                        subsample =  .7\n                        \n    )\n\nxgb_model <-train(label~.,\n                 data=train,\n                 method=\"xgbTree\",\n                 trControl= xgb_ctrl,\n                 tuneGrid=xgb_grid,\n                 verbosity=0,\n                 metric=\"ROC\",\n                 nthread =4\n                     \n    )\n\nIncreasing cut of increases the precision. A greater fraction of those who will be predicted that they have cancer will turn out that they have, but the algorithm is likely to have lower recall. If we want to avoid too many cases of people cancer being predicted that they do not have cancer. It will be very bad to tell someone that they do not have cancer but they have. If we lower the probability let say to 0.3 then we want to make sure that even if there is a 30% chance you have cancer then you should be flagged.\n\nresample_xgb <- thresholder(xgb_model, \n                              threshold = seq(.0, 1, by = 0.01), \n                              final = TRUE, \n                              statistics = \"all\")\n\nggplot(resample_xgb , aes(x = prob_threshold, y = F1, col = \"F1\")) + \n  geom_point() + \n  geom_point(aes(y = Sensitivity, col = \"Sensitivity\"))+\n  scale_x_continuous(breaks = seq(0, 1, by =.1))\n\n\n\n\n\npred_xgb <-predict(xgb_model, newdata = test, type = \"prob\")\npred_xgb1 <- ifelse(pred_xgb[, \"M\"] > 0.4, \"M\", \"B\")\npred_xgb1 <- factor(pred_xgb1, levels = levels(test$label))\n\nconfusionMatrix(pred_xgb1,test$label,  positive = \"M\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   M   B\n         M  59   2\n         B   1 109\n                                       \n               Accuracy : 0.982        \n                 95% CI : (0.95, 0.996)\n    No Information Rate : 0.649        \n    P-Value [Acc > NIR] : <2e-16       \n                                       \n                  Kappa : 0.962        \n                                       \n Mcnemar's Test P-Value : 1            \n                                       \n            Sensitivity : 0.983        \n            Specificity : 0.982        \n         Pos Pred Value : 0.967        \n         Neg Pred Value : 0.991        \n             Prevalence : 0.351        \n         Detection Rate : 0.345        \n   Detection Prevalence : 0.357        \n      Balanced Accuracy : 0.983        \n                                       \n       'Positive' Class : M"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#learning-curves",
    "href": "posts/breast_cancer_prediction/cancer_data.html#learning-curves",
    "title": "Predict whether the cancer is benign or malignant",
    "section": "Learning Curves",
    "text": "Learning Curves\n\nsets <- seq(from = 50, to = nrow(train), by = 50)\nsets[length(sets)] <-nrow(train) \ntrain.err <- c()\ntest.err<- c()\ntune_grid <- expand.grid( nrounds = 50, max_depth = 50, eta = 0.06, gamma = 0.01, \n                         colsample_bytree = 0.6, min_child_weight = 1, subsample = 0.7)\nfor (i in 1:length(sets)) {\n    \n    traini = train[1:sets[i],]\n    fit_svm <- train(label ~., data = traini, metric=\"Accuracy\", method = \"svmRadial\",\n                 trControl = trainControl(method = \"none\", summaryFunction = twoClassSummary,\n                                          classProbs = TRUE),\n                 tuneGrid = expand_grid( sigma = 0.0075, C = 5),\n                 )\n    \n    # fit_svm <-train(label~.,\n    #              data=traini,\n    #              method=\"xgbTree\",\n    #              trControl= xgb_ctrl,\n    #              tuneGrid= tune_grid ,\n    #              verbose=T,\n    #              metric=\"ROC\",\n    #              nthread =3\n    #                  \n    # )\n    pred_train = predict(fit_svm, newdata = traini, type = \"prob\")\n    pred_train = ifelse(pred_train[[\"M\"]] > 0.4, \"M\", \"B\")\n    train.err[i] =1 -  mean(pred_train == traini$label)\n    pred_test = predict(fit_svm, newdata = test, type = 'prob')\n    pred_test = ifelse(pred_test[, \"M\"] > 0.4, \"M\", \"B\")\n    test.err[i] = 1 - mean(test$label == pred_test)\n    \n    cat(i,\" \")\n    \n}\n\n1  2  3  4  5  6  7  \n\ntrain.err\n\n[1] 0.00000 0.03000 0.03333 0.01500 0.02000 0.02000 0.02261\n\nmatplot(sets, cbind(test.err, train.err), pch = 19, col = c(\"red\", \"blue\"),\n        type = \"b\", ylab = \"Error\", xlab = \"Train sample size\", main = \"SVM Learning Curves\")\nlegend(\"topright\", legend = c(\"Test\", \"Train\"), pch = 19, col = c(\"red\", \"blue\"))"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#error-analysis",
    "href": "posts/breast_cancer_prediction/cancer_data.html#error-analysis",
    "title": "Predict whether the cancer is benign or malignant",
    "section": "Error Analysis",
    "text": "Error Analysis\nLook at the examples that the algorithm misclassified to see if there is a trend. Generally you are trying to find out the weak points of your algorithm. Checking why your algorithm is making those errors. For instance, from the boxplots below the malignant tumors that were misclassified had lower radius mean compared to mislassified benign tumors. This contrary to what we saw in the first boxplots graph.\n\ndf <- data.frame(cancer[-train_sample,], pred_svm) %>%\n    setDT()\n\n\ntest_mis_svm <- df[(diagnosis == \"M\" & pred_svm == 0) |( diagnosis == \"B\" & pred_svm == \"M\")]\n\n\n# test_mis_svm_m <- melt(test_mis_svm, \n#                 id.vars = c(\"diagnosis\", \"pred_svm\"))\n# \n# ggplot(test_mis_svm_m , aes(x = pred_svm, y = value))+\n#     geom_boxplot() + facet_wrap(~variable, scales = \"free_y\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Personal Blog",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting loan defaults\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2020\n\n\nmburu\n\n\n\n\n\n\n  \n\n\n\n\nPredict whether the cancer is benign or malignant\n\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2019\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssociation analysis\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2018\n\n\nMburu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/association_analysis/association_analysis.html",
    "href": "posts/association_analysis/association_analysis.html",
    "title": "Association analysis",
    "section": "",
    "text": "Maybe you have heard of a grocery store that carried out analysis and found out that men who buy diapers between 5pm to 7pm were more likely to buy beer. The grocery store then moved the beer isle closer to the diaper isle and beer sales increased by 35%. This is called association analysis which was motivated by retail store data.\nIn this blog I will explore the basics of association analysis. The goal is to find out:\n\nItems frequently bought together in association analysis this is called support. Let say you have ten transactions and in those ten 3 transactions have maize floor, rice and bread the the support for maize floor, rice and bread is 3/10 = 0.3. This is just marginal probability. In other terms the percentage of transactions these items were bought together.\nIn this example the support is written as Support({bread, maize floor} –> {rice} ). In general this is written as Support of item one and item 2 is Support({item1} –> {item 2}). Item 1 and item 2 may contain one or more items.\nWe also want to find out if someone bought a set of items what other set of item(s) were they likely to buy. In association analysis this is called confidence. In our above example let say that you find the proportion of transactions that contained maize floor and bread are 0.4. Then the confidence is the proportion of those transactions with maize floor, bread and rice/proportion of transactions that contained maize floor and bread. Then the confidence is 0.3/0.4 which is 0.75. In other word 75% of those who bought maize floor and bread also bought rice.\n\nConfidence in this example is denoted as Confidence({bread, maize floor} –> {rice} ) and in general this is Confidence({item 1} –> {item 2} ).\n\nThe lift refers to how the chances of rice being purchased increased given that maize floor and bread are purchased. So the lift of rice is confidence of rice/support(rice). Support of rice is the number of transactions that contain rice.\n\nLift({Item 1} -> {Item 2 }) = (Confidence(Item1 -> Item2)) / (Support(Item2))\n\n\nTo make sense of all these I’m going to use a bakery to find association rules between items bought manually and then towards the end I will use r package arules which uses apriori algorithm to find association between items bought. The data set is available on kaggle as BreadBasket_DMS. We start by first having a glimpse of this data set.\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(data.table)\nlibrary(DT)\n\ndat <- setDT(read.csv( \"BreadBasket_DMS.csv\" ))\n\ndat <- dat[Item != \"NONE\"]\nhead(dat[sample(1:nrow(dat), 10)]) %>% data_table()\n\n\n\n\n\n\n\n\nFirst step is to transform the data set into wide format. Column headers will be items sold in the bakery and the rows will be populated with 1 and 0 indicating whether that item was bought for that transaction.\n\ndat2 <- dcast(Date+Time+Transaction~Item, data = dat, fun.aggregate = length)\n#dat2[, NONE := NULL]\n\nsample_cols <- sample(4:ncol(dat2), 5)\n\nitem_names <- names(dat2)[4:97]\n\ndat2[, (item_names) := lapply(.SD, function(x) ifelse(x==0, 0, 1)), .SDcols = item_names]\n\nhead(dat2[, c(1:3, sample_cols), with = F]) %>% data_table()\n\n\n\n\n\n\n\n\n\nOn average a transaction has 2 items. The median is also 2, this shows that atleast 50% of the transactions contained 2 or more items and atleast 25% of the transactions have 1 item.\n\nnumber_items <- rowSums(dat2[, 4:97, with =F])\n\ndat2[, number_items := number_items]\n\nhist(number_items, col = \"black\", main = \"Number of items bought\")\n\n\n\nsumStats <- dat2 %>%\n    summarise(Average = round(mean(number_items), 2), Stdev = round(sd(number_items), 2),\n              Median= median(number_items),\n              Minimum = min(number_items), Maximum = max(number_items),\n              First_qurtile = quantile(number_items,0.25,  na.rm = T),\n              Third_qurtile=quantile(number_items,0.75,  na.rm = T))\n\ndata_table(sumStats)\n\n\n\n\n\n\n\n\n\nTable below shows top ten most bought items and about 47.84% of the transactions contained coffee. Coffee was the most popular item in this bakery followed by bread.\n\nn_transacts_item_in <- colSums(dat2[, item_names, with = F])\n\ndata.frame(item = names(n_transacts_item_in),\n           number =n_transacts_item_in) %>%\n    mutate(Percentage = round(number/nrow(dat2)*100, 2)) %>%\n    arrange(desc(number)) %>% head(10) %>% data_table()\n\n\n\n\n\n\nSince we have transformed the data in the wide format and every transaction is in it’s row we can visualize how the baskets look like. This is done by extracting the column names for the transactions where the value is 1. For each transaction, 1 represent that item being in that transaction.\n\nitems_bought <- apply(dat2[, 4:97, with =F], 1, paste, collapse = \"\", sep = \"\")\n\nlist_items <- vector(mode = \"list\", length = length(items_bought))\nfor (i in 1:length(items_bought)) {\n    index <- unlist(gregexpr(\"1\", items_bought[i]))\n    items_transaction_i <- item_names[index]\n    items_transaction_i <- paste(items_transaction_i, sep = \" \", collapse = \" , \")\n    list_items[[i]] <- items_transaction_i\n}\n\nhead(unlist(list_items))\n\n[1] \"Bread\"                         \"Scandinavian\"                 \n[3] \"Cookies , Hot chocolate , Jam\" \"Muffin\"                       \n[5] \"Bread , Coffee , Pastry\"       \"Medialuna , Muffin , Pastry\"  \n\n\nData frame below shows how the baskets look like. Only 10 randomly selected rows are displayed. Items_bought column shows the baskets.\n\ndat2[, items_bought := unlist(list_items) ]\n\nhead(dat2[sample(1:nrow(dat2), 10),\n          .(Transaction, number_items,items_bought)]) %>%\n  data_table()\n\n\n\n\n\n\nA small example which I will work out manually to see what is the support for ({coffee, bread} –> {jam}). Generally I want to see how many transactions contained these 3 items. 0.12% of the transactions contained {bread, coffee, jam}\n\nmy_item_set <- Hmisc::Cs(Coffee , Jam , Bread)\n\nidx_sample <- grep( \"Transactio|^Coffee$|^Jam$|^Bread$\", names(dat2))\n\n\n\nitem_set_dat <- dat2[, idx_sample, with = F] \n\n#some transaction bought more thanone of \nitem_set_dat[, (my_item_set) := lapply(.SD, function(x) ifelse(x==0, 0, 1)), .SDcols = my_item_set]\n\nitem_set_dat[, total_items := rowSums(item_set_dat[, 2:4, with = F]) ]\n\nsupport_coffee_bread_jam <- table(item_set_dat$total_items)[\"3\"]/9531 \n\nsupport_coffee_bread_jam\n\n          3 \n0.001154129 \n\n\nTo calculate confidence({bread, coffee} –> {jam}) we should also calculate the support of ({bread, coffee}) which is the prorpotion of bread and coffee appearing together in the transactions which is about 8.9% of the transactions.\n\nitem_set_dat[, coffee_bread := rowSums(item_set_dat[, c(\"Bread\", \"Coffee\"), with = F]) ]\n\nhead(item_set_dat, 2)\n\n   Transaction Bread Coffee Jam total_items coffee_bread\n1:           1     1      0   0           1            1\n2:           2     0      0   0           0            0\n\nsupport_coffee_bread <- table(item_set_dat$coffee_bread)[\"2\"]/9531 \n\nsupport_coffee_bread\n\n         2 \n0.08939251 \n\n\n1.3% of the people who bought bread and coffee also bought jam. This is the confidence of({bread, coffee} –> {jam}) For statisticians this can be translated as conditional probability. In conditional probability notations P(Jam/bread, coffee) which is probability you will buy jam given that you have already bought bread and coffee. In association analysis we have {bread, coffee} >>{jam} bread and coffee implies jam. So the confidence measures the strength/probability of this implication.\n\nconfidence <- support_coffee_bread_jam/support_coffee_bread \n\nconfidence * 100\n\n      3 \n1.29108 \n\n\nUsing package arules we find the 10 rules with the highest confidence in descending order. Confidence({Toast} –>{Coffee}) had the highest confidence of 0.70440252. About 70.44% of the transactions that contained toast also contained coffee.\n\nlibrary(arules)\n\ntransactions <- as(split(dat$Item, dat$Transaction), \"transactions\")\n\nassoc_rules <- apriori(transactions,\n                 parameter = list(supp = 0.02, conf = 0.04, target = \"rules\"))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n       0.04    0.1    1 none FALSE            TRUE       5    0.02      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 189 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[94 item(s), 9465 transaction(s)] done [0.00s].\nsorting and recoding items ... [19 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 done [0.00s].\nwriting ... [38 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\nassoc_rules <- sort(assoc_rules, by='confidence', decreasing = TRUE)\n\n\ninspect(assoc_rules[1:30]) #%>% broom::tidy() %>% kable\n\n     lhs                rhs         support    confidence coverage   lift     \n[1]  {Toast}         => {Coffee}    0.02366614 0.70440252 0.03359746 1.4724315\n[2]  {Medialuna}     => {Coffee}    0.03518225 0.56923077 0.06180666 1.1898784\n[3]  {Pastry}        => {Coffee}    0.04754358 0.55214724 0.08610671 1.1541682\n[4]  {Juice}         => {Coffee}    0.02060222 0.53424658 0.03856313 1.1167500\n[5]  {Sandwich}      => {Coffee}    0.03824617 0.53235294 0.07184363 1.1127916\n[6]  {Cake}          => {Coffee}    0.05472795 0.52695829 0.10385631 1.1015151\n[7]  {Cookies}       => {Coffee}    0.02820919 0.51844660 0.05441099 1.0837229\n[8]  {Hot chocolate} => {Coffee}    0.02958267 0.50724638 0.05832013 1.0603107\n[9]  {}              => {Coffee}    0.47839408 0.47839408 1.00000000 1.0000000\n[10] {Tea}           => {Coffee}    0.04986793 0.34962963 0.14263074 0.7308402\n[11] {Pastry}        => {Bread}     0.02916006 0.33865031 0.08610671 1.0349774\n[12] {}              => {Bread}     0.32720549 0.32720549 1.00000000 1.0000000\n[13] {Bread}         => {Coffee}    0.09001585 0.27510494 0.32720549 0.5750592\n[14] {Cake}          => {Tea}       0.02377179 0.22889115 0.10385631 1.6047813\n[15] {Cake}          => {Bread}     0.02334918 0.22482197 0.10385631 0.6870972\n[16] {Tea}           => {Bread}     0.02810354 0.19703704 0.14263074 0.6021813\n[17] {Coffee}        => {Bread}     0.09001585 0.18816254 0.47839408 0.5750592\n[18] {Tea}           => {Cake}      0.02377179 0.16666667 0.14263074 1.6047813\n[19] {}              => {Tea}       0.14263074 0.14263074 1.00000000 1.0000000\n[20] {Coffee}        => {Cake}      0.05472795 0.11439929 0.47839408 1.1015151\n[21] {Coffee}        => {Tea}       0.04986793 0.10424028 0.47839408 0.7308402\n[22] {}              => {Cake}      0.10385631 0.10385631 1.00000000 1.0000000\n[23] {Coffee}        => {Pastry}    0.04754358 0.09938163 0.47839408 1.1541682\n[24] {Bread}         => {Pastry}    0.02916006 0.08911850 0.32720549 1.0349774\n[25] {}              => {Pastry}    0.08610671 0.08610671 1.00000000 1.0000000\n[26] {Bread}         => {Tea}       0.02810354 0.08588957 0.32720549 0.6021813\n[27] {Coffee}        => {Sandwich}  0.03824617 0.07994700 0.47839408 1.1127916\n[28] {Coffee}        => {Medialuna} 0.03518225 0.07354240 0.47839408 1.1898784\n[29] {}              => {Sandwich}  0.07184363 0.07184363 1.00000000 1.0000000\n[30] {Bread}         => {Cake}      0.02334918 0.07135938 0.32720549 0.6870972\n     count\n[1]   224 \n[2]   333 \n[3]   450 \n[4]   195 \n[5]   362 \n[6]   518 \n[7]   267 \n[8]   280 \n[9]  4528 \n[10]  472 \n[11]  276 \n[12] 3097 \n[13]  852 \n[14]  225 \n[15]  221 \n[16]  266 \n[17]  852 \n[18]  225 \n[19] 1350 \n[20]  518 \n[21]  472 \n[22]  983 \n[23]  450 \n[24]  276 \n[25]  815 \n[26]  266 \n[27]  362 \n[28]  333 \n[29]  680 \n[30]  221 \n\n\nI hope with this small example you can now understand how association analysis works."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Okay I’m excited to finally sit down and work on my blog!. I hope to continually add interesting."
  },
  {
    "objectID": "notes/busara_task/busara data analysis.html",
    "href": "notes/busara_task/busara data analysis.html",
    "title": "Exploratory Data Analysis for Busara task",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\nlibrary(data.table)\nlibrary(knitr)\nlibrary(ggthemes)\nlibrary(broom)\n#library(data_tableExtra)\n\n\nTask 1\n\nUnderstanding the demographics of company xyz.\nAt least half are youth average age = 33.5\nAt least half earn 5557\n\n\nxyz <- setDT(read_csv(\"XYZ.csv\"))\n\nxyz_sub <- xyz[, .(Gender, Age, Income)]\n\nxyz_subm <- melt(xyz_sub, id.vars = \"Gender\")\n\n\n\nSummary Statistics Age and Income\n\nxyz_subm %>% group_by(variable) %>%\n    summarise(Average = mean(value), Median = median(value),\n              Min = min(value), Max = max(value)) %>%\n    \n    data_table() #%>% data_table_styling() %>%\n\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\n% gender Gap\n\nMale/Female % al most equal\nThere are 5.2% more men than women\n\n\ngender <- xyz %>% group_by(Gender) %>%\n    summarise(freq = n()) %>%\n    mutate(Perc = round(freq/sum(freq) * 100, 2))\n\ngender %>% data_table() #%>% data_table_styling() %>%\n\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\n% gender Gap\n\ngender[, -2] %>% spread(Gender, Perc) %>% \n    mutate(Percentage_Gender_Gap = Male - Female) %>% \n    data_table() #%>% data_table_styling() %>%\n\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n#Single Ladies Nyeri\n\n12(2.4% of the company employees) single ladies from Nyeri county\n\n\nsingle_nyeri <- xyz[Gender == \"Female\" & Marital_Status == \"Single\" & County == \"Nyeri\",]\nnrow(xyz)\n\n[1] 500\n\ncat(\"The Number of single ladies in Nyeri is \", nrow(single_nyeri))\n\nThe Number of single ladies in Nyeri is  12\n\n\n\n\nSummary Statistics Single Ladies Nyeri\n\nAverage age 36 and medium income is about $50\n\n\nsingle_nyeri %>%\n    summarise(Average_Age = mean(Age), Median_Income = median(Income)) %>%\n    data_table() #%>%  data_table_styling() %>%\n\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nNumber of Juniors\n\n28 juniors\n\n\njuniors_26 <-xyz[!grepl(\"Operartions|Data\",Department)  & xyz$Age < 26 & grepl(\"Junior\", Role),]\n\ncat(\"The Number of juniors \", nrow(juniors_26))\n\nThe Number of juniors  28\n\njuniors_26 %>% group_by(Department) %>%\n    summarise(freq = n()) %>%\n    mutate(Perc = round(freq/sum(freq) * 100, 2)) %>%\n    data_table() #%>% data_table_styling() %>%\n\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"40%\")\n\n\n\nDifference in mean income between male and female\n\nThe Operations has the biggest difference in mean income\nFemale/Males average earnings in different departments\n\n\nincome_gender <- xyz %>% group_by(Gender, Department) %>%\n    summarise(Average = mean(Income))\n\n\nincome_gender_dcast <- dcast(Department ~ Gender, data = income_gender) \n\nincome_gender_dcast %>% mutate( Difference = Male - Female) %>%\n    data_table()#%>% data_table_styling() %>%\n\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"45%\")\n\n\n\nFunction to plot categorical variables\n\nbar_plot <- function(data, title,...) {\n    #load ggplot2\n    #function takes a data frame\n    #and other arguments that ggplot\n    #function from ggplot2 takes\n    # the other arguments are aesthetic mappings\n    require(ggplot2)\n    ggplot(data) + geom_bar(aes(...))+\n        ggtitle(title)+\n        ggthemes::theme_hc()+\n        ggthemes::scale_fill_hc()+\n        theme(legend.position = \"none\")\n        \n}\n\n\n\nFunction to plot categorical variables test 1\n\nbar_plot(xyz, Department, title = \"Department Distribution\", fill = Department)\n\n\n\n\n\n\nFunction to plot categorical variables test 2\n\nbar_plot(xyz, Gender, title = \"Gender Distibution\", fill = Gender)\n\n\n\n\n\n\nTask 2\nRead Files\n\nRead files using the patterns\n\n\nmy_files <- dir(path = \"Education\",pattern = \"^Chi|^Sch|^Persi|Secon|^Progr|Pri\")\n\nmy_files <- paste0(\"Education/\", my_files)\n\nlibrary(readxl)\n\nlist_files <- list()\n\nfor (i in 1:length(my_files)) {\n    \n    \n    x = read_excel(my_files[i]) \n    id = grep(\"Country Name\", x$`Data Source`)\n    nms <- x[id,]\n    names(x) <- nms %>% as.character()\n    list_files[[i]] <- x[-c(1:id),] \n    cat(\"...\")\n    \n}\n\n......................................................\n\n\n\n\nCombine Files\n\nSince files are stored in a list combine them\n\n\ndf_world <- rbindlist(list_files) %>% setDT()\n\ndf_world_melt <- melt(df_world, id.vars = names(df_world)[1:4])\n\nnms2 <- Hmisc::Cs(Country_Name, Country_Code,   \n          Indicator_Name,   Indicator_Code, Year,   Indicator_value)\n\nnames(df_world_melt) <- nms2\n\ndf_world_melt[,  Year := as.numeric(as.character(df_world_melt$Year))]\n\n\n\nHead output data frame\n\nhead(df_world_melt) %>% data_table() #%>% \n\n\n\n\n\n    #data_table_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nHead output kenya data\n\nkenya_2011 <- df_world_melt[Country_Name == \"Kenya\" & Year >= 2011]\n\nhead(kenya_2011) #%>% data_table() %>%\n\n   Country_Name Country_Code\n1:        Kenya          KEN\n2:        Kenya          KEN\n3:        Kenya          KEN\n4:        Kenya          KEN\n5:        Kenya          KEN\n6:        Kenya          KEN\n                                               Indicator_Name    Indicator_Code\n1:                    Children out of school, primary, female    SE.PRM.UNER.FE\n2: Persistence to last grade of primary, female (% of cohort) SE.PRM.PRSL.FE.ZS\n3:   Persistence to last grade of primary, male (% of cohort) SE.PRM.PRSL.MA.ZS\n4:  Primary completion rate, female (% of relevant age group) SE.PRM.CMPT.FE.ZS\n5:    Primary completion rate, male (% of relevant age group) SE.PRM.CMPT.MA.ZS\n6:                Progression to secondary school, female (%) SE.SEC.PROG.FE.ZS\n   Year Indicator_value\n1: 2011            <NA>\n2: 2011            <NA>\n3: 2011            <NA>\n4: 2011            <NA>\n5: 2011            <NA>\n6: 2011            <NA>\n\n    #data_table_styling() %>%\n    #scroll_box(width = \"100%\", height = \"70%\")\n\n\n\nHead output kenya data and saving files\n\nwrite.csv(head(kenya_2011, 15), file = \"kenya data.csv\", row.names = F)\n\nkenya_2011_na <- kenya_2011[!is.na(kenya_2011$Indicator_value),]\nwrite.csv(head(kenya_2011_na, 15), file = \"kenya data without na.csv\", row.names = F)\n\nhead(kenya_2011_na) %>% data_table() #%>%\n\n\n\n\n\n   # data_table_styling() %>%\n    #scroll_box(width = \"100%\", height = \"70%\")\n\n#Task 3\n\nfigari_sheet1 <- read_excel(\"Figari Bank.xlsx\" ) %>% setDT()\n\nfigari_sheet2 <- read_excel(\"Figari Bank.xlsx\", sheet = 2 ) %>% setDT()\n\nfigari_sheet2[,  Dates := as.Date(Dates, origin = \"1900-01-01\")]\n\nfigari_sheet2[,  year := year(Dates)]\n\n\nfigari_sheet2[,  month := month(Dates)]\nfigari_sheet2[, month := ifelse(nchar(month) == 1 ,paste(0, month), month)]\n\nfigari_sheet2[,  week_day := as.POSIXlt(Dates)$wday+1]\nfigari_sheet2[, week_day := ifelse(nchar(week_day) == 1 ,paste(0, week_day), week_day)]\nfigari_sheet2[,  week_no := week(Dates)]\n\nfigari_sheet2[, week_no := ifelse(nchar(week_no) == 1 ,paste(0, week_no), week_no)]\n\nfigari_sheet2[,  day_month := format(Dates, \"%d\")]\n\nfigari_sheet2_m <- melt(figari_sheet2[, c(3:9), with = F], id.vars = c(\"Amount\", \"Saving Mode\"))\n\n\n\nTask 3 Plots\n\nTime series will enable us too see if there is seasonal/cyclic effects/trend\nweek number after every two weeks, maybe end month\nSmoothing/decoposing often needed to see trend\n\n\nfigari_dat <- figari_sheet2_m %>% group_by(`Saving Mode`,variable, value) %>%\n    summarise(Average = mean(Amount)) \ntitles <- levels(as.factor(figari_dat$`Saving Mode`))\ntitles <- paste(\"Average Savings for\", titles)\nfigari_dat_split <- split(figari_dat, figari_dat$`Saving Mode`)\nplots_figari <- list()\nfor ( i in 1:length(figari_dat_split)) {\n    this = figari_dat_split[[i]]\n    #write.csv(this, file = \"this.csv\", row.names = F)\n   plots_figari[[i]] <- ggplot(this, aes(value, Average)) +\n       facet_wrap(~variable, scales = \"free_x\", ncol = 1)+\n       geom_line(data = this, aes(value, Average, group = 1)) +\n       ggthemes::theme_hc()+\n       labs(x = \"\", y = \"Average amount saved (KES)\", title = titles[i])# +\n       #theme(axis.text.x = element_text(angle = 30, vjust = 1, hjust = 1))\n    \n}\n\n\n\nAverage Amount saved at the Bank\n\nYear no visible trend/ few years\nfirst two months higher savings\nlowest between month 3 to month 8\nSave more from day 2 - day 4\nWeek 1- 3 more savings drops to week 10\nmore save less in around day 10 of the month\ndecreasing trend trend from day 4 -10\n\n\nplots_figari[[2]]\n\n\n\n\n\n\nAverage Amount saved at the Agent\n\nSave more from March to June\nIncreasing trend from week 1 to 23 then decreasing\nSave less towards end of a month\n\n\nplots_figari[[1]]\n\n\n\n\n\n\nAverage Amount saved Mobile money\n\non average\nsave less from month 3 to 6\nsave less from week 9 to 22\n\n\nplots_figari[[3]]\n\n\n\n\n\n\nEnd Month Savings Favourite tool\n\nI’m thinking about the number of times someone saves. Average maybe skewed.\nWomen prefer to save using agent\nIn regions no Nyeri\n\n\nnames(figari_sheet2)[1] = names(figari_sheet1)[1]\n\nfigari_comb <- merge(figari_sheet2, figari_sheet1, by = \"CustomerID\")\n\nend_month <- figari_comb %>% \n    group_by(day_month, `Saving Mode`) %>%\n    summarise(Freq = n()) %>%\n    mutate(perc = round(100 * Freq/sum(Freq), 2)) %>% ungroup()\n#The number of times one deposits\nggplot(end_month, aes(day_month, Freq )) +\n    geom_line(aes(color =`Saving Mode`, group =`Saving Mode` ), size = 1)+\n    theme_hc()+\n    scale_color_hc(name = \"\") +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\nHistogram Deposits\n\nWhat you would expect.\n\n\ndeposits <- figari_sheet2[, .(freq = .N), by = CustomerID] \n\n#approximmately poison\nhist(deposits$freq, col = \"black\",\n     main = \"Deposits\", \n     xlab = \"Deposits\")\n\n\n\n\n\n\nSubset People who have made one deposit\n\nfigari_deposits <- merge(deposits, figari_sheet1, by = \"CustomerID\")\nfigari_deposits_one <- figari_deposits[freq == 1] \n\n\n\nDemographic characteristics of those who have only made one deposit\n\n\nGender\n\nfigari_deposits_one %>% group_by(Gender) %>%\n    summarise(freq= n()) %>%\n    mutate(Perc =round(freq/sum(freq) *100, 2) ) %>%\n    data_table()# %>%\n\n\n\n\n\n    #data_table_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nRegion\n\nfigari_deposits_one %>% group_by(Region) %>%\n    summarise(freq= n()) %>% \n    mutate(Perc =round(freq/sum(freq) *100, 2) ) %>%\n    data_table() #%>%\n\n\n\n\n\n    #data_table_styling() %>%\n    #scroll_box(width = \"100%\", height = \"100%\")\n\n\n\nAge\n\nfigari_deposits_one %>% \n    summarise(Mean= round(mean(Age), 2), Median  = median(Age)) %>%\n    data_table() #%>%\n\n\n\n\n\n    #data_table_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nTask 4\n\n\nProject Motivation\nData has the potential to transform business and drive the creation of business value. It can be used for a range of tasks such visualization relationships between variables to predicting if an event will occur. The later is one of the heavily reaserched areas in recent times. The reason for this is that data has grown exponentially and so does the computing power. Banks and financial institutions used data analytics for a range of value such as fraud detetction customer segment, recruiting, credit scoring and so on.\nIn this study I will use Bogoza data set to build a credit model where an applicat will be avaluated on whether they will default or not.\nHigh accuracy for this model will be required because predicting false positives will eventually cause a business to make a loss and false negatives means that the financial instituion looses business.\n\n\nData Cleaning\nFirst step is data cleaning. This ensures that columns are consistent. For instance the target variable had values such as Y y yes where all of them represent yes.\n\n#some algorithms like xgboost take numeric data\n#you can convert binary vars to 1,0\n# and form dummie variables using library dummies\n#for variables with more than 2 categories\nborogoza <- setDT(read_csv( \"Bagorogoza Loan.csv\"))\n\nborogoza[, Target := ifelse(grepl(\"y|Y\", Target), 1, 0)]\n\nborogoza[, Gender := ifelse(grepl(\"^m$|^male$\", tolower(Gender)), 0, 1)]\n\nborogoza[, Married := ifelse(grepl(\"Yes\",Married), 1, 0)]\n\nborogoza[, Education := ifelse(grepl(\"not\", tolower(Education)), 0, 1)]\n\nborogoza[, Self_Employed := ifelse(grepl(\"Yes\",Self_Employed), 1, 0)]\n\nborogoza[, Property_Area := ifelse(grepl(\"rural\",tolower(Property_Area)), \"Rural\", Property_Area)]\n\nborogoza[, Property_Area := ifelse(grepl(\"semi\",tolower(Property_Area)), \"Semi-urban\", Property_Area)]\n\nborogoza[, Property_Area := ifelse(grepl(\"^urban$\",tolower(Property_Area)), \"Urban\", Property_Area)]\n\n\n\nVariable Selection\n\nWhere we run descripte statistics\n\n\n\nVisualize Categorical variables\nVisualization and summary statistics is an impostant step before fitting any model as this will give you a glimpse of how the variables are associated with target variable. In this case I will use stacked barplot as from them you can see if the prorpotions of defaulters and non defaulters is equal in defferent categories of a variable. From the graphs we can see that the prorpotion of defaulters and non defaulters is defferent for the different credit history categories. This is aslo seen in the prorpety area. From the categorical variables we can therefore conclude that one of the best predictors is credit history.\n\nnumeric_vars <- Hmisc::Cs(ApplicantIncome,CoapplicantIncome, LoanAmount )\n\nnms_bo <- names(borogoza)[-1]\n\ncat_vars <- nms_bo[!nms_bo %in% numeric_vars]\n\n\nborogoza_catm <- melt(borogoza[, cat_vars, with = F], id.vars = \"Target\")\n\nborogoza_catm_perc <-borogoza_catm  %>%  group_by(variable, value, Target) %>%\n    summarise(freq= n()) %>% mutate(perc =round(freq/sum(freq) *100, 2) )\n\nlibrary(ggthemes)\nggplot(borogoza_catm_perc, aes(value, perc, fill = factor(Target) )) +\n    geom_bar(stat = \"identity\") +facet_wrap(~variable, scales = \"free_x\")+\n    scale_fill_economist(name = \"\")+\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\nVisualize numeric variables\nFor the numeric variables boxplot help us visualize which distribution is different from the other. Non overlapping boxplot for defaulters and non defaulters may indicate that the mean/median values in the two groups was significantly different. From this we can see that it’s unlikely that education and self employment affect loan repayment and for this we drop this two variables\n\nborogoza_numm <- melt(borogoza[, c(numeric_vars, \"Target\"), with = F], id.vars = \"Target\")\n\nggplot(borogoza_numm, aes(as.factor(Target), scale(value ))) +\n    geom_boxplot() +facet_wrap(~variable, scales = \"free_y\")\n\n\n\n\n\n\nOne-Hot Encoding for categorical variables with more than 2 levels\nIn this step variables with more than two categories are converted to dummies variables. The first column in each category is dropped as it’s linearly dependent with the second column.\n\nchars <- unlist(lapply(borogoza[, -1, with = F], is.character)) \n\nchars <- nms_bo[chars]\n\nlibrary(dummies)\nborogoza_dummy <- dummy.data.frame(borogoza, names = c(chars, \"Loan_Amount_Term\")) %>%\n    setDT()\n\n\nborogoza_dummy[, Loan_ID := NULL]\nborogoza_dummy[, Loan_Amount_Term36 := NULL]\nborogoza_dummy[, `Property_AreaSemi-urban` := NULL]\nborogoza_dummy[, `Dependents1` := NULL]\n\n\n\nScale variables\nIt’s important to scale your variables since it leads to faster convergence and since some algorithm use distances to find decision boundary this means that variables with big values will have a big influence.\n\nxvars <- names(borogoza_dummy)[!names(borogoza_dummy) %in% \"Target\"]\nborogoza_dummy[, (xvars) := lapply(.SD, function(x) scale(x)), .SDcols = xvars ]\n\n\n\nSplit test and train sets\nThis is important as it helps evaluate your model on data it has never seen. The model will be trained on one set(training set) and tested using test set.\n\nset.seed(200) # for reproducibility\ntrain_sample <- sample(1:nrow(borogoza_dummy), round(0.7*nrow(borogoza_dummy)))\n\ntrain <- borogoza_dummy[train_sample,]\ntest <- borogoza_dummy[-train_sample,]\n\n\n\nFit Logistic Regression\nLogistic regression was fit to predict the probability of someone defaulting. The advantages of logistic regression is interpret table, ie you can see the association between a predictor and response value, it also gives a probability. This is very important when you want to have your own cut off point eg you want to label someone as a defaulter if you the predicted probability is more than 0.7. This increases precision but lowers recall. Using step wise selection the model was used to select the variables that best predict loan default.\n\nfit_glm <- glm(Target ~ Married + CoapplicantIncome + Loan_Amount_Term60 + \n    Loan_Amount_Term180 + Loan_Amount_Term300 + \n     Loan_Amount_Term360 + Credit_History + Property_AreaRural + \n     Property_AreaUrban ,data = train, family = binomial)\n\nborogoza_dummy <- borogoza_dummy[, .(Target,Married , CoapplicantIncome , Loan_Amount_Term60 , \n     Loan_Amount_Term180 , Loan_Amount_Term300 , \n     Loan_Amount_Term360 , Credit_History , Property_AreaRural , \n     Property_AreaUrban)]\n\n\ntrain <- borogoza_dummy[train_sample,]\ntest <- borogoza_dummy[-train_sample,]\nfit_glm %>% broom::tidy() %>% data_table()# %>%\n\n\n\n\n\n    # data_table_styling() %>%\n    # scroll_box(width = \"100%\", height = \"100%\")\n\n#MASS::stepAIC(fit_glm)\n\nThe estimate column shows the log odds. Positive values means that the variable makes it more likely for a person to repay their loan negative values means that the person is less likely to repay.\n\n\nConfusion Matrix Logistic regression\nThe confusion matrix evaluate correctly classified cases. A perfect fit will have all values in the main diagnose while the entries of lower/upper triangular should be zeros. In this case we have 14 cases of false positives and 7 cases of false negatives the accuracy of the model is 0.82 with and f1 score of 0.87. F1 score is a very important evaluation metric where there is unbalanced classes.\n\nlibrary(caret)\npred_glm <- predict(fit_glm,newdata = test)\n\npred_glm <- ifelse(pred_glm>0.7, 1 , 0)\n\ntable(test$Target, pred_glm) %>% data_table()# %>%\n\n\n\n\n\n    #data_table_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nAccuracy Logistic regression\n\nlibrary(broom)\nlibrary(pROC)\ntable(test$Target, pred_glm) %>% \n    confusionMatrix(positive = \"1\") %>% \n    tidy() %>% data_table()# %>%\n\n\n\n\n\n    # data_table_styling() %>%\n    # scroll_box(width = \"100%\", height = \"100%\")\n\n\n\nArea under curve\nThis is important as it will help you know if the suffers from high false negatives or false positives. A value greater than 0.8 is normally desired in this case we achieve 0.74.\n\nroc(as.numeric(test$Target), pred_glm, print.auc=T, print.auc.y=0.5, levels =0:1 ) \n\n\nCall:\nroc.default(response = as.numeric(test$Target), predictor = pred_glm,     levels = 0:1, print.auc = T, print.auc.y = 0.5)\n\nData: pred_glm in 39 controls (as.numeric(test$Target) 0) < 76 cases (as.numeric(test$Target) 1).\nArea under the curve: 0.7367\n\n\n\n\nCross Validation SVM\nNext we fit Support vector machine model. We start by finding the best parameters using cross validation. We use 10 fold this where train set is randomly split into 10 sets. In each cases one of the 1 set is used as a validation/test set while the other 9 are used to train the model.\n\nlibrary(e1071)\ntune.out = tune(svm, as.factor(Target)~., data = train, kernel =\"radial\", \n                type =\"C-classification\",\n                ranges =list (cost=c(0.01, 0.1, 1 ,5 ,  10),\n                              gamma = c(0.01,  0.1, 1 ,5 )))\n\nsummary(tune.out)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost gamma\n    5  0.01\n\n- best performance: 0.1595442 \n\n- Detailed performance results:\n    cost gamma     error dispersion\n1   0.01  0.01 0.2747863 0.06706580\n2   0.10  0.01 0.2747863 0.06706580\n3   1.00  0.01 0.1596866 0.06510749\n4   5.00  0.01 0.1595442 0.06956599\n5  10.00  0.01 0.1595442 0.06956599\n6   0.01  0.10 0.2747863 0.06706580\n7   0.10  0.10 0.1967236 0.07767111\n8   1.00  0.10 0.1670940 0.06780670\n9   5.00  0.10 0.1633903 0.06543082\n10 10.00  0.10 0.1633903 0.06543082\n11  0.01  1.00 0.2747863 0.06706580\n12  0.10  1.00 0.2747863 0.06706580\n13  1.00  1.00 0.2041311 0.06754684\n14  5.00  1.00 0.2004274 0.07183672\n15 10.00  1.00 0.2078348 0.07591537\n16  0.01  5.00 0.2747863 0.06706580\n17  0.10  5.00 0.2747863 0.06706580\n18  1.00  5.00 0.2078348 0.06963224\n19  5.00  5.00 0.2152422 0.07506115\n20 10.00  5.00 0.2189459 0.08024003\n\n\n\n\nConfusion Matrix SVM\n\nfit_svm <- svm(as.factor(Target)~., data = train, cost =5  , gamma = .01, \n               kernel = \"radial\", type =\"C-classification\")\n\n\npred_svm <-predict(fit_svm, newdata = test)\ntable(test$Target, pred_svm) %>% data_table() #%>%\n\n\n\n\n\n   # data_table_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nArea under curve\n\nroc(test$Target, as.numeric(pred_svm), print.auc=T, print.auc.y=0.5, levels =0:1 ) \n\n\nCall:\nroc.default(response = test$Target, predictor = as.numeric(pred_svm),     levels = 0:1, print.auc = T, print.auc.y = 0.5)\n\nData: as.numeric(pred_svm) in 39 controls (test$Target 0) < 76 cases (test$Target 1).\nArea under the curve: 0.692\n\n\n\n\nAccuracy SVM\n\ntable(test$Target, pred_svm) %>% \n    confusionMatrix(positive = \"1\") %>% \n    tidy() %>% data_table() #%>%\n\n\n\n\n\n    #data_table_styling() %>%\n    #scroll_box(width = \"100%\", height = \"100%\")\n\n\n\nValidation Curves\nThe two models almost give equal results based on accuracy, f1 score and area under the curve. In this section we will evaluate the models using learning curves to see if they suffer from high variance or bias. In this case the model suffers from high bias. It’s evident that adding more data won’t solve accuracy problems. In this case additional features would help.\n\nsets <- seq(from = 50, to = nrow(train), by = 50)\nsets[length(sets)] <-nrow(train) \ntrain.err <- c()\ntest.err<- c()\nfor (i in 1:length(sets)) {\n    \n    traini = train[1:sets[i],]\n    fit_svm <- svm(as.factor(Target)~., data = traini, cost =5  , gamma = .01, \n               kernel = \"radial\", type =\"C-classification\")\n\n    \n    pred_train = predict(fit_svm, newdata = traini)\n    train.err[i] =1 -  mean(pred_train == traini$Target)\n    pred_test <- predict(fit_svm, newdata = test)\n    test.err[i] = 1 - mean(test$Target == pred_test)\n    \n    cat(i,\" \")\n    \n}\n\n1  2  3  4  5  \n\nmatplot(sets, cbind(test.err, train.err), pch = 19, col = c(\"red\", \"blue\"),\n        type = \"b\", ylab = \"Error\", xlab = \"Train sample size\", main = \"SVM Training and Validation errors\")\nlegend(\"topright\", legend = c(\"Test\", \"Train\"), pch = 19, col = c(\"red\", \"blue\"))\n\n\n\n\n\n\nDeployment\nOther model like Xgboost which uses boosting and bagging could first be used to see if the model performs better on this data. The problem could after this be intergrated with a loan evaluation software where it can help loan officers decide if the will award a loan."
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html",
    "href": "datacamp/imputations_in_R/imputations_in_R.html",
    "title": "Handling Missing Data with Imputations in R",
    "section": "",
    "text": "knitr::opts_chunk$set(\n    echo = TRUE,\n    message = FALSE,\n    warning = FALSE\n)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(janitor)\nlibrary(ggthemes)\nlibrary(here)\nlibrary(lubridate)\nlibrary(knitr)\nlibrary(broom)\n\n\n\nMissing data is a common problem and dealing with it appropriately is extremely important. Ignoring the missing data points or filling them incorrectly may cause the models to work in unexpected ways and cause the predictions and inferences to be biased.\nIn this chapter, you will be working with the biopics dataset. It contains information on a number of biographical movies, including their earnings, subject characteristics and some other variables. Some of the data points are, however, missing. The original data comes with the fivethirtyeight R package, but in this course, you will work with a slightly preprocessed version.\nIn this exercise, you will get to know the dataset and fit a linear regression model to explain a movie’s earnings. Let’s begin!\n\n\n\nbiopics <- read_csv(\"data/biopics.csv\")\n# Print first 10 observations\nhead(biopics, 10) %>%\n    kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\nyear\nearnings\nsub_num\nsub_type\nsub_race\nnon_white\nsub_sex\n\n\n\n\nUK\n1971\nNA\n1\nCriminal\nNA\n0\nMale\n\n\nUS/UK\n2013\n56.700\n1\nOther\nAfrican\n1\nMale\n\n\nUS/UK\n2010\n18.300\n1\nAthlete\nNA\n0\nMale\n\n\nCanada\n2014\nNA\n1\nOther\nWhite\n0\nMale\n\n\nUS\n1998\n0.537\n1\nOther\nNA\n0\nMale\n\n\nUS\n2008\n81.200\n1\nOther\nother\n1\nMale\n\n\nUK\n2002\n1.130\n1\nMusician\nWhite\n0\nMale\n\n\nUS\n2013\n95.000\n1\nAthlete\nAfrican\n1\nMale\n\n\nUS\n1994\n19.600\n1\nAthlete\nNA\n0\nMale\n\n\nUS/UK\n1987\n1.080\n2\nAuthor\nNA\n0\nMale\n\n\n\n\n\n\n\n\n\n# Get the number of missing values per variable\nbiopics %>%\n    is.na() %>% \n    colSums()\n\n  country      year  earnings   sub_num  sub_type  sub_race non_white   sub_sex \n        0         0       324         0         0       197         0         0 \n\n\n\n\n\n\n# Fit linear regression to predict earnings\nmodel_1 <- lm(earnings ~ country + year + sub_type, \n              data = biopics)\n\nsummary(model_1)\n\n\nCall:\nlm(formula = earnings ~ country + year + sub_type, data = biopics)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.283 -20.466  -5.251   6.871 285.210 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(>|t|)   \n(Intercept)                 -743.2411   273.2831  -2.720  0.00682 **\ncountryCanada/UK              -6.9648    19.5228  -0.357  0.72146   \ncountryUK                      7.0207    15.4945   0.453  0.65071   \ncountryUS                     30.9079    15.0039   2.060  0.04004 * \ncountryUS/Canada              31.6905    18.8308   1.683  0.09316 . \ncountryUS/UK                  23.7589    15.4580   1.537  0.12508   \ncountryUS/UK/Canada           -4.8187    29.6967  -0.162  0.87118   \nyear                           0.3783     0.1359   2.784  0.00562 **\nsub_typeActivist             -21.7103    13.0520  -1.663  0.09701 . \nsub_typeActor                -41.6236    16.8004  -2.478  0.01364 * \nsub_typeActress              -34.9628    17.5264  -1.995  0.04673 * \nsub_typeActress / activist     7.1816    37.6378   0.191  0.84877   \nsub_typeArtist               -25.2620    13.8543  -1.823  0.06898 . \nsub_typeAthlete              -10.7316    12.1242  -0.885  0.37661   \nsub_typeAthlete / military    66.3717    37.6682   1.762  0.07882 . \nsub_typeAuthor               -25.9330    12.6080  -2.057  0.04034 * \nsub_typeAuthor (poet)        -17.1963    17.1851  -1.001  0.31759   \nsub_typeComedian             -29.3344    18.3419  -1.599  0.11053   \nsub_typeCriminal              -7.3534    12.2475  -0.600  0.54857   \nsub_typeGovernment           -16.9917    23.5048  -0.723  0.47016   \nsub_typeHistorical            -4.0166    12.6665  -0.317  0.75133   \nsub_typeJournalist           -30.6610    28.0016  -1.095  0.27418   \nsub_typeMedia                -15.7588    16.7744  -0.939  0.34806   \nsub_typeMedicine               5.0987    21.0749   0.242  0.80895   \nsub_typeMilitary              15.1616    14.0730   1.077  0.28196   \nsub_typeMilitary / activist   29.8300    37.6688   0.792  0.42888   \nsub_typeMusician             -21.1765    12.1482  -1.743  0.08206 . \nsub_typeOther                -17.5989    11.4405  -1.538  0.12476   \nsub_typePolitician           -21.0700    37.6688  -0.559  0.57623   \nsub_typeSinger                 1.0769    14.9161   0.072  0.94248   \nsub_typeTeacher               42.4600    37.6407   1.128  0.25997   \nsub_typeWorld leader           0.5964    16.2407   0.037  0.97072   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 36 on 405 degrees of freedom\n  (324 observations deleted due to missingness)\nMultiple R-squared:  0.1799,    Adjusted R-squared:  0.1171 \nF-statistic: 2.865 on 31 and 405 DF,  p-value: 1.189e-06\n\n\n\n\n\n\nYou are interested in how well the model you’ve just built fits the data. To measure this, you want to calculate the median absolute difference between the true and predicted earnings. You run the following line of code:\nAs some observations were removed from the model, the two vectors inside abs() have different lengths, and so the entries of the shorter one get replicated to enable the subtraction. Consequently, the resulting number has no meaning. Analyzing models fit to incomplete data can be treacherous\n\n\nmedian(abs(biopics$earnings - model_1$fitted.values), na.rm = TRUE)\n\n[1] 21.66698\n\n\n\n\n\n\nChoosing the best of multiple competing models can be tricky if these models are built on incomplete data. In this exercise, you will extend the model you have built previously by adding one more explanatory variable: the race of the movie’s subject. Then, you will try to compare it to the previous model.\nAs a reminder, this is how you have fitted the first model:\n\nmodel_1 <- lm(earnings ~ country + year + sub_type, data = biopics) Let’s see if we can judge whether adding the race variable improves the model!\n\n\n\n\n# Fit linear regression to predict earnings\nmodel_2 <- lm(earnings ~ country + year + sub_type + sub_race, \n              data = biopics)\n\n# Print summaries of both models\n\nsummary(model_2)\n\n\nCall:\nlm(formula = earnings ~ country + year + sub_type + sub_race, \n    data = biopics)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-58.323 -16.237  -4.018   5.614 200.234 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                -139.27034  287.97218  -0.484 0.629031    \ncountryCanada/UK              4.00206   18.25641   0.219 0.826643    \ncountryUK                    13.84774   14.91395   0.929 0.353943    \ncountryUS                    31.42015   14.32201   2.194 0.029069 *  \ncountryUS/Canada             18.29811   18.65109   0.981 0.327403    \ncountryUS/UK                 29.40669   14.79424   1.988 0.047817 *  \ncountryUS/UK/Canada           5.28487   34.26999   0.154 0.877553    \nyear                          0.08053    0.14277   0.564 0.573156    \nsub_typeActivist            -22.70696   13.91011  -1.632 0.103718    \nsub_typeActor               -37.18944   16.80696  -2.213 0.027722 *  \nsub_typeActress             -29.08213   17.54697  -1.657 0.098561 .  \nsub_typeActress / activist   22.74806   34.10892   0.667 0.505370    \nsub_typeArtist              -16.16366   14.44232  -1.119 0.264019    \nsub_typeAthlete               1.82705   13.21810   0.138 0.890163    \nsub_typeAthlete / military   81.76200   33.27768   2.457 0.014619 *  \nsub_typeAuthor              -16.89061   13.34913  -1.265 0.206817    \nsub_typeAuthor (poet)       -10.46216   17.81790  -0.587 0.557562    \nsub_typeComedian            -29.04858   19.58703  -1.483 0.139185    \nsub_typeCriminal             -3.63899   13.49577  -0.270 0.787636    \nsub_typeGovernment           -3.98375   21.53144  -0.185 0.853347    \nsub_typeHistorical           -1.84026   13.64400  -0.135 0.892806    \nsub_typeJournalist          -19.52435   25.70076  -0.760 0.448085    \nsub_typeMedia               -23.58188   18.39661  -1.282 0.200952    \nsub_typeMedicine             19.79476   33.28029   0.595 0.552465    \nsub_typeMilitary            -11.90055   15.58559  -0.764 0.445772    \nsub_typeMusician            -11.87866   12.76816  -0.930 0.352999    \nsub_typeOther                -8.26334   12.46291  -0.663 0.507854    \nsub_typePolitician          -13.12470   33.28805  -0.394 0.693677    \nsub_typeSinger               12.59513   15.42311   0.817 0.414829    \nsub_typeTeacher              52.19210   33.25064   1.570 0.117624    \nsub_typeWorld leader          5.70258   15.84955   0.360 0.719272    \nsub_raceAsian               -33.21461   17.04703  -1.948 0.052365 .  \nsub_raceHispanic            -25.63976    9.37824  -2.734 0.006657 ** \nsub_raceMid Eastern          -0.75224   11.54403  -0.065 0.948091    \nsub_raceMulti racial        -26.03619    9.67832  -2.690 0.007571 ** \nsub_raceother               -23.90532   12.36017  -1.934 0.054113 .  \nsub_raceWhite               -20.10327    5.90967  -3.402 0.000767 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31.01 on 280 degrees of freedom\n  (444 observations deleted due to missingness)\nMultiple R-squared:  0.2566,    Adjusted R-squared:  0.161 \nF-statistic: 2.684 on 36 and 280 DF,  p-value: 3.145e-06\n\n\n\nThe two models are not comparable, because each of them is based on a different data sample.\nWith incomplete datasets, changing the model’s architecture can impact the set of observations that are actually used by the model. This might prevent us from comparing different models.\n\n\n\n\n\nIn this exercise, you will face six different scenarios in which some data are missing. Try assigning each of them to the most likely missing data mechanism. As a refresher, here are some general guidelines:\n\nIf the reason for missingness is purely random, it’s MCAR.\nIf the reason for missingness can be explained by another variable, it’s MAR.\nIf the reason for missingness depends on the missing value itself, it’s MNAR.\n\nFurther explanation from Missing data mechanisms\n\nMissing completely at random (MCAR). When data are MCAR, the fact that the data are missing is independent of the observed and unobserved data. In other words, no systematic differences exist between participants with missing data and those with complete data. For example, some participants may have missing laboratory values because a batch of lab samples was processed improperly. In these instances, the missing data reduce the analyzable population of the study and consequently, the statistical power, but do not introduce bias: when data are MCAR, the data which remain can be considered a simple random sample of the full data set of interest. MCAR is generally regarded as a strong and often unrealistic assumption.\nMissing at random (MAR). When data are MAR, the fact that the data are missing is systematically related to the observed but not the unobserved data.15 For example, a registry examining depression may encounter data that are MAR if male participants are less likely to complete a survey about depression severity than female participants. That is, if probability of completion of the survey is related to their sex (which is fully observed) but not the severity of their depression, then the data may be regarded as MAR. Complete case analyses, which are based on only observations for which all relevant data are present and no fields are missing, of a data set containing MAR data may or may not result in bias. If the complete case analysis is biased, however, proper accounting for the known factors (in the above example, sex) can produce unbiased results in analysis.\nMissing not at random (MNAR). When data are MNAR, the fact that the data are missing is systematically related to the unobserved data, that is, the missingness is related to events or factors which are not measured by the researcher. To extend the previous example, the depression registry may encounter data that are MNAR if participants with severe depression are more likely to refuse to complete the survey about depression severity. As with MAR data, complete case analysis of a data set containing MNAR data may or may not result in bias; if the complete case analysis is biased, however, the fact that the sources of missing data are themselves unmeasured means that (in general) this issue cannot be addressed in analysis and the estimate of effect will likely be biased.\n\n ## t-test for MAR: data preparation Great work on classifying the missing data mechanisms in the last exercise! Of all three, MAR is arguably the most important one to detect, as many imputation methods assume the data are MAR. This exercise will, therefore, focus on testing for MAR.\nYou will be working with the familiar biopics data. The goal is to test whether the number of missing values in earnings differs per subject’s gender. In this exercise, you will only prepare the data for the t-test. First, you will create a dummy variable indicating missingness in earnings. Then, you will split it per gender by first filtering the data to keep one of the genders, and then pulling the dummy variable. For filtering, it might be helpful to print biopics’s head() in the console and examine the gender variable.\n\n# Create a dummy variable for missing earnings\nbiopics <- biopics %>% \n  mutate(missing_earnings = ifelse(is.na(earnings), TRUE, FALSE))\n\n# Pull the missing earnings dummy for males\nmissing_earnings_males <- biopics %>% \n  filter(sub_sex == \"Male\") %>% \n  pull(missing_earnings)\n\n# Pull the missing earnings dummy for females\nmissing_earnings_females <- biopics %>% \n  filter(sub_sex == \"Female\") %>% \n  pull(missing_earnings)\n\n# Run the t-test\nt.test(missing_earnings_males, missing_earnings_females)\n\n\n    Welch Two Sample t-test\n\ndata:  missing_earnings_males and missing_earnings_females\nt = 1.1116, df = 294.39, p-value = 0.2672\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.03606549  0.12969214\nsample estimates:\nmean of x mean of y \n0.4366438 0.3898305 \n\n\n\nNotice how the missing earnings percentage is not significantly different for both genders, even though the sample values (at the bottom of the test’s output) differ by almost 5 percentage points. Also, keep in mind that the conclusion that the data are not MAR is only valid for the specific variables we have tested.\n\n\n\n\nThe aggregation plot provides the answer to the basic question one may ask about an incomplete dataset: in which combinations of variables the data are missing, and how often? It is very useful for gaining a high-level overview of the missingness patterns. For example, it makes it immediately visible if there is some combination of variables that are often missing together, which might suggest some relation between them.\nIn this exercise, you will first draw the aggregation plot for the biopics data and then practice making conclusions based on it. Let’s do some plotting!\n\n# Load the VIM package\nlibrary(VIM)\n\n# Draw an aggregation plot of biopics\nbiopics %>% \n    aggr(combined = TRUE, numbers = TRUE)\n\n\n\n\n\n10% of the observations have missing values in both earnings and sub_race.\nThere are more missing values in sub_race than in earnings. This is false\n42% of the observations have no missing entries.\nThere are exactly two variables in the biopics data that have missing values.\nThis one is false! It is actually the other way round, there are more missing values in earnings. You can see it from the bars above the plot. Now that you have a high-level overview of the missingness in the data, let’s look more closely at specific variables!\n\n\n\n\nThe aggregation plot you have drawn in the previous exercise gave you some high-level overview of the missing data. If you are interested in the interaction between specific variables, a spine plot is the way to go. It allows you to study the percentage of missing values in one variable for different values of the other, which is conceptually very similar to the t-tests you have been running in the previous lesson.\nIn this exercise, you will draw a spine plot to investigate the percentage of missing data in earnings for different categories of sub_race. Is there more missing data on earnings for some specific races of the movie’s main character? Let’s find out! The VIM package has already been loaded for you.\n\n# Draw a spine plot to analyse missing values in earnings by sub_race\nbiopics %>% \n    select(sub_race, earnings) %>% as.data.frame() %>%\n    spineMiss()\n\n\n\n\n\n\n\nIn the vast majority of movies, the main character is white.\nWhen the main subject is African, we are the most likely to have complete earnings information.\nAs far as earnings and sub_race are concerned, the data seem to be MAR.\nThe race that appears most rarely in the data has around 40% of earnings missing.\n\n\nThis one is false! The scarcest race is Asian, as this bar is the thinnest. The missing earnings, however, amount to around 20%, not 40%. Let’s build upon the idea of a spine plot to create one more visualization in the next exercise!\n\n\n\n\n\nThe spine plot you have created in the previous exercise allows you to study missing data patterns between two variables at a time. This idea is generalized to more variables in the form of a mosaic plot.\nIn this exercise, you will start by creating a dummy variable indicating whether the United States was involved in the production of each movie. To do this, you will use the grepl() function, which checks if the string passed as its first argument is present in the object passed as its second argument. Then, you will draw a mosaic plot to see if the subject’s gender correlates with the amount of missing data on earnings for both US and non-US movies.\nThe biopics data as well as the VIM package are already loaded for you. Let’s do some exploratory plotting!\nNote that a propriety display_image()function has been created to return the output from the latest VIMpackage version. Make sure to expand the HTML Viewer section.\n\n# Prepare data for plotting and draw a mosaic plot\nbiopics %>%\n    # Create a dummy variable for US-produced movies\n    mutate(is_US_movie = grepl(\"US\", country)) %>%\n    # Draw mosaic plot\n    mosaicMiss(highlight = \"earnings\", \n             plotvars = c(\"is_US_movie\", \"sub_sex\"))\n\n\n\n# Return plot from latest VIM package - expand the HTML viewer section\n#display_image()\n\n\nBefore you expand the output, notice how, for non-US movies, there is less missing data on earnings for movies featuring females. This doesn’t look MCAR! You are now done with Chapter 1 and ready to take a deep dive into imputation methods."
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#smelling-the-danger-of-mean-imputation",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#smelling-the-danger-of-mean-imputation",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Smelling the danger of mean imputation",
    "text": "Smelling the danger of mean imputation\nOne of the most popular imputation methods is the mean imputation, in which missing values in a variable are replaced with the mean of the observed values in this variable. However, in many cases this simple approach is a poor choice. Sometimes a quick look at the data can already alert you to the dangers of mean-imputing.\nIn this chapter, you will be working with a subsample of the Tropical Atmosphere Ocean (tao) project data. The dataset consists of atmospheric measurements taken in two different time periods at five different locations. The data comes with the VIM package.\nIn this exercise you will familiarize yourself with the data and perform a simple analysis that will indicate what the consequences of mean imputation could be. Let’s take a look at the tao data!\n\nPrint first 10 observations\n\ntao <- read.csv(\"data/tao.csv\")\n# Print first 10 observations\nhead(tao, 10)\n\n   year latitude longitude sea_surface_temp air_temp humidity uwind vwind\n1  1997        0      -110            27.59    27.15     79.6  -6.4   5.4\n2  1997        0      -110            27.55    27.02     75.8  -5.3   5.3\n3  1997        0      -110            27.57    27.00     76.5  -5.1   4.5\n4  1997        0      -110            27.62    26.93     76.2  -4.9   2.5\n5  1997        0      -110            27.65    26.84     76.4  -3.5   4.1\n6  1997        0      -110            27.83    26.94     76.7  -4.4   1.6\n7  1997        0      -110            28.01    27.04     76.5  -2.0   3.5\n8  1997        0      -110            28.04    27.11     78.3  -3.7   4.5\n9  1997        0      -110            28.02    27.21     78.6  -4.2   5.0\n10 1997        0      -110            28.05    27.25     76.9  -3.6   3.5\n\n\n\n\nGet the number of missing values per column\n\n# Get the number of missing values per column\ntao %>%\n  is.na() %>% \n  colSums()\n\n            year         latitude        longitude sea_surface_temp \n               0                0                0                3 \n        air_temp         humidity            uwind            vwind \n              81               93                0                0 \n\n\n\n\nCalculate the number of missing values in air_temp per year\n\n# Calculate the number of missing values in air_temp per year\ntao %>% \n  group_by(year) %>% \n  summarize(num_miss = sum(is.na(air_temp))) %>%\n    kable()\n\n\n\n\nyear\nnum_miss\n\n\n\n\n1993\n4\n\n\n1997\n77"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#mean-imputing-the-temperature",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#mean-imputing-the-temperature",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Mean-imputing the temperature",
    "text": "Mean-imputing the temperature\nMean imputation can be a risky business. If the variable you are mean-imputing is correlated with other variables, this correlation might be destroyed by the imputed values. You saw it looming in the previous exercise when you analyzed the air_temp variable.\nTo find out whether these concerns are valid, in this exercise you will perform mean imputation on air_temp, while also creating a binary indicator for where the values are imputed. It will come in handy in the next exercise, when you will be assessing your imputation’s performance. Let’s fill in those missing values!\n\ntao_imp <- tao %>% \n  # Create a binary indicator for missing values in air_temp\n  mutate(air_temp_imp = ifelse(is.na(air_temp), TRUE, FALSE)) %>% \n  # Impute air_temp with its mean\n  mutate(air_temp = ifelse(is.na(air_temp), mean(air_temp, na.rm = TRUE), air_temp))\n\n# Print the first 10 rows of tao_imp\nhead(tao_imp, 10) %>%\n    head() %>%\n    kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nlatitude\nlongitude\nsea_surface_temp\nair_temp\nhumidity\nuwind\nvwind\nair_temp_imp\n\n\n\n\n1997\n0\n-110\n27.59\n27.15\n79.6\n-6.4\n5.4\nFALSE\n\n\n1997\n0\n-110\n27.55\n27.02\n75.8\n-5.3\n5.3\nFALSE\n\n\n1997\n0\n-110\n27.57\n27.00\n76.5\n-5.1\n4.5\nFALSE\n\n\n1997\n0\n-110\n27.62\n26.93\n76.2\n-4.9\n2.5\nFALSE\n\n\n1997\n0\n-110\n27.65\n26.84\n76.4\n-3.5\n4.1\nFALSE\n\n\n1997\n0\n-110\n27.83\n26.94\n76.7\n-4.4\n1.6\nFALSE\n\n\n\n\n\n\nAssessing imputation quality with margin plot\nIn the last exercise, you have mean-imputed air_temp and added an indicator variable to denote which values were imputed, called air_temp_imp. Time to see how well this works.\nUpon examining the tao data, you might have noticed that it also contains a variable called sea_surface_temp, which could reasonably be expected to be positively correlated with air_temp. If that’s the case, you would expect these two temperatures to be both high or both low at the same time. Imputing mean air temperature when the sea temperature is high or low would break this relation.\nTo find out, in this exercise you will select the two temperature variables and the indicator variable and use them to draw a margin plot. Let’s assess the mean imputation!\n\n# Draw a margin plot of air_temp vs sea_surface_temp\ntao_imp %>% \n  select(air_temp, sea_surface_temp, air_temp_imp) %>%\n  marginplot(delimiter = \"imp\")\n\n\n\n\nQuestion\n\nJudging by the margin plot you have drawn, what’s wrong with this mean imputation?\nPossible Answers\n\n\nAll the imputed air_temp values are the same, no matter the sea_surface_temp. This breaks the correlation between these two variables.\nThe imputed values are located in the space where there is no observed data, which makes them outliers.\nThe variance of the imputed data differs from the one of observed data.\nAll three above answers are correct. correct\n\n\nNotice how air and sea surface temperatures correlate. Imputing average air temperature in the observations where sea surface temperature is high creates clearly outlying data points and destroys the relation between these two variables. If the sea surface temperature is high, we would like to impute air temperature values that are also high. Head over to the upcoming video to learn a method that is able to do that!\n\n\n\nThe problem of mean imputation\n\nggplot(tao_imp, aes(air_temp, sea_surface_temp, color = air_temp_imp))+\n    geom_point()+\n    scale_color_brewer(name = \"Imputed\", type = \"qual\", palette = \"Dark2\")+\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\nVanilla hot-deck\nHot-deck imputation is a simple method that replaces every missing value in a variable by the last observed value in this variable. It’s very fast, as only one pass through the data is needed, but in its simplest form, hot-deck may sometimes break relations between the variables.\nIn this exercise, you will try it out on the tao dataset. You will hot-deck-impute missing values in the air temperature column air_temp and then draw a margin plot to analyze the relation between the imputed values with the sea surface temperature column sea_surface_temp. Let’s see how it works!\n\n# Load VIM package\nlibrary(VIM)\n\n# Impute air_temp in tao with hot-deck imputation\ntao_imp <- hotdeck(tao, variable = \"air_temp\")\n\n# Check the number of missing values in each variable\ntao_imp %>% \n    is.na() %>% \n    colSums()\n\n            year         latitude        longitude sea_surface_temp \n               0                0                0                3 \n        air_temp         humidity            uwind            vwind \n               0               93                0                0 \n    air_temp_imp \n               0 \n\n# Draw a margin plot of air_temp vs sea_surface_temp\ntao_imp %>% \n    select(air_temp, sea_surface_temp, air_temp_imp) %>% \n    marginplot(delimiter = \"imp\")\n\n\n\n\n\nDoes the imputation look good? Notice the observations in the top left part of the plot with imputed air_temp and high sea_surface_temp. These observations must have been preceded by ones with low air_temp in the data frame, and so after hot-deck imputation, they ended up being outliers with low air_temp and high sea_surface_temp.\n\n\n\nHot-deck tricks & tips I: imputing within domains\nOne trick that may help when hot-deck imputation breaks the relations between the variables is imputing within domains. What this means is that if the variable to be imputed is correlated with another, categorical variable, one can simply run hot-deck separately for each of its categories.\nFor instance, you might expect air temperature to depend on time, as we are seeing the average temperatures rising due to global warming. The time indicator you have available in the tao data is a categorical variable, year. Let’s first check if the average air temperature is different in each of the two studied years and then run hot-deck within year domains. Finally, you will draw the margin plot again to assess the imputation performance.\n\n# Calculate mean air_temp per year\ntao %>% \n    group_by(year) %>% \n    summarize(average_air_temp = mean(air_temp, na.rm = TRUE)) %>%\n    kable()\n\n\n\n\nyear\naverage_air_temp\n\n\n\n\n1993\n23.36596\n\n\n1997\n27.10979\n\n\n\n\n# Hot-deck-impute air_temp in tao by year domain\ntao_imp <- hotdeck(tao, variable = \"air_temp\", domain_var = \"year\")\n\n# Draw a margin plot of air_temp vs sea_surface_temp\ntao_imp %>% \n    select(air_temp, sea_surface_temp, air_temp_imp) %>% \n    marginplot(delimiter = \"imp\")\n\n\n\n\n\nThe results look much better this time. However, if you look at the top right corner of the plot, you will see that the variance in the imputed (orange) values is somewhat larger than among the observed (blue) values. Let’s see if we can improve even further in the next exercise\n\n\n\nHot-deck tricks & tips II: sorting by correlated variables\nAnother trick that can boost the performance of hot-deck imputation is sorting the data by variables correlated to the one we want to impute.\nFor instance, in all the margin plots you have been drawing recently, you have seen that air temperature is strongly correlated with sea surface temperature, which makes a lot of sense. You can exploit this knowledge to improve your hot-deck imputation. If you first order the data by sea_surface_temp, then every imputed air_temp value will come from a donor with a similar sea_surface_temp. Let’s see how this will work!\n\n# Hot-deck-impute air_temp in tao ordering by sea_surface_temp\ntao_imp <- hotdeck(tao, \n                   variable = \"air_temp\", \n                   ord_var = \"sea_surface_temp\")\n\n# Draw a margin plot of air_temp vs sea_surface_temp\ntao_imp %>% \n    select(air_temp, sea_surface_temp, air_temp_imp) %>% \n    marginplot(delimiter = \"imp\")\n\n\n\n\n\nThis time the imputation seems not to impact the relation between air and sea temperatures: if not for the colors, you likely wouldn’t know which ones are the imputed values. Hot-deck imputation, possibly enhanced with domain-imputing or sorting, is a fast and simple method that can serve you well in many situations. However, sometimes you may need a more complex approach. Head over to the next video to learn about k-Nearest-Neighbors imputation!\n\n\n\nJust a little experiment\n\n# Hot-deck-impute air_temp in tao ordering by sea_surface_temp\ntao_imp <- hotdeck(tao, \n                   variable = \"air_temp\", \n                   ord_var = \"sea_surface_temp\",\n                   domain_var = \"year\")\n\n# Draw a margin plot of air_temp vs sea_surface_temp\ntao_imp %>% \n    select(air_temp, sea_surface_temp, air_temp_imp) %>% \n    marginplot(delimiter = \"imp\")\n\n\n\n\n\n\nChoosing the number of neighbors\nk-Nearest-Neighbors (or kNN) imputation fills the missing values in an observation based on the values coming from the k other observations that are most similar to it. The number of these similar observations, called neighbors, that are considered is a parameter that has to be chosen beforehand.\nHow to choose k? One way is to try different values and see how they impact the relations between the imputed and observed data.\nLet’s try imputing humidity in the tao data using three different values of k and see how the imputed values fit the relation between humidity and sea_surface_temp.\n\nImpute humidity with kNN imputation using 30 neighbors and draw a marginplot() of sea_surface_temp vs humidity.\n\n\n# Impute humidity using 30 neighbors\ntao_imp <- kNN(tao, k = 30, variable = \"humidity\")\n\n# Draw a margin plot of sea_surface_temp vs humidity\ntao_imp %>% \n    select(sea_surface_temp, humidity, humidity_imp) %>% \n    marginplot(delimiter = \"imp\", main = \"k = 30\")\n\n\n\n\n\nImpute humidity with kNN imputation using 15 neighbors and draw a margin plot of sea_surface_temp vs humidity.\n\n\n# Impute humidity using 15 neighbors\ntao_imp <- kNN(tao, k = 15, variable = \"humidity\")\n\n# Draw a margin plot of sea_surface_temp vs humidity\ntao_imp %>% \n    select(sea_surface_temp, humidity, humidity_imp) %>% \n    marginplot(delimiter = \"imp\", main = \"k = 15\")\n\n\n\n\n\nImpute humidity with kNN imputation using 5 neighbors and draw a margin plot of sea_surface_temp vs humidity.\n\n\n# Impute humidity using 5 neighbors\ntao_imp <- kNN(tao, k = 5, variable = \"humidity\")\n\n# Draw a margin plot of sea_surface_temp vs humidity\ntao_imp %>% \n    select(sea_surface_temp, humidity, humidity_imp) %>% \n    marginplot(delimiter = \"imp\", main = \"k = 5\")\n\n\n\n\n\nYou can browse through the three plots you have just drawn. The last one seems to capture the most variation in the data, so you should be good to use k = 5 in this case. Let’s look at how we can improve on this default kNN imputation with some tricks!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#knn-tricks-tips-i-weighting-donors",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#knn-tricks-tips-i-weighting-donors",
    "title": "Handling Missing Data with Imputations in R",
    "section": "kNN tricks & tips I: weighting donors",
    "text": "kNN tricks & tips I: weighting donors\nA variation of kNN imputation that is frequently applied uses the so-called distance-weighted aggregation. What this means is that when we aggregate the values from the neighbors to obtain a replacement for a missing value, we do so using the weighted mean and the weights are inverted distances from each neighbor. As a result, closer neighbors have more impact on the imputed value.\nIn this exercise, you will apply the distance-weighted aggregation while imputing the tao data. This will only require passing two additional arguments to the kNN() function. Let’s try it out!\n\n# Load the VIM package\nlibrary(VIM)\n\n# Impute humidity with kNN using distance-weighted mean\ntao_imp <- kNN(tao, \n               k = 5, \n               variable = \"humidity\", \n               numFun = weighted.mean,\n               weightDist = TRUE)\n\ntao_imp %>% \n    select(sea_surface_temp, humidity, humidity_imp) %>% \n    marginplot(delimiter = \"imp\")\n\n\n\n\n\nDistance-weighted aggregation makes the kNN imputation more robust to situations where an observation is unique in some way and doesn’t have many very similar neighbors. In such cases, the least similar neighbors get assigned a small weight and contribute less to the imputed values. Head over to the last exercise of this chapter to learn one more trick that makes kNN more robust and accurate!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#knn-tricks-tips-ii-sorting-variables",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#knn-tricks-tips-ii-sorting-variables",
    "title": "Handling Missing Data with Imputations in R",
    "section": "kNN tricks & tips II: sorting variables",
    "text": "kNN tricks & tips II: sorting variables\nAs the k-Nearest Neighbors algorithm loops over the variables in the data to impute them, it computes distances between observations using other variables, some of which have already been imputed in the previous steps. This means that if the variables located earlier in the data have a lot of missing values, then the subsequent distance calculation is based on a lot of imputed values. This introduces noise to the distance calculation.\nFor this reason, it is a good practice to sort the variables increasingly by the number of missing values before performing kNN imputation. This way, each distance calculation is based on as much observed data and as little imputed data as possible.\nLet’s try this out on the tao data!\n\n# Get tao variable names sorted by number of NAs\nvars_by_NAs <- tao %>%\n  is.na() %>%\n  colSums() %>%\n  sort(decreasing = FALSE) %>% \n  names()\nvars_by_NAs\n\n[1] \"year\"             \"latitude\"         \"longitude\"        \"uwind\"           \n[5] \"vwind\"            \"sea_surface_temp\" \"air_temp\"         \"humidity\"        \n\n# Sort tao variables and feed it to kNN imputation\ntao_imp <- tao %>% \n  select(vars_by_NAs) %>% \n  kNN(k = 5)\n\ntao_imp %>% \n    select(sea_surface_temp, humidity, humidity_imp) %>% \n    marginplot(delimiter = \"imp\")\n\n\n\n\n\nThe kNN you have just coded should be more accurate and robust against faulty imputations, so remember to sort your variables first before performing kNN imputation! This brings us to the end of this chapter. Keep it up! See you in Chapter 3, where you will learn to use statistical and machine learning models to impute missing values!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#linear-regression-imputation",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#linear-regression-imputation",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Linear regression imputation",
    "text": "Linear regression imputation\nSometimes, you can use domain knowledge, previous research or simply your common sense to describe the relations between the variables in your data. In such cases, model-based imputation is a great solution, as it allows you to impute each variable according to a statistical model that you can specify yourself, taking into account any assumptions you might have about how the variables impact each other.\nFor continuous variables, a popular model choice is linear regression. It doesn’t restrict you to linear relations though! You can always include a square or a logarithm of a variable in the predictors. In this exercise, you will work with the simputation package to run a single linear regression imputation on the tao data and analyze the results. Let’s give it a try!\n\n# Load the simputation package\nlibrary(simputation)\n\n# Impute air_temp and humidity with linear regression\nformula <- air_temp + humidity ~ year + latitude + sea_surface_temp\ntao_imp <- impute_lm(tao, formula)\n\n# Check the number of missing values per column\ntao_imp %>% \n  is.na() %>% \n  colSums()\n\n            year         latitude        longitude sea_surface_temp \n               0                0                0                3 \n        air_temp         humidity            uwind            vwind \n               3                2                0                0 \n\n# Print rows of tao_imp in which air_temp or humidity are still missing \ntao_imp %>% \n  filter(is.na(air_temp) | is.na(humidity)) %>%\n    kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nlatitude\nlongitude\nsea_surface_temp\nair_temp\nhumidity\nuwind\nvwind\n\n\n\n\n1993\n0\n-95\nNA\nNA\nNA\n-5.6\n3.1\n\n\n1993\n0\n-95\nNA\nNA\nNA\n-6.3\n0.5\n\n\n1993\n-2\n-95\nNA\nNA\n89.9\n-3.4\n2.4\n\n\n\n\n\n\nLinear regression fails when at least one of the predictors is missing. In this case, it was sea_surface_temp. In the next exercise, you will fix it by initializing the missing values before running impute_lm()"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#initializing-missing-values-iterating-over-variables",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#initializing-missing-values-iterating-over-variables",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Initializing missing values & iterating over variables",
    "text": "Initializing missing values & iterating over variables\nAs you have just seen, running impute_lm() might not fill-in all the missing values. To ensure you impute all of them, you should initialize the missing values with a simple method, such as the hot-deck imputation you learned about in the previous chapter, which simply feeds forward the last observed value.\nMoreover, a single imputation is usually not enough. It is based on the basic initialized values and could be biased. A proper approach is to iterate over the variables, imputing them one at a time in the locations where they were originally missing.\nIn this exercise, you will first initialize the missing values with hot-deck imputation and then loop five times over air_temp and humidity from the tao data to impute them with linear regression. Let’s get to it!\n\n# Initialize missing values with hot-deck\ntao_imp <- hotdeck(tao)\n\n# Create boolean masks for where air_temp and humidity are missing\nmissing_air_temp <- tao_imp$air_temp_imp\nmissing_humidity <-  tao_imp$humidity_imp\n\nfor (i in 1:5) {\n  # Set air_temp to NA in places where it was originally missing and re-impute it\n  tao_imp$air_temp[missing_air_temp] <- NA\n  tao_imp <- impute_lm(tao_imp, air_temp ~ year + latitude + sea_surface_temp + humidity)\n  # Set humidity to NA in places where it was originally missing and re-impute it\n  tao_imp$humidity[missing_humidity] <- NA\n  tao_imp <- impute_lm(tao_imp, humidity ~ year + latitude + sea_surface_temp + air_temp)\n}\n\n\nThat’s a professional approach to model-based imputation you have just coded! But how do we know that 5 is the proper number of iterations to run? Let’s look at the convergence in the next exercise!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#detecting-convergence",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#detecting-convergence",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Detecting convergence",
    "text": "Detecting convergence\nGreat job iterating over the variables in the last exercise! But how many iterations are needed? When the imputed values don’t change with the new iteration, we can stop.\nYou will now extend your code to compute the differences between the imputed variables in subsequent iterations. To do this, you will use the Mean Absolute Percentage Change function, defined for you as follows:\nmapc <- function(a, b) { mean(abs(b - a) / a, na.rm = TRUE) } mapc() outputs a single number that tells you how much b differs from a. You will use it to check how much the imputed variables change across iterations. Based on this, you will decide how many of them are needed!\nThe boolean masks missing_air_temp and missing_humidity are available for you, as is the hotdeck-initialized tao_imp data.\n\nmapc <- function(a, b) {\n  mean(abs(b - a) / a, na.rm = TRUE)\n}\n\ndiff_air_temp <- c()\ndiff_humidity <- c()\n\nfor (i in 1:5) {\n  # Assign the outcome of the previous iteration (or initialization) to prev_iter\n  prev_iter <- tao_imp\n  # Impute air_temp and humidity at originally missing locations\n  tao_imp$air_temp[missing_air_temp] <- NA\n  tao_imp <- impute_lm(tao_imp, air_temp ~ year + latitude + sea_surface_temp + humidity)\n  tao_imp$humidity[missing_humidity] <- NA\n  tao_imp <- impute_lm(tao_imp, humidity ~ year + latitude + sea_surface_temp + air_temp)\n  # Calculate MAPC for air_temp and humidity and append them to previous iteration's MAPCs\n  diff_air_temp <- c(diff_air_temp, mapc(prev_iter$air_temp, tao_imp$air_temp))\n  diff_humidity <- c(diff_humidity, mapc(prev_iter$humidity, tao_imp$humidity))\n}\n\ndf_diff  <- data.frame(diff_air_temp, diff_humidity)\nplot_diffs <- function(a, b) {\n  data.frame(\"mapc\" = c(a, b),\n             \"Variable\" = c(rep(\"air_temp\", length(a)),\n                            rep(\"humidity\", length(b))),\n             \"Iterations\" = c(1:length(a), 1:length(b))) %>% \n    ggplot(aes(Iterations, mapc, color = Variable)) +\n    geom_line(size = 1.5) +\n    ylab(\"Mean absolute percentage change\") +\n    ggtitle(\"Changes in imputed variables' values across iterations\") +\n    theme(legend.position = \"bottom\")\n}\n\nplot_diffs(diff_air_temp, diff_humidity)\n\n\n\n\n\nTwo are enough, as the third one brings virtually no change anymore!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#logistic-regression-imputation",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#logistic-regression-imputation",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Logistic regression imputation",
    "text": "Logistic regression imputation\nA popular choice for imputing binary variables is logistic regression. Unfortunately, there is no function similar to impute_lm() that would do it. That’s why you’ll write such a function yourself!\nLet’s call the function impute_logreg(). Its first argument will be a data frame df, whose missing values have been initialized and only containing missing values in the column to be imputed. The second argument will be a formula for the logistic regression model.\nThe function will do the following:\nKeep the locations of missing values. Build the model. Make predictions. Replace missing values with predictions. Don’t worry about the line creating imp_var - this is just a way to extract the name of the column to impute from the formula. Let’s do some functional programming!\n\nimpute_logreg <- function(df, formula) {\n  # Extract name of response variable\n  imp_var <- as.character(formula[2])\n  # Save locations where the response is missing\n  missing_imp_var <- is.na(df[imp_var])\n  # Fit logistic regression mode\n  logreg_model <- glm(formula, data = df, family = binomial)\n  # Predict the response and convert it to 0s and 1s\n  preds <- predict(logreg_model, type = \"response\")\n  preds <- ifelse(preds >= 0.5, 1, 0)\n  # Impute missing values with predictions\n  df[missing_imp_var, imp_var] <-preds[missing_imp_var]\n  return(df)\n}"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#drawing-from-conditional-distribution",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#drawing-from-conditional-distribution",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Drawing from conditional distribution",
    "text": "Drawing from conditional distribution\nSimply calling predict() on a model will always return the same value for the same values of the predictors. This results in a small variability in imputed data. In order to increase it, so that the imputation replicates the variability from the original data, we can draw from the conditional distribution. What this means is that instead of always predicting 1 whenever the model outputs a probability larger than 0.5, we can draw the prediction from a binomial distribution described by the probability returned by the model.\nYou will work on the code you have written in the previous exercise. The following line was removed:\npreds <- ifelse(preds >= 0.5, 1, 0) Your task is to fill its place with drawing from a binomial distribution. That’s just one line of code!\n\n impute_logreg <- function(df, formula) {\n  # Extract name of response variable\n  imp_var <- as.character(formula[2])\n  # Save locations where the response is missing\n  missing_imp_var <- is.na(df[imp_var])\n  # Fit logistic regression mode\n  logreg_model <- glm(formula, data = df, family = binomial)\n  # Predict the response\n  preds <- predict(logreg_model, type = \"response\")\n  # Sample the predictions from binomial distribution\n  preds <- rbinom(length(preds), size = 1, prob = preds)\n  # Impute missing values with predictions\n  df[missing_imp_var, imp_var] <- preds[missing_imp_var]\n  return(df)\n}\n\n\nDrawing from the conditional distribution will make the imputed data’s variability more similar to the one of original, observed data. With this powerful function at hand, you are now ready to design a model-based imputation flow that takes care of both continuous and binary variables. Let’s do it in the next exercise!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#model-based-imputation-with-multiple-variable-types",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#model-based-imputation-with-multiple-variable-types",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Model-based imputation with multiple variable types",
    "text": "Model-based imputation with multiple variable types\nGreat job on writing the function to implement logistic regression imputation with drawing from conditional distribution. That’s pretty advanced statistics you have coded! In this exercise, you will combine what you learned so far about model-based imputation to impute different types of variables in the tao data.\nYour task is to iterate over variables just like you have done in the previous chapter and impute two variables:\nis_hot, a new binary variable that was created out of air_temp, which is 1 if air_temp is at or above 26 degrees and is 0 otherwise; humidity, a continuous variable you are already familiar with. You will have to use the linear regression function you have learned before, as well as your own function for logistic regression. Let’s get to it!\n\n# Initialize missing values with hot-deck\ntao <- tao %>% \n    mutate(is_hot = ifelse(air_temp > 26, 1, 0))\ntao_imp <- hotdeck(tao)\n\n# Create boolean masks for where is_hota and humidity are missing\nmissing_is_hot <- tao_imp$is_hot_imp\nmissing_humidity <- tao_imp$humidity_imp\n\nfor (i in 1:3) {\n  # Set is_hot to NA in places where it was originally missing and re-impute it\n  tao_imp$is_hot[missing_is_hot] <- NA\n  tao_imp <- impute_logreg(tao_imp, is_hot ~ sea_surface_temp)\n  # Set humidity to NA in places where it was originally missing and re-impute it\n  tao_imp$humidity[missing_humidity] <- NA\n  tao_imp <- impute_lm(tao_imp, \n  humidity ~ sea_surface_temp + air_temp)\n}\n\n\nYou have used the simputation package where possible, filling the gaps with your own programming, in order to run a model-based imputation that takes care of both continuous and binary variables, additionally inreasing variability in imputed data in the latter case. Well done! Let’s continue to the final lesson of this chapter, where you will learn how to use tree-based machine learning models for imputation."
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#imputing-with-random-forests",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#imputing-with-random-forests",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Imputing with random forests",
    "text": "Imputing with random forests\nA machine learning approach to imputation might be both more accurate and easier to implement compared to traditional statistical models. First, it doesn’t require you to specify relationships between variables. Moreover, machine learning models such as random forests are able to discover highly complex, non-linear relations and exploit them to predict missing values.\nIn this exercise, you will get acquainted with the missForest package, which builds a separate random forest to predict missing values for each variable, one by one. You will call the imputing function on the biographic movies data, biopics, which you have worked with earlier in the course and then extract the filled-in data as well as the estimated imputation errors.\nLet’s plant some random forests!\n\n# Load the missForest package\nbiopics <- read_csv(\"data/biopics.csv\")\nlibrary(missForest)\n\ncont_lev <- c(\"UK\", \"US/UK\", \"Canada US\", \n           \"Canada/UK\", \"US/Canada\", \"US/UK/Canada\")\n\nbiopics <- biopics %>%\n    mutate(country = factor(country, levels = cont_lev))\nbiopics <- biopics %>%\n    mutate_if(is.character, factor)\n# Impute biopics data using missForest\nbiopics <- as.data.frame(biopics)\nimp_res <- missForest(biopics)\n\n# Extract imputed data and check for missing values\nimp_data <- imp_res$ximpnhanes_imp\nprint(sum(is.na(imp_data)))\n\n[1] 0\n\n# Extract and print imputation errors\nimp_err <- imp_res$OOBerror\nprint(imp_err)\n\n    NRMSE       PFC \n0.0189669 0.1374846 \n\n\nNote that missForest() outputs a list and you have to manually extract the imputed data - it’s a common mistake to overlook it when building a data processing pipeline. Also, take a look at the errors. Can you tell which variables have been imputed particularly well? Let’s look at it more closely in the next exercise!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#variable-wise-imputation-errors",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#variable-wise-imputation-errors",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Variable-wise imputation errors",
    "text": "Variable-wise imputation errors\nIn the previous exercise you have extracted the estimated imputation errors from missForest’s output. This gave you two numbers:\nthe normalized root mean squared error (NRMSE) for all continuous variables; the proportion of falsely classified entries (PFC) for all categorical variables. However, it could well be that the imputation model performs great for one continuous variable and poor for another! To diagnose such cases, it is enough to tell missForest to produce variable-wise error estimates. This is done by setting the variablewise argument to TRUE.\nThe biopics data and missForest package have already been loaded for you, so let’s take a closer look at the errors!\n\n# Impute biopics data with missForest computing per-variable errors\nimp_res <- missForest(biopics, variablewise = TRUE)\n\n# Extract and print imputation errors\nper_variable_errors <- imp_res$OOBerror\nprint(per_variable_errors)\n\n         PFC          MSE          MSE          MSE          PFC          PFC \n   0.3740157    0.0000000 1182.3263805    0.0000000    0.0000000    0.1613475 \n         MSE          PFC \n   0.0000000    0.0000000 \n\n# Rename errors' columns to include variable names\nnames(per_variable_errors) <- paste(names(biopics), \n                                    names(per_variable_errors),\n                                    sep = \"_\")\n\n# Print the renamed errors\nprint(per_variable_errors)\n\n  country_PFC      year_MSE  earnings_MSE   sub_num_MSE  sub_type_PFC \n    0.3740157     0.0000000  1182.3263805     0.0000000     0.0000000 \n sub_race_PFC non_white_MSE   sub_sex_PFC \n    0.1613475     0.0000000     0.0000000"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#speed-accuracy-trade-off",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#speed-accuracy-trade-off",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Speed-accuracy trade-off",
    "text": "Speed-accuracy trade-off\nIn the last video, you have seen there are two knobs you can tune to influence the performance of the random forests:\nNumber of decision trees in each forest. Number of variables used for splitting within decision trees. Increasing each of them might improve the accuracy of the imputation model, but it will also require more time to run. In this exercise, you will explore these ideas yourself by fitting missForest() to the biopics data twice with different settings. As you follow the instructions, pay attention to the errors you will be printing, and to the time the code takes to run.\n\n# Set number of trees to 50 and number of variables used for splitting to 6\nimp_res <- missForest(biopics, ntree = 5, mtry = 2)\n\n# Print the resulting imputation errors\nprint(imp_res$OOBerror)\n\n     NRMSE        PFC \n0.02214787 0.19046987 \n\n# Set number of trees to 50 and number of variables used for splitting to 6\nimp_res <- missForest(biopics, ntree = 50, mtry = 6)\n\n# Print the resulting imputation errors\nprint(imp_res$OOBerror)\n\n     NRMSE        PFC \n0.02030573 0.13344991 \n\n\n\nCompare the errors and the run times of the two imputation models. Can you see a relation? There ain’t no such thing as a free lunch, they say. To get a more precise imputation, you had to spend more in computing time! Congratulations on finishing the chapter! See you in the final chapter, where you will learn to incorporate uncertainty from imputation into your analyses and predictions."
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#wrapping-imputation-modeling-in-a-function",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#wrapping-imputation-modeling-in-a-function",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Wrapping imputation & modeling in a function",
    "text": "Wrapping imputation & modeling in a function\nWhenever you perform any analysis or modeling on imputed data, you should account for the uncertainty from imputation. Running a model on a dataset imputed only once ignores the fact that imputation estimates the missing values with uncertainty. Standard errors from such a model tend to be too small. The solution to this is multiple imputation and one way to implement it is by bootstrapping.\nIn the upcoming exercises, you will work with the familiar biopics data. The goal is to use multiple imputation by bootstrapping and linear regression to see if, based on the data at hand, biographical movies featuring females earn less than those about males.\nLet’s start with writing a function that creates a bootstrap sample, imputes it, and fits a linear regression model.\n\ncalc_gender_coef <- function(data, indices) {\n  # Get bootstrap sample\n  data_boot <- data[indices, ]\n  # Impute with kNN imputation\n  data_imp <- kNN(data_boot, k = 5)\n  # Fit linear regression\n  linear_model <- lm(earnings ~ sub_sex + sub_type + year,data = data_imp)\n  # Extract and return gender coefficient\n  gender_coefficient <- coef(linear_model)[2]\n  return(gender_coefficient)\n}\n\nThe calc_gender_coef() function you have just coded takes the data and bootstrap indices as inputs, and outputs our statistic of interest - the impact of gender on earnings from linear regression. You can now feed this function to the bootstrapping algorithm!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#running-the-bootstrap",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#running-the-bootstrap",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Running the bootstrap",
    "text": "Running the bootstrap\nGood job writing calc_gender_coef() in the last exercise! This function creates a bootstrap sample, imputes it and, outputs the linear regression coefficient describing the impact of movie subject’s being a female on the movie’s earnings.\nIn this exercise, you will use the boot package in order to obtain a bootstrapped distribution of such coefficients. The spread of this distribution will capture the uncertainty from imputation. You will also look at how the bootstrapped distribution differs from a single-time imputation and regression. Let’s do some bootstrapping!\n\n# Load the boot library\nlibrary(boot)\n\n# Run bootstrapping on biopics data\nboot_results <- boot(biopics, statistic = calc_gender_coef, R = 50)\n\n# Print and plot bootstrapping results\nprint(boot_results)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = biopics, statistic = calc_gender_coef, R = 50)\n\n\nBootstrap Statistics :\n    original     bias    std. error\nt1* 1.875142 0.09830848    3.345934\n\nplot(boot_results)\n\n\n\n# Calculate and print confidence interval\nboot_ci <- boot.ci(boot_results, conf = .95, type = \"norm\")\nprint(boot_ci)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 50 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_results, conf = 0.95, type = \"norm\")\n\nIntervals : \nLevel      Normal        \n95%   (-4.781,  8.335 )  \nCalculations and Intervals on Original Scale\n\n\n\nIf you had run the kNN imputation and the regression analysis on biopics data only once, you would have obtained the female-coefficient of -1.45 (called ‘original’ in the console output), suggesting that movies about females indeed earn less. However, correcting for the uncertainty from imputation, you have obtained the distribution that covers both negative and postive values!\n\n\nBootstrapping confidence intervals\nHaving bootstrapped the distribution of the female-effect coefficient in the last exercise, you can now use it to estimate a confidence interval. It will allow you to make the following assessment about your data: “Given the uncertainty from imputation, we are 95% sure that the female-effect on earnings is between a and b”, where a and b are the lower and upper bounds of the interval.\nIn the last exercise, you have run bootstrapping with R = 50 replicates. In most applications, however, this is not enough. In this exercise, you can use boot_results that were prepared for you using 1000 replicates. First, you will look at the bootstrapped distribution to see if it looks normal. If so, you can then rely on the normal distribution to calculate the confidence interval.\n\n# Run bootstrapping on biopics data\nboot_results <- boot(biopics, statistic = calc_gender_coef, R = 1000)\n\n# Print and plot bootstrapping results\nprint(boot_results)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = biopics, statistic = calc_gender_coef, R = 1000)\n\n\nBootstrap Statistics :\n    original     bias    std. error\nt1* 2.311883 -0.5428357    3.600594\n\nplot(boot_results)\n\n\n\n# Calculate and print confidence interval\nboot_ci <- boot.ci(boot_results, conf = .95, type = \"norm\")\nprint(boot_ci)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_results, conf = 0.95, type = \"norm\")\n\nIntervals : \nLevel      Normal        \n95%   (-4.202,  9.912 )  \nCalculations and Intervals on Original Scale\n\n\n\nDespite the coefficient leaning to be a negative relationship, bootstrap replicates show that some movies with female leads actually earn more! Accounting for the uncertainty from imputation, you cannot be 100% sure about the direction of this relation, even though a single analysis suggests otherwise."
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#the-mice-flow-mice---with---pool",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#the-mice-flow-mice---with---pool",
    "title": "Handling Missing Data with Imputations in R",
    "section": "The mice flow: mice - with - pool",
    "text": "The mice flow: mice - with - pool\nMultiple imputation by chained equations, or MICE, allows us to estimate the uncertainty from imputation by imputing a data set multiple times with model-based imputation, while drawing from conditional distributions. This way, each imputed data set is slightly different. Then, an analysis is conducted on each of them and the results are pooled together, yielding the quantities of interest, alongside their confidence intervals that reflect the imputation uncertainty.\nIn this exercise, you will practice the typical MICE flow: mice() - with() - pool(). You will perform a regression analysis on the biopics data to see which subject occupation, sub_type, is associated with highest movie earnings. Let’s play with mice!\n\n# Load mice package\nlibrary(mice)\n\n# Impute biopics with mice using 5 imputations\nbiopics_multiimp <- mice(biopics, m = 5, seed = 3108)\n\n\n iter imp variable\n  1   1  country  earnings  sub_race\n  1   2  country  earnings  sub_race\n  1   3  country  earnings  sub_race\n  1   4  country  earnings  sub_race\n  1   5  country  earnings  sub_race\n  2   1  country  earnings  sub_race\n  2   2  country  earnings  sub_race\n  2   3  country  earnings  sub_race\n  2   4  country  earnings  sub_race\n  2   5  country  earnings  sub_race\n  3   1  country  earnings  sub_race\n  3   2  country  earnings  sub_race\n  3   3  country  earnings  sub_race\n  3   4  country  earnings  sub_race\n  3   5  country  earnings  sub_race\n  4   1  country  earnings  sub_race\n  4   2  country  earnings  sub_race\n  4   3  country  earnings  sub_race\n  4   4  country  earnings  sub_race\n  4   5  country  earnings  sub_race\n  5   1  country  earnings  sub_race\n  5   2  country  earnings  sub_race\n  5   3  country  earnings  sub_race\n  5   4  country  earnings  sub_race\n  5   5  country  earnings  sub_race\n\n# Fit linear regression to each imputed data set \nlm_multiimp <- with(biopics_multiimp,  lm(earnings~year+sub_type ))\n\n# Pool and summarize regression results\nlm_pooled <- pool(lm_multiimp)\nsummary(lm_pooled, conf.int = TRUE, conf.level = 0.95)\n\n                             term     estimate  std.error   statistic\n1                     (Intercept) -408.5476080 188.003023 -2.17309063\n2                            year    0.2195188   0.091547  2.39788059\n3  sub_typeAcademic (Philosopher)  -12.0393151  39.386794 -0.30566883\n4                sub_typeActivist  -12.5649024  12.493282 -1.00573272\n5                   sub_typeActor  -23.5461708  15.296101 -1.53935771\n6                 sub_typeActress  -17.5662047  12.764488 -1.37617778\n7      sub_typeActress / activist   20.3881286  35.764655  0.57006362\n8                  sub_typeArtist  -22.4452374  13.451761 -1.66857238\n9                 sub_typeAthlete   -0.8963086  13.009378 -0.06889712\n10     sub_typeAthlete / military   82.4367906  35.569407  2.31763188\n11                 sub_typeAuthor  -19.1635360  12.262989 -1.56271336\n12          sub_typeAuthor (poet)  -20.0620382  14.444056 -1.38894771\n13               sub_typeComedian  -18.1686840  16.980795 -1.06995483\n14               sub_typeCriminal   -5.8479945  13.420469 -0.43575187\n15             sub_typeGovernment   16.0450423  46.390726  0.34586746\n16             sub_typeHistorical   -3.7623435  11.294642 -0.33310871\n17             sub_typeJournalist  -22.8565780  26.181388 -0.87300866\n18                  sub_typeMedia   -7.0627461  18.145706 -0.38922410\n19               sub_typeMedicine   19.1339721  20.966662  0.91259031\n20               sub_typeMilitary   27.6733819  17.764776  1.55776701\n21    sub_typeMilitary / activist   41.9247600  35.860752  1.16909873\n22               sub_typeMusician  -14.1096056  11.123075 -1.26849868\n23                  sub_typeOther  -12.5957619  11.127539 -1.13194499\n24             sub_typePolitician   -8.9752400  35.860752 -0.25028030\n25                 sub_typeSinger   -2.3189932  14.863480 -0.15601953\n26                sub_typeTeacher   55.5076474  35.777696  1.55145953\n27           sub_typeWorld leader    1.9363047  14.102239  0.13730477\n           df    p.value         2.5 %      97.5 %\n1    8.441788 0.05977554 -838.16570234  21.0704863\n2    8.986358 0.04007631    0.01237714   0.4266604\n3   78.086760 0.76067020  -90.45102629  66.3723961\n4   23.732194 0.32468917  -38.36517557  13.2353708\n5   23.125611 0.13728938  -55.17905857   8.0867171\n6   63.616800 0.17359176  -43.06915926   7.9367499\n7  533.362518 0.56887459  -49.86873468  90.6449919\n8   17.334211 0.11316311  -50.78434701   5.8938723\n9   11.560021 0.94624889  -29.36142352  27.5688062\n10 610.794724 0.02079891   12.58361683 152.2899644\n11  18.269586 0.13527579  -44.89989577   6.5728238\n12  40.490599 0.17244132  -49.24354989   9.1194734\n13  68.008128 0.28842244  -52.05326005  15.7158921\n14  10.341265 0.67197238  -35.61744245  23.9214534\n15   5.438799 0.74242360 -100.36959617 132.4596809\n16  19.622883 0.74258502  -27.35161361  19.8269266\n17 401.472498 0.38318021  -74.32631797  28.6131621\n18  10.959555 0.70456670  -47.01916501  32.8936729\n19  11.569949 0.38008106  -26.73749557  65.0054398\n20   7.029288 0.16307001  -14.29818398  69.6449479\n21 499.260259 0.24292173  -28.53182460 112.3813447\n22  20.977532 0.21851670  -37.24281435   9.0236032\n23  15.905797 0.27443480  -36.19645010  11.0049264\n24 499.260259 0.80247354  -79.43182460  61.4813447\n25  16.388102 0.87792336  -33.76762529  29.1296390\n26 528.585147 0.12139010  -14.77627920 125.7915739\n27  27.018773 0.89180799  -26.99815808  30.8707674\n\n\nYou have followed the mice - with - pool flow to impute, model and pool the results. Now take a look at the console output: a couple of sub_types have a positive impact on earnings. However, accounting for imputation uncertainty with 95% confidence, we are never sure of these effects, as the lower bounds are negative! With one exception: for sub_typeAthlete / military, both upper and lower bounds are positive. What we can say for sure is thus that movies about military athletes are popular!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#choosing-default-models",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#choosing-default-models",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Choosing default models",
    "text": "Choosing default models\nMICE creates a separate imputation model for each variable in the data. What kind of model it is depends on the type of the variable in question. A popular way to specify the kinds of models we want to use is set a default model for each of the four variable types.\nYou can do this by passing the defaultMethod argument to mice(), which should be a vector of length 4 containing the default imputation methods for:\nContinuous variables, Binary variables, Categorical variables (unordered factors), Factor variables (ordered factors). In this exercise, you will take advantage of mice’s documentation to view the list of available methods and to pick the desired ones for the algorithm to use. Let’s do some model selection!\n\n# Impute biopics using the methods specified in the instruction\nbiopics_multiimp <- mice(biopics, m = 20, \n                         defaultMethod = c(\"cart\", \"lda\", \"pmm\", \"polr\"))\n\n\n iter imp variable\n  1   1  country  earnings  sub_race\n  1   2  country  earnings  sub_race\n  1   3  country  earnings  sub_race\n  1   4  country  earnings  sub_race\n  1   5  country  earnings  sub_race\n  1   6  country  earnings  sub_race\n  1   7  country  earnings  sub_race\n  1   8  country  earnings  sub_race\n  1   9  country  earnings  sub_race\n  1   10  country  earnings  sub_race\n  1   11  country  earnings  sub_race\n  1   12  country  earnings  sub_race\n  1   13  country  earnings  sub_race\n  1   14  country  earnings  sub_race\n  1   15  country  earnings  sub_race\n  1   16  country  earnings  sub_race\n  1   17  country  earnings  sub_race\n  1   18  country  earnings  sub_race\n  1   19  country  earnings  sub_race\n  1   20  country  earnings  sub_race\n  2   1  country  earnings  sub_race\n  2   2  country  earnings  sub_race\n  2   3  country  earnings  sub_race\n  2   4  country  earnings  sub_race\n  2   5  country  earnings  sub_race\n  2   6  country  earnings  sub_race\n  2   7  country  earnings  sub_race\n  2   8  country  earnings  sub_race\n  2   9  country  earnings  sub_race\n  2   10  country  earnings  sub_race\n  2   11  country  earnings  sub_race\n  2   12  country  earnings  sub_race\n  2   13  country  earnings  sub_race\n  2   14  country  earnings  sub_race\n  2   15  country  earnings  sub_race\n  2   16  country  earnings  sub_race\n  2   17  country  earnings  sub_race\n  2   18  country  earnings  sub_race\n  2   19  country  earnings  sub_race\n  2   20  country  earnings  sub_race\n  3   1  country  earnings  sub_race\n  3   2  country  earnings  sub_race\n  3   3  country  earnings  sub_race\n  3   4  country  earnings  sub_race\n  3   5  country  earnings  sub_race\n  3   6  country  earnings  sub_race\n  3   7  country  earnings  sub_race\n  3   8  country  earnings  sub_race\n  3   9  country  earnings  sub_race\n  3   10  country  earnings  sub_race\n  3   11  country  earnings  sub_race\n  3   12  country  earnings  sub_race\n  3   13  country  earnings  sub_race\n  3   14  country  earnings  sub_race\n  3   15  country  earnings  sub_race\n  3   16  country  earnings  sub_race\n  3   17  country  earnings  sub_race\n  3   18  country  earnings  sub_race\n  3   19  country  earnings  sub_race\n  3   20  country  earnings  sub_race\n  4   1  country  earnings  sub_race\n  4   2  country  earnings  sub_race\n  4   3  country  earnings  sub_race\n  4   4  country  earnings  sub_race\n  4   5  country  earnings  sub_race\n  4   6  country  earnings  sub_race\n  4   7  country  earnings  sub_race\n  4   8  country  earnings  sub_race\n  4   9  country  earnings  sub_race\n  4   10  country  earnings  sub_race\n  4   11  country  earnings  sub_race\n  4   12  country  earnings  sub_race\n  4   13  country  earnings  sub_race\n  4   14  country  earnings  sub_race\n  4   15  country  earnings  sub_race\n  4   16  country  earnings  sub_race\n  4   17  country  earnings  sub_race\n  4   18  country  earnings  sub_race\n  4   19  country  earnings  sub_race\n  4   20  country  earnings  sub_race\n  5   1  country  earnings  sub_race\n  5   2  country  earnings  sub_race\n  5   3  country  earnings  sub_race\n  5   4  country  earnings  sub_race\n  5   5  country  earnings  sub_race\n  5   6  country  earnings  sub_race\n  5   7  country  earnings  sub_race\n  5   8  country  earnings  sub_race\n  5   9  country  earnings  sub_race\n  5   10  country  earnings  sub_race\n  5   11  country  earnings  sub_race\n  5   12  country  earnings  sub_race\n  5   13  country  earnings  sub_race\n  5   14  country  earnings  sub_race\n  5   15  country  earnings  sub_race\n  5   16  country  earnings  sub_race\n  5   17  country  earnings  sub_race\n  5   18  country  earnings  sub_race\n  5   19  country  earnings  sub_race\n  5   20  country  earnings  sub_race\n\n# Print biopics_multiimp\nprint(biopics_multiimp)\n\nClass: mids\nNumber of multiple imputations:  20 \nImputation methods:\n  country      year  earnings   sub_num  sub_type  sub_race non_white   sub_sex \n    \"pmm\"        \"\"    \"cart\"        \"\"        \"\"     \"pmm\"        \"\"        \"\" \nPredictorMatrix:\n         country year earnings sub_num sub_type sub_race non_white sub_sex\ncountry        0    1        1       1        1        1         1       1\nyear           1    0        1       1        1        1         1       1\nearnings       1    1        0       1        1        1         1       1\nsub_num        1    1        1       0        1        1         1       1\nsub_type       1    1        1       1        0        1         1       1\nsub_race       1    1        1       1        1        0         1       1\nNumber of logged events:  300 \n  it im      dep meth\n1  1  1  country  pmm\n2  1  1 earnings cart\n3  1  1 sub_race  pmm\n4  1  2  country  pmm\n5  1  2 earnings cart\n6  1  2 sub_race  pmm\n                                                                                                                                           out\n1 sub_typeActress / activist, sub_typeAthlete / military, sub_typeGovernment, sub_typeMilitary / activist, sub_typePolitician, sub_typeTeacher\n2                                                                                             countryCanada US, sub_typeAcademic (Philosopher)\n3                                                                                                countryCanada US, sub_typeMilitary / activist\n4 sub_typeActress / activist, sub_typeAthlete / military, sub_typeGovernment, sub_typeMilitary / activist, sub_typePolitician, sub_typeTeacher\n5                                                                                             countryCanada US, sub_typeAcademic (Philosopher)\n6                                                                                                countryCanada US, sub_typeMilitary / activist\n\n\n\nThe ability to specify imputation models might come in handy when you see some specific methods underperforming. Another factor infuencing how the imputation methods work is the set of predictors they use. Let’s look at how to set these in the next exercise."
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#using-predictor-matrix",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#using-predictor-matrix",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Using predictor matrix",
    "text": "Using predictor matrix\nAn important decision that needs to be taken when using model-based imputation is which variables should be included as predictors, and in which models. In mice(), this is governed by the predictor matrix and by default, all variables are used to impute all others.\nIn case of many variables in the data or little time to do a proper model selection, you can use mice’s functionality to create a predictor matrix based on the correlations between the variables. This matrix can then be passed to mice(). In this exercise, you will practice exactly this: you will first build a predictor matrix such that each variable will be imputed using variables most correlated to it; then, you will feed your predictor matrix to the imputing function. Let’s try this simple model selection!\n\n# Create predictor matrix with minimum correlation of 0.1\npred_mat <- quickpred(biopics, mincor = 0.1)\n\n# Impute biopics with mice\nbiopics_multiimp <- mice(biopics, \n                         m = 10, \n                         predictorMatrix = pred_mat,\n                         seed = 3108)\n\n\n iter imp variable\n  1   1  country  earnings  sub_race\n  1   2  country  earnings  sub_race\n  1   3  country  earnings  sub_race\n  1   4  country  earnings  sub_race\n  1   5  country  earnings  sub_race\n  1   6  country  earnings  sub_race\n  1   7  country  earnings  sub_race\n  1   8  country  earnings  sub_race\n  1   9  country  earnings  sub_race\n  1   10  country  earnings  sub_race\n  2   1  country  earnings  sub_race\n  2   2  country  earnings  sub_race\n  2   3  country  earnings  sub_race\n  2   4  country  earnings  sub_race\n  2   5  country  earnings  sub_race\n  2   6  country  earnings  sub_race\n  2   7  country  earnings  sub_race\n  2   8  country  earnings  sub_race\n  2   9  country  earnings  sub_race\n  2   10  country  earnings  sub_race\n  3   1  country  earnings  sub_race\n  3   2  country  earnings  sub_race\n  3   3  country  earnings  sub_race\n  3   4  country  earnings  sub_race\n  3   5  country  earnings  sub_race\n  3   6  country  earnings  sub_race\n  3   7  country  earnings  sub_race\n  3   8  country  earnings  sub_race\n  3   9  country  earnings  sub_race\n  3   10  country  earnings  sub_race\n  4   1  country  earnings  sub_race\n  4   2  country  earnings  sub_race\n  4   3  country  earnings  sub_race\n  4   4  country  earnings  sub_race\n  4   5  country  earnings  sub_race\n  4   6  country  earnings  sub_race\n  4   7  country  earnings  sub_race\n  4   8  country  earnings  sub_race\n  4   9  country  earnings  sub_race\n  4   10  country  earnings  sub_race\n  5   1  country  earnings  sub_race\n  5   2  country  earnings  sub_race\n  5   3  country  earnings  sub_race\n  5   4  country  earnings  sub_race\n  5   5  country  earnings  sub_race\n  5   6  country  earnings  sub_race\n  5   7  country  earnings  sub_race\n  5   8  country  earnings  sub_race\n  5   9  country  earnings  sub_race\n  5   10  country  earnings  sub_race\n\n# Print biopics_multiimp\nprint(biopics_multiimp)\n\nClass: mids\nNumber of multiple imputations:  10 \nImputation methods:\n  country      year  earnings   sub_num  sub_type  sub_race non_white   sub_sex \n\"polyreg\"        \"\"     \"pmm\"        \"\"        \"\" \"polyreg\"        \"\"        \"\" \nPredictorMatrix:\n         country year earnings sub_num sub_type sub_race non_white sub_sex\ncountry        0    1        1       0        0        0         0       0\nyear           0    0        0       0        0        0         0       0\nearnings       1    1        0       0        0        1         1       0\nsub_num        0    0        0       0        0        0         0       0\nsub_type       0    0        0       0        0        0         0       0\nsub_race       0    1        1       1        1        0         1       0\nNumber of logged events:  100 \n  it im      dep    meth                         out\n1  1  1 earnings     pmm            countryCanada US\n2  1  1 sub_race polyreg sub_typeMilitary / activist\n3  1  2 earnings     pmm            countryCanada US\n4  1  2 sub_race polyreg sub_typeMilitary / activist\n5  1  3 earnings     pmm            countryCanada US\n6  1  3 sub_race polyreg sub_typeMilitary / activist\n\n\n\nLook at the predictor matrix you’ve used that is printed in the console. Which variables have been used as predictors to impute earnings?\n\n\ncountry, year and non_white\nProvides an easy way to set up predictor matrices, but if you can afford it, you should try to choose the predictors based on the insights from data analytics or on domain knowledge."
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#analyzing-missing-data-patterns",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#analyzing-missing-data-patterns",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Analyzing missing data patterns",
    "text": "Analyzing missing data patterns\nThe first step in working with incomplete data is to gain some insights into the missingness patterns, and a good way to do it is with visualizations. You will start your analysis of the africa data with employing the VIM package to create two visualizations: the aggregation plot and the spine plot. They will tell you how many data are missing, in which variables and configurations, and whether we can say something about the missing data mechanism. Let’s kick off with some plotting!\n\nafrica <- read.csv( \"data/africa_clean.csv\")\n# Load VIM\nlibrary(VIM)\n\n# Draw a combined aggregation plot of africa\nafrica %>%\n  aggr(combined = TRUE, numbers = TRUE)\n\n\n\n\n\nQuestion\n\nBased on the aggregation plot you have just created, which of the following statements is TRUE?\nans Whenever gdp_pc is missing, trade is missing too\n\n\n# Draw a spine plot of country vs trade\nafrica %>% \n  select(country ,trade) %>%\n  spineMiss()\n\n\n\n\n\n\nQuestion\n\nBased on the spine plot you have just created, which of the following statements is TRUE?\nCorrect, there are not that many missing values! Also, notice from the spine plot that the africa data seem to be MAR - at least with respect to the GDP and country, which means it can be imputed. Let’s try multiple imputation to fill-in the missing values in the next exercise!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#imputing-and-inspecting-outcomes",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#imputing-and-inspecting-outcomes",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Imputing and inspecting outcomes",
    "text": "Imputing and inspecting outcomes\nGood job on visualizing missing data in the previous exercise! You have discovered there are some missing entries in GDP, gdp_pc, and trade as percentage of GDP, trade. Also, you suspect the data are MAR, and thus imputable. In this exercise, you will make use of multiple imputation from the mice package to impute the africa data. Then, you will draw a strip plot of gdp_pc vs trade to see if the imputed data do not break the relation between these variables. Let mice do the job!\n\n# Load mice\nlibrary(mice)\n\n# Impute africa with mice\nafrica_multiimp <- mice(africa, m = 5, defaultMethod = \"cart\", seed = 3108)\n\n\n iter imp variable\n  1   1  gdp_pc  trade\n  1   2  gdp_pc  trade\n  1   3  gdp_pc  trade\n  1   4  gdp_pc  trade\n  1   5  gdp_pc  trade\n  2   1  gdp_pc  trade\n  2   2  gdp_pc  trade\n  2   3  gdp_pc  trade\n  2   4  gdp_pc  trade\n  2   5  gdp_pc  trade\n  3   1  gdp_pc  trade\n  3   2  gdp_pc  trade\n  3   3  gdp_pc  trade\n  3   4  gdp_pc  trade\n  3   5  gdp_pc  trade\n  4   1  gdp_pc  trade\n  4   2  gdp_pc  trade\n  4   3  gdp_pc  trade\n  4   4  gdp_pc  trade\n  4   5  gdp_pc  trade\n  5   1  gdp_pc  trade\n  5   2  gdp_pc  trade\n  5   3  gdp_pc  trade\n  5   4  gdp_pc  trade\n  5   5  gdp_pc  trade\n\n# Draw a stripplot of gdp_pc versus trade\nstripplot(africa_multiimp, gdp_pc ~ trade | .imp, pch = 20, cex = 2)\n\n\n\n\n\nt seems the imputation works well: there are small clusters in the scatter plots, likely corresponding to different countries. Each imputed data point fits into one of the clusters, instead of being an outlier somewhere between the clusters. Having done the imputation, you can now proceed to modeling!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#inference-with-imputed-data",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#inference-with-imputed-data",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Inference with imputed data",
    "text": "Inference with imputed data\nIn the last exercise, you have run mice to multiply impute the africa data. In this one, you will implement the other two steps of the mice - with - pool flow you’ve learned about earlier in the course. The model of interest is a linear regression that explains the GDP, gdp_pc, with other variables. You are particularly interested in the coefficient of civil liberties, civlib. Is more liberty associated with more economic growth once we incorporate the uncertainty from imputation? Let’s find out!\n\n# Fit linear regression to each imputed data set\nlm_multiimp <- with(africa_multiimp, lm(gdp_pc ~ country + year + trade + infl + civlib))\n\n# Pool regression results\nlm_pooled <- pool(lm_multiimp)\n\n# Summarize pooled results\nsummary(lm_pooled, conf.int = TRUE, conf.level = 0.90)\n\n              term      estimate    std.error  statistic       df      p.value\n1      (Intercept) -30068.441810 5618.7067367 -5.3514880 107.7805 4.954407e-07\n2   countryBurundi     67.199848   61.9274336  1.0851386 107.8599 2.802797e-01\n3  countryCameroon    650.257144   58.5640793 11.1033444 107.1503 1.591166e-19\n4     countryCongo   1343.288063  113.7299489 11.8112078 106.8653 4.209726e-21\n5   countrySenegal    527.686999   78.7955032  6.6969177 106.7578 1.027035e-09\n6    countryZambia    415.136577   83.9727225  4.9437075 107.0560 2.853378e-06\n7             year     15.335776    2.8263293  5.4260402 107.7640 3.574950e-07\n8            trade      4.941659    1.5893889  3.1091568 105.7726 2.410907e-03\n9             infl     -4.347124    0.9876079 -4.4016696 108.0009 2.533732e-05\n10          civlib    -49.411852  132.1837617 -0.3738118 107.4442 7.092809e-01\n             5 %          95 %\n1  -39390.518971 -20746.364650\n2     -35.544192    169.943888\n3     553.087683    747.426605\n4    1154.583058   1531.993068\n5     396.945387    658.428610\n6     275.808050    554.465103\n7      10.646566     20.024986\n8       2.304247      7.579071\n9      -5.985649     -2.708598\n10   -268.725783    169.902078\n\n\n\nQuestion\n\nBased on the summary of the pooled regression results that you have just printed to the console, which of the following statements about the civil liberties in Africa is false?\nCorrect, this one is false! Since the lower and upper bounds have different signs, we cannot be sure of the direction of the effect. Congratulations, you have come a long way and learned a lot. Well done! Let’s sum it all up in the final video of the course."
  }
]