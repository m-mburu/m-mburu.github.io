[
  {
    "objectID": "datacamp.html",
    "href": "datacamp.html",
    "title": "Mburu Blog",
    "section": "",
    "text": "Sampling in R\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nModeling with tidymodels in R\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nCoding the forward propagation algorithm\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nUntitled\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Data Science in Python\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nHandling Missing Data with Imputations in R\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nFunctions in R\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nCensus data in r with tidycensus\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nThe reduction in weekly working hours in Europe\n\n\nLooking at the development between 1996 and 2006\n\n\n\n\n\n\n\n\n\nInsert your name here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWriting Efficient R Code\n\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunicating with Data in the Tidyverse\n\n\n\n\n\n\n\n\n\n\n\n\nAug 11, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Linear and Logistic Regression in R\n\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple and Logistic Regression in R\n\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnomaly Detection in R\n\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScalable Data Processing in R\n\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork analysis in r\n\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML with tree based models in r\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to TensorFlow in R\n\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nMburu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv/cv_mburu.html",
    "href": "cv/cv_mburu.html",
    "title": "Mburu Blog",
    "section": "",
    "text": "Data Analyst, with substantial experience in data management, statistical analysis and machine learning. I have worked in healthcare with a bias towards malnutrition, financial diaries research in low income areas and agricultural research. I have gained in-depth knowledge of using statistical/programming languages such as R, Python and SQL for analysis. I enjoy using data to solve problems and I’m always researching on new methods/tools/skills to help me become efficient in my work."
  },
  {
    "objectID": "cv/cv_mburu.html#data-manager-kemri-wellcome-trust-may-2019-present",
    "href": "cv/cv_mburu.html#data-manager-kemri-wellcome-trust-may-2019-present",
    "title": "Mburu Blog",
    "section": "Data Manager, KEMRI WELLCOME TRUST May 2019 – present:",
    "text": "Data Manager, KEMRI WELLCOME TRUST May 2019 – present:\n\nRole covers a mixture of data management, data analysis and machine learning. In my current role I work under the CHAIN Network where we are focused on optimizing the management and care of the sick and undernourished children in resource-limited settings to improve survival, growth and development. CHAIN study has 9 sites in Africa and Asia.\n\n\nData Management:\n\nI was in a team that developed dashboards for data management using R programming and shiny server. This was achieved by creating automated scripts that were connected to MySQL database and Redcap to check spurious values and for reporting. This ensured that we had an automated way to flag outliers, validation and automation of report generation. This led to a very low data entry error rate of less than 2%. It also helped in that we had a continuous process for data cleaning which made it easier to catch most of the errors early.\n\n\n\nManagement/Mentorship:\n\nTaught data clerks and lab technicians on using dashboards to double check entry outlier values or generate reports. I also encouraged them to use project management software to raise tasks to the data team which led us to know early when problems occurred.\n\n\n\nData Visualization:\n\nCreate maps to display malnutrition patterns/prevalence in study countries to see the most vulnerable communities, using R packages such as sf, raster, ggplot2, leaflet and tmap.\nUsed plotly and ggplot2, shiny to visualize various study variables patterns and trends. This helped us to dedicate more time to discuss results, research more and try different models for fitting data.\n\n\n\nData Mining:\n\nExtraction of data sets from various health demographic data bases such as demographic and health surveys database, world population database, National Oceanic and Atmospheric Administration to derive various processed variables. This was useful in finding patterns between weather and hospital visits, how diseases are distributed spatially, how spatial distribution of diseases has changed over years.\n\n\n\nMachine Learning:\n\nWorking closely with clinicians to create models that predict high risk children using clinical features, water sanitation and hygiene variables. Clustering methods such as k means clustering was used to group patients that are similar given clinical measurements of features such as complete blood count, blood biochemistry and symptoms. This is important as it helped identify features of high-risk groups.\n\n\n\nSpatial Statistics:\n\nWorking closely with the population health unit to model travel times and travel distances to hospitals for study participants."
  },
  {
    "objectID": "cv/cv_mburu.html#data-analyststatistician-low-income-financial-transformation-june-2016-may-2019",
    "href": "cv/cv_mburu.html#data-analyststatistician-low-income-financial-transformation-june-2016-may-2019",
    "title": "Mburu Blog",
    "section": "Data Analyst/Statistician, Low Income Financial Transformation June 2016 – May 2019:",
    "text": "Data Analyst/Statistician, Low Income Financial Transformation June 2016 – May 2019:\n\nLow-Income Financial Transformation (L-IFT) is a for-profit social business. The company specializes in a diaries research methodology that can be applied to a range of purposes, such as impact measurement, product development, customer satisfaction gauging, and programme design. L-IFT is primarily focused on financial inclusion, digital finance and strengthening the financial sector of the countries where it works. L-IFT also has expertise and is building data in the fields of energy, livelihoods, youth, entrepreneurship, SME development and the intersection of health and financial management.\nAt L-IFT I received a lot of experience in finances of low-income areas. How fluctuating the finances are, savings patterns and how calamities affect they lives.\n\n\nData Management:\n\nThis is important step in research as it ensured that the data used in analysis is of the highest quality. This is was achieved by creating data collection databases and reviewing each question with the data collection teams. Monitoring of surveys, flagging inconsistencies in the data entry and raising any queries with the researchers in the field.\n\n\n\nData Mining/Statistical Computation:\n\nOur surveys were done after every two weeks with a follow up period of more than 6 months. This meant that the data sets produced were large, complex and messy. I Introduced R programming in our company to solve this. This helped with the analysis as we could do more complex analysis for instance digging into passed data and making comparisons to see the trends. R being an open source meant that we had access to actively developed packages for all kinds of analysis. It also meant we had power to do tasks such as visualizing all data variables using frameworks such as shiny.\n\n\n\nData Analysis/Machine Learning:\n\nThis was my day to day job. I worked with the IT department to have shiny server installed in our servers which meant that we could produce come up with dashboards. We were able to stream twitter data in real time. This enabled us to advise on the trending topics that our company could take advantage of in terms of advertising.\nAnalysing twitter data to see the impact of influencers by using metrics such as follower’s quotient, interactions to user tweets for instance the summary statistics of number of retweets or favourites a user has received over a duration of time.\nUsed text analysis methods such as sentiment analysis, topic modelling on open ended questions from surveys.\nI researched extensively on the best ways to visualize and analyse different kind of data sets. This helped us use methods such clustering to identify groups of clients for microfinance institutions in low income settings."
  },
  {
    "objectID": "cv/cv_mburu.html#qualifications",
    "href": "cv/cv_mburu.html#qualifications",
    "title": "Mburu Blog",
    "section": "QUALIFICATIONS",
    "text": "QUALIFICATIONS\n\nBachelor of Science in Statistics, University of Nairobi, 2012 – 2016:\n\nArea of study included Exploratory data analysis, Statistical modelling, computation and data analysis, analysis and design of experiments, Time series analysis\n\n\n\nSpatial Data: Data Camp\n\nArea of study included visualizing spatial data using ggplot2, tmap and leaflet, spatial analysis with sf and raster, interactive maps with leaflet and spatial statistics, such as point pattern analysis, areal statistics and geo-statistics\n\n\n\nMachine Learning: Coursera\n\nArea of study included supervised learning methods such as regression methods such linear regression, tree-based methods, classification, logistic regression support vector machines tree-based methods neural networks, dimension reduction with pca, clustering methods such k means clustering.\n\n\n\nPublications\n\nPeter Gachoki, Moses Mburu, and Moses Muraya, “Predictive Modelling of Benign and Malignant Tumors Using Binary Logistic, Support Vector Machine and Extreme Gradient Boosting Models.” American Journal of Applied Mathematics and Statistics, vol. 7, no. 6 (2019): 196-204. doi: 10.12691/ajams-7-6-2"
  },
  {
    "objectID": "datacamp/regression_r/regression.html",
    "href": "datacamp/regression_r/regression.html",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "",
    "text": "We use the lm() function to fit linear models to data. In this case, we want to understand how the price of MarioKart games sold at auction varies as a function of not only the number of wheels included in the package, but also whether the item is new or used. Obviously, it is expected that you might have to pay a premium to buy these new. But how much is that premium? Can we estimate its value after controlling for the number of wheels?\nWe will fit a parallel slopes model using lm(). In addition to the data argument, lm() needs to know which variables you want to include in your regression model, and how you want to include them. It accomplishes this using a formula argument. A simple linear regression formula looks like y ~ x, where y is the name of the response variable, and x is the name of the explanatory variable. Here, we will simply extend this formula to include multiple explanatory variables. A parallel slopes model has the form y ~ x + z, where z is a categorical explanatory variable, and x is a numerical explanatory variable.\nThe output from lm() is a model object, which when printed, will show the fitted coefficients.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(openintro)\nlibrary(broom)\nlibrary(pander)\ndata( mariokart, package = \"openintro\")\nmario_kart <- mariokart\n\nmario_kart <- mario_kart %>% mutate(total_pr := ifelse(total_pr > 100, NA, total_pr))\n# Explore the data\nglimpse(mario_kart)\n\nRows: 143\nColumns: 12\n$ id          <dbl> 150377422259, 260483376854, 320432342985, 280405224677, 17…\n$ duration    <int> 3, 7, 3, 3, 1, 3, 1, 1, 3, 7, 1, 1, 1, 1, 7, 7, 3, 3, 1, 7…\n$ n_bids      <int> 20, 13, 16, 18, 20, 19, 13, 15, 29, 8, 15, 15, 13, 16, 6, …\n$ cond        <fct> new, used, new, new, new, new, used, new, used, used, new,…\n$ start_pr    <dbl> 0.99, 0.99, 0.99, 0.99, 0.01, 0.99, 0.01, 1.00, 0.99, 19.9…\n$ ship_pr     <dbl> 4.00, 3.99, 3.50, 0.00, 0.00, 4.00, 0.00, 2.99, 4.00, 4.00…\n$ total_pr    <dbl> 51.55, 37.04, 45.50, 44.00, 71.00, 45.00, 37.02, 53.99, 47…\n$ ship_sp     <fct> standard, firstClass, firstClass, standard, media, standar…\n$ seller_rate <int> 1580, 365, 998, 7, 820, 270144, 7284, 4858, 27, 201, 4858,…\n$ stock_photo <fct> yes, yes, no, yes, yes, yes, yes, yes, yes, no, yes, yes, …\n$ wheels      <int> 1, 1, 1, 1, 2, 0, 0, 2, 1, 1, 2, 2, 2, 2, 1, 0, 1, 1, 2, 2…\n$ title       <fct> \"~~ Wii MARIO KART &amp; WHEEL ~ NINTENDO Wii ~ BRAND NEW …\n\n# fit parallel slopes\n\nmod_mario <- lm(total_pr ~ wheels + cond, data = mario_kart)"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#reasoning-about-two-intercepts",
    "href": "datacamp/regression_r/regression.html#reasoning-about-two-intercepts",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Reasoning about two intercepts",
    "text": "Reasoning about two intercepts\nThe mario_kart data contains several other variables. The totalPr, startPr, and shipPr variables are numeric, while the cond and stockPhoto variables are categorical.\nWhich formula will result in a parallel slopes model?\n\ntotalPr ~ shipPr + stockPhoto"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#using-geom_line-and-augment",
    "href": "datacamp/regression_r/regression.html#using-geom_line-and-augment",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Using geom_line() and augment()",
    "text": "Using geom_line() and augment()\nParallel slopes models are so-named because we can visualize these models in the data space as not one line, but two parallel lines. To do this, we’ll draw two things:\na scatterplot showing the data, with color separating the points into groups a line for each value of the categorical variable Our plotting strategy is to compute the fitted values, plot these, and connect the points to form a line. The augment() function from the broom package provides an easy way to add the fitted values to our data frame, and the geom_line() function can then use that data frame to plot the points and connect them.\nNote that this approach has the added benefit of automatically coloring the lines appropriately to match the data.\nYou already know how to use ggplot() and geom_point() to make the scatterplot. The only twist is that now you’ll pass your augment()-ed model as the data argument in your ggplot() call. When you add your geom_line(), instead of letting the y aesthetic inherit its values from the ggplot() call, you can set it to the .fitted column of the augment()-ed model. This has the advantage of automatically coloring the lines for you.\n\n# Augment the model\naugmented_mod <- augment(mod_mario)\nglimpse(augmented_mod)\n\nRows: 141\nColumns: 10\n$ .rownames  <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"1…\n$ total_pr   <dbl> 51.55, 37.04, 45.50, 44.00, 71.00, 45.00, 37.02, 53.99, 47.…\n$ wheels     <int> 1, 1, 1, 1, 2, 0, 0, 2, 1, 1, 2, 2, 2, 2, 1, 0, 1, 1, 2, 0,…\n$ cond       <fct> new, used, new, new, new, new, used, new, used, used, new, …\n$ .fitted    <dbl> 49.60260, 44.01777, 49.60260, 49.60260, 56.83544, 42.36976,…\n$ .resid     <dbl> 1.9473995, -6.9777674, -4.1026005, -5.6026005, 14.1645592, …\n$ .hat       <dbl> 0.02103158, 0.01250410, 0.02103158, 0.02103158, 0.01915635,…\n$ .sigma     <dbl> 4.902339, 4.868399, 4.892414, 4.881308, 4.750591, 4.899816,…\n$ .cooksd    <dbl> 1.161354e-03, 8.712334e-03, 5.154337e-03, 9.612441e-03, 5.5…\n$ .std.resid <dbl> 0.40270893, -1.43671086, -0.84838977, -1.15857953, 2.926332…\n\n# scatterplot, with color\ndata_space <- ggplot(augmented_mod, aes(x = wheels, y = total_pr , color = cond )) + \n  geom_point()\n  \n# single call to geom_line()\ndata_space + \n  geom_line(aes(y = .fitted))"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#intercept-interpretation",
    "href": "datacamp/regression_r/regression.html#intercept-interpretation",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Intercept interpretation",
    "text": "Intercept interpretation\nRecall that the cond variable is either new or used. Here are the fitted coefficients from your model:\nCall: lm(formula = totalPr ~ wheels + cond, data = mario_kart)\nCoefficients: (Intercept) wheels condused\n42.370 7.233 -5.585\nChoose the correct interpretation of the coefficient on condused:\n\nThe expected price of a used MarioKart is $5.58 less than that of a new one with the same number of wheels.\nFor each additional wheel, the expected price of a MarioKart increases by $7.23 regardless of whether it is new or used.\n\nSyntax from math The babies data set contains observations about the birthweight and other characteristics of children born in the San Francisco Bay area from 1960–1967.\nWe would like to build a model for birthweight as a function of the mother’s age and whether this child was her first (parity == 0). Use the mathematical specification below to code the model in R.\n\\[birthweight = \\beta_0 + \\beta_1 * age  + \\beta_2 * parity + \\epsilon\\]\n\ndata( babies, package = \"openintro\")\n\nmod <- lm(bwt~ age+parity, data = babies)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n118.3\n2.788\n42.43\n3.957e-243\n\n\nage\n0.06315\n0.09577\n0.6594\n0.5097\n\n\nparity\n-1.652\n1.271\n-1.3\n0.1937"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#syntax-from-plot",
    "href": "datacamp/regression_r/regression.html#syntax-from-plot",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Syntax from plot",
    "text": "Syntax from plot\nThis time, we’d like to build a model for birthweight as a function of the length of gestation and the mother’s smoking status. Use the plot to inform your model specification.\n\nggplot(babies, aes(gestation, bwt, color = factor(smoke)))+\n    geom_point()\n\n\n\nmod <- lm(bwt~ gestation + smoke, data = babies)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.9317\n8.152\n-0.1143\n0.909\n\n\ngestation\n0.4429\n0.02902\n15.26\n3.156e-48\n\n\nsmoke\n-8.088\n0.9527\n-8.49\n5.963e-17\n\n\n\n\n\nR-squared vs. adjusted R-squared Two common measures of how well a model fits to data are \\[R^2\\] (the coefficient of determination) and the adjusted \\[R^2\\] . The former measures the percentage of the variability in the response variable that is explained by the model. To compute this, we define\n\\[R^2 = 1 - \\frac{sse}{sst} \\] where SSE and SST are the sum of the squared residuals, and the total sum of the squares, respectively. One issue with this measure is that the can only decrease as new variable are added to the model, while the SST depends only on the response variable and therefore is not affected by changes to the model. This means that you can increase \\[R^2\\] by adding any additional variable to your model—even random noise.\nThe adjusted \\[R^2\\] includes a term that penalizes a model for each additional explanatory variable (where is the number of explanatory variables). We can see both measures in the output of the summary() function on our model object.\n\n# R^2 and adjusted R^2\nsummary(mod_mario)\n\n\nCall:\nlm(formula = total_pr ~ wheels + cond, data = mario_kart)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.0078  -3.0754  -0.8254   2.9822  14.1646 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  42.3698     1.0651  39.780  < 2e-16 ***\nwheels        7.2328     0.5419  13.347  < 2e-16 ***\ncondused     -5.5848     0.9245  -6.041 1.35e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.887 on 138 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7165,    Adjusted R-squared:  0.7124 \nF-statistic: 174.4 on 2 and 138 DF,  p-value: < 2.2e-16\n\n# add random noise\nmario_kart_noisy <- mario_kart %>% \nmutate(noise = rnorm(n = nrow(mario_kart)))\n  \n# compute new model\nmod2_mario2 <- lm(total_pr ~ wheels + cond+noise, data = mario_kart_noisy)\n\n# new R^2 and adjusted R^2\nsummary(mod2_mario2)\n\n\nCall:\nlm(formula = total_pr ~ wheels + cond + noise, data = mario_kart_noisy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.6919  -3.2414  -0.7768   2.7511  12.8040 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  42.3422     1.0647  39.769  < 2e-16 ***\nwheels        7.1972     0.5425  13.266  < 2e-16 ***\ncondused     -5.3883     0.9414  -5.724  6.3e-08 ***\nnoise         0.5403     0.4968   1.088    0.279    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.884 on 137 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7189,    Adjusted R-squared:  0.7128 \nF-statistic: 116.8 on 3 and 137 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#prediction",
    "href": "datacamp/regression_r/regression.html#prediction",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Prediction",
    "text": "Prediction\nOnce we have fit a regression model, we can use it to make predictions for unseen observations or retrieve the fitted values. Here, we explore two methods for doing the latter.\nA traditional way to return the fitted values (i.e. the y ’s) is to run the predict() function on the model object. This will return a vector of the fitted values. Note that predict() will take an optional newdata argument that will allow you to make predictions for observations that are not in the original data.\nA newer alternative is the augment() function from the broom package, which returns a data.frame with the response varible (), the relevant explanatory variables (the ’s), the fitted value ( ) and some information about the residuals (). augment() will also take a newdata argument that allows you to make predictions.\n\n# return a vector\nlibrary(knitr)\n\npredict(mod_mario)\n\n       1        2        3        4        5        6        7        8 \n49.60260 44.01777 49.60260 49.60260 56.83544 42.36976 36.78493 56.83544 \n       9       10       11       12       13       14       15       16 \n44.01777 44.01777 56.83544 56.83544 56.83544 56.83544 44.01777 36.78493 \n      17       18       19       21       22       23       24       25 \n49.60260 49.60260 56.83544 36.78493 56.83544 56.83544 56.83544 44.01777 \n      26       27       28       29       30       31       32       33 \n56.83544 36.78493 36.78493 36.78493 49.60260 36.78493 36.78493 44.01777 \n      34       35       36       37       38       39       40       41 \n51.25061 44.01777 44.01777 36.78493 44.01777 56.83544 56.83544 49.60260 \n      42       43       44       45       46       47       48       49 \n44.01777 51.25061 56.83544 56.83544 44.01777 56.83544 36.78493 36.78493 \n      50       51       52       53       54       55       56       57 \n44.01777 56.83544 36.78493 44.01777 42.36976 36.78493 36.78493 44.01777 \n      58       59       60       61       62       63       64       66 \n44.01777 36.78493 36.78493 56.83544 36.78493 56.83544 36.78493 51.25061 \n      67       68       69       70       71       72       73       74 \n56.83544 44.01777 58.48345 51.25061 49.60260 44.01777 49.60260 56.83544 \n      75       76       77       78       79       80       81       82 \n56.83544 51.25061 44.01777 36.78493 36.78493 36.78493 44.01777 56.83544 \n      83       84       85       86       87       88       89       90 \n44.01777 65.71629 44.01777 56.83544 36.78493 49.60260 49.60260 36.78493 \n      91       92       93       94       95       96       97       98 \n44.01777 36.78493 51.25061 44.01777 36.78493 51.25061 42.36976 56.83544 \n      99      100      101      102      103      104      105      106 \n51.25061 44.01777 51.25061 56.83544 56.83544 56.83544 36.78493 49.60260 \n     107      108      109      110      111      112      113      114 \n51.25061 44.01777 56.83544 49.60260 36.78493 44.01777 51.25061 56.83544 \n     115      116      117      118      119      120      121      122 \n64.06828 44.01777 49.60260 44.01777 49.60260 51.25061 42.36976 44.01777 \n     123      124      125      126      127      128      129      130 \n56.83544 44.01777 49.60260 44.01777 51.25061 56.83544 56.83544 49.60260 \n     131      132      133      134      135      136      137      138 \n56.83544 36.78493 44.01777 44.01777 36.78493 56.83544 36.78493 44.01777 \n     139      140      141      142      143 \n36.78493 51.25061 49.60260 36.78493 56.83544 \n\n# return a data frame\n\naugment(mod_mario)%>% head() %>% kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.rownames\ntotal_pr\nwheels\ncond\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n1\n51.55\n1\nnew\n49.60260\n1.947399\n0.0210316\n4.902340\n0.0011614\n0.4027089\n\n\n2\n37.04\n1\nused\n44.01777\n-6.977767\n0.0125041\n4.868399\n0.0087123\n-1.4367109\n\n\n3\n45.50\n1\nnew\n49.60260\n-4.102601\n0.0210316\n4.892414\n0.0051543\n-0.8483898\n\n\n4\n44.00\n1\nnew\n49.60260\n-5.602601\n0.0210316\n4.881308\n0.0096124\n-1.1585795\n\n\n5\n71.00\n2\nnew\n56.83544\n14.164559\n0.0191563\n4.750591\n0.0557493\n2.9263328\n\n\n6\n45.00\n0\nnew\n42.36976\n2.630240\n0.0474932\n4.899816\n0.0050537\n0.5514192"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#thought-experiments",
    "href": "datacamp/regression_r/regression.html#thought-experiments",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Thought experiments",
    "text": "Thought experiments\nSuppose that after going apple picking you have 12 apples left over. You decide to conduct an experiment to investigate how quickly they will rot under certain conditions. You place six apples in a cool spot in your basement, and leave the other six on the window sill in the kitchen. Every week, you estimate the percentage of the surface area of the apple that is rotten or moldy.\nConsider the following models:\n\\[rot = \\beta_0 + \\beta_1 *t + \\beta_2 * temp + \\epsilon \\]\nand\n\\[rot = \\beta_0 + \\beta_1 *t + \\beta_2 * temp + \\beta_2 * temp *t +  \\epsilon \\]\n\nThe rate at which apples rot will vary based on the temperature."
  },
  {
    "objectID": "datacamp/regression_r/regression.html#fitting-a-model-with-interaction",
    "href": "datacamp/regression_r/regression.html#fitting-a-model-with-interaction",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Fitting a model with interaction",
    "text": "Fitting a model with interaction\nIncluding an interaction term in a model is easy—we just have to tell lm() that we want to include that new variable. An expression of the form\nlm(y ~ x + z + x:z, data = mydata)\nwill do the trick. The use of the colon (:) here means that the interaction between and will be a third term in the model.\n\n# include interaction\n\nmod <- lm(total_pr ~cond + duration + cond:duration, data = mario_kart)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n58.27\n1.366\n42.64\n5.832e-81\n\n\ncondused\n-17.12\n2.178\n-7.86\n1.014e-12\n\n\nduration\n-1.966\n0.4488\n-4.38\n2.342e-05\n\n\ncondused:duration\n2.325\n0.5484\n4.239\n4.102e-05"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#visualizing-interaction-models",
    "href": "datacamp/regression_r/regression.html#visualizing-interaction-models",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Visualizing interaction models",
    "text": "Visualizing interaction models\nInteraction allows the slope of the regression line in each group to vary. In this case, this means that the relationship between the final price and the length of the auction is moderated by the condition of each item.\nInteraction models are easy to visualize in the data space with ggplot2 because they have the same coefficients as if the models were fit independently to each group defined by the level of the categorical variable. In this case, new and used MarioKarts each get their own regression line. To see this, we can set an aesthetic (e.g. color) to the categorical variable, and then add a geom_smooth() layer to overlay the regression line for each color.\n\n# interaction plot\nggplot(mario_kart, aes(duration, total_pr, color = cond)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = 0)"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#consequences-of-simpsons-paradox",
    "href": "datacamp/regression_r/regression.html#consequences-of-simpsons-paradox",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Consequences of Simpson’s paradox",
    "text": "Consequences of Simpson’s paradox\nIn the simple linear regression model for average SAT score, (total) as a function of average teacher salary (salary), the fitted coefficient was -5.02 points per thousand dollars. This suggests that for every additional thousand dollars of salary for teachers in a particular state, the expected SAT score for a student from that state is about 5 points lower.\nIn the model that includes the percentage of students taking the SAT, the coefficient on salary becomes 1.84 points per thousand dollars. Choose the correct interpretation of this slope coefficient.\n\nFor every additional thousand dollars of salary for teachers in a particular state, the expected SAT score for a student from that state is about 2 points higher, after controlling for the percentage of students taking the SAT."
  },
  {
    "objectID": "datacamp/regression_r/regression.html#simpsons-paradox-in-action",
    "href": "datacamp/regression_r/regression.html#simpsons-paradox-in-action",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Simpson’s paradox in action",
    "text": "Simpson’s paradox in action\nA mild version of Simpson’s paradox can be observed in the MarioKart auction data. Consider the relationship between the final auction price and the length of the auction. It seems reasonable to assume that longer auctions would result in higher prices, since—other things being equal—a longer auction gives more bidders more time to see the auction and bid on the item.\nHowever, a simple linear regression model reveals the opposite: longer auctions are associated with lower final prices. The problem is that all other things are not equal. In this case, the new MarioKarts—which people pay a premium for—were mostly sold in one-day auctions, while a plurality of the used MarioKarts were sold in the standard seven-day auctions.\nOur simple linear regression model is misleading, in that it suggests a negative relationship between final auction price and duration. However, for the used MarioKarts, the relationship is positive.\n\nslr <- ggplot(mario_kart, aes(y = total_pr, x = duration)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n# model with one slope\nmod <- lm(total_pr ~ duration, data = mario_kart)\n\n# plot with two slopes\nslr + aes(color = cond)"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#fitting-a-mlr-model",
    "href": "datacamp/regression_r/regression.html#fitting-a-mlr-model",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Fitting a MLR model",
    "text": "Fitting a MLR model\nIn terms of the R code, fitting a multiple linear regression model is easy: simply add variables to the model formula you specify in the lm() command.\nIn a parallel slopes model, we had two explanatory variables: one was numeric and one was categorical. Here, we will allow both explanatory variables to be numeric.\n\n# Fit the model using duration and startPr\n\nmod <- lm(total_pr~ start_pr + duration, data = mario_kart)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n51.03\n1.179\n43.28\n3.666e-82\n\n\nstart_pr\n0.233\n0.04364\n5.339\n3.756e-07\n\n\nduration\n-1.508\n0.2555\n-5.902\n2.645e-08"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#tiling-the-plane",
    "href": "datacamp/regression_r/regression.html#tiling-the-plane",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Tiling the plane",
    "text": "Tiling the plane\nOne method for visualizing a multiple linear regression model is to create a heatmap of the fitted values in the plane defined by the two explanatory variables. This heatmap will illustrate how the model output changes over different combinations of the explanatory variables.\nThis is a multistep process:\nFirst, create a grid of the possible pairs of values of the explanatory variables. The grid should be over the actual range of the data present in each variable. We’ve done this for you and stored the result as a data frame called grid. Use augment() with the newdata argument to find the ’s corresponding to the values in grid. Add these to the data_space plot by using the fill aesthetic and geom_tile().\n\n# add predictions to grid\nprice_hats <- augment(mod, newdata = grid)\n\n# tile the plane\ndata_space + \n  geom_tile(data = price_hats, aes(fill = .fitted), alpha = 0.5)"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#models-in-3d",
    "href": "datacamp/regression_r/regression.html#models-in-3d",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Models in 3D",
    "text": "Models in 3D\nAn alternative way to visualize a multiple regression model with two numeric explanatory variables is as a plane in three dimensions. This is possible in R using the plotly package.\nWe have created three objects that you will need:\nx: a vector of unique values of duration y: a vector of unique values of startPr plane: a matrix of the fitted values across all combinations of x and y Much like ggplot(), the plot_ly() function will allow you to create a plot object with variables mapped to x, y, and z aesthetics. The add_markers() function is similar to geom_point() in that it allows you to add points to your 3D plot.\nNote that plot_ly uses the pipe (%>%) operator to chain commands together.\n\n# draw the 3D scatterplot\np <- plot_ly(data = mario_kart, z = ~totalPr, x = ~duration, y = ~startPr, opacity = 0.6) %>%\n  add_markers() \n  \n# draw the plane\np %>%\n  add_surface(x = ~x, y = ~y, z = ~plane, showscale = FALSE)"
  },
  {
    "objectID": "datacamp/anomaly_detection_R/anomaly_detection.html",
    "href": "datacamp/anomaly_detection_R/anomaly_detection.html",
    "title": "Anomaly Detection in R",
    "section": "",
    "text": "In this exercise, you’ll explore the river dataset which will be used throughout this chapter to illustrate the use of common anomaly detection techniques. The river data is a data.frame that contains the following three columns:\nindex - integers describing the order of the nitrate observations nitrate - monthly concentrations of dissolved nitrate found in a river month - a factor containing the month for each nitrate observation You will explore the nitrate column using summary statistics and boxplots to assess whether there may be point anomalies present.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(DT)\nlibrary(outliers)\nlibrary(AnomalyDetection)\nlibrary(FNN)\nsource(\"river_data.R\")\nhead(river) %>% data_table()\n\n\n\n\n\n# Summary statistics of river nitrate concentrations\nsummary(river$nitrate)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.5920  0.9485  1.0680  1.0649  1.1700  1.8970 \n\n# Plot the distribution of nitrate concentration\n\nboxplot(river$nitrate)\n\n\n\n\n\n\n\nBefore using Grubbs’ test, you should first check that the observations are plausibly normal. The hist() function in R returns a histogram of the observations, which will help you to form a judgement about the normal assumption. hist() is used as follows\nhist(data, xlab = “My x-axis label”, breaks = 30)\ndata is a vector of numeric values breaks the number of break points used to assign the data to bins xlab an optional character string for the x-axis label\n\n# Separate the histogram into 40 bins \nhist(river$nitrate, xlab = \"Nitrate concentration\", breaks = 40)\n\n\n\n\n\n\n\nWe’ve now checked that the data are normal. Now let’s apply Grubbs’ outlier test!\nGrubbs’ test assesses whether the value that is farthest from the mean is an outlier - the value could be either the maximum or minimum value. The test is performed using the grubbs.test() function from the outliers package:\ngrubbs.test(x)\nx is the input data as a numeric vector\n\n# Apply Grubbs' test to the river nitrate data\ngrubbs.test(river$nitrate)\n\n\n    Grubbs test for one outlier\n\ndata:  river$nitrate\nG = 4.72676, U = 0.92269, p-value = 0.000211\nalternative hypothesis: highest value 1.897 is an outlier\n\n\n\nYou can now use Grubbs’ test to check for single outliers. Remember, the lower the p-value returned by the test, the higher the likelihood that the point tested was an outlier.\n\n\n\n\nGrubbs’ test found that the maximum value could be an outlier, but what if there are more? Further outliers can be found by repeating Grubbs’ test, after removing any previously identified outliers from the data.\nTo identify the point that was tested, the which.min() or which.max() functions can be used to find the index containing the largest or smallest value - remember we know which of these it is from the Grubbs’ test output.\n\n# Apply Grubbs' test to the nitrate data\ngrubbs.test(river$nitrate)\n\n\n    Grubbs test for one outlier\n\ndata:  river$nitrate\nG = 4.72676, U = 0.92269, p-value = 0.000211\nalternative hypothesis: highest value 1.897 is an outlier\n\n# Use which.max to find row index of the max\nwhich.max(river$nitrate)\n\n[1] 156\n\n# Runs Grubbs' test excluding row 156\ngrubbs.test(river$nitrate[-156])\n\n\n    Grubbs test for one outlier\n\ndata:  river$nitrate[-156]\nG = 3.42983, U = 0.95915, p-value = 0.07756\nalternative hypothesis: highest value 1.643 is an outlier\n\n# Print the value tested in the second Grubbs' test\nmax(river$nitrate[-156])\n\n[1] 1.643\n\n\n\n\n\nThe first step when analyzing time series should be to construct graphical summaries that provide insight into important features such as trend, seasonality and possible anomalies. In this exercise, you’ll use several plots to explore thenitrate concentrations as a time series and to identify the period of any seasonal patterns that might be present.\n\n# View contents of dataset\nhead(river)\n\n   nitrate   months index\n1:   1.581  January     1\n2:   1.323 February     2\n3:   1.140    March     3\n4:   1.245    April     4\n5:   1.072      May     5\n6:   1.483     June     6\n\n# Show the time series of nitrate concentrations with time\nplot(nitrate ~ index, data = river, type = \"o\")\n\n\n\n# Create a boxplot of nitrate against months\nboxplot(nitrate~months, data = river)\n\n\n\n\n\n\n\n# Calculate the mean nitrate by month\nmonthly_mean <- tapply(river$nitrate, river$months, FUN = mean)\nmonthly_mean\n\n    April    August  December  February   January      July      June     March \n1.0166250 0.9380833 1.2264167 1.1838400 1.2163600 0.9810417 0.9792083 1.1050400 \n      May  November   October September \n0.9978333 1.0962500 1.0360000 0.9885833 \n\n# Plot the monthly means \nplot(monthly_mean, type = \"o\", xlab = \"Month\", ylab = \"Monthly mean\")\n\n\n\n\n\n\n\n\n# Create a boxplot of nitrate against months\nboxplot(nitrate~months, data = river)\n\n\n\n\n\n\n\n\nYou’ve identified a repeating seasonal cycle in the nitrate data, with a period of 12 months. You are now ready to apply the Seasonal-Hybrid ESD algorithm to find out if there are anomalies present in the data.\n\n# Run Seasonal-Hybrid ESD for nitrate concentrations\nAnomalyDetectionVec(river$nitrate,\n                    period = 12, \n                    direction = 'both', \n                    plot = T)\n\n$anoms\n  index anoms\n1     6 1.483\n2    53 1.533\n3   156 1.897\n\n$plot\n\n\n\n\n\n\n\n\nThe Seasonal-Hybrid ESD algorithm provides some useful output for understanding where the anomalies occur within the data.\nIn particular, the AnomalyDetectionVec() function generates output as a list with the two elements\n$anoms, a data frame containing the columns index and anoms $plot a plot of the time series with anomalies highlighted (if plot = T)\n\n# Use Seasonal-Hybrid ESD for nitrate concentrations\nriver_anomalies <- AnomalyDetectionVec(x = river$nitrate, period = 12, direction = 'both', plot = T)\n\n# Print the anomalies\nriver_anomalies$anoms\n\n  index anoms\n1     6 1.483\n2    53 1.533\n3   156 1.897\n\n# Print the plot\nprint(river_anomalies$plot)\n\n\n\n\n\n\nRecall when using Grubbs’ test on the river nitrate data, that only row 156 was found to be anomalous, while Seasonal-Hybrid ESD identified 2 further high-valued anomalies. Which of the following provides the best explanation for the difference between the two approaches?\n\nSpot on! Grubbs’ test can only take an extreme value as a candidate for an outlier, while Seasonal-Hybrid ESD explicitly accounts for the repeating seasonal patterns. Therefore, it is likely that the extra anomalies have been identified as extreme with respect to the seasonal pattern in the data."
  },
  {
    "objectID": "datacamp/anomaly_detection_R/anomaly_detection.html#distance-and-density-based-anomaly-detection",
    "href": "datacamp/anomaly_detection_R/anomaly_detection.html#distance-and-density-based-anomaly-detection",
    "title": "Anomaly Detection in R",
    "section": "Distance and density based anomaly detection",
    "text": "Distance and density based anomaly detection\n\nExploring wine\nThroughout this chapter, you’ll explore techniques for anomaly detection with a new data set called wine. Each row of this data set refers to a wine whose chemical composition is described by two numeric fields:\npH: how acidic the wine is alcohol: the wine’s alcohol content (%)\n\nwine <- fread(\"data/wine.csv\")\nwine <- wine[, .(pH, alcohol)]\n# View the contents of the wine data\nhead(wine)\n\n     pH alcohol\n1: 3.00     8.8\n2: 3.30     9.5\n3: 3.26    10.1\n4: 3.19     9.9\n5: 3.19     9.9\n6: 3.26    10.1\n\n# Scatterplot of wine pH against alcohol\nplot(pH ~ alcohol, data = wine)\n\n\n\n\n\n\nkNN distance matrix\nThe kNN distance matrix is a necessary prior step to producing the kNN distance score. The distance matrix has\nrows, where is the number of data points columns, where is the user-chosen number of neighbors. The entry in row i and column j of the distance matrix is the distance between point i and its jth nearest neighbor.\n\n# Calculate the 5 nearest neighbors distance\nwine_nn <- get.knn(wine, k = 5)\n\n# View the distance matrix\nhead(wine_nn$nn.dist)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]    0    0    0    0    0\n[3,]    0    0    0    0    0\n[4,]    0    0    0    0    0\n[5,]    0    0    0    0    0\n[6,]    0    0    0    0    0\n\n# Distance from wine 5 to nearest neighbor\nwine_nn$nn.dist[5, 1]\n\n[1] 0\n\n# Row index of wine 5's nearest neighbor \nrow_idx = wine_nn$nn.ind[5, 1]\nrow_idx\n\n[1] 9800\n\n# Return data for wine 5 and its nearest neighbor\nwine[c(5, row_idx), ] %>% data_table()\n\n\n\n\n\n\n\n\nkNN distance score\nOnce the kNN distance matrix is available, the nearest neighbor distance score can be calculated by averaging the nearest neighbor distances for each point.\nLarge values of the distance score can be interpreted as indicating the presence of unusual or anomalous points.\n\n# Calculate the 5 nearest neighbors distance\nwine_nn <- get.knn(wine, k = 5)\n\n# Create score by averaging distances\nwine_nnd <- rowMeans(wine_nn$nn.dist)\nhist(wine_nnd)\n\n\n\n# Print row index of the most anomalous point\nwhich.max(wine_nnd)\n\n[1] 3919\n\n\n\n\nStandardizing features\nIt is important to ensure that the feature inputs to the kNN distance calculation are standardized using the scale() function. Standardization ensures that features with large mean or variance do not disproportionately influence the kNN distance score.\n\n# Without standardization, features have different scales\nsummary(wine)\n\n       pH           alcohol     \n Min.   :2.720   Min.   : 8.00  \n 1st Qu.:3.090   1st Qu.: 9.50  \n Median :3.180   Median :10.40  \n Mean   :3.188   Mean   :10.51  \n 3rd Qu.:3.280   3rd Qu.:11.40  \n Max.   :3.820   Max.   :14.20  \n\n# Standardize the wine columns\nwine_scaled <- scale(wine)\n\n# Standardized features have similar means and quartiles\nsummary(wine_scaled)\n\n       pH              alcohol        \n Min.   :-3.10130   Min.   :-2.04323  \n 1st Qu.:-0.65081   1st Qu.:-0.82425  \n Median :-0.05475   Median :-0.09286  \n Mean   : 0.00000   Mean   : 0.00000  \n 3rd Qu.: 0.60755   3rd Qu.: 0.71979  \n Max.   : 4.18393   Max.   : 2.99522  \n\n\n\n\nAppending the kNN score\nNow you’ve standardized your input features, it’s time to create the kNN distance score for the standardized wine data and append it as a new column.\nIn this exercise, the 5 nearest neighbor distance score is already available in the object wine_nnd. So that the score is easily available for visualization or further analysis, you’ll append the score as a new column to the unstandardized data.\n\n# Calculate the 5 nearest neighbors distance\nwine_nn <- get.knn(wine_scaled, k = 5)\n\n# Create score by averaging distances\nwine_nnd <- rowMeans(wine_nn$nn.dist)\n\n# Print the 5-nearest neighbor distance score\nwine_nnd[1:5]\n\n[1] 0 0 0 0 0\n\n# Append the score as a new column \nwine$score <- wine_nnd\n\n\n\nVisualizing kNN distance score\nThe kNN distance score can be hard to interpret by simply eyeballing a set of values. It’s helpful to use scatterplots to visualize the kNN distance score to understand how the score works. When interpreting the plot, the relative size of the kNN distance score is more informative than the absolute value.\nThe wine data has been loaded with the kNN distance score appended from the previous exercise.\n\n# Scatterplot showing pH, alcohol and kNN score\nplot(pH ~ alcohol, data = wine, cex = sqrt(score), pch = 20)\n\n\n\n\n\n\nLOF calculation\nkNN is useful for finding global anomalies, but is less able to surface local outliers. In this exercise, you’ll practice using the lof() function to calculate local outlier factors for the wine data.\nlof() has the arguments:\nx: the data for scoring, k: the number of neighbors used to calculate the LOF.\n\nlibrary(dbscan)\n# Calculate the LOF for wine data\nwine_lof <- lof(scale(wine), 5)\n\n# Append the LOF score as a new column\nwine$score <- wine_lof\n\n\n\nLOF visualization\nAs with kNN distance scoring, a scatterplot can be a useful visual aid for understanding why a low or high score has been assigned. In this exercise, the LOF score is visualized by scaling the points according to the size of the score. You should notice some differences in the location of points with highest scores compared to kNN distance.\nThe wine data for this exercise already contains the score column appended in the previous exercise.\n\n# Scatterplot showing pH, alcohol and LOF score\nplot(pH ~ alcohol, data = wine, pch = 20, cex = sqrt(score))\n\n\n\n\n\n\nLOF vs kNN\nIt is common to look first at the points with highest anomaly scores before taking any action. When several algorithms are used, the points with highest scores may differ.\nIn this final exercise, you’ll calculate new LOF and kNN distance scores for the wine data, and print the highest scoring point for each.\n\n# Scaled wine data\n#wine_scaled <- scale(wine)\n\n# Calculate and append kNN distance as a new column\nwine_nn <- get.knn(wine_scaled, k = 10)\nwine$score_knn <- rowMeans(wine_nn$nn.dist)     \n\n# Calculate and append LOF as a new column\nwine$score_lof <- lof(wine_scaled, k = 10)\n\n# Find the row location of highest kNN\nwhich.max(wine$score_knn)\n\n[1] 2957\n\n# Find the row location of highest LOF\nwhich.max(wine$score_lof)\n\n[1] 15"
  },
  {
    "objectID": "datacamp/anomaly_detection_R/anomaly_detection.html#isolation-forest",
    "href": "datacamp/anomaly_detection_R/anomaly_detection.html#isolation-forest",
    "title": "Anomaly Detection in R",
    "section": "Isolation forest",
    "text": "Isolation forest\n\nFit and predict with an isolation tree\nThe two most important functions to know when fitting an isolation tree are iForest() to fit and predict() to generate an isolation score. In this exercise, you’ll use these two functions to explore isolated points in the wine data set.\n\nlibrary(isofor)\n# Build an isolation tree \nwine_tree <- iForest(wine_scaled, nt = 1)\n\n# Create isolation score\nwine$tree_score <- predict(wine_tree, newdata = wine_scaled) \n\n# Histogram plot of the scores\n\nhist(wine$tree_score, breaks = 40)\n\n\n\n\n\nIsolation forest scores are values between 0 and 1. High scores near to 1 indicate possible anomalies, while scores between 0 and 0.5 do not.\n\n\n\nFit an isolation forest\nAn isolation forest is a collection of isolation trees, and uses exactly the same commands that you used in the previous lesson to grow a single isolation tree. In this exercise, you’ll practice fitting an isolation forest to the wine data.\nWhen growing an isolation forest you should to pay particular attention to the number of trees and the number of points sampled to grow each tree.\n\n# Fit isolation forest\nwine_forest <- iForest(wine, nt = 100, phi = 200)\n\n# Create isolation score from forest\nwine_score <- predict(wine_forest, newdata = wine)\n\n# Append score to the wine data\nwine$score <- wine_score\n\n\n\nChecking convergence\nThe anomaly score from an isolation forest usually don’t change after a certain number of trees have been grown. This is called convergence, and can be checked by comparing the scores generated by forests with different numbers of trees. If the scores differ greatly, then this might suggest that more trees are required.\nIn this exercise, the scores for isolation forests with different numbers of trees have been already calculated for you and are contained in the data frame wine_scores.\n\nwine <- fread(\"data/wine.csv\")\nwine <- wine[, .(pH, alcohol)]\n# View the contents of the wine scores\n# Fit isolation forest\nwine_forest_2000 <- iForest(wine, nt = 200, phi = 200)\ntrees_2000 <- predict(wine_forest_2000, newdata = wine)\n\nwine_forest_1000 <- iForest(wine, nt = 100, phi = 200)\n\ntrees_1000 <- predict(wine_forest_1000, newdata = wine)\nwine_scores <-  data.frame(trees_2000,  trees_1000)\nhead(wine_scores)\n\n  trees_2000 trees_1000\n1  0.5302965  0.5414763\n2  0.4509511  0.4434861\n3  0.4434579  0.4403694\n4  0.4336313  0.4255291\n5  0.4336313  0.4255291\n6  0.4434579  0.4403694\n\n# Score scatterplot 2000 vs 1000 trees \nplot(trees_2000 ~ trees_1000, data = wine_scores)\n\n# Add reference line of equality\nabline(a = 0, b = 1)\n\n\n\n\n\n\nA grid of points\nIn the video, you saw how it can be instructive to use a contour plot over a grid of points to see what the anomaly score might have been at locations other than where the data occurred.\nIn this exercise you’ll create and visualize a grid of points across the region of interest. The grid you create will be used in the exercise that follows to visualize predicted isolation scores.\n\n# Sequence of values for pH and alcohol\nph_seq <- seq(min(wine$pH), max(wine$pH), length.out = 25)\nalcohol_seq <- seq(min(wine$alcohol),  max(wine$alcohol), length.out = 25)\n\n# Create a data frame of grid coordinates\nwine_grid <- expand.grid(pH = ph_seq, alcohol = alcohol_seq)\n\n# Visualise the grid using a scatterplot\nplot(pH~alcohol, data = wine_grid, pch = 20)\n\n\n\n\n\n\nPrediction over a grid\nIn this exercise, you’ll use an isolation forest to obtain an anomaly score for the grid of points you created in the last exercise. Getting the anomaly score is the final preparatory step required to visualize the isolation scores.\nThe data frame wine_grid and fitted isolation forest wine_forest from the previous exercise are preloaded.\n\n# Calculate isolation score at grid locations\nwine_grid$score <- predict(wine_forest_2000, wine_grid)\n\n\n\nAnomaly contours\nYou now have the key ingredients to produce a contour plot, which is a powerful tool for viewing how the anomaly score varies across the region spanned by the data points.\nThe wine_grid data from the previous exercise has been preloaded for you to use.\n\nlibrary(lattice)\n# Contour plot of isolation scores\ncontourplot(score ~ pH + alcohol, wine_grid, region = TRUE)"
  },
  {
    "objectID": "datacamp/anomaly_detection_R/anomaly_detection.html#comparing-performance",
    "href": "datacamp/anomaly_detection_R/anomaly_detection.html#comparing-performance",
    "title": "Anomaly Detection in R",
    "section": "Comparing performance",
    "text": "Comparing performance\n\nThyroid data\nIn this chapter, you’ll explore a new data set called thyroid. These data contain examples of thyroid hormone measurements for 1000 patients, and a column called label indicating the presence of thyroid disease. It is expected that unusual hormone measurements can be used to detect disease.\nThe overall goal is to determine whether an anomaly score based on hormone measurements could be used to detect thyroid disease. In this exercise, you’ll use plotting techniques to visually assess the distribution of thyroid disease.\n\nthyroid <- fread(\"data/thyroid.csv\")\n# View contents of thryoid data\nhead(thyroid)\n\n   label        TSH        T3       TT4       T4U        FTI       TBG\n1:     0 -0.2559334 -6.783703 -1.983614 -1.288439 -1.2181574 -1.443646\n2:     0 -1.3971053 -7.659171 -1.273372 -1.110363 -0.6250937 -1.750020\n3:     0 -0.7039581 -5.631023 -1.500762 -1.453953 -0.6427933 -2.082726\n4:     0 -0.3894648 -6.378238 -1.854402 -1.741635 -1.0986123 -1.994618\n5:     0 -1.4415570 -7.659171 -1.419084 -1.139142 -1.0986123 -1.396179\n6:     0 -0.3130918 -7.659171 -1.916923 -1.628306 -1.4294665 -1.617668\n\n# Tabulate the labels\ntable(thyroid$label)\n\n\n  0   1 \n978  22 \n\n# Proportion of thyroid cases\nprop_disease <- 22/(978+22)\n\n\n\nVisualizing thyroid disease\nIn previous chapters, we considered data with two or fewer features. As the number of features increases, it becomes harder to visually assess whether individual points are anomalous.\nIn cases where anomaly labels are available, it is important to use scatterplots to visualize how the distribution of anomalies varies with different combinations of features.\n\n# Scatterplot showing TT4, TBG and anomaly labels\nplot(TT4 ~ TBG, data = thyroid, pch = 20, col = label + 1)\n\n\n\n\n\nThe disease cases seem to occur at extreme values of the hormone measurements. Next you’ll build an anomaly score to capture this!\n\n\n\nAnomaly score\nYour visualization suggested that thyroid disease could be detected from anomalous hormone measurements.\nIn this exercise you’ll use an isolation forest to generate an anomaly score for thyroid levels, and compare the resulting score against the true disease status.\n\n# Fit isolation forest\nthyroid_xdf <- thyroid[, .SD, .SDcols = !\"label\"]\nthyroid_forest <- iForest(thyroid[, .SD, .SDcols = !\"label\"], nt = 200, phi = 100)\n\n# Anomaly score \nthyroid$iso_score <- predict(thyroid_forest, thyroid[, -1])\n\n# Boxplot of the anomaly score against labels\nboxplot(iso_score ~ label, data = thyroid, col = \"olivedrab4\")\n\n\n\n\n\nThe boxplot showed that the anomaly scores were generally higher for the thyroid disease cases. This adds further weight to the claim that the disease cases could be detected from anomalous hormone levels.\n\n\n\nBinarized scores\nIt’s worthwhile to compare the performance of more than one anomaly detection algorithm before deciding which to use.\nIn this exercise, you’ll construct a pair of binary anomaly scores based on local outlier factor (LOF) and the isolation forest. The isolation score vector iso_score generated in the previous exercise is preloaded for you to use.\n\niso_score <- thyroid$iso_score\n# Scale the measurement columns of thyroid\nscaled_thyroid_measurements <- scale(thyroid_xdf)\n\n# Create a LOF score for the measurements\nlof_score <- lof(scaled_thyroid_measurements, k = 10)\n                 \n# Calculate high threshold for lof_score\nhigh_lof <- quantile(lof_score, probs = 0.98) \n\n# Append binary LOF score to thyroid data\nthyroid$binary_lof <- as.numeric(lof_score >= high_lof)\n                 \n# Calculate high threshold for iso_score\nhigh_iso <- quantile(iso_score, probs = 0.98)  \n\n# Append binary isolation score to thyroid data\nthyroid$binary_iso <- as.numeric(iso_score> high_iso )         \n\n\nAlthough the 98th percentile was chosen here, other thresholds could have been used. The choice might be constrained by the number of potential anomalies you have time to check, or the cost of missing an anomaly.\n\n\n\nCross-tabulate binary scores\nThe table() function tallies the number of occurrences of combinations of values across one or more vectors. The orientation of the output table depends on the order that the input vectors are specified. For example in the table\ntable(currency, country)\n    country\ncurrency UK USA $ 0 1 £ 1 0 the rows are indexed by currency because this appears first in the table() function.\n\n# Tabulate agreement of label and binary isolation score \ntable(thyroid$label, thyroid$binary_iso)\n\n   \n      0   1\n  0 971   7\n  1   9  13\n\n# Tabulate agreement of label and binary LOF score \ntable(thyroid$label, thyroid$binary_lof)\n\n   \n      0   1\n  0 958  20\n  1  22   0\n\n# Proportion of binary_iso and label that agree\niso_prop <- mean(thyroid$label == thyroid$binary_iso)\n\n# Proportion of binary_lof and label that agree\nlof_prop <- mean(thyroid$label == thyroid$binary_lof)\n\n\nAlthough the measure of agreement is intuitive, it can be misleadingly high when anomalies are very rare. In the next exercise, you’ll try other ways to quantify the success of the algorithm.\n\n\n\nThyroid precision and recall\nCross-tabulating the agreement between a binary score and a known label is a great way to understand how well the algorithm performs. Precision and recall are two further measures based on the table that give more insight into how well the score performs.\nIn this exercise, you’ll explore precision and recall using the thyroid data. The binary_lof and binary_iso scores created in the previous exercises are available to use as columns in the thyroid data. The code used to tabulate agreements in the previous exercise is also included.\n\n# Tabulation for the binary isolation score\ntable(thyroid$label, thyroid$binary_iso)\n\n   \n      0   1\n  0 971   7\n  1   9  13\n\n# Precision for the isolation score\nprecision_iso <- 12 / (8 + 12)\n# Recall for the isolation score\nrecall_iso <- 12 / (10+12)\n\n# Tabulation for the binary lof score\ntable(thyroid$label, thyroid$binary_lof)\n\n   \n      0   1\n  0 958  20\n  1  22   0\n\n# Precision for the binary lof score\nprecision_lof <- 0 / (20 + 0)\n# Recall for the binary lof score\nrecall_lof <- 0 / (22+0)\n\n\n\nConverting character to factor\nBoth LOF and isolation forest can be trained using data containing categorical features, but it’s easier if these are converted to factors first.\nIn this exercise, you’ll revisit the thyroid data which contains some additional categorical features that will need to be converted.\n\n# Print the column classes in thyroid\nsapply(X = thyroid, FUN = class)\n\n     label        TSH         T3        TT4        T4U        FTI        TBG \n \"integer\"  \"numeric\"  \"numeric\"  \"numeric\"  \"numeric\"  \"numeric\"  \"numeric\" \n iso_score binary_lof binary_iso \n \"numeric\"  \"numeric\"  \"numeric\" \n\n# Convert column with character class to factor\nthyroid$age <- as.factor(thyroid$age)\nthyroid$sex <- as.factor(thyroid$sex)\n\n# Check that all columns are factor or numeric\nsapply(X = thyroid, FUN = class)\n\n     label        TSH         T3        TT4        T4U        FTI        TBG \n \"integer\"  \"numeric\"  \"numeric\"  \"numeric\"  \"numeric\"  \"numeric\"  \"numeric\" \n iso_score binary_lof binary_iso        age        sex \n \"numeric\"  \"numeric\"  \"numeric\"   \"factor\"   \"factor\" \n\n\n\n\nIsolation forest with factors\nAs you saw in the video, an isolation forest can accept categorical features as input, but only if they are encoded as factor variables.\nIn this exercise, the thyroid data you edited in the previous exercise is preloaded. To be extra careful, you should first check that all of the features are numeric or factor before attempting to train an isolation forest.\n\n# Check the class of age column\nclass(thyroid$age)\n\n[1] \"factor\"\n\n# Check the class of sex column\nclass(thyroid$sex)\n\n[1] \"factor\"\n\n# Fit an isolation forest with 100 trees\nthyroid_for <- iForest(thyroid[,-1], 100)\n\n\nBeing able to incorporate categorical features greatly extends the range of applications in which isolation forests can be used.\n\n\n\nLOF with factors\nThe lof() function can accept either a numeric data frame or a distance matrix as input to calculate LOF scores. In this exercise, you’ll practice calculating a distance matrix using Gower’s distance, which can then be passed to the lof() function for scoring.\nAs in the previous exercise, the thyroid data with character columns converted to factors has been preloaded for you to use.\n\n# Calculate Gower's distance matrix\nlibrary(cluster)\nthyroid_dist <- daisy(thyroid[, -1], metric = \"gower\")\n\n# Generate LOF scores for thyroid data\nthyroid_lof <- lof(thyroid_dist, k = 10)\n\n# Range of values in the distance matrix\nrange(as.matrix(thyroid_dist))\n\n[1] 0.0000000 0.7039034"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html",
    "href": "datacamp/sampling_R/sampling_R.html",
    "title": "Sampling in R",
    "section": "",
    "text": "Sampling is an important technique in your statistical arsenal. It isn’t always appropriate though—you need to know when to use it and when to work with the whole dataset.\n\nWhich of the following is not a good scenario to use sampling?\nwhen data set is small"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html#simple-sampling-with-dplyr",
    "href": "datacamp/sampling_R/sampling_R.html#simple-sampling-with-dplyr",
    "title": "Sampling in R",
    "section": "Simple sampling with dplyr",
    "text": "Simple sampling with dplyr\nThroughout this chapter you’ll be exploring song data from Spotify. Each row of the dataset represents a song, and there are 41656 rows. Columns include the name of the song, the artists who performed it, the release year, and attributes of the song like its duration, tempo, and danceability. We’ll start by looking at the durations.\nYour first task is to sample the song dataset and compare a calculation on the whole population and on a sample.\nspotify_population is available and dplyr is loaded.\n\nlibrary(tidyverse)\nlibrary(fst)\nlibrary(knitr)\nspotify_population <- read_fst(\"data/spotify_2000_2020.fst\")\n# View the whole population dataset\n\n# Sample 1000 rows from spotify_population\nspotify_sample <- slice_sample(spotify_population, n = 10)\n\n\n# See the result\nkable(spotify_sample)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nacousticness\nartists\ndanceability\nduration_ms\nduration_minutes\nenergy\nexplicit\nid\ninstrumentalness\nkey\nliveness\nloudness\nmode\nname\npopularity\nrelease_date\nspeechiness\ntempo\nvalence\nyear\n\n\n\n\n0.00665\n[‘Hurt’]\n0.507\n281373\n4.689550\n0.684\n0\n0N48Iqm5s1DVkiBSF2e2et\n2.85e-05\n8\n0.1150\n-6.230\n0\nFalls Apart\n43\n2006-01-01\n0.0506\n146.021\n0.371\n2006\n\n\n0.00312\n[‘Kevin Gates’]\n0.728\n247320\n4.122000\n0.550\n1\n5KJ7TOWSkcRCbgnRmGatnA\n0.00e+00\n2\n0.0871\n-6.049\n1\nIDGAF\n45\n2013-04-09\n0.3790\n139.863\n0.305\n2013\n\n\n0.05350\n[‘J Dilla’]\n0.230\n119387\n1.989783\n0.770\n1\n4jVqbLx0MvlIaj3h2D872X\n1.56e-04\n1\n0.6440\n-6.349\n1\nDon’t Cry\n53\n2006-02-07\n0.3370\n173.442\n0.588\n2006\n\n\n0.82500\n[‘Los Panchos’]\n0.844\n180627\n3.010450\n0.561\n0\n1HlX3tDP7eJBs5CJx4XIIk\n9.19e-04\n6\n0.1030\n-7.677\n0\nBésame Mucho\n44\n2008-01-26\n0.0378\n121.755\n0.936\n2008\n\n\n0.41400\n[‘Zac Brown Band’]\n0.400\n227560\n3.792667\n0.465\n0\n2q40yq9UxqYGKuVZnynPxV\n2.24e-05\n1\n0.0739\n-6.530\n1\nCold Hearted\n53\n2010-09-20\n0.0264\n166.010\n0.316\n2010\n\n\n0.69500\n[‘Tom Odell’]\n0.445\n244360\n4.072667\n0.537\n1\n3JvKfv6T31zO0ini8iNItO\n1.60e-05\n4\n0.0944\n-8.532\n0\nAnother Love\n75\n2013-06-24\n0.0400\n122.764\n0.131\n2013\n\n\n0.00282\n[‘WALK THE MOON’]\n0.629\n232733\n3.878883\n0.721\n0\n4LgIcna8LRrR3DOomihYMY\n1.40e-06\n11\n0.1280\n-6.292\n1\nShiver Shiver\n46\n2012-06-19\n0.0326\n100.208\n0.854\n2012\n\n\n0.04240\n[‘Blur’]\n0.529\n233373\n3.889550\n0.455\n0\n79PrPZu9zWyc1qwUwXchVl\n3.64e-03\n0\n0.3640\n-8.603\n1\nOut of Time\n54\n2003\n0.0274\n138.678\n0.318\n2003\n\n\n0.13000\n[‘Taylor Swift’]\n0.459\n261800\n4.363333\n0.459\n0\n6HWYtS215rxaaMjvpyG18W\n2.30e-06\n6\n0.1130\n-4.126\n1\nYou’re Not Sorry\n46\n2008-11-11\n0.0275\n134.018\n0.281\n2008\n\n\n0.02020\n[‘Black Stone Cherry’]\n0.394\n230707\n3.845117\n0.834\n0\n5lIqY3Yb3bPj5z1tiUIiCJ\n0.00e+00\n1\n0.3090\n-3.296\n1\nLonely Train\n52\n2006-07-17\n0.0767\n123.600\n0.438\n2006"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html#simple-sampling-with-dplyr-1",
    "href": "datacamp/sampling_R/sampling_R.html#simple-sampling-with-dplyr-1",
    "title": "Sampling in R",
    "section": "Simple sampling with dplyr",
    "text": "Simple sampling with dplyr\nThroughout this chapter you’ll be exploring song data from Spotify. Each row of the dataset represents a song, and there are 41656 rows. Columns include the name of the song, the artists who performed it, the release year, and attributes of the song like its duration, tempo, and danceability. We’ll start by looking at the durations.\nYour first task is to sample the song dataset and compare a calculation on the whole population and on a sample.\nspotify_population is available and dplyr is loaded.\n\n# Calculate the mean duration in mins from spotify_population\nmean_dur_pop <- summarize(spotify_population, mean(duration_minutes))\n\n\n# Calculate the mean duration in mins from spotify_sample\nmean_dur_samp <- summarize(spotify_sample, mean(duration_minutes))\n\n\n# See the results\nmean_dur_pop\n\n  mean(duration_minutes)\n1               3.852152\n\nmean_dur_samp\n\n  mean(duration_minutes)\n1                 3.7654"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html#simple-sampling-with-base-r",
    "href": "datacamp/sampling_R/sampling_R.html#simple-sampling-with-base-r",
    "title": "Sampling in R",
    "section": "Simple sampling with base-R",
    "text": "Simple sampling with base-R\nWhile dplyr provides great tools for sampling data frames, if you want to work with vectors you can use base-R.\nLet’s turn it up to eleven and look at the loudness property of each song.\nspotify_population is available.\n\n# From previous step\nloudness_pop <- spotify_population$loudness\nloudness_samp <- sample(loudness_pop, size = 100)\n\n# Calculate the standard deviation of loudness_pop\nsd_loudness_pop <- sd(loudness_pop)\n\n# Calculate the standard deviation of loudness_samp\nsd_loudness_samp <- sd(loudness_samp)\n\n# See the results\nsd_loudness_pop\n\n[1] 4.524076\n\nsd_loudness_samp\n\n[1] 3.908033"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html#are-findings-from-the-sample-generalizable",
    "href": "datacamp/sampling_R/sampling_R.html#are-findings-from-the-sample-generalizable",
    "title": "Sampling in R",
    "section": "Are findings from the sample generalizable?",
    "text": "Are findings from the sample generalizable?\nYou just saw how convenience sampling—collecting data via the easiest method can result in samples that aren’t representative of the whole population. Equivalently, this means findings from the sample are not generalizable to the whole population. Visualizing the distributions of the population and the sample can help determine whether or not the sample is representative of the population.\nThe Spotify dataset contains a column named acousticness, which is a confidence measure from zero to one of whether the track is acoustic, that is, it was made with instruments that aren’t plugged in. Here, you’ll look at acousticness in the total population of songs, and in a sample of those songs.\nspotify_population and spotify_mysterious_sample are available; dplyr and ggplot2 are loaded.\n\nggplot(spotify_population, aes(acousticness))+\n    geom_histogram(binwidth = 0.01)\n\n\n\n\n\nggplot(spotify_population, aes(duration_minutes))+\n    geom_histogram(binwidth = 0.5)"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html#generating-random-numbers",
    "href": "datacamp/sampling_R/sampling_R.html#generating-random-numbers",
    "title": "Sampling in R",
    "section": "Generating random numbers",
    "text": "Generating random numbers\nYou’ve seen sample() and it’s dplyr cousin, slice_sample() for generating pseudo-random numbers from a set of values. A related task is to generate random numbers that follow a statistical distribution, like the uniform distribution or the normal distribution.\nEach random number generation function has a name beginning with “r”. It’s first argument is the number of numbers to generate, but other arguments are distribution-specific. Free hint: Try args(runif) and args(rnorm) to see what arguments you need to pass to those functions.\nn_numbers is available and set to 5000; ggplot2 is loaded.\n\nn_numbers <- 5000\n# Generate random numbers from ...\nrandoms <- data.frame(\n  # a uniform distribution from -3 to 3\n  uniform =runif(n_numbers, -3, 3),\n  # a normal distribution with mean 5 and sd 2\n  normal = rnorm(n_numbers, mean = 5, sd = 2)\n)\n\n\n# Plot a histogram of uniform values, binwidth 0.25\nggplot(randoms, aes(uniform)) +\n    geom_histogram(binwidth = 0.25)\n\n\n\n# Plot a histogram of normal values, binwidth 0.5\nggplot(randoms, aes(normal)) +\n    geom_histogram(binwidth = 0.5)"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html#understanding-random-seeds",
    "href": "datacamp/sampling_R/sampling_R.html#understanding-random-seeds",
    "title": "Sampling in R",
    "section": "Understanding random seeds",
    "text": "Understanding random seeds\nWhile random numbers are important for many analyses, they create a problem: the results you get can vary slightly. This can cause awkward conversations with your boss when your script for calculating the sales forecast gives different answers each time.\nSetting the seed to R’s random number generator helps avoid such problems by making the random number generation reproducible. - The values of x are different to those of y.\n\nset.seed(123)\nx <- rnorm(5)\ny <- rnorm(5)\nx\n\n[1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774\n\ny\n\n[1]  1.7150650  0.4609162 -1.2650612 -0.6868529 -0.4456620\n\n\n\nx and y have identical values.\n\n\nset.seed(123)\nx <- rnorm(5)\nset.seed(123)\ny <- rnorm(5)\nx\n\n[1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774\n\ny\n\n[1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774\n\n\n\nx and y have identical values.\n\n\nset.seed(123)\nx <- c(rnorm(5), rnorm(5))\nset.seed(123)\ny <- rnorm(10)\nx\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n\ny\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html",
    "href": "datacamp/tidymodels/tidymodels.html",
    "title": "Modeling with tidymodels in R",
    "section": "",
    "text": "The rsample package is designed to create training and test datasets. Creating a test dataset is important for estimating how a trained model will likely perform on new data. It also guards against overfitting, where a model memorizes patterns that exist only in the training data and performs poorly on new data.\nIn this exercise, you will create training and test datasets from the home_sales data. This data contains information on homes sold in the Seattle, Washington area between 2015 and 2016.\nThe outcome variable in this data is selling_price.\nThe tidymodels package will be pre-loaded in every exercise in the course. The home_sales tibble has also been loaded for you.\n\n\n\nTidy model packages\n\n\n\nhome_sales <- readRDS(\"home_sales.rds\")\n\n# Create a data split object\nhome_split <- initial_split(home_sales, \n                            prop = 0.7, \n                            strata = selling_price)\n\n# Create the training data\nhome_training <- home_split %>%\n  training()\n\n# Create the test data\nhome_test <- home_split %>% \n  testing()\n\n# Check number of rows in each dataset\nnrow(home_training)\n\n[1] 1042\n\nnrow(home_test)\n\n[1] 450\n\n\nDistribution of outcome variable values Stratifying by the outcome variable when generating training and test datasets ensures that the outcome variable values have a similar range in both datasets.\nSince the original data is split at random, stratification avoids placing all the expensive homes in home_sales into the test dataset, for example. In this case, your model would most likely perform poorly because it was trained on less expensive homes.\nIn this exercise, you will calculate summary statistics for the selling_price variable in the training and test datasets. The home_training and home_test tibbles have been loaded from the previous exercise.\n\n# Distribution of selling_price in training data\nlibrary(knitr)\nsummary_func <- function(df){\n      df %>% summarize(min_sell_price = min(selling_price),\n            max_sell_price = max(selling_price),\n            mean_sell_price = mean(selling_price),\n            sd_sell_price = sd(selling_price)) %>%\n    kable()\n\n}\nhome_training %>% \n    summary_func()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n478986.2\n80915.09\n\n\n\n\nhome_test %>% \n  summary_func()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n479312.1\n81216.26"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#fitting-a-linear-regression-model",
    "href": "datacamp/tidymodels/tidymodels.html#fitting-a-linear-regression-model",
    "title": "Modeling with tidymodels in R",
    "section": "Fitting a linear regression model",
    "text": "Fitting a linear regression model\nThe parsnip package provides a unified syntax for the model fitting process in R.\nWith parsnip, it is easy to define models using the various packages, or engines, that exist in the R ecosystem.\nIn this exercise, you will define a parsnip linear regression object and train your model to predict selling_price using home_age and sqft_living as predictor variables from the home_sales data.\nThe home_training and home_test tibbles that you created in the previous lesson have been loaded into this session.\n\n# Specify a linear regression model, linear_model\nlinear_model <- linear_reg() %>% \n  # Set the model engine\n  set_engine('lm') %>% \n  # Set the model mode\n  set_mode('regression')\n\n# Train the model with the training data\nlm_fit <- linear_model %>% \n  fit(selling_price~ home_age + sqft_living,\n      data = home_training)\n\n# Print lm_fit to view model information\ntidy(lm_fit) %>%\n    kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n291856.6392\n7408.987519\n39.392243\n0\n\n\nhome_age\n-1484.9767\n174.432209\n-8.513203\n0\n\n\nsqft_living\n102.7973\n2.678621\n38.376944\n0"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#predicting-home-selling-prices",
    "href": "datacamp/tidymodels/tidymodels.html#predicting-home-selling-prices",
    "title": "Modeling with tidymodels in R",
    "section": "Predicting home selling prices",
    "text": "Predicting home selling prices\nAfter fitting a model using the training data, the next step is to use it to make predictions on the test dataset. The test dataset acts as a new source of data for the model and will allow you to evaluate how well it performs.\nBefore you can evaluate model performance, you must add your predictions to the test dataset.\nIn this exercise, you will use your trained model, lm_fit, to predict selling_price in the home_test dataset.\nYour trained model, lm_fit, as well as the test dataset, home_test have been loaded into your session.\n\n# Predict selling_price\nhome_predictions <- predict(lm_fit,\n                        new_data = home_test)\n\n# View predicted selling prices\n#home_predictions\n\n# Combine test data with predictions\nhome_test_results <- home_test %>% \n  select(selling_price, home_age, sqft_living) %>% \n  bind_cols(home_predictions)\n\n# View results\nhome_test_results %>% \n    head()\n\n# A tibble: 6 × 4\n  selling_price home_age sqft_living   .pred\n          <dbl>    <dbl>       <dbl>   <dbl>\n1        411000       18        1130 381288.\n2        635000        4        3350 630288.\n3        425000       11        1920 472893.\n4        525000       16        2100 483971.\n5        381000       25        1680 427432.\n6        540000       22        2220 487397."
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#model-performance-metrics",
    "href": "datacamp/tidymodels/tidymodels.html#model-performance-metrics",
    "title": "Modeling with tidymodels in R",
    "section": "Model performance metrics",
    "text": "Model performance metrics\nEvaluating model results is an important step in the modeling process. Model evaluation should be done on the test dataset in order to see how well a model will generalize to new datasets.\nIn the previous exercise, you trained a linear regression model to predict selling_price using home_age and sqft_living as predictor variables. You then created the home_test_results tibble using your trained model on the home_test data.\nIn this exercise, you will calculate the RMSE and R squared metrics using your results in home_test_results.\nThe home_test_results tibble has been loaded into your session.\n\n# Print home_test_results\n#home_test_results\n\n# Caculate the RMSE metric\nhome_test_results %>% \n  rmse(truth = selling_price, estimate =.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      48709.\n\n# Calculate the R squared metric\nhome_test_results %>% \n  rsq(truth = selling_price, estimate =.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.640"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#r-squared-plot",
    "href": "datacamp/tidymodels/tidymodels.html#r-squared-plot",
    "title": "Modeling with tidymodels in R",
    "section": "R squared plot",
    "text": "R squared plot\nIn the previous exercise, you got an R squared value of 0.651. The R squared metric ranges from 0 to 1, 0 being the worst and 1 the best.\nCalculating the R squared value is only the first step in studying your model’s predictions.\nMaking an R squared plot is extremely important because it will uncover potential problems with your model, such as non-linear patterns or regions where your model is either over or under-predicting the outcome variable.\nIn this exercise, you will create an R squared plot of your model’s performance.\nThe home_test_results tibble has been loaded into your session.\n\n# Create an R squared plot of model performance\nggplot(home_test_results, aes(x = selling_price, y = .pred)) +\n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred()  +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#complete-model-fitting-process-with-last_fit",
    "href": "datacamp/tidymodels/tidymodels.html#complete-model-fitting-process-with-last_fit",
    "title": "Modeling with tidymodels in R",
    "section": "Complete model fitting process with last_fit()",
    "text": "Complete model fitting process with last_fit()\nIn this exercise, you will train and evaluate the performance of a linear regression model that predicts selling_price using all the predictors available in the home_sales tibble.\nThis exercise will give you a chance to perform the entire model fitting process with tidymodels, from defining your model object to evaluating its performance on the test data.\nEarlier in the chapter, you created an rsample object called home_split by passing the home_sales tibble into initial_split(). The home_split object contains the instructions for randomly splitting home_sales into training and test sets.\nThe home_sales tibble, and home_split object have been loaded into this session.\n\n# Define a linear regression model\nlinear_model <- linear_reg() %>% \n  set_engine('lm') %>% \n  set_mode('regression')\n\n# Train linear_model with last_fit()\nlinear_fit <- linear_model %>% \n  last_fit(selling_price ~ ., split = home_split)\n\n# Collect predictions and view results\npredictions_df <- linear_fit %>% collect_predictions()\npredictions_df %>% head()\n\n# A tibble: 6 × 5\n  id                 .pred  .row selling_price .config             \n  <chr>              <dbl> <int>         <dbl> <chr>               \n1 train/test split 398796.     3        411000 Preprocessor1_Model1\n2 train/test split 693533.     4        635000 Preprocessor1_Model1\n3 train/test split 439551.     9        425000 Preprocessor1_Model1\n4 train/test split 505907.    13        525000 Preprocessor1_Model1\n5 train/test split 392617.    18        381000 Preprocessor1_Model1\n6 train/test split 513498.    20        540000 Preprocessor1_Model1\n\n# Make an R squared plot using predictions_df\nggplot(predictions_df, aes(x = selling_price, y = .pred)) + \n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred() +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#data-resampling",
    "href": "datacamp/tidymodels/tidymodels.html#data-resampling",
    "title": "Modeling with tidymodels in R",
    "section": "Data resampling",
    "text": "Data resampling\nThe first step in a machine learning project is to create training and test datasets for model fitting and evaluation. The test dataset provides an estimate of how your model will perform on new data and helps to guard against overfitting.\nYou will be working with the telecom_df dataset which contains information on customers of a telecommunications company. The outcome variable is canceled_service and it records whether a customer canceled their contract with the company. The predictor variables contain information about customers’ cell phone and internet usage as well as their contract type and monthly charges.\nThe telecom_df tibble has been loaded into your session.\n\ntelecom_df <- readRDS(\"telecom_df.rds\")\n# Create data split object\ntelecom_split <- initial_split(telecom_df, prop = 0.75,\n                     strata = canceled_service)\n\n# Create the training data\ntelecom_training <- telecom_split %>% \n  training()\n\n# Create the test data\ntelecom_test <- telecom_split %>% \n  testing()\n\n# Check the number of rows\nnrow(telecom_training)\n\n[1] 731\n\nnrow(telecom_test)\n\n[1] 244"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#fitting-a-logistic-regression-model",
    "href": "datacamp/tidymodels/tidymodels.html#fitting-a-logistic-regression-model",
    "title": "Modeling with tidymodels in R",
    "section": "Fitting a logistic regression model",
    "text": "Fitting a logistic regression model\nIn addition to regression models, the parsnip package also provides a general interface to classification models in R.\nIn this exercise, you will define a parsnip logistic regression object and train your model to predict canceled_service using avg_call_mins, avg_intl_mins, and monthly_charges as predictor variables from the telecom_df data.\nThe telecom_training and telecom_test tibbles that you created in the previous lesson have been loaded into this session.\n\n# Specify a logistic regression model\nlogistic_model <- logistic_reg() %>% \n  # Set the engine\n  set_engine('glm') %>% \n  # Set the mode\n  set_mode('classification')\n\n# Fit to training data\nlogistic_fit <- logistic_model %>% \n  fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n      data = telecom_training)\n\n# Print model fit object\nlogistic_fit %>% tidy()\n\n# A tibble: 4 × 5\n  term            estimate std.error statistic  p.value\n  <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)      1.87      0.581       3.22  1.28e- 3\n2 avg_call_mins   -0.0104    0.00130    -8.05  8.42e-16\n3 avg_intl_mins    0.0223    0.00311     7.18  6.94e-13\n4 monthly_charges  0.00231   0.00472     0.488 6.25e- 1"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#combining-test-dataset-results",
    "href": "datacamp/tidymodels/tidymodels.html#combining-test-dataset-results",
    "title": "Modeling with tidymodels in R",
    "section": "Combining test dataset results",
    "text": "Combining test dataset results\nEvaluating your model’s performance on the test dataset gives insights into how well your model predicts on new data sources. These insights will help you communicate your model’s value in solving problems or improving decision making.\nBefore you can calculate classification metrics such as sensitivity or specificity, you must create a results tibble with the required columns for yardstick metric functions.\nIn this exercise, you will use your trained model to predict the outcome variable in the telecom_test dataset and combine it with the true outcome values in the canceled_service column.\nYour trained model, logistic_fit, and test dataset, telecom_test, have been loaded from the previous exercise.\n\n# Predict outcome categories\nclass_preds <- predict(logistic_fit, new_data = telecom_test,\n                       type = 'class')\n\n# Obtain estimated probabilities for each outcome value\nprob_preds <- predict(logistic_fit, new_data = telecom_test, \n                      type = 'prob')\n\n# Combine test set results\ntelecom_results <- telecom_test %>% \n  select(canceled_service) %>% \n  bind_cols(class_preds, prob_preds)\n\n# View results tibble\ntelecom_results %>%\n    head()\n\n# A tibble: 6 × 4\n  canceled_service .pred_class .pred_yes .pred_no\n  <fct>            <fct>           <dbl>    <dbl>\n1 yes              no              0.366    0.634\n2 yes              no              0.143    0.857\n3 no               no              0.225    0.775\n4 yes              yes             0.556    0.444\n5 no               no              0.383    0.617\n6 yes              no              0.363    0.637"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#evaluating-performance-with-yardstick",
    "href": "datacamp/tidymodels/tidymodels.html#evaluating-performance-with-yardstick",
    "title": "Modeling with tidymodels in R",
    "section": "Evaluating performance with yardstick",
    "text": "Evaluating performance with yardstick\nIn the previous exercise, you calculated classification metrics from a sample confusion matrix. The yardstick package was designed to automate this process.\nFor classification models, yardstick functions require a tibble of model results as the first argument. This should include the actual outcome values, predicted outcome values, and estimated probabilities for each value of the outcome variable.\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate performance metrics.\nThe telecom_results tibble has been loaded into your session.\n\n# Calculate the confusion matrix\nconf_mat(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n          Truth\nPrediction yes  no\n       yes  32  22\n       no   50 140\n\n# Calculate the accuracy\naccuracy(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.705\n\n# Calculate the sensitivity\n\nsens(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.390\n\n# Calculate the specificity\n\nspec(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 spec    binary         0.864"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#creating-custom-metric-sets",
    "href": "datacamp/tidymodels/tidymodels.html#creating-custom-metric-sets",
    "title": "Modeling with tidymodels in R",
    "section": "Creating custom metric sets",
    "text": "Creating custom metric sets\nThe yardstick package also provides the ability to create custom sets of model metrics. In cases where the cost of obtaining false negative errors is different from the cost of false positive errors, it may be important to examine a specific set of performance metrics.\nInstead of calculating accuracy, sensitivity, and specificity separately, you can create your own metric function that calculates all three at the same time.\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate a custom set of performance metrics. You will also use a confusion matrix to calculate all available binary classification metrics in tidymodelsall at once.\nThe telecom_results tibble has been loaded into your session.\n\n# Create a custom metric function\ntelecom_metrics <- metric_set(accuracy, sens, spec)\n\n# Calculate metrics using model results tibble\ntelecom_metrics(telecom_results, \n                truth = canceled_service,\n                estimate = .pred_class)\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.705\n2 sens     binary         0.390\n3 spec     binary         0.864\n\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class) %>% \n  # Pass to the summary() function\n  summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary         0.705\n 2 kap                  binary         0.278\n 3 sens                 binary         0.390\n 4 spec                 binary         0.864\n 5 ppv                  binary         0.593\n 6 npv                  binary         0.737\n 7 mcc                  binary         0.290\n 8 j_index              binary         0.254\n 9 bal_accuracy         binary         0.627\n10 detection_prevalence binary         0.221\n11 precision            binary         0.593\n12 recall               binary         0.390\n13 f_meas               binary         0.471"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#plotting-the-confusion-matrix",
    "href": "datacamp/tidymodels/tidymodels.html#plotting-the-confusion-matrix",
    "title": "Modeling with tidymodels in R",
    "section": "Plotting the confusion matrix",
    "text": "Plotting the confusion matrix\nCalculating performance metrics with the yardstick package provides insight into how well a classification model is performing on the test dataset. Most yardstick functions return a single number that summarizes classification performance.\nMany times, it is helpful to create visualizations of the confusion matrix to more easily communicate your results.\nIn this exercise, you will make a heat map and mosaic plot of the confusion matrix from your logistic regression model on the telecom_df dataset.\nYour model results tibble, telecom_results, has been loaded into your session.\n\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a heat map\n  autoplot(type = \"heatmap\")\n\n\n\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a mosaic plot\n  autoplot(type = \"mosaic\")"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#roc-curves-and-area-under-the-roc-curve",
    "href": "datacamp/tidymodels/tidymodels.html#roc-curves-and-area-under-the-roc-curve",
    "title": "Modeling with tidymodels in R",
    "section": "ROC curves and area under the ROC curve",
    "text": "ROC curves and area under the ROC curve\nROC curves are used to visualize the performance of a classification model across a range of probability thresholds. An ROC curve with the majority of points near the upper left corner of the plot indicates that a classification model is able to correctly predict both the positive and negative outcomes correctly across a wide range of probability thresholds.\nThe area under this curve provides a letter grade summary of model performance.\nIn this exercise, you will create an ROC curve from your logistic regression model results and calculate the area under the ROC curve with yardstick.\nYour model results tibble, telecom_results has been loaded into your session.\n\n# Calculate metrics across thresholds\nthreshold_df <- telecom_results %>% \n  roc_curve(truth = canceled_service, .pred_yes)\n\n# View results\nthreshold_df %>%\n  head()\n\n# A tibble: 6 × 3\n  .threshold specificity sensitivity\n       <dbl>       <dbl>       <dbl>\n1  -Inf          0                 1\n2     0.0175     0                 1\n3     0.0398     0.00617           1\n4     0.0415     0.0123            1\n5     0.0485     0.0185            1\n6     0.0525     0.0247            1\n\n# Plot ROC curve\nthreshold_df %>% \n  autoplot()\n\n\n\n# Calculate ROC AUC\nroc_auc(telecom_results, truth = canceled_service, .pred_yes)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.747\n\n\nStreamlining the modeling process The last_fit() function is designed to streamline the modeling workflow in tidymodels. Instead of training your model on the training data and building a results tibble using the test data, last_fit() accomplishes this with one function.\nIn this exercise, you will train the same logistic regression model as you fit in the previous exercises, except with the last_fit() function.\nYour data split object, telecom_split, and model specification, logistic_model, have been loaded into your session.\n\n# Train model with last_fit()\ntelecom_last_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n           split = telecom_split)\n\n# View test set metrics\ntelecom_last_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.705 Preprocessor1_Model1\n2 roc_auc  binary         0.747 Preprocessor1_Model1"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#collecting-predictions-and-creating-custom-metrics",
    "href": "datacamp/tidymodels/tidymodels.html#collecting-predictions-and-creating-custom-metrics",
    "title": "Modeling with tidymodels in R",
    "section": "Collecting predictions and creating custom metrics",
    "text": "Collecting predictions and creating custom metrics\nUsing the last_fit() modeling workflow also saves time in collecting model predictions. Instead of manually creating a tibble of model results, there are helper functions that extract this information automatically.\nIn this exercise, you will use your trained model, telecom_last_fit, to create a tibble of model results on the test dataset as well as calculate custom performance metrics.\nYou trained model, telecom_last_fit, has been loaded into this session.\n\n# Collect predictions\nlast_fit_results <- telecom_last_fit %>% \n  collect_predictions()\n\n# View results\nlast_fit_results %>%\n  head()\n\n# A tibble: 6 × 7\n  id               .pred_yes .pred_no  .row .pred_class canceled_service .config\n  <chr>                <dbl>    <dbl> <int> <fct>       <fct>            <chr>  \n1 train/test split     0.366    0.634     2 no          yes              Prepro…\n2 train/test split     0.143    0.857     4 no          yes              Prepro…\n3 train/test split     0.225    0.775     6 no          no               Prepro…\n4 train/test split     0.556    0.444     9 yes         yes              Prepro…\n5 train/test split     0.383    0.617    13 no          no               Prepro…\n6 train/test split     0.363    0.637    17 no          yes              Prepro…\n\n# Custom metrics function\nlast_fit_metrics <- metric_set(accuracy, sens,\n                               spec, roc_auc)\n\n# Calculate metrics\nlast_fit_metrics(last_fit_results,\n                 truth = canceled_service,\n                 estimate = .pred_class,\n                 .pred_yes)\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.705\n2 sens     binary         0.390\n3 spec     binary         0.864\n4 roc_auc  binary         0.747"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#complete-modeling-workflow",
    "href": "datacamp/tidymodels/tidymodels.html#complete-modeling-workflow",
    "title": "Modeling with tidymodels in R",
    "section": "Complete modeling workflow",
    "text": "Complete modeling workflow\nIn this exercise, you will use the last_fit() function to train a logistic regression model and evaluate its performance on the test data by assessing the ROC curve and the area under the ROC curve.\nSimilar to previous exercises, you will predict canceled_service in the telecom_df data, but with an additional predictor variable to see if you can improve model performance.\nThe telecom_df tibble, telecom_split, and logistic_model objects from the previous exercises have been loaded into your workspace. The telecom_split object contains the instructions for randomly splitting the telecom_df tibble into training and test sets. The logistic_model object is a parsnip specification of a logistic regression model.\n\n# Train a logistic regression model\nlogistic_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins + avg_intl_mins + monthly_charges + months_with_company, \n           split = telecom_split)\n\n# Collect metrics\nlogistic_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.787 Preprocessor1_Model1\n2 roc_auc  binary         0.845 Preprocessor1_Model1\n\n# Collect model predictions\nlogistic_fit %>% \n  collect_predictions() %>% \n  # Plot ROC curve\n  roc_curve(truth = canceled_service, .pred_yes) %>% \n  autoplot()"
  },
  {
    "objectID": "datacamp/efficient_R_code/efficient_R_code.html",
    "href": "datacamp/efficient_R_code/efficient_R_code.html",
    "title": "Writing Efficient R Code",
    "section": "",
    "text": "One of the relatively easy optimizations available is to use an up-to-date version of R. In general, R is very conservative, so upgrading doesn’t break existing code. However, a new version will often provide free speed boosts for key functions.\nThe version command returns a list that contains (among other things) the major and minor version of R currently being used.\n\n# Print the R version details using version\nversion\n\n               _                                          \nplatform       x86_64-pc-linux-gnu                        \narch           x86_64                                     \nos             linux-gnu                                  \nsystem         x86_64, linux-gnu                          \nstatus         Patched                                    \nmajor          4                                          \nminor          2.2                                        \nyear           2022                                       \nmonth          11                                         \nday            10                                         \nsvn rev        83330                                      \nlanguage       R                                          \nversion.string R version 4.2.2 Patched (2022-11-10 r83330)\nnickname       Innocent and Trusting                      \n\n# Assign the variable major to the major component\nmajor <- version$major\n\n# Assign the variable minor to the minor component\nminor <- version$minor"
  },
  {
    "objectID": "datacamp/efficient_R_code/efficient_R_code.html#comparing-read-times-of-csv-and-rds-files",
    "href": "datacamp/efficient_R_code/efficient_R_code.html#comparing-read-times-of-csv-and-rds-files",
    "title": "Writing Efficient R Code",
    "section": "Comparing read times of CSV and RDS files",
    "text": "Comparing read times of CSV and RDS files\nOne of the most common tasks we perform is reading in data from CSV files. However, for large CSV files this can be slow. One neat trick is to read in the data and save as an R binary file (rds) using saveRDS(). To read in the rds file, we use readRDS().\nNote: Since rds is R’s native format for storing single objects, you have not introduced any third-party dependencies that may change in the future.\nTo benchmark the two approaches, you can use system.time(). This function returns the time taken to evaluate any R expression. For example, to time how long it takes to calculate the square root of the numbers from one to ten million, you would write the following:\n\n# How long does it take to read movies from CSV?\nsystem.time(read.csv(\"movies.csv\"))\n\n   user  system elapsed \n  0.122   0.012   0.134 \n\n# How long does it take to read movies from RDS?\nsystem.time(readRDS(\"movies.rds\"))\n\n   user  system elapsed \n  0.031   0.000   0.031"
  },
  {
    "objectID": "datacamp/efficient_R_code/efficient_R_code.html#elapsed-time",
    "href": "datacamp/efficient_R_code/efficient_R_code.html#elapsed-time",
    "title": "Writing Efficient R Code",
    "section": "Elapsed time",
    "text": "Elapsed time\nUsing system.time() is convenient, but it does have its drawbacks when comparing multiple function calls. The microbenchmark package solves this problem with the microbenchmark() function.\n\n# Load the microbenchmark package\nlibrary(microbenchmark)\n\n# Compare the two functions\ncompare <- microbenchmark(read.csv(\"movies.csv\"), \n                          readRDS(\"movies.rds\"), \n                          times = 100)\n\n# Print compare\ncompare\n\nUnit: milliseconds\n                   expr       min        lq      mean    median        uq\n read.csv(\"movies.csv\") 123.26020 126.89352 134.24088 129.80037 133.89713\n  readRDS(\"movies.rds\")  29.68107  30.55256  31.89255  31.08048  32.63884\n       max neval\n 165.80678   100\n  57.83575   100\n\n\nMy hardware For many problems your time is the expensive part. If having a faster computer makes you more productive, it can be cost effective to buy one. However, before you splash out on new toys for yourself, your boss/partner may want to see some numbers to justify the expense. Measuring the performance of your computer is called benchmarking, and you can do that with the benchmarkme package.\n\n# Load the benchmarkme package\nlibrary(benchmarkme)\n\n# Assign the variable ram to the amount of RAM on this machine\nram <- get_ram()\nram\n\n16.5 GB\n\n# Assign the variable cpu to the cpu specs\ncpu <- get_cpu()\ncpu\n\n$vendor_id\n[1] \"GenuineIntel\"\n\n$model_name\n[1] \"11th Gen Intel(R) Core(TM) i7-11370H @ 3.30GHz\"\n\n$no_of_cores\n[1] 8"
  },
  {
    "objectID": "datacamp/efficient_R_code/efficient_R_code.html#benchmark-datacamps-machine",
    "href": "datacamp/efficient_R_code/efficient_R_code.html#benchmark-datacamps-machine",
    "title": "Writing Efficient R Code",
    "section": "Benchmark DataCamp’s machine",
    "text": "Benchmark DataCamp’s machine\nThe benchmarkme package allows you to run a set of standardized benchmarks and compare your results to other users. One set of benchmarks tests is reading and writing speeds.\nThe function call\nres = benchmark_io(runs = 1, size = 5) records the length of time it takes to read and write a 5MB file.\n\n# Run the io benchmark\nres <- benchmark_io(runs = 1, size = 50)\n\nPreparing read/write io\n\n\n# IO benchmarks (2 tests) for size 50 MB:\n\n\n     Writing a csv with 6250000 values: 3.9 (sec).\n\n\n     Reading a csv with 6250000 values: 1.36 (sec).\n\n# Plot the results\nplot(res)\n\nYou are ranked 1 out of 119 machines.\n\n\nPress return to get next plot \n\n\nYou are ranked 2 out of 119 machines."
  },
  {
    "objectID": "datacamp/efficient_R_code/efficient_R_code.html#benchmark-r-operations",
    "href": "datacamp/efficient_R_code/efficient_R_code.html#benchmark-r-operations",
    "title": "Writing Efficient R Code",
    "section": "Benchmark r operations",
    "text": "Benchmark r operations\n\n# Run each benchmark 3 times\nres <- benchmark_std(runs = 10)\n\n# Programming benchmarks (5 tests):\n\n\n    3,500,000 Fibonacci numbers calculation (vector calc): 0.114 (sec).\n\n\n    Grand common divisors of 1,000,000 pairs (recursion): 0.355 (sec).\n\n\n    Creation of a 3,500 x 3,500 Hilbert matrix (matrix calc): 0.157 (sec).\n\n\n    Creation of a 3,000 x 3,000 Toeplitz matrix (loops): 0.728 (sec).\n\n\n    Escoufier's method on a 60 x 60 matrix (mixed): 0.534 (sec).\n\n\n# Matrix calculation benchmarks (5 tests):\n\n\n    Creation, transp., deformation of a 5,000 x 5,000 matrix: 0.271 (sec).\n\n\n    2,500 x 2,500 normal distributed random matrix^1,000: 0.143 (sec).\n\n\n    Sorting of 7,000,000 random values: 0.511 (sec).\n\n\n    2,500 x 2,500 cross-product matrix (b = a' * a): 0.932 (sec).\n\n\n    Linear regr. over a 5,000 x 500 matrix (c = a \\ b'): 0.088 (sec).\n\n\n# Matrix function benchmarks (5 tests):\n\n\n    Cholesky decomposition of a 3,000 x 3,000 matrix: 0.571 (sec).\n\n\n    Determinant of a 2,500 x 2,500 random matrix: 0.653 (sec).\n\n\n    Eigenvalues of a 640 x 640 random matrix: 0.276 (sec).\n\n\n    FFT over 2,500,000 random values: 0.166 (sec).\n\n\n    Inverse of a 1,600 x 1,600 random matrix: 0.558 (sec).\n\nplot(res)\n\nYou are ranked 1 out of 749 machines.\n\n\nPress return to get next plot \n\n\nYou are ranked 2 out of 747 machines.\n\n\n\n\n\nPress return to get next plot \n\n\nYou are ranked 46 out of 747 machines."
  },
  {
    "objectID": "datacamp/efficient_R_code/efficient_R_code.html#timings---growing-a-vector",
    "href": "datacamp/efficient_R_code/efficient_R_code.html#timings---growing-a-vector",
    "title": "Writing Efficient R Code",
    "section": "Timings - growing a vector",
    "text": "Timings - growing a vector\nGrowing a vector is one of the deadly sins in R; you should always avoid it.\nThe growing() function defined below generates n random standard normal numbers, but grows the size of the vector each time an element is added!\nNote: Standard normal numbers are numbers drawn from a normal distribution with mean 0 and standard deviation 1.\nn <- 30000 # Slow code growing <- function(n) { x <- NULL for(i in 1:n) x <- c(x, rnorm(1)) x }\n\ngrowing <- function(n) {\n    x = NULL\n    for(i in 1:n) \n        x = c(x, rnorm(1))\n    x\n}\n\n# Use <- with system.time() to store the result as res_grow\nsystem.time(res_grow <- growing(30000))\n\n   user  system elapsed \n  0.643   0.000   0.642 \n\n\nTimings - pre-allocation In the previous exercise, growing the vector took around 2 seconds. How long does it take when we pre-allocate the vector? The pre_allocate() function is defined below.\n\nn <- 30000\n# Fast code\npre_allocate <- function(n) {\n    x <- numeric(n) # Pre-allocate\n    for(i in 1:n) \n        x[i] <- rnorm(1)\n    x\n}\n\n\n# Use <- with system.time() to store the result as res_allocate\nn <- 30000\nsystem.time(res_allocate <- pre_allocate(n))\n\n   user  system elapsed \n  0.033   0.000   0.033"
  },
  {
    "objectID": "datacamp/efficient_R_code/efficient_R_code.html#vectorized-code-multiplication",
    "href": "datacamp/efficient_R_code/efficient_R_code.html#vectorized-code-multiplication",
    "title": "Writing Efficient R Code",
    "section": "Vectorized code: multiplication",
    "text": "Vectorized code: multiplication\nThe following piece of code is written like traditional C or Fortran code. Instead of using the vectorized version of multiplication, it uses a for loop.\nYour job is to make this code more “R-like” by vectorizing it.\n\nx <- rnorm(10)\nx2 <- numeric(length(x))\nfor(i in 1:10)\n    x2[i] <- x[i] * x[i]\n# Store your answer as x2_imp\nx2_imp <- x*x"
  },
  {
    "objectID": "datacamp/efficient_R_code/efficient_R_code.html#vectorized-code-calculating-a-log-sum",
    "href": "datacamp/efficient_R_code/efficient_R_code.html#vectorized-code-calculating-a-log-sum",
    "title": "Writing Efficient R Code",
    "section": "Vectorized code: calculating a log-sum",
    "text": "Vectorized code: calculating a log-sum\nA common operation in statistics is to calculate the sum of log probabilities. The following code calculates the log-sum (the sum of the logs)."
  },
  {
    "objectID": "datacamp/efficient_R_code/efficient_R_code.html#data-frames-and-matrices---column-selection",
    "href": "datacamp/efficient_R_code/efficient_R_code.html#data-frames-and-matrices---column-selection",
    "title": "Writing Efficient R Code",
    "section": "Data frames and matrices - column selection",
    "text": "Data frames and matrices - column selection\nAll values in a matrix must have the same data type, which has efficiency implications when selecting rows and columns.\nSuppose we have two objects, mat (a matrix) and df (a data frame).\n\n# Which is faster, mat[, 1] or df[, 1]? \nmicrobenchmark(mat[, 1], df[, 1])"
  },
  {
    "objectID": "datacamp/efficient_R_code/efficient_R_code.html#row-timings",
    "href": "datacamp/efficient_R_code/efficient_R_code.html#row-timings",
    "title": "Writing Efficient R Code",
    "section": "Row timings",
    "text": "Row timings\nSimilar to the previous example, the objects mat and df are a matrix and a data frame, respectively. Using microbenchmark(), how long does it take to select the first row from each of these objects?\nTo make the comparison fair, just use mat[1, ] and df[1, ].\n\nInteresting! Accessing a row of a data frame is much slower than accessing that of a matrix, more so than when accessing a column from each data type. This is because the values of a column of a data frame must be the same data type, whereas that of a row doesn’t have to be. Do you see the pattern here?\n\n\n# Which is faster, mat[, 1] or df[, 1]? \nmicrobenchmark(mat[1, ], df[1, ])"
  },
  {
    "objectID": "datacamp/efficient_R_code/efficient_R_code.html#profvis-in-action",
    "href": "datacamp/efficient_R_code/efficient_R_code.html#profvis-in-action",
    "title": "Writing Efficient R Code",
    "section": "Profvis in action",
    "text": "Profvis in action\nExamine the code on the right that performs a standard data analysis. It loads and selects data, plots the data of interest, and adds in a regression line.\n\n# Load the data set\ndata(movies, package = \"ggplot2movies\") \n\n# Load the profvis package\nlibrary(profvis)\n\n# Profile the following code with the profvis function\nprofvis({\n  # Load and select data\n  comedies <- movies[movies$Comedy == 1, ]\n\n  # Plot data of interest\n  plot(comedies$year, comedies$rating)\n\n  # Loess regression line\n  model <- loess(rating ~ year, data = comedies)\n  j <- order(comedies$year)\n  \n  # Add fitted line to the plot\n  lines(comedies$year[j], model$fitted[j], col = \"red\")\n})    ## Remember the closing brackets!"
  },
  {
    "objectID": "datacamp/network-analysis-in-r/network_analysis_r.html",
    "href": "datacamp/network-analysis-in-r/network_analysis_r.html",
    "title": "Network analysis in r",
    "section": "",
    "text": "Here you will learn how to create an igraph ‘object’ from data stored in an edgelist. The data are friendships in a group of students. You will also learn how to make a basic visualization of the network.\nEach row of the friends dataframe represents an edge in the network.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(igraph)\nlibrary(knitr)\nfriends <- fread(\"friends.csv\")\n\n\n# Inspect the first few rows of the dataframe 'friends'\nhead(friends) %>% kable\n\n# Convert friends dataframe to a matrix\nfriends.mat <- as.matrix(friends)\n\n# Convert friends matrix to an igraph object\ng <- graph.edgelist(friends.mat, directed = FALSE)\n\n\n# Make a very basic plot of the network\nplot(g)"
  },
  {
    "objectID": "datacamp/network-analysis-in-r/network_analysis_r.html#neighbors",
    "href": "datacamp/network-analysis-in-r/network_analysis_r.html#neighbors",
    "title": "Network analysis in r",
    "section": "Neighbors",
    "text": "Neighbors\nOften in network analysis it is important to explore the patterning of connections that exist between vertices. One way is to identify neighboring vertices of each vertex. You can then determine which neighboring vertices are shared even by unconnected vertices indicating how two vertices may have an indirect relationship through others. In this exercise you will learn how to identify neighbors and shared neighbors between pairs of vertices.\n\n# Identify all neighbors of vertex 12 regardless of direction\nneighbors(g, '12', mode = c('all'))\n\n# Identify other vertices that direct edges towards vertex 12\nneighbors(g, '12', mode = c('in'))\n\n# Identify any vertices that receive an edge from vertex 42 and direct an edge to vertex 124\nn1 <-neighbors(g, '42', mode = c('out'))\nn2 <- neighbors(g, '124', mode = c('in'))\nintersection(n1, n2)"
  },
  {
    "objectID": "datacamp/network-analysis-in-r/network_analysis_r.html#finding-longest-path-between-two-vertices",
    "href": "datacamp/network-analysis-in-r/network_analysis_r.html#finding-longest-path-between-two-vertices",
    "title": "Network analysis in r",
    "section": "Finding longest path between two vertices",
    "text": "Finding longest path between two vertices\n\nWhat is the longest possible path in a network referred to as?\nDiameter"
  },
  {
    "objectID": "datacamp/network-analysis-in-r/network_analysis_r.html#network-density-and-average-path-length",
    "href": "datacamp/network-analysis-in-r/network_analysis_r.html#network-density-and-average-path-length",
    "title": "Network analysis in r",
    "section": "Network density and average path length",
    "text": "Network density and average path length\nThe first graph level metric you will explore is the density of a graph. This is essentially the proportion of all potential edges between vertices that actually exist in the network graph. It is an indicator of how well connected the vertices of the graph are.\nAnother measure of how interconnected a network is average path length. This is calculated by determining the mean of the lengths of the shortest paths between all pairs of vertices in the network. The longest path length between any pair of vertices is called the diameter of the network graph. You will calculate the diameter and average path length of the original graph g.\n\n# Get density of a graph\ngd <- edge_density(g)\n\n# Get the diameter of the graph g\ndiameter(g, directed = FALSE)\n\n# Get the average path length of the graph g\ng.apl <- mean_distance(g, directed = FALSE)\ng.apl\n\n\nIf a graph has 7 vertices there are 21 possible edges in the network. If 14 edges exist, what is the density of the network?\n0.67"
  },
  {
    "objectID": "datacamp/network-analysis-in-r/network_analysis_r.html#randomization-quiz",
    "href": "datacamp/network-analysis-in-r/network_analysis_r.html#randomization-quiz",
    "title": "Network analysis in r",
    "section": "Randomization quiz",
    "text": "Randomization quiz\n\nRandomization tests enable you to identify:\nWhether features of your original network are particularly unusual."
  },
  {
    "objectID": "datacamp/network-analysis-in-r/network_analysis_r.html#what-does-assortativity-measure",
    "href": "datacamp/network-analysis-in-r/network_analysis_r.html#what-does-assortativity-measure",
    "title": "Network analysis in r",
    "section": "What does assortativity measure",
    "text": "What does assortativity measure\n\nHow likely vertices are to connect to others that share some attribute in common."
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html",
    "title": "ML with tree based models in r",
    "section": "",
    "text": "Let’s get started and build our first classification tree. A classification tree is a decision tree that performs a classification (vs regression) task. You will train a decision tree model to understand which loan applications are at higher risk of default using a subset of the German Credit Dataset. The response variable, called “default”, indicates whether the loan went into a default or not, which means this is a binary classification problem (there are just two classes). You will use the rpart package to fit the decision tree and the rpart.plot package to visualize the tree.\n\n# Look at the data\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(rpart)\nlibrary(rpart.plot)\ncredit <- fread(\"credit.csv\", stringsAsFactors = TRUE) \ncredit[, default := factor(default)]\ncreditsub <- credit[sample(1:nrow(credit), 522),]\nstr(creditsub)\n\nClasses 'data.table' and 'data.frame':  522 obs. of  17 variables:\n $ checking_balance    : Factor w/ 4 levels \"1 - 200 DM\",\"< 0 DM\",..: 1 1 4 3 2 4 4 1 4 1 ...\n $ months_loan_duration: int  30 18 24 12 8 18 36 36 18 6 ...\n $ credit_history      : Factor w/ 5 levels \"critical\",\"good\",..: 5 2 4 5 1 2 4 2 2 1 ...\n $ purpose             : Factor w/ 6 levels \"business\",\"car\",..: 5 2 2 1 3 5 5 2 2 4 ...\n $ amount              : int  3496 2779 2538 609 1164 2051 4463 6948 2662 932 ...\n $ savings_balance     : Factor w/ 5 levels \"100 - 500 DM\",..: 4 3 3 3 3 3 3 3 5 5 ...\n $ employment_duration : Factor w/ 5 levels \"1 - 4 years\",..: 1 1 4 3 4 3 1 1 2 2 ...\n $ percent_of_income   : int  4 1 4 4 3 4 4 2 4 1 ...\n $ years_at_residence  : int  2 3 4 1 4 1 2 2 3 3 ...\n $ age                 : int  34 21 47 26 51 33 26 35 32 39 ...\n $ other_credit        : Factor w/ 3 levels \"bank\",\"none\",..: 3 2 2 2 1 2 2 2 2 2 ...\n $ housing             : Factor w/ 3 levels \"other\",\"own\",..: 2 3 2 2 1 2 2 3 2 2 ...\n $ existing_loans_count: int  1 1 2 1 2 1 2 1 1 2 ...\n $ job                 : Factor w/ 4 levels \"management\",\"skilled\",..: 2 2 4 3 1 2 1 1 2 4 ...\n $ dependents          : int  2 1 2 1 2 1 1 1 1 1 ...\n $ phone               : Factor w/ 2 levels \"no\",\"yes\": 2 2 1 1 2 1 2 2 1 1 ...\n $ default             : Factor w/ 2 levels \"no\",\"yes\": 1 1 2 2 1 1 2 1 1 1 ...\n - attr(*, \".internal.selfref\")=<externalptr> \n\n# Create the model\ncredit_model <- rpart(formula = default ~ ., \n                      data = creditsub, \n                      method = \"class\")\n\n# Display the results\nrpart.plot(x = credit_model, yesno = 2, type = 0, extra = 0)"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#traintest-split",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#traintest-split",
    "title": "ML with tree based models in r",
    "section": "Train/test split",
    "text": "Train/test split\nFor this exercise, you’ll randomly split the German Credit Dataset into two pieces: a training set (80%) called credit_train and a test set (20%) that we will call credit_test. We’ll use these two sets throughout the chapter.\n\n# Total number of rows in the credit data frame\nn <- nrow(credit)\n\n# Number of rows for the training set (80% of the dataset)\nn_train <- round(.8 * n) \n\n# Create a vector of indices which is an 80% random sample\nset.seed(123)\ntrain_indices <- sample(1:n, n_train)\n\n# Subset the credit data frame to training indices only\ncredit_train <- credit[train_indices, ]  \n  \n# Exclude the training indices to create the test set\ncredit_test <- credit[-train_indices, ]  \n\n# Train the model (to predict 'default')\ncredit_model <- rpart(formula = default ~., \n                      data = credit_train, \n                      method = \"class\")\n\n# Look at the model output                      \nprint(credit_model)\n\nn= 800 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 800 230 no (0.7125000 0.2875000)  \n    2) checking_balance=> 200 DM,unknown 365  48 no (0.8684932 0.1315068) *\n    3) checking_balance=1 - 200 DM,< 0 DM 435 182 no (0.5816092 0.4183908)  \n      6) months_loan_duration< 22.5 259  85 no (0.6718147 0.3281853)  \n       12) credit_history=critical,good,poor 235  68 no (0.7106383 0.2893617)  \n         24) months_loan_duration< 11.5 70  11 no (0.8428571 0.1571429) *\n         25) months_loan_duration>=11.5 165  57 no (0.6545455 0.3454545)  \n           50) amount>=1282 112  30 no (0.7321429 0.2678571) *\n           51) amount< 1282 53  26 yes (0.4905660 0.5094340)  \n            102) purpose=business,education,furniture/appliances 34  12 no (0.6470588 0.3529412) *\n            103) purpose=car,renovations 19   4 yes (0.2105263 0.7894737) *\n       13) credit_history=perfect,very good 24   7 yes (0.2916667 0.7083333) *\n      7) months_loan_duration>=22.5 176  79 yes (0.4488636 0.5511364)  \n       14) savings_balance=> 1000 DM,unknown 29   7 no (0.7586207 0.2413793) *\n       15) savings_balance=100 - 500 DM,500 - 1000 DM,< 100 DM 147  57 yes (0.3877551 0.6122449)  \n         30) months_loan_duration< 47.5 119  54 yes (0.4537815 0.5462185)  \n           60) amount>=2313.5 93  45 no (0.5161290 0.4838710)  \n            120) amount< 3026 19   5 no (0.7368421 0.2631579) *\n            121) amount>=3026 74  34 yes (0.4594595 0.5405405)  \n              242) percent_of_income< 2.5 38  15 no (0.6052632 0.3947368)  \n                484) purpose=business,car,education 23   6 no (0.7391304 0.2608696) *\n                485) purpose=car0,furniture/appliances,renovations 15   6 yes (0.4000000 0.6000000) *\n              243) percent_of_income>=2.5 36  11 yes (0.3055556 0.6944444) *\n           61) amount< 2313.5 26   6 yes (0.2307692 0.7692308) *\n         31) months_loan_duration>=47.5 28   3 yes (0.1071429 0.8928571) *"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#compute-confusion-matrix",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#compute-confusion-matrix",
    "title": "ML with tree based models in r",
    "section": "Compute confusion matrix",
    "text": "Compute confusion matrix\nAs discussed in the previous video, there are a number of different metrics by which you can measure the performance of a classification model. In this exercise, we will evaluate the performance of the model using test set classification error. A confusion matrix is a convenient way to examine the per-class error rates for all classes at once. The confusionMatrix() function from the caret package prints both the confusion matrix and a number of other useful classification metrics such as “Accuracy” (fraction of correctly classified instances). The caret package has been loaded for you.\n\nlibrary(caret)\n# Generate predicted classes using the model object\nclass_prediction <- predict(object = credit_model,  \n                        newdata = credit_test,   \n                        type = \"class\")  \n                            \n# Calculate the confusion matrix for the test set\n\nclass_prediction <- factor(class_prediction, levels = levels(credit_test$default) )\nconfusionMatrix(data = class_prediction,       \n                reference = credit_test$default, positive = \"yes\")  \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  117  44\n       yes  13  26\n                                          \n               Accuracy : 0.715           \n                 95% CI : (0.6471, 0.7764)\n    No Information Rate : 0.65            \n    P-Value [Acc > NIR] : 0.03046         \n                                          \n                  Kappa : 0.3023          \n                                          \n Mcnemar's Test P-Value : 7.08e-05        \n                                          \n            Sensitivity : 0.3714          \n            Specificity : 0.9000          \n         Pos Pred Value : 0.6667          \n         Neg Pred Value : 0.7267          \n             Prevalence : 0.3500          \n         Detection Rate : 0.1300          \n   Detection Prevalence : 0.1950          \n      Balanced Accuracy : 0.6357          \n                                          \n       'Positive' Class : yes"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#compare-models-with-a-different-splitting-criterion",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#compare-models-with-a-different-splitting-criterion",
    "title": "ML with tree based models in r",
    "section": "Compare models with a different splitting criterion",
    "text": "Compare models with a different splitting criterion\nTrain two models that use a different splitting criterion and use the validation set to choose a “best” model from this group. To do this you’ll use the parms argument of the rpart() function. This argument takes a named list that contains values of different parameters you can use to change how the model is trained. Set the parameter split to control the splitting criterion.\n\n# Train a gini-based model\ncredit_model1 <- rpart(formula = default ~ ., \n                       data = credit_train, \n                       method = \"class\",\n                       parms = list(split = \"gini\"))\n\n# Train an information-based model\ncredit_model2 <- rpart(formula = default ~ ., \n                       data = credit_train, \n                       method = \"class\",\n                       parms = list(split = \"information\"))\n\n# Generate predictions on the validation set using the gini model\npred1 <- predict(object = credit_model1, \n             newdata = credit_test,\n             type = \"class\")    \n\n# Generate predictions on the validation set using the information model\npred2 <- predict(object = credit_model2, \n             newdata = credit_test,\n             type = \"class\") \n\ndt_preds <- predict(object = credit_model2, \n             newdata = credit_test,\n             type = \"prob\") \n\n# Compare classification error\nlibrary(Metrics)\nce(actual = credit_test$default, \n   predicted = pred1)\n\n[1] 0.285\n\nce(actual = credit_test$default, \n   predicted = pred2)  \n\n[1] 0.285"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#the-response-is-final_grade-numeric-from-0-to-20-output-target.",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#the-response-is-final_grade-numeric-from-0-to-20-output-target.",
    "title": "ML with tree based models in r",
    "section": "The response is final_grade (numeric: from 0 to 20, output target).",
    "text": "The response is final_grade (numeric: from 0 to 20, output target).\nAfter initial exploration, split the data into training, validation, and test sets. In this chapter, we will introduce the idea of a validation set, which can be used to select a “best” model from a set of competing models. In Chapter 1, we demonstrated a simple way to split the data into two pieces using the sample() function. In this exercise, we will take a slightly different approach to splitting the data that allows us to split the data into more than two parts (here, we want three: train, validation, test). We still use the sample() function, but instead of sampling the indices themselves, we will assign each row to either the training, validation or test sets according to a probability distribution. The dataset grade is already in your workspace.\n\ngrade <- read.csv(\"grade.csv\")\n# Look at the data\nstr(grade)\n\n'data.frame':   395 obs. of  8 variables:\n $ final_grade: num  3 3 5 7.5 5 7.5 5.5 3 9.5 7.5 ...\n $ age        : int  18 17 15 15 16 16 16 17 15 15 ...\n $ address    : chr  \"U\" \"U\" \"U\" \"U\" ...\n $ studytime  : int  2 2 2 3 2 2 2 2 2 2 ...\n $ schoolsup  : chr  \"yes\" \"no\" \"yes\" \"no\" ...\n $ famsup     : chr  \"no\" \"yes\" \"no\" \"yes\" ...\n $ paid       : chr  \"no\" \"no\" \"yes\" \"yes\" ...\n $ absences   : int  6 4 10 2 4 10 0 6 0 0 ...\n\n# Set seed and create assignment\nset.seed(1)\nassignment <- sample(1:3, size = nrow(grade), prob = c(.7, .15, .15), replace = TRUE)\n\n# Create a train, validation and tests from the original data frame \ngrade_train <- grade[assignment == 1, ]    # subset grade to training indices only\ngrade_valid <- grade[assignment == 2, ]  # subset grade to validation indices only\ngrade_test <- grade[assignment == 3, ]   # subset grade to test indices only"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-regression-tree-model",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-regression-tree-model",
    "title": "ML with tree based models in r",
    "section": "Train a regression tree model",
    "text": "Train a regression tree model\nIn this exercise, we will use the grade_train dataset to fit a regression tree using rpart() and visualize it using rpart.plot(). A regression tree plot looks identical to a classification tree plot, with the exception that there will be numeric values in the leaf nodes instead of predicted classes. This is very similar to what we did previously in Chapter 1. When fitting a classification tree, we use method = “class”, however, when fitting a regression tree, we need to set method = “anova”. By default, the rpart() function will make an intelligent guess as to what the method value should be based on the data type of your response column, but it’s recommened that you explictly set the method for reproducibility reasons (since the auto-guesser may change in the future). The grade_train training set is loaded into the workspace.\n\n# Train the model\ngrade_model <- rpart(formula = final_grade ~ ., \n                     data = grade_train, \n                     method = \"anova\")\n\n# Look at the model output                      \nprint(grade_model)\n\nn= 282 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 282 1519.49700 5.271277  \n   2) absences< 0.5 82  884.18600 4.323171  \n     4) paid=no 50  565.50500 3.430000  \n       8) famsup=yes 22  226.36360 2.272727 *\n       9) famsup=no 28  286.52680 4.339286 *\n     5) paid=yes 32  216.46880 5.718750  \n      10) age>=17.5 10   82.90000 4.100000 *\n      11) age< 17.5 22   95.45455 6.454545 *\n   3) absences>=0.5 200  531.38000 5.660000  \n     6) absences>=13.5 42  111.61900 4.904762 *\n     7) absences< 13.5 158  389.43670 5.860759  \n      14) schoolsup=yes 23   50.21739 4.847826 *\n      15) schoolsup=no 135  311.60000 6.033333  \n        30) studytime< 3.5 127  276.30710 5.940945 *\n        31) studytime>=3.5 8   17.00000 7.500000 *\n\n# Plot the tree model\nrpart.plot(x = grade_model, yesno = 2, type = 0, extra = 0)"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-a-regression-tree-model",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-a-regression-tree-model",
    "title": "ML with tree based models in r",
    "section": "Evaluate a regression tree model",
    "text": "Evaluate a regression tree model\nPredict the final grade for all students in the test set. The grade is on a 0-20 scale. Evaluate the model based on test set RMSE (Root Mean Squared Error). RMSE tells us approximately how far away our predictions are from the true values.\n\n# Generate predictions on a test set\npred <- predict(object = grade_model,   # model object \n                newdata = grade_test)  # test dataset\n\n# Compute the RMSE\nrmse(actual = grade_test$final_grade, \n     predicted = pred)\n\n[1] 2.278249"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#tuning-the-model",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#tuning-the-model",
    "title": "ML with tree based models in r",
    "section": "Tuning the model",
    "text": "Tuning the model\nTune (or “trim”) the model using the prune() function by finding the best “CP” value (CP stands for “Complexity Parameter”).\n\n# Plot the \"CP Table\"\nplotcp(grade_model)\n\n\n\n# Print the \"CP Table\"\nprint(grade_model$cptable)\n\n          CP nsplit rel error    xerror       xstd\n1 0.06839852      0 1.0000000 1.0066743 0.09169976\n2 0.06726713      1 0.9316015 1.0185398 0.08663026\n3 0.03462630      2 0.8643344 0.8923588 0.07351895\n4 0.02508343      3 0.8297080 0.9046335 0.08045100\n5 0.01995676      4 0.8046246 0.8920489 0.08153881\n6 0.01817661      5 0.7846679 0.9042142 0.08283114\n7 0.01203879      6 0.7664912 0.8833557 0.07945742\n8 0.01000000      7 0.7544525 0.8987112 0.08200148\n\n# Retrieve optimal cp value based on cross-validated error\nopt_index <- which.min(grade_model$cptable[, \"xerror\"])\ncp_opt <- grade_model$cptable[opt_index, \"CP\"]\n\n# Prune the model (to optimized cp value)\ngrade_model_opt <- prune(tree = grade_model, \n                         cp = cp_opt)\n                          \n# Plot the optimized model\nrpart.plot(x = grade_model_opt, yesno = 2, type = 0, extra = 0)"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#generate-a-grid-of-hyperparameter-values",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#generate-a-grid-of-hyperparameter-values",
    "title": "ML with tree based models in r",
    "section": "Generate a grid of hyperparameter values",
    "text": "Generate a grid of hyperparameter values\nUse expand.grid() to generate a grid of maxdepth and minsplit values.\n\n# Establish a list of possible values for minsplit and maxdepth\nminsplit <- seq(1, 4, 1)\nmaxdepth <- seq(1, 6, 1)\n\n# Create a data frame containing all combinations \nhyper_grid <- expand.grid(minsplit = minsplit, maxdepth = maxdepth)\n\n# Check out the grid\nhead(hyper_grid)\n\n  minsplit maxdepth\n1        1        1\n2        2        1\n3        3        1\n4        4        1\n5        1        2\n6        2        2\n\n# Print the number of grid combinations\nnrow(hyper_grid)\n\n[1] 24"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#generate-a-grid-of-models",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#generate-a-grid-of-models",
    "title": "ML with tree based models in r",
    "section": "Generate a grid of models",
    "text": "Generate a grid of models\nIn this exercise, we will write a simple loop to train a “grid” of models and store the models in a list called grade_models. R users who are familiar with the apply functions in R could think about how this loop could be easily converted into a function applied to a list as an extra-credit thought experiment.\n\n# Number of potential models in the grid\nnum_models <- nrow(hyper_grid)\n\n# Create an empty list to store models\ngrade_models <- list()\n\n# Write a loop over the rows of hyper_grid to train the grid of models\nfor (i in 1:num_models) {\n\n    # Get minsplit, maxdepth values at row i\n    minsplit <- hyper_grid$minsplit[i]\n    maxdepth <- hyper_grid$maxdepth[i]\n\n    # Train a model and store in the list\n    grade_models[[i]] <- rpart(formula = final_grade ~ ., \n                               data = grade_train, \n                               method = \"anova\",\n                               minsplit = minsplit,\n                               maxdepth = maxdepth)\n}\n\nEvaluate the grid Earlier in the chapter we split the dataset into three parts: training, validation and test.\nA dataset that is not used in training is sometimes referred to as a “holdout” set. A holdout set is used to estimate model performance and although both validation and test sets are considered to be holdout data, there is a key difference:\nJust like a test set, a validation set is used to evaluate the performance of a model. The difference is that a validation set is specifically used to compare the performance of a group of models with the goal of choosing a “best model” from the group. All the models in a group are evaluated on the same validation set and the model with the best performance is considered to be the winner. Once you have the best model, a final estimate of performance is computed on the test set. A test set should only ever be used to estimate model performance and should not be used in model selection. Typically if you use a test set more than once, you are probably doing something wrong.\n\n# Number of potential models in the grid\nnum_models <- length(grade_models)\n\n# Create an empty vector to store RMSE values\nrmse_values <- c()\n\n# Write a loop over the models to compute validation RMSE\nfor (i in 1:num_models) {\n\n    # Retrieve the i^th model from the list\n    model <- grade_models[[i]]\n    \n    # Generate predictions on grade_valid \n    pred <- predict(object = model,\n                    newdata = grade_valid)\n    \n    # Compute validation RMSE and add to the \n    rmse_values[i] <- rmse(actual = grade_valid$final_grade, \n                           predicted = pred)\n}\n\n# Identify the model with smallest validation set RMSE\nbest_model <- grade_models[[which.min(rmse_values)]]\n\n# Print the model paramters of the best model\nbest_model$control\n\n$minsplit\n[1] 2\n\n$minbucket\n[1] 1\n\n$cp\n[1] 0.01\n\n$maxcompete\n[1] 4\n\n$maxsurrogate\n[1] 5\n\n$usesurrogate\n[1] 2\n\n$surrogatestyle\n[1] 0\n\n$maxdepth\n[1] 1\n\n$xval\n[1] 10\n\n# Compute test set RMSE on best_model\npred <- predict(object = best_model,\n                newdata = grade_test)\nrmse(actual = grade_test$final_grade, \n     predicted = pred)\n\n[1] 2.124109"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-bagged-tree-model",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-bagged-tree-model",
    "title": "ML with tree based models in r",
    "section": "Train a bagged tree model",
    "text": "Train a bagged tree model\nLet’s start by training a bagged tree model. You’ll be using the bagging() function from the ipred package. The number of bagged trees can be specified using the nbagg parameter, but here we will use the default (25). If we want to estimate the model’s accuracy using the “out-of-bag” (OOB) samples, we can set the the coob parameter to TRUE. The OOB samples are the training obsevations that were not selected into the bootstrapped sample (used in training). Since these observations were not used in training, we can use them instead to evaluate the accuracy of the model (done automatically inside the bagging() function).\n\nlibrary(ipred)\n# Bagging is a randomized model, so let's set a seed (123) for reproducibility\nset.seed(123)\n\n# Train a bagged model\ncredit_model <- bagging(formula = default ~ ., \n                        data = credit_train,\n                        coob = TRUE)\n\n# Print the model\nprint(credit_model)\n\n\nBagging classification trees with 25 bootstrap replications \n\nCall: bagging.data.frame(formula = default ~ ., data = credit_train, \n    coob = TRUE)\n\nOut-of-bag estimate of misclassification error:  0.2537"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#prediction-and-confusion-matrix",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#prediction-and-confusion-matrix",
    "title": "ML with tree based models in r",
    "section": "Prediction and confusion matrix",
    "text": "Prediction and confusion matrix\nAs you saw in the video, a confusion matrix is a very useful tool for examining all possible outcomes of your predictions (true positive, true negative, false positive, false negative). In this exercise, you will predict those who will default using bagged trees. You will also create the confusion matrix using the confusionMatrix() function from the caret package. It’s always good to take a look at the output using the print() function.\n\n# Generate predicted classes using the model object\nclass_prediction <- predict(object = credit_model,    \n                            newdata = credit_test,  \n                            type = \"class\")  # return classification labels\n\n# Print the predicted classes\nprint(class_prediction)\n\n  [1] no  no  no  no  yes no  no  no  no  no  no  no  no  yes no  no  no  no \n [19] no  no  yes no  no  no  no  no  yes no  no  no  no  no  no  no  no  no \n [37] yes yes no  yes no  yes no  no  no  no  no  no  no  yes no  yes no  yes\n [55] yes no  yes no  yes no  no  yes no  no  yes yes no  yes no  no  no  yes\n [73] yes no  no  no  no  no  no  yes no  no  no  no  yes no  no  yes no  no \n [91] no  no  no  yes yes no  no  no  no  no  no  yes no  no  yes no  no  no \n[109] no  no  no  no  no  no  no  no  no  no  no  no  yes no  yes no  no  yes\n[127] yes no  yes no  no  no  no  no  yes no  yes yes no  no  no  no  yes no \n[145] no  no  yes no  no  no  no  yes no  no  no  no  no  no  no  yes no  no \n[163] yes no  yes no  no  no  no  no  no  no  no  no  no  no  no  no  no  no \n[181] no  no  yes yes yes no  yes no  no  no  no  no  yes no  no  no  yes no \n[199] no  yes\nLevels: no yes\n\n# Calculate the confusion matrix for the test set\nconfusionMatrix(data =  class_prediction,     \n                reference =  credit_test$default, positive = \"yes\")  \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  119  33\n       yes  11  37\n                                          \n               Accuracy : 0.78            \n                 95% CI : (0.7161, 0.8354)\n    No Information Rate : 0.65            \n    P-Value [Acc > NIR] : 4.557e-05       \n                                          \n                  Kappa : 0.4787          \n                                          \n Mcnemar's Test P-Value : 0.001546        \n                                          \n            Sensitivity : 0.5286          \n            Specificity : 0.9154          \n         Pos Pred Value : 0.7708          \n         Neg Pred Value : 0.7829          \n             Prevalence : 0.3500          \n         Detection Rate : 0.1850          \n   Detection Prevalence : 0.2400          \n      Balanced Accuracy : 0.7220          \n                                          \n       'Positive' Class : yes"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#predict-on-a-test-set-and-compute-auc",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#predict-on-a-test-set-and-compute-auc",
    "title": "ML with tree based models in r",
    "section": "Predict on a test set and compute AUC",
    "text": "Predict on a test set and compute AUC\nIn binary classification problems, we can predict numeric values instead of class labels. In fact, class labels are created only after you use the model to predict a raw, numeric, predicted value for a test point. The predicted label is generated by applying a threshold to the predicted value, such that all tests points with predicted value greater than that threshold get a predicted label of “1” and, points below that threshold get a predicted label of “0”. In this exercise, generate predicted values (rather than class labels) on the test set and evaluate performance based on AUC (Area Under the ROC Curve). The AUC is a common metric for evaluating the discriminatory ability of a binary classification model.\n\n# Generate predictions on the test set\npred <- predict(object = credit_model,\n                newdata = credit_test,\n                type = \"prob\")\n\n# `pred` is a matrix\nclass(pred)\n\n[1] \"matrix\" \"array\" \n\n# Look at the pred format\nhead(pred)\n\n       no  yes\n[1,] 0.92 0.08\n[2,] 0.92 0.08\n[3,] 1.00 0.00\n[4,] 1.00 0.00\n[5,] 0.16 0.84\n[6,] 0.84 0.16\n\n# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)\ncredit_ipred_model_test_auc <- auc(actual = ifelse(credit_test$default == \"yes\", 1, 0), \n    predicted = pred[,\"yes\"])  \n\ncredit_ipred_model_test_auc\n\n[1] 0.8084066"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#cross-validate-a-bagged-tree-model-in-caret",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#cross-validate-a-bagged-tree-model-in-caret",
    "title": "ML with tree based models in r",
    "section": "Cross-validate a bagged tree model in caret",
    "text": "Cross-validate a bagged tree model in caret\nUse caret::train() with the “treebag” method to train a model and evaluate the model using cross-validated AUC. The caret package allows the user to easily cross-validate any model across any relevant performance metric. In this case, we will use 5-fold cross validation and evaluate cross-validated AUC (Area Under the ROC Curve).\n\n# Specify the training configuration\nctrl_bag <- trainControl(method = \"cv\",     # Cross-validation\n                     number = 5,      # 5 folds\n                     classProbs = TRUE,                  # For AUC\n                     summaryFunction = twoClassSummary)  # For AUC\n\n# Cross validate the credit model using \"treebag\" method; \n# Track AUC (Area under the ROC curve)\nset.seed(1)  # for reproducibility\ncredit_caret_model <- train(default ~ .,\n                            data = credit_train, \n                            method = \"treebag\",\n                            metric = \"ROC\",\n                            trControl = ctrl_bag)\n\n# Look at the model object\nprint(credit_caret_model)\n\nBagged CART \n\n800 samples\n 16 predictor\n  2 classes: 'no', 'yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 640, 640, 640, 640, 640 \nResampling results:\n\n  ROC       Sens       Spec     \n  0.744508  0.8736842  0.4173913\n\n# Inspect the contents of the model list \nnames(credit_caret_model)\n\n [1] \"method\"       \"modelInfo\"    \"modelType\"    \"results\"      \"pred\"        \n [6] \"bestTune\"     \"call\"         \"dots\"         \"metric\"       \"control\"     \n[11] \"finalModel\"   \"preProcess\"   \"trainingData\" \"ptype\"        \"resample\"    \n[16] \"resampledCM\"  \"perfNames\"    \"maximize\"     \"yLimits\"      \"times\"       \n[21] \"levels\"       \"terms\"        \"coefnames\"    \"contrasts\"    \"xlevels\"     \n\n# Print the CV AUC\ncredit_caret_model$results[,\"ROC\"]\n\n[1] 0.744508"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#generate-predictions-from-the-caret-model",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#generate-predictions-from-the-caret-model",
    "title": "ML with tree based models in r",
    "section": "Generate predictions from the caret model",
    "text": "Generate predictions from the caret model\nGenerate predictions on a test set for the caret model.\n\n# Generate predictions on the test set\nbag_preds <- predict(object = credit_caret_model, \n                newdata = credit_test,\n                type = \"prob\")\n\n# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)\ncredit_caret_model_test_auc <- auc(actual = ifelse(credit_test$default == \"yes\", 1, 0), \n                    predicted = pred[,\"yes\"])\n\ncredit_caret_model_test_auc\n\n[1] 0.8084066"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#compare-test-set-performance-to-cv-performance",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#compare-test-set-performance-to-cv-performance",
    "title": "ML with tree based models in r",
    "section": "Compare test set performance to CV performance",
    "text": "Compare test set performance to CV performance\nIn this excercise, you will print test set AUC estimates that you computed in previous exercises. These two methods use the same code underneath, so the estimates should be very similar.\nThe credit_ipred_model_test_auc object stores the test set AUC from the model trained using the ipred::bagging() function. The credit_caret_model_test_auc object stores the test set AUC from the model trained using the caret::train() function with method = “treebag”. Lastly, we will print the 5-fold cross-validated estimate of AUC that is stored within the credit_caret_model object. This number will be a more accurate estimate of the true model performance since we have averaged the performance over five models instead of just one.\nOn small datasets like this one, the difference between test set model performance estimates and cross-validated model performance estimates will tend to be more pronounced. When using small data, it’s recommended to use cross-validated estimates of performance because they are more stable.\n\n# Print ipred::bagging test set AUC estimate\nprint(credit_ipred_model_test_auc)\n\n[1] 0.8084066\n\n# Print caret \"treebag\" test set AUC estimate\nprint(credit_caret_model_test_auc)\n\n[1] 0.8084066\n\n# Compare to caret 5-fold cross-validated AUC\ncredit_caret_model$results[, \"ROC\"]\n\n[1] 0.744508"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-random-forest-model",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-random-forest-model",
    "title": "ML with tree based models in r",
    "section": "Train a Random Forest model",
    "text": "Train a Random Forest model\nHere you will use the randomForest() function from the randomForest package to train a Random Forest classifier to predict loan default.\n\nlibrary(randomForest)\n# Train a Random Forest\nset.seed(1)  # for reproducibility\ncredit_model <- randomForest(formula = default ~ ., \n                             data = credit_train)\n                             \n# Print the model output                             \nprint(credit_model)\n\n\nCall:\n randomForest(formula = default ~ ., data = credit_train) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n        OOB estimate of  error rate: 24.12%\nConfusion matrix:\n     no yes class.error\nno  521  49  0.08596491\nyes 144  86  0.62608696"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-out-of-bag-error",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-out-of-bag-error",
    "title": "ML with tree based models in r",
    "section": "Evaluate out-of-bag error",
    "text": "Evaluate out-of-bag error\nHere you will plot the OOB error as a function of the number of trees trained, and extract the final OOB error of the Random Forest model from the trained model object.\n\n# Grab OOB error matrix & take a look\nerr <- credit_model$err.rate\nhead(err)\n\n           OOB        no       yes\n[1,] 0.3310105 0.2400000 0.5402299\n[2,] 0.3519313 0.2283951 0.6338028\n[3,] 0.3164129 0.1912833 0.6067416\n[4,] 0.3130564 0.1886792 0.6142132\n[5,] 0.3039890 0.1776062 0.6172249\n[6,] 0.2957560 0.1713222 0.6036866\n\n# Look at final OOB error rate (last row in err matrix)\noob_err <- err[nrow(err), \"OOB\"]\nprint(oob_err)\n\n    OOB \n0.24125 \n\n# Plot the model trained in the previous exercise\nplot(credit_model)\n\n# Add a legend since it doesn't have one by default\nlegend(x = \"right\", \n       legend = colnames(err),\n       fill = 1:ncol(err))"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-model-performance-on-a-test-set",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-model-performance-on-a-test-set",
    "title": "ML with tree based models in r",
    "section": "Evaluate model performance on a test set",
    "text": "Evaluate model performance on a test set\nUse the caret::confusionMatrix() function to compute test set accuracy and generate a confusion matrix. Compare the test set accuracy to the OOB accuracy.\n\n# Generate predicted classes using the model object\nclass_prediction <- predict(object = credit_model,   # model object \n                            newdata = credit_test,  # test dataset\n                            type = \"class\") # return classification labels\n                            \n# Calculate the confusion matrix for the test set\ncm <- confusionMatrix(data = class_prediction,       # predicted classes\n                      reference = credit_test$default, positive = \"yes\")  # actual classes\nprint(cm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  121  39\n       yes   9  31\n                                          \n               Accuracy : 0.76            \n                 95% CI : (0.6947, 0.8174)\n    No Information Rate : 0.65            \n    P-Value [Acc > NIR] : 0.0005292       \n                                          \n                  Kappa : 0.4146          \n                                          \n Mcnemar's Test P-Value : 2.842e-05       \n                                          \n            Sensitivity : 0.4429          \n            Specificity : 0.9308          \n         Pos Pred Value : 0.7750          \n         Neg Pred Value : 0.7562          \n             Prevalence : 0.3500          \n         Detection Rate : 0.1550          \n   Detection Prevalence : 0.2000          \n      Balanced Accuracy : 0.6868          \n                                          \n       'Positive' Class : yes             \n                                          \n\n# Compare test set accuracy to OOB accuracy\npaste0(\"Test Accuracy: \", cm$overall[1])\n\n[1] \"Test Accuracy: 0.76\"\n\npaste0(\"OOB Accuracy: \", 1 - oob_err)\n\n[1] \"OOB Accuracy: 0.75875\""
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#advantage-of-oob-error",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#advantage-of-oob-error",
    "title": "ML with tree based models in r",
    "section": "Advantage of OOB error",
    "text": "Advantage of OOB error\nWhat is the main advantage of using OOB error instead of validation or test error? - If you evaluate your model using OOB error, then you don’t need to create a separate test set"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-test-set-auc",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-test-set-auc",
    "title": "ML with tree based models in r",
    "section": "Evaluate test set AUC",
    "text": "Evaluate test set AUC\nIn Chapter 3, we learned about the AUC metric for evaluating binary classification models. In this exercise, you will compute test set AUC for the Random Forest model.\n\n# Generate predictions on the test set\npred <- predict(object = credit_model,\n            newdata = credit_test,\n            type = \"prob\")\n\n# `pred` is a matrix\nclass(pred)\n\n[1] \"matrix\" \"array\"  \"votes\" \n\n# Look at the pred format\nhead(pred)\n\n     no   yes\n1 0.910 0.090\n2 0.892 0.108\n3 0.992 0.008\n4 0.952 0.048\n5 0.224 0.776\n6 0.846 0.154\n\n# Compute the AUC (`actual` must be a binary 1/0 numeric vector)\nauc(actual = ifelse(credit_test$default == \"yes\", 1, 0), \n    predicted = pred[,\"yes\"])                    \n\n[1] 0.8175824"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#tuning-a-random-forest-via-mtry",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#tuning-a-random-forest-via-mtry",
    "title": "ML with tree based models in r",
    "section": "Tuning a Random Forest via mtry",
    "text": "Tuning a Random Forest via mtry\nIn this exercise, you will use the randomForest::tuneRF() to tune mtry (by training several models). This function is a specific utility to tune the mtry parameter based on OOB error, which is helpful when you want a quick & easy way to tune your model. A more generic way of tuning Random Forest parameters will be presented in the following exercise.\n\n# Execute the tuning process\nset.seed(1)              \nres <- tuneRF(x = subset(credit_train, select = -default),\n              y = credit_train$default,\n              ntreeTry = 500)\n\nmtry = 4  OOB error = 24.12% \nSearching left ...\nmtry = 2    OOB error = 24.5% \n-0.01554404 0.05 \nSearching right ...\nmtry = 8    OOB error = 23.87% \n0.01036269 0.05 \n\n\n\n\n# Look at results\nprint(res)\n\n      mtry OOBError\n2.OOB    2  0.24500\n4.OOB    4  0.24125\n8.OOB    8  0.23875\n\n# Find the mtry value that minimizes OOB Error\nmtry_opt <- res[,\"mtry\"][which.min(res[,\"OOBError\"])]\nprint(mtry_opt)\n\n8.OOB \n    8 \n\n# If you just want to return the best RF model (rather than results)\n# you can set `doBest = TRUE` in `tuneRF()` to return the best RF model\n# instead of a set performance matrix."
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#tuning-a-random-forest-via-tree-depth",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#tuning-a-random-forest-via-tree-depth",
    "title": "ML with tree based models in r",
    "section": "Tuning a Random Forest via tree depth",
    "text": "Tuning a Random Forest via tree depth\nIn Chapter 2, we created a manual grid of hyperparameters using the expand.grid() function and wrote code that trained and evaluated the models of the grid in a loop. In this exercise, you will create a grid of mtry, nodesize and sampsize values. In this example, we will identify the “best model” based on OOB error. The best model is defined as the model from our grid which minimizes OOB error. Keep in mind that there are other ways to select a best model from a grid, such as choosing the best model based on validation AUC. However, for this exercise, we will use the built-in OOB error calculations instead of using a separate validation set.\n\n# Establish a list of possible values for mtry, nodesize and sampsize\nmtry <- seq(4, ncol(credit_train) * 0.8, 2)\nnodesize <- seq(3, 8, 2)\nsampsize <- nrow(credit_train) * c(0.7, 0.8)\n\n# Create a data frame containing all combinations \nhyper_grid <- expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize)\n\n# Create an empty vector to store OOB error values\noob_err <- c()\n\n# Write a loop over the rows of hyper_grid to train the grid of models\nfor (i in 1:nrow(hyper_grid)) {\n\n    # Train a Random Forest model\n    model <- randomForest(formula = default ~ ., \n                          data = credit_train,\n                          mtry = hyper_grid$mtry[i],\n                          nodesize = hyper_grid$nodesize[i],\n                          sampsize = hyper_grid$sampsize[i])\n                          \n    # Store OOB error for the model                      \n    oob_err[i] <- model$err.rate[nrow(model$err.rate), \"OOB\"]\n}\n\n# Identify optimal set of hyperparmeters based on OOB error\nopt_i <- which.min(oob_err)\nprint(hyper_grid[opt_i,])\n\n   mtry nodesize sampsize\n17    6        3      640"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#bagged-trees-vs.-boosted-trees",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#bagged-trees-vs.-boosted-trees",
    "title": "ML with tree based models in r",
    "section": "Bagged trees vs. boosted trees",
    "text": "Bagged trees vs. boosted trees\nWhat is the main difference between bagged trees and boosted trees?\n\nBoosted trees improve the model fit by considering past fits and bagged trees do not"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-gbm-model",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-gbm-model",
    "title": "ML with tree based models in r",
    "section": "Train a GBM model",
    "text": "Train a GBM model\nHere you will use the gbm() function to train a GBM classifier to predict loan default. You will train a 10,000-tree GBM on the credit_train dataset, which is pre-loaded into your workspace. Using such a large number of trees (10,000) is probably not optimal for a GBM model, but we will build more trees than we need and then select the optimal number of trees based on early performance-based stopping. The best GBM model will likely contain fewer trees than we started with. For binary classification, gbm() requires the response to be encoded as 0/1 (numeric), so we will have to convert from a “no/yes” factor to a 0/1 numeric response column.\nAlso, the the gbm() function requires the user to specify a distribution argument. For a binary classification problem, you should set distribution = “bernoulli”. The Bernoulli distribution models a binary response.\n\nlibrary(gbm)\n# Convert \"yes\" to 1, \"no\" to 0\ncredit_train$default <- ifelse(as.character(credit_train$default) == \"yes\", 1, 0) \n\n# Train a 10000-tree GBM model\nset.seed(1)\ncredit_model <- gbm(formula = default ~ ., \n                    distribution = \"bernoulli\", \n                    data = credit_train,\n                    n.trees =  10000)\n                    \n# Print the model object                    \nprint(credit_model)\n\ngbm(formula = default ~ ., distribution = \"bernoulli\", data = credit_train, \n    n.trees = 10000)\nA gradient boosted model with bernoulli loss function.\n10000 iterations were performed.\nThere were 16 predictors of which 16 had non-zero influence.\n\n# summary() prints variable importance\nsummary(credit_model)\n\n\n\n\n                                      var    rel.inf\namount                             amount 22.0897595\nage                                   age 17.9626175\ncredit_history             credit_history 10.6369658\npurpose                           purpose 10.2584546\nemployment_duration   employment_duration  8.8596192\nchecking_balance         checking_balance  6.4650840\nmonths_loan_duration months_loan_duration  5.8863990\nsavings_balance           savings_balance  3.7722735\njob                                   job  2.9418015\nother_credit                 other_credit  2.8613862\nhousing                           housing  2.5237773\nyears_at_residence     years_at_residence  2.3409228\npercent_of_income       percent_of_income  1.7687143\nphone                               phone  0.6373101\nexisting_loans_count existing_loans_count  0.5870700\ndependents                     dependents  0.4078447"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#prediction-using-a-gbm-model",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#prediction-using-a-gbm-model",
    "title": "ML with tree based models in r",
    "section": "Prediction using a GBM model",
    "text": "Prediction using a GBM model\nThe gbm package uses a predict() function to generate predictions from a model, similar to many other machine learning packages in R. When you see a function like predict() that works on many different types of input (a GBM model, a RF model, a GLM model, etc), that indicates that predict() is an “alias” for a GBM-specific version of that function. The GBM specific version of that function is predict.gbm(), but for convenience sake, we can just use predict() (either works).\nOne thing that’s particular to the predict.gbm() however, is that you need to specify the number of trees used in the prediction. There is no default, so you have to specify this manually. For now, we can use the same number of trees that we specified when training the model, which is 10,000 (though this may not be the optimal number to use).\nAnother argument that you can specify is type, which is only relevant to Bernoulli and Poisson distributed outcomes. When using Bernoulli loss, the returned value is on the log odds scale by default and for Poisson, it’s on the log scale. If instead you specify type = “response”, then gbm converts the predicted values back to the same scale as the outcome. This will convert the predicted values into probabilities for Bernoulli and expected counts for Poisson.\n\n# Since we converted the training response col, let's also convert the test response col\ncredit_test$default <- ifelse(credit_test$default == \"yes\", 1, 0) \n\n# Generate predictions on the test set\npreds1 <- predict(object = credit_model, \n                  newdata = credit_test,\n                  n.trees = 10000 )\n\n# Generate predictions on the test set (scale to response)\npreds2 <- predict(object = credit_model, \n                  newdata = credit_test,\n                  n.trees = 10000,\n                  type = \"response\")\n\n# Compare the range of the two sets of predictions\nrange(preds1)\n\n[1] -6.004812  4.646991\n\nrange(preds2)\n\n[1] 0.002460783 0.990500685"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-test-set-auc-1",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-test-set-auc-1",
    "title": "ML with tree based models in r",
    "section": "Evaluate test set AUC",
    "text": "Evaluate test set AUC\nCompute test set AUC of the GBM model for the two sets of predictions. We will notice that they are the same value. That’s because AUC is a rank-based metric, so changing the actual values does not change the value of the AUC.\nHowever, if we were to use a scale-aware metric like RMSE to evaluate performance, we would want to make sure we converted the predictions back to the original scale of the response.\n\n# Generate the test set AUCs using the two sets of preditions & compare\nauc(actual = credit_test$default, predicted = preds1)  #default\n\n[1] 0.7142857\n\nauc(actual = credit_test$default, predicted = preds2)  #rescaled\n\n[1] 0.7142857"
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#early-stopping-in-gbms",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#early-stopping-in-gbms",
    "title": "ML with tree based models in r",
    "section": "Early stopping in GBMs",
    "text": "Early stopping in GBMs\nUse the gbm.perf() function to estimate the optimal number of boosting iterations (aka n.trees) for a GBM model object using both OOB and CV error. When you set out to train a large number of trees in a GBM (such as 10,000) and you use a validation method to determine an earlier (smaller) number of trees, then that’s called “early stopping”. The term “early stopping” is not unique to GBMs, but can describe auto-tuning the number of iterations in an iterative learning algorithm.\n\n# Optimal ntree estimate based on OOB\nntree_opt_oob <- gbm.perf(object = credit_model, \n                          method = \"OOB\", \n                          oobag.curve = TRUE)\n\n\n\n\n\n\n# Train a CV GBM model\nset.seed(1)\ncredit_model_cv <- gbm(formula = default ~ ., \n                       distribution = \"bernoulli\", \n                       data = credit_train,\n                       n.trees = 10000,\n                       cv.folds = 5)\n\n# Optimal ntree estimate based on CV\nntree_opt_cv <- gbm.perf(object = credit_model_cv , \n                         method = \"cv\")\n\n\n\n# Compare the estimates                         \nprint(paste0(\"Optimal n.trees (OOB Estimate): \", ntree_opt_oob))                         \n\n[1] \"Optimal n.trees (OOB Estimate): 76\"\n\nprint(paste0(\"Optimal n.trees (CV Estimate): \", ntree_opt_cv))\n\n[1] \"Optimal n.trees (CV Estimate): 127\""
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#oob-vs-cv-based-early-stopping",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#oob-vs-cv-based-early-stopping",
    "title": "ML with tree based models in r",
    "section": "OOB vs CV-based early stopping",
    "text": "OOB vs CV-based early stopping\nIn the previous exercise, we used OOB error and cross-validated error to estimate the optimal number of trees in the GBM. These are two different ways to estimate the optimal number of trees, so in this exercise we will compare the performance of the models on a test set. We can use the same model object to make both of these estimates since the predict.gbm() function allows you to use any subset of the total number of trees (in our case, the total number is 10,000).\n\n# Generate predictions on the test set using ntree_opt_oob number of trees\npreds1 <- predict(object = credit_model, \n                  newdata = credit_test,\n                  n.trees = ntree_opt_oob)\n                  \n# Generate predictions on the test set using ntree_opt_cv number of trees\ngbm_preds <- predict(object = credit_model, \n                  newdata = credit_test,\n                  n.trees = ntree_opt_cv)   \n\n# Generate the test set AUCs using the two sets of preditions & compare\nauc1 <- auc(actual = credit_test$default, predicted = preds1)  #OOB\nauc2 <- auc(actual = credit_test$default, predicted =gbm_preds)  #CV \n\n# Compare AUC \nprint(paste0(\"Test set AUC (OOB): \", auc1))                         \n\n[1] \"Test set AUC (OOB): 0.802527472527472\"\n\nprint(paste0(\"Test set AUC (CV): \", auc2))\n\n[1] \"Test set AUC (CV): 0.792527472527473\""
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#compare-all-models-based-on-auc",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#compare-all-models-based-on-auc",
    "title": "ML with tree based models in r",
    "section": "Compare all models based on AUC",
    "text": "Compare all models based on AUC\nIn this final exercise, we will perform a model comparison across all types of models that we’ve learned about so far: Decision Trees, Bagged Trees, Random Forest and Gradient Boosting Machine (GBM). The models were all trained on the same training set, credit_train, and predictions were made for the credit_test dataset.\nWe have pre-loaded four sets of test set predictions, generated using the models we trained in previous chapters (one for each model type). The numbers stored in the prediction vectors are the raw predicted values themselves – not the predicted class labels. Using the raw predicted values, we can calculate test set AUC for each model and compare the results.\n\n#credit_train$default <-  factor(credit_train$default, levels  = c(0, 1))\n#credit_test$default <-  factor(credit_test$default, levels = c(0, 1))\n\nmyFolds <- createFolds(credit_train$default, k = 5)\n# tuneGridRf <- data.frame(\n#   .mtry = c(2, 3, 7),\n#   .splitrule = \"variance\",\n#   .min.node.size = 5\n# )\n\nmyControl <- trainControl(\n  method = \"cv\",\n  number = 5,\n  summaryFunction = twoClassSummary,\n  classProbs = TRUE, # IMPORTANT!\n  verboseIter = FALSE,\n  index = myFolds\n)\n\ny =  factor(credit_train$default,levels = c(1,0),  labels = c(\"yes\", \"no\"))\nx <- credit_train %>% select(-default)\nmodel_rf <- train(x =  x ,\n                  y= y,\n                  method = \"ranger\",\n                  classification = TRUE,\n                  metric = \"ROC\",\n                  trControl = myControl)\n \nrf_preds <- predict(model_rf, newdata = credit_test, type = \"prob\")\n\nrf_preds <- rf_preds[, \"yes\"]\n\nmodel_bag <- train(x = x, \n                   y = y,\n                   method = \"treebag\",\n                   metric = \"ROC\",\n                   trControl = myControl)\n\n\nbag_preds <- predict(model_bag, newdata = credit_test, type = \"prob\")\n\nbag_preds  <-bag_preds[, \"yes\"]\n\n\nhyperparams_gbm <- expand.grid(n.trees = seq(100,500, by = 50), \n                           interaction.depth = 1:7, \n                           shrinkage = seq(0.1, 0.9, by = .1), \n                           n.minobsinnode = seq(10, 30, 10))\nmodel_gbm <- train(x = x,\n                   y = y,\n                   method = \"gbm\",\n                   #tuneGrid = hyperparams_gbm,\n                   metric = \"ROC\",\n                   trControl = myControl)\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2650            -nan     0.1000    0.0039\n     2        1.2427            -nan     0.1000    0.0095\n     3        1.2226            -nan     0.1000    0.0072\n     4        1.2078            -nan     0.1000    0.0015\n     5        1.1963            -nan     0.1000    0.0018\n     6        1.1816            -nan     0.1000    0.0039\n     7        1.1712            -nan     0.1000    0.0007\n     8        1.1601            -nan     0.1000    0.0007\n     9        1.1506            -nan     0.1000   -0.0010\n    10        1.1402            -nan     0.1000    0.0044\n    20        1.0561            -nan     0.1000    0.0007\n    40        0.9714            -nan     0.1000   -0.0006\n    60        0.9192            -nan     0.1000   -0.0048\n    80        0.8788            -nan     0.1000   -0.0041\n   100        0.8515            -nan     0.1000   -0.0014\n   120        0.8191            -nan     0.1000   -0.0056\n   140        0.7927            -nan     0.1000   -0.0041\n   150        0.7834            -nan     0.1000   -0.0037\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2528            -nan     0.1000    0.0081\n     2        1.2246            -nan     0.1000    0.0064\n     3        1.1934            -nan     0.1000    0.0106\n     4        1.1729            -nan     0.1000    0.0022\n     5        1.1475            -nan     0.1000    0.0070\n     6        1.1305            -nan     0.1000    0.0000\n     7        1.1180            -nan     0.1000    0.0005\n     8        1.0926            -nan     0.1000    0.0005\n     9        1.0726            -nan     0.1000    0.0045\n    10        1.0576            -nan     0.1000   -0.0013\n    20        0.9674            -nan     0.1000   -0.0046\n    40        0.8488            -nan     0.1000   -0.0153\n    60        0.7727            -nan     0.1000   -0.0039\n    80        0.6981            -nan     0.1000   -0.0045\n   100        0.6502            -nan     0.1000   -0.0015\n   120        0.6014            -nan     0.1000   -0.0036\n   140        0.5684            -nan     0.1000   -0.0033\n   150        0.5464            -nan     0.1000   -0.0009\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2438            -nan     0.1000    0.0155\n     2        1.2147            -nan     0.1000    0.0054\n     3        1.1900            -nan     0.1000    0.0060\n     4        1.1626            -nan     0.1000    0.0052\n     5        1.1412            -nan     0.1000    0.0017\n     6        1.1199            -nan     0.1000    0.0026\n     7        1.0992            -nan     0.1000    0.0009\n     8        1.0813            -nan     0.1000   -0.0046\n     9        1.0672            -nan     0.1000   -0.0023\n    10        1.0506            -nan     0.1000   -0.0080\n    20        0.9000            -nan     0.1000   -0.0032\n    40        0.7530            -nan     0.1000   -0.0073\n    60        0.6553            -nan     0.1000   -0.0054\n    80        0.5664            -nan     0.1000   -0.0073\n   100        0.5062            -nan     0.1000   -0.0059\n   120        0.4603            -nan     0.1000   -0.0057\n   140        0.4111            -nan     0.1000   -0.0043\n   150        0.3856            -nan     0.1000   -0.0035\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0493            -nan     0.1000    0.0012\n     2        1.0312            -nan     0.1000    0.0065\n     3        1.0190            -nan     0.1000   -0.0028\n     4        1.0054            -nan     0.1000    0.0000\n     5        0.9983            -nan     0.1000   -0.0054\n     6        0.9814            -nan     0.1000   -0.0030\n     7        0.9722            -nan     0.1000    0.0000\n     8        0.9649            -nan     0.1000    0.0024\n     9        0.9579            -nan     0.1000    0.0004\n    10        0.9509            -nan     0.1000    0.0011\n    20        0.9017            -nan     0.1000   -0.0067\n    40        0.8232            -nan     0.1000   -0.0027\n    60        0.7655            -nan     0.1000   -0.0019\n    80        0.7302            -nan     0.1000   -0.0026\n   100        0.6907            -nan     0.1000   -0.0018\n   120        0.6691            -nan     0.1000   -0.0074\n   140        0.6566            -nan     0.1000   -0.0030\n   150        0.6459            -nan     0.1000   -0.0042\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0321            -nan     0.1000    0.0031\n     2        1.0123            -nan     0.1000   -0.0019\n     3        0.9904            -nan     0.1000    0.0021\n     4        0.9813            -nan     0.1000   -0.0088\n     5        0.9736            -nan     0.1000   -0.0005\n     6        0.9543            -nan     0.1000    0.0044\n     7        0.9453            -nan     0.1000   -0.0038\n     8        0.9327            -nan     0.1000   -0.0033\n     9        0.9221            -nan     0.1000   -0.0018\n    10        0.9059            -nan     0.1000    0.0043\n    20        0.8193            -nan     0.1000   -0.0042\n    40        0.6906            -nan     0.1000   -0.0047\n    60        0.6227            -nan     0.1000   -0.0049\n    80        0.5567            -nan     0.1000   -0.0028\n   100        0.5066            -nan     0.1000   -0.0055\n   120        0.4600            -nan     0.1000   -0.0059\n   140        0.4173            -nan     0.1000   -0.0066\n   150        0.4008            -nan     0.1000   -0.0024\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0386            -nan     0.1000    0.0032\n     2        1.0199            -nan     0.1000    0.0012\n     3        1.0082            -nan     0.1000   -0.0025\n     4        0.9934            -nan     0.1000   -0.0032\n     5        0.9699            -nan     0.1000   -0.0042\n     6        0.9500            -nan     0.1000   -0.0011\n     7        0.9247            -nan     0.1000    0.0039\n     8        0.9062            -nan     0.1000   -0.0055\n     9        0.8931            -nan     0.1000   -0.0046\n    10        0.8790            -nan     0.1000   -0.0041\n    20        0.7603            -nan     0.1000   -0.0044\n    40        0.6094            -nan     0.1000   -0.0032\n    60        0.5139            -nan     0.1000   -0.0026\n    80        0.4287            -nan     0.1000   -0.0012\n   100        0.3686            -nan     0.1000   -0.0040\n   120        0.3319            -nan     0.1000   -0.0055\n   140        0.2831            -nan     0.1000   -0.0024\n   150        0.2612            -nan     0.1000   -0.0024\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1727            -nan     0.1000    0.0048\n     2        1.1566            -nan     0.1000    0.0042\n     3        1.1399            -nan     0.1000    0.0027\n     4        1.1174            -nan     0.1000    0.0048\n     5        1.0990            -nan     0.1000    0.0023\n     6        1.0846            -nan     0.1000    0.0023\n     7        1.0754            -nan     0.1000    0.0018\n     8        1.0645            -nan     0.1000    0.0035\n     9        1.0550            -nan     0.1000   -0.0008\n    10        1.0466            -nan     0.1000   -0.0008\n    20        0.9777            -nan     0.1000    0.0019\n    40        0.8611            -nan     0.1000   -0.0011\n    60        0.7978            -nan     0.1000   -0.0013\n    80        0.7429            -nan     0.1000   -0.0040\n   100        0.7077            -nan     0.1000   -0.0046\n   120        0.6889            -nan     0.1000   -0.0027\n   140        0.6654            -nan     0.1000   -0.0016\n   150        0.6559            -nan     0.1000   -0.0011\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1591            -nan     0.1000    0.0058\n     2        1.1253            -nan     0.1000    0.0114\n     3        1.0902            -nan     0.1000    0.0116\n     4        1.0662            -nan     0.1000    0.0025\n     5        1.0593            -nan     0.1000   -0.0112\n     6        1.0380            -nan     0.1000    0.0034\n     7        1.0167            -nan     0.1000    0.0040\n     8        0.9910            -nan     0.1000    0.0052\n     9        0.9677            -nan     0.1000    0.0079\n    10        0.9546            -nan     0.1000   -0.0062\n    20        0.8419            -nan     0.1000   -0.0025\n    40        0.6865            -nan     0.1000   -0.0037\n    60        0.6017            -nan     0.1000   -0.0013\n    80        0.5465            -nan     0.1000   -0.0042\n   100        0.4891            -nan     0.1000   -0.0037\n   120        0.4468            -nan     0.1000   -0.0030\n   140        0.4141            -nan     0.1000   -0.0066\n   150        0.3985            -nan     0.1000   -0.0030\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1740            -nan     0.1000   -0.0038\n     2        1.1288            -nan     0.1000    0.0121\n     3        1.0920            -nan     0.1000    0.0046\n     4        1.0637            -nan     0.1000    0.0088\n     5        1.0287            -nan     0.1000    0.0107\n     6        1.0025            -nan     0.1000   -0.0025\n     7        0.9746            -nan     0.1000    0.0011\n     8        0.9501            -nan     0.1000    0.0005\n     9        0.9284            -nan     0.1000   -0.0074\n    10        0.9116            -nan     0.1000   -0.0016\n    20        0.7753            -nan     0.1000   -0.0005\n    40        0.6060            -nan     0.1000   -0.0030\n    60        0.4993            -nan     0.1000    0.0012\n    80        0.4251            -nan     0.1000   -0.0042\n   100        0.3696            -nan     0.1000   -0.0020\n   120        0.3210            -nan     0.1000   -0.0020\n   140        0.2723            -nan     0.1000   -0.0012\n   150        0.2511            -nan     0.1000   -0.0026\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2101            -nan     0.1000   -0.0015\n     2        1.1790            -nan     0.1000    0.0183\n     3        1.1480            -nan     0.1000    0.0163\n     4        1.1148            -nan     0.1000    0.0101\n     5        1.0826            -nan     0.1000    0.0125\n     6        1.0723            -nan     0.1000    0.0024\n     7        1.0499            -nan     0.1000    0.0121\n     8        1.0395            -nan     0.1000   -0.0027\n     9        1.0187            -nan     0.1000    0.0074\n    10        1.0063            -nan     0.1000    0.0043\n    20        0.9174            -nan     0.1000   -0.0016\n    40        0.8156            -nan     0.1000   -0.0010\n    60        0.7405            -nan     0.1000   -0.0031\n    80        0.7003            -nan     0.1000   -0.0016\n   100        0.6655            -nan     0.1000   -0.0023\n   120        0.6434            -nan     0.1000   -0.0027\n   140        0.6147            -nan     0.1000   -0.0048\n   150        0.6067            -nan     0.1000   -0.0012\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1551            -nan     0.1000    0.0184\n     2        1.1113            -nan     0.1000    0.0181\n     3        1.0712            -nan     0.1000    0.0178\n     4        1.0338            -nan     0.1000    0.0092\n     5        1.0134            -nan     0.1000    0.0058\n     6        0.9854            -nan     0.1000    0.0107\n     7        0.9625            -nan     0.1000    0.0084\n     8        0.9432            -nan     0.1000    0.0041\n     9        0.9335            -nan     0.1000   -0.0023\n    10        0.9221            -nan     0.1000   -0.0025\n    20        0.8041            -nan     0.1000   -0.0017\n    40        0.6691            -nan     0.1000   -0.0066\n    60        0.5943            -nan     0.1000   -0.0049\n    80        0.5265            -nan     0.1000   -0.0049\n   100        0.4683            -nan     0.1000   -0.0041\n   120        0.4215            -nan     0.1000   -0.0018\n   140        0.3862            -nan     0.1000   -0.0031\n   150        0.3698            -nan     0.1000   -0.0018\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1719            -nan     0.1000    0.0194\n     2        1.1111            -nan     0.1000    0.0264\n     3        1.0633            -nan     0.1000    0.0174\n     4        1.0323            -nan     0.1000    0.0050\n     5        1.0085            -nan     0.1000    0.0025\n     6        0.9997            -nan     0.1000   -0.0087\n     7        0.9728            -nan     0.1000    0.0075\n     8        0.9311            -nan     0.1000    0.0128\n     9        0.9227            -nan     0.1000   -0.0048\n    10        0.9037            -nan     0.1000    0.0010\n    20        0.7400            -nan     0.1000    0.0013\n    40        0.5865            -nan     0.1000   -0.0055\n    60        0.4703            -nan     0.1000   -0.0030\n    80        0.3800            -nan     0.1000   -0.0036\n   100        0.3205            -nan     0.1000   -0.0015\n   120        0.2803            -nan     0.1000   -0.0023\n   140        0.2401            -nan     0.1000   -0.0015\n   150        0.2183            -nan     0.1000   -0.0013\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1683            -nan     0.1000    0.0149\n     2        1.1484            -nan     0.1000    0.0054\n     3        1.1290            -nan     0.1000    0.0081\n     4        1.1046            -nan     0.1000    0.0100\n     5        1.0993            -nan     0.1000   -0.0032\n     6        1.0877            -nan     0.1000    0.0007\n     7        1.0761            -nan     0.1000    0.0006\n     8        1.0694            -nan     0.1000    0.0002\n     9        1.0624            -nan     0.1000   -0.0023\n    10        1.0458            -nan     0.1000   -0.0009\n    20        0.9643            -nan     0.1000   -0.0015\n    40        0.8710            -nan     0.1000   -0.0019\n    60        0.8263            -nan     0.1000   -0.0067\n    80        0.7926            -nan     0.1000   -0.0050\n   100        0.7772            -nan     0.1000   -0.0060\n   120        0.7532            -nan     0.1000   -0.0031\n   140        0.7267            -nan     0.1000   -0.0032\n   150        0.7225            -nan     0.1000   -0.0028\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1726            -nan     0.1000    0.0081\n     2        1.1329            -nan     0.1000    0.0210\n     3        1.0909            -nan     0.1000    0.0075\n     4        1.0686            -nan     0.1000    0.0064\n     5        1.0454            -nan     0.1000    0.0060\n     6        1.0313            -nan     0.1000   -0.0003\n     7        1.0107            -nan     0.1000    0.0060\n     8        0.9922            -nan     0.1000    0.0039\n     9        0.9778            -nan     0.1000   -0.0013\n    10        0.9617            -nan     0.1000    0.0008\n    20        0.8664            -nan     0.1000   -0.0030\n    40        0.7598            -nan     0.1000   -0.0031\n    60        0.6822            -nan     0.1000   -0.0028\n    80        0.6271            -nan     0.1000    0.0005\n   100        0.5784            -nan     0.1000   -0.0030\n   120        0.5264            -nan     0.1000   -0.0046\n   140        0.4798            -nan     0.1000   -0.0042\n   150        0.4659            -nan     0.1000   -0.0067\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1689            -nan     0.1000    0.0036\n     2        1.1327            -nan     0.1000    0.0121\n     3        1.0929            -nan     0.1000    0.0143\n     4        1.0462            -nan     0.1000    0.0100\n     5        1.0181            -nan     0.1000    0.0078\n     6        0.9997            -nan     0.1000    0.0011\n     7        0.9750            -nan     0.1000    0.0056\n     8        0.9586            -nan     0.1000   -0.0003\n     9        0.9303            -nan     0.1000    0.0071\n    10        0.9179            -nan     0.1000   -0.0073\n    20        0.8060            -nan     0.1000   -0.0029\n    40        0.6746            -nan     0.1000   -0.0055\n    60        0.5787            -nan     0.1000   -0.0073\n    80        0.4995            -nan     0.1000   -0.0056\n   100        0.4327            -nan     0.1000   -0.0024\n   120        0.3789            -nan     0.1000   -0.0030\n   140        0.3350            -nan     0.1000   -0.0035\n   150        0.3100            -nan     0.1000   -0.0036\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1819            -nan     0.1000    0.0092\n     2        1.1663            -nan     0.1000    0.0076\n     3        1.1517            -nan     0.1000    0.0061\n     4        1.1427            -nan     0.1000    0.0039\n     5        1.1339            -nan     0.1000    0.0044\n     6        1.1280            -nan     0.1000    0.0022\n     7        1.1230            -nan     0.1000    0.0019\n     8        1.1151            -nan     0.1000    0.0023\n     9        1.1083            -nan     0.1000    0.0030\n    10        1.0985            -nan     0.1000    0.0035\n    20        1.0537            -nan     0.1000    0.0010\n    40        1.0014            -nan     0.1000   -0.0008\n    50        0.9848            -nan     0.1000   -0.0002\n\ngbm_preds <- predict(model_gbm, newdata = credit_test, type = \"prob\")\n\ngbm_preds  <- gbm_preds[, \"yes\"]\n\nmodel_dt <- train(x = x,\n                   y = y,\n                   method = \"rpart\",\n                   metric = \"ROC\",\n                   trControl = myControl)\n\n\ndt_preds <- predict(model_dt, newdata = credit_test, type = \"prob\")\ndt_preds  <- dt_preds[, \"yes\"]\n\n\n# Generate the test set AUCs using the two sets of predictions & compare\nactual <- credit_test$default\ndt_auc <- auc(actual = actual, predicted = dt_preds)\nbag_auc <- auc(actual = actual, predicted = bag_preds)\nrf_auc <- auc(actual = actual, predicted = rf_preds)\ngbm_auc <- auc(actual = actual, predicted = gbm_preds)\n# \n# # Print results\nsprintf(\"Decision Tree Test AUC: %.3f\", dt_auc)\n\n[1] \"Decision Tree Test AUC: 0.770\"\n\nsprintf(\"Bagged Trees Test AUC: %.3f\", bag_auc)\n\n[1] \"Bagged Trees Test AUC: 0.803\"\n\nsprintf(\"Random Forest Test AUC: %.3f\", rf_auc)\n\n[1] \"Random Forest Test AUC: 0.814\"\n\nsprintf(\"GBM Test AUC: %.3f\", gbm_auc)\n\n[1] \"GBM Test AUC: 0.804\""
  },
  {
    "objectID": "datacamp/ml_tree_methods_r/ml_tree_methods.html#plot-compare-roc-curves",
    "href": "datacamp/ml_tree_methods_r/ml_tree_methods.html#plot-compare-roc-curves",
    "title": "ML with tree based models in r",
    "section": "Plot & compare ROC curves",
    "text": "Plot & compare ROC curves\nWe conclude this course by plotting the ROC curves for all the models (one from each chapter) on the same graph. The ROCR package provides the prediction() and performance() functions which generate the data required for plotting the ROC curve, given a set of predictions and actual (true) values.\nThe more “up and to the left” the ROC curve of a model is, the better the model. The AUC performance metric is literally the “Area Under the ROC Curve”, so the greater the area under this curve, the higher the AUC, and the better-performing the model is.\n\nlibrary(ROCR)\n# List of predictions\npreds_list <- list(dt_preds, bag_preds, rf_preds, gbm_preds)\n\n# List of actual values (same for all)\nm <- length(preds_list)\n\nactuals_list <- rep(list(credit_test$default), m)\n\n# Plot the ROC curves\npred <- prediction(preds_list, actuals_list)\nrocs <- performance(pred, \"tpr\", \"fpr\")\nplot(rocs, col = as.list(1:m), main = \"Test Set ROC Curves\")\nlegend(x = \"bottomright\", \n       legend = c(\"Decision Tree\", \"Bagged Trees\", \"Random Forest\", \"GBM\"),\n       fill = 1:m)"
  },
  {
    "objectID": "datacamp/introduction_deeplearning/final miniproject.html",
    "href": "datacamp/introduction_deeplearning/final miniproject.html",
    "title": "Mburu Blog",
    "section": "",
    "text": "def Option():\n    print(\"\"\"OPTIONS:\\n\n            [R]: Prompts the user to load a file (in this project S.typhimurium fasta file)\\n\n            [S]:  It gets the compliment of a dna\\n\n            [W]:  gets the revervse complement of a dna\\n\n            [I]:  prints rest of ORF in a DNA sequence\\n\n            [A]: gets the rest of ORF in one frame of a sequence\\n\n            [K]  gets the rest of ORF in all frames sequences\\n\n            [J]  gets the rest of ORF in one frame in both strands\\n\n            [M]  gets the longest ORF\\n\n            [N]  Computes the maximum length of the longest ORF over num_trials shuffles of the specfied DNA sequence\\n\n            [B]  Computes the Protein encoded by a sequence of DNA\\n\n            [F]   Returns the amino acid sequences that are likely coded by the specified dna\n            [H]:  Give the use of all the Options\\n\n            [Q]:  To exit the program\\n\"\"\")\nOption()\n\nOPTIONS:\n\n            [R]: Prompts the user to load a file (in this project S.typhimurium fasta file)\n\n            [S]:  It gets the compliment of a dna\n\n            [W]:  gets the revervse complement of a dna\n\n            [I]:  prints rest of ORF in a DNA sequence\n\n            [A]: gets the rest of ORF in one frame of a sequence\n\n            [K]  gets the rest of ORF in all frames sequences\n\n            [J]  gets the rest of ORF in one frame in both strands\n\n            [M]  gets the longest ORF\n\n            [N]  Computes the maximum length of the longest ORF over num_trials shuffles of the specfied DNA sequence\n\n            [B]  Computes the Protein encoded by a sequence of DNA\n\n            [F]   Returns the amino acid sequences that are likely coded by the specified dna\n            [H]:  Give the use of all the Options\n\n            [Q]:  To exit the program\n\n\n\n\n\ndef H():\n    Option()\nH()\n\nOPTIONS:\n\n            [R]: Prompts the user to load a file (in this project S.typhimurium fasta file)\n\n            [S]:  It gets the compliment of a dna\n\n            [W]:  gets the revervse complement of a dna\n\n            [I]:  prints rest of ORF in a DNA sequence\n\n            [A]: gets the rest of ORF in one frame of a sequence\n\n            [K]  gets the rest of ORF in all frames sequences\n\n            [J]  gets the rest of ORF in one frame in both strands\n\n            [M]  gets the longest ORF\n\n            [N]  Computes the maximum length of the longest ORF over num_trials shuffles of the specfied DNA sequence\n\n            [B]  Computes the Protein encoded by a sequence of DNA\n\n            [F]   Returns the amino acid sequences that are likely coded by the specified dna\n            [H]:  Give the use of all the Options\n\n            [Q]:  To exit the program\n\n\n\n\n#provide a nucleotide sequnce\nnucleotide ='ATGGCTAGCGATGCGAGCCCTACCGTGACCGATCCATGAGAGATGCTCTAGGCTATGAATGACGTAGCG'\ndef get_complement(nucleotide):\n    \"\"\" Returns the complementary nucleotide\n\n        nucleotide: a nucleotide (A, C, G, or T) represented as a string\n        returns: the complementary nucleotide\n    >>> get_complement('A')\n    'T'\n    >>> get_complement('C')\n    'G'\n    \"\"\"\n    complement = nucleotide.replace(\"A\",\"t\")   #V#replace with small letters and the make uppercase to prevent confusion\n    complement = complement.replace(\"T\",\"a\")\n    complement = complement.replace(\"C\",\"g\")\n    complement = complement.replace(\"G\",\"c\")\n    complement = complement.upper()  #return to upper case\n    return complement\nget_complement(nucleotide)\n\n'TACCGATCGCTACGCTCGGGATGGCACTGGCTAGGTACTCTCTACGAGATCCGATACTTACTGCATCGC'\n\n\n\nget_reverse_complement(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\n'GGATCCCATGAACGTCTTTTAGCGTGGCGCCTTCACTAACGGCATCCCAAATCATTTCCGCAACACGTTCTTCGCTGACATTATTTTGATAATCCATTACTTACTCCTGTTATCTGTCACCGACTTTGTAGAACTTAACGACTGCGTTTATCTGATGCAGTTATTAAACCCCGACGGTGGTTAGTGAACATTCAAAAAACGCCCAATGAATACATCGCTACTGCCTTACGCGGCTCAATGCCGTACCTCGTTTTCTTGTGGCTGAATAACGTCTTTGCCCGCGTTTTCTACCTCTTCCAGCCAAACCAGAAGACGTAAAACTTCATCAATTTCTTCCAGACTCACCAGATCATAACGGCGATGGGTTTTGAAAAGACTGCGCGCCAGTTTGATATCGACGATCACAGGTACGCCAACCTTCTCCGCATAGGCGCGGACGGCCAGTGCGCGCTGATTCGTTTCATACACCGAGATCATCGGAATCGGCATCAATTCGGGTTTAAAATAAATCCCGATCGTAATATGCGTGGGGTTGGCAACAATCAGGCGTGAGTTTTCAATATCAGATTTCACCTGTTCAGACAGAATTTCCATATGAACTTCACGTCTTTTAGATTTAACCTCTGGGTTCCCTTCCTGCTCCTTCATTTCACGCTTCACTTCTTCCTTATCCATTTTCATATCTTTCATGGTCAGGAAATATTCCGCAATAGCATCCAATAATAAGACAATCAATGCGCAAGCAAGGCAAGTTAATACCAATGCGAGGAGAAGTTCACGCCAAATGACGGCAATACCTACAATATTGCCATTTAGCTGAGAAAAGATTTCAACCTTATATTTCTTCCAGCAAATGATGGCGGCCACCACAAAGGATGAGAGATACAGTAGGGTTTTGACCGTATCTTTAACCGTGCGCATACTAAAAAGTTTTTTTGCCCCTTCTACCGGGTTTAACGCCGATAAATTAGGCTTTAATGCTTCTGTCGCCAGCACAAAACCGGCCTGTAATAACGCCGGTAATGCGGAACACACTAAGCAGAGCAGCATAAATGGAATCAGATATTTTAACCCTATCCCAAAAACGGCCAAACTGTAGTCAGCCATGCTCTGATCAAAATTATCCGCAATAATGATCTTAATTATCCCCATAAACTCATTAAATGAGCCATACGACACCAGATAGGCAATTCCTCCCAGCGTCAGGCAGGCGATAATGAGATCTTTACTTTTAAATGACTGGCCTTTTTTAGCGGAGTCTTCCAGCCGTTTTTTAGTCGGTTTTTCTGTTTTATTCGAGGACATGCGTCGCCCCTCGCTCGTAAAACCAACTGCTTAACCCTGTGGCCTGGAAAGAGAGTCGCAGTACATTGTCCGGTAGTACCGGAGAGAAATAAAGCAGCATAATTAAAACGGCAATACCGCTTTTTACCGTCAGTGAAATCGCAAAAGCGTTCATTTGCGGAGCAAAGCGCGACAATAAACCCAGGAATACTTCTGACAGCAACAGCACTAATACCACCGGACTGGCCAGAACCAAGGCGTTTTGAGCCACCTGATTAATAAACGTTAATAGCGGCGGTAATGAAGGCGTGCACTCGTTCATCGGATCGCATAGCTGATAGCTTTTATTTAACACGTCAACCATCGTGACCAGACCGCCGTTTTGTAAATAAACGACAGCGGCAAACATATTCAGGAAATTAGCCATTTCCGAGGTATCAATACCGTTTGCCGGATCGATACTACTACTTAGCGTTGCCCCTCGCTGGTTATCGATAATACAACCCAGCGCATGCATAACCCAAAAAGGCCATGACAGCAGACAGCCCAGCATGACGCCTACCGCCGCTTCTTGCAGAACTAACGGGATCATCGCCACCGATAAAAACGGCGGCGCCTCGTTCAATGCATGCGGCCATACTCCCAATGCCACCAGGATGATAATGGCGTTTCTCGGCGCACCGCTTAATACCCCGCTATTCAAAAACGGCAGGAAGAAAAAAATCGGCGCCACGCGAGCAAACCCTAGCGCCGCAGACGCAACCAGGTGATGAATTTCAAAGTACAACGCGTAAAACATTTTTTACCCCTTAGCCAACGCCAGGAATATCACCTGACGCCCGTAAGAGAGTAAAACTTCGCCATACCAGCCAGACAGTAAAAACAAGCATAAACACACGCCAAGTAATTTAATGCCAAAAGGCAGCGTCTGTTCCTGTAATTGCGTTACCGTCTGGAATAACCCTACCAGGAGGCCGATAATCGTTGCGACAATCGTCGGCCACCCTGACAGGATCAAAACAAGATAGAGCGCCTTATTACCTGCAAACACTAAATCATCCATTTAACTATCCCGTCTCGTAATGATGTCATGTTGCAATGTCCATATACTGTAATATCAATCCCTTAGACAGTAAGGTCCAGCCATCAAGCGCGACAAAAAGCACCAGCTTAATAGGTGTAGATATCGTCACCGGACTCATCATCATCATCCCCAGCGCCAGTAGCACGCTGGATACCACCAGGTCGACGACGACAAAGGGCAAATAAAGATAAAAACCAATTTTAAACGCGCTTTTTATTTCGCTCAGCGCATAAGCAGGTAATAACGCAAATATTGAAGGTTTTTCAATTTCATCTTTGTCACGCTTTACCGTCTCGGTCTCTTCTCCATACTGACGCTTCAGTTGCGCGTTTTCAAAAAACTGAACTAACTCGCGATCTGAATATTTGATCAGATAATCGCGATAACCATCCAGACCTTCATCAACGTGTTTACTTAATGATGAAATATCATTAAAGGTGACATCTTCGTCCTCAAAATAGACGTAGGCATCATGCATTATGGGCCACATAACAAACATAGAAAGCAGCAATGCGACGCCGTTAAGCGTCATATTTGAAGGTATCTGCTGTAATCCCAGGGCGTTACGCACCATGACAAATACAATAGAAAATTTAACGAAACAGGTTCCTGACGCAATAATAAATGGCAACAGGGTGGAAAATGCCAGTAAGGCAATTAATGAGATATCATTCCCCATTACCAGACTCGCTCAGCCATTCATGGATCTCAACGCCTAAGGTGTCATTCATCTGTACCAGTTCGCCATTACCCAGCAAAACACCATTCGCCATAATTTCAACGTTAAGTTCAGCATTGGTCGGCAGTGATAATAGCTGTTGCTGCCCCATGGCTTCGAGTTCGGCGAGGGTAACGTTCTTACGATACAAAACAAATTCCAGTTTGACGGGCAATTGATTCAAGCCAGGCAGAGTTTCTGCAGTTTCAGTTGTATTATTTTCTTCTTCGATATGTTGAATATCTAACGTTTCCACAATAATTCCCCCTTCAACACGGTTGAAATGACCTAACTTTTTCGCGTAGCAATAAACTTCCGCACGGGAAGTACGAATCAGGAGTACATCTCCGATCCCGATTCGGCCCAGCAACGAACGCTGCGTATCACTGCTACCGATTACAAAGCGCAACGGCCAACGCAGCATTTTCGGCCTGCCGCCCCCGACTGCAGGCAGTTCAGGAAGATGCTCAAACCACAGGCCGCCCCGATCGCTCATAATGTGCAACAATTTCCCTTCCGGCAGCGCGCTTCCCGGTACGGGGTTCTCTACGCATAAACGCCGACAGGACAAATGCGGCACGGGCAACTCAAACGGTCGCTCTGTTGCAGCAAGCCAGGGAACGACCAGGTGCTCAGCGCCAGCAGAAACCGCCGCCCCAGCCAGAGCGGGAGAGACATGCTCAAGCCAGTCCCCAGGTTTAATCCAGGCCGACCACCGTTTTTCTGCATCGCTCAACCGAACCCACATTCCCTGTCGCGTCGGATATTCCAGCGTCGCTTCCCGGCCATGGCGCTGGCATTCTGTCGCGGTTTGCGCCAATAGCCATTCGCGACGATCAATCTGTCTCACACGCAATGACATCAGGCGTCATCCTCCTCGCCAGATTGCTGTCTGTGCTGTTGCTGCTGCGGATTTTGTTGATCGTCTCGCGTCAGGTGCCAGCGCTGGGGATTACCGTTTTGCCATTGATCATGCAAACGATGTTCAACCTGCGTATTTGACGGTATTAACGAAAACTCCCCTGCTTGCCGCGCCTGAATATTGACGGAATAGTCATTTCCCCAGCGCTGAAAACGGTAAGTCAGCGAGCTATCCTCTCCTTTCACGCCATCGGCAGTGGGAAAAATAGTCATCATCGGCTTTGATTGCGCCGCTAAAGGCATTTTTTCATCGCCGCCGGTTAATTGGCTAAGATCGGCGATAGTGGTTGGTTGCAGCGGAAGCTGAGAAACATCTTTAACCTTTTTATGATCTTTATCTTCAGGCTTACCGGTATTGGCTGCGGCCATTCGGGCAGGTGCGACATCCCGCGCCAGCGGCGCGCCCTCTTTACGAACGCCTTCGCCCGCGATGGCTTTATTATCCCCAGGCAATGCCTTGATATTATCGTCAGAGATTCCCGTGGCGTTATCGGTTACTTCACTAACGGATTCCACAGCTTTTAAATCAGCAGATAATTTTTTACCGCTTACGCTTTCTAACGGCCTATTTTTAGACGATAGCAACGCTGCGGATTTATCTACTTTGGCCTCCGCAGAGATCAAACCGACAGATTTTTCAGCAGTGACTTTCAACAGTTTTTCAGCAATCCTGAGTTCGCTTTTTCCGTTATGATGCAGACCAGAAACGTTGCCATTGTGATGTTCTGATTTCGCTGGCGCGCCATGTCGCCATGCCGCCAGTAATAACGGTAAAGCCGTTTCTTTATGCATTACGAAAGCATCGCCATAGTCGCGATCTTTTTTATCACCGGAATATTCTGTCTTATGTTTTTCCACCGCTTTTTTTAATGCTTCTGATAAACCGCCAACCTCATCCTGCTGCGGCAGTAAAATGTTCCCGGATGAACTGACAGCTGACACATCGCCCATTAAATTATCTCCTCTGACTCGGCCTCTTCCTGCTGTATCTCTCGCTGGATATAGAATCTTTTCTGACGGATTATCCAGCGTTGATAGTTCCCTTCTTTGCGCAACCAATATTTACTTTTTTTCTGAAACTCTTCCCTTTTCTTTTCCAGCTCGCTCCGTTTTTCCTGAATTTGTATAATCTGGAGTTCTAAATCTTTTATCTGCCGGCGAACAATAGACTGCTTACGTAATAACGTATAAATTTCCTCACGACTGAGCTGTCTGTTTTCTGCACGCAGCGTATCTAATAACAATTTCAGACCCGCTATTTGTTCAAGGATCGCCTCCTCCTCGGCCTGCAGCCCGCGGTCCTCATCCTGATAGCGAAGTAATATCGACTCACACTGTGAATGAAATACCGTACAGCGCCGCTGCAATACTTTAATTCTGGTCAGCGAATGCATTCATACCGCTCAACGTGTCATCAAAGGATGAATACTGCGCTACCGGCTGGCATAACCAGGCTTTCAGGCTATCCCGCATCTGCATCGCCCGATCGTTATCGATATTTTCGCCAGGACGATATTCTCCCAAGTCAATGAAAAGCTGGAGCTCTTCCAAACGCGTCATTAATTTACGCACGGCAGATGCCTGTTCAGCATGTGTCGGCGTCGTGACTTGTCCAAAAACGCGGCTTACGCTTTTCAGTACATCGATTGCCGGGTAATGTCCCTGCCCGGCCAGCTTTCTGCTCAGATACAGGTGACCGTCAAGGATAGAGCGAATTTCATCCGCCATCGGGTCCGCCTCTTCCTCGCTTTCCAGCAGTACCGTATAAAAGGCAGTAATGCTTCCCTCGCTGGTCGCCCCTGGGCGTTCCAGCAAGCGGGGCAAATTATCGAATACGGAGGCGGGATAACCTCGACGAGCCGGACGCTCTCCCGACGCCAGTGCCACGTCTCGCAAAGCACGCGCATAACGGGTCATGGAATCGATAAAAAGCACGACCCGTTTTCCCTGGTCGCGAAAATATTCCGCTACGGTTGTCGCCAGTTGCGCCGCATTGCAGCGATCGACCGAGGGGAAATCGGAAGTGGCAAAAACCAGCACGCATTTTTCTTTCTTATGCGAAGCGCGCAACATATCCACGAATTCAGTGACCTCACGGCCTCGTTCACCGATAAGACCGATAACAAAGACATCCGCCTCCGTTTGCTCGATCAGCATATGCATCAGCATGGTCTTACCGCATCCTGCGGAGGCAAAAATGCCCATTCGCTGGCCTACGCCACAGGTCAATAACCCGTCAATCGCGCGCACACCGGTAATCAGCGGTTCACGGACGCCAACGCGTGAAGCGTAAGACGGCGGTGCGACATCAATAACGCGTTCTTCGCTAATCGGCGCCACTTCAGGGGTAAAACGCTCAACGATTTTCCCTGTCGGATCC'\n\n\n\ndef load_seq(fasta_file):\n    \"\"\" Reads a FASTA file and returns the DNA sequence as a string.\n\n    fasta_file: the path to the FASTA file containing the DNA sequence\n    returns: the DNA sequence as a string\n    \"\"\"\n    retval = \"\"\n    f = open(fasta_file) # assign f to the opened fata file\n    lines = f.readlines()   #assin lines to the lists created\n    for l in lines[1:]:  #remove the ist line in fa(header)\n        retval += l[0:-1] #append the lista without header to the empty list\n    f.close()\n    return retval\n\n\nload_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa')\n\n'GGATCCGACAGGGAAAATCGTTGAGCGTTTTACCCCTGAAGTGGCGCCGATTAGCGAAGAACGCGTTATTGATGTCGCACCGCCGTCTTACGCTTCACGCGTTGGCGTCCGTGAACCGCTGATTACCGGTGTGCGCGCGATTGACGGGTTATTGACCTGTGGCGTAGGCCAGCGAATGGGCATTTTTGCCTCCGCAGGATGCGGTAAGACCATGCTGATGCATATGCTGATCGAGCAAACGGAGGCGGATGTCTTTGTTATCGGTCTTATCGGTGAACGAGGCCGTGAGGTCACTGAATTCGTGGATATGTTGCGCGCTTCGCATAAGAAAGAAAAATGCGTGCTGGTTTTTGCCACTTCCGATTTCCCCTCGGTCGATCGCTGCAATGCGGCGCAACTGGCGACAACCGTAGCGGAATATTTTCGCGACCAGGGAAAACGGGTCGTGCTTTTTATCGATTCCATGACCCGTTATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGATAATTTGCCCCGCTTGCTGGAACGCCCAGGGGCGACCAGCGAGGGAAGCATTACTGCCTTTTATACGGTACTGCTGGAAAGCGAGGAAGAGGCGGACCCGATGGCGGATGAAATTCGCTCTATCCTTGACGGTCACCTGTATCTGAGCAGAAAGCTGGCCGGGCAGGGACATTACCCGGCAATCGATGTACTGAAAAGCGTAAGCCGCGTTTTTGGACAAGTCACGACGCCGACACATGCTGAACAGGCATCTGCCGTGCGTAAATTAATGACGCGTTTGGAAGAGCTCCAGCTTTTCATTGACTTGGGAGAATATCGTCCTGGCGAAAATATCGATAACGATCGGGCGATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAATTAAAGTATTGCAGCGGCGCTGTACGGTATTTCATTCACAGTGTGAGTCGATATTACTTCGCTATCAGGATGAGGACCGCGGGCTGCAGGCCGAGGAGGAGGCGATCCTTGAACAAATAGCGGGTCTGAAATTGTTATTAGATACGCTGCGTGCAGAAAACAGACAGCTCAGTCGTGAGGAAATTTATACGTTATTACGTAAGCAGTCTATTGTTCGCCGGCAGATAAAAGATTTAGAACTCCAGATTATACAAATTCAGGAAAAACGGAGCGAGCTGGAAAAGAAAAGGGAAGAGTTTCAGAAAAAAAGTAAATATTGGTTGCGCAAAGAAGGGAACTATCAACGCTGGATAATCCGTCAGAAAAGATTCTATATCCAGCGAGAGATACAGCAGGAAGAGGCCGAGTCAGAGGAGATAATTTAATGGGCGATGTGTCAGCTGTCAGTTCATCCGGGAACATTTTACTGCCGCAGCAGGATGAGGTTGGCGGTTTATCAGAAGCATTAAAAAAAGCGGTGGAAAAACATAAGACAGAATATTCCGGTGATAAAAAAGATCGCGACTATGGCGATGCTTTCGTAATGCATAAAGAAACGGCTTTACCGTTATTACTGGCGGCATGGCGACATGGCGCGCCAGCGAAATCAGAACATCACAATGGCAACGTTTCTGGTCTGCATCATAACGGAAAAAGCGAACTCAGGATTGCTGAAAAACTGTTGAAAGTCACTGCTGAAAAATCTGTCGGTTTGATCTCTGCGGAGGCCAAAGTAGATAAATCCGCAGCGTTGCTATCGTCTAAAAATAGGCCGTTAGAAAGCGTAAGCGGTAAAAAATTATCTGCTGATTTAAAAGCTGTGGAATCCGTTAGTGAAGTAACCGATAACGCCACGGGAATCTCTGACGATAATATCAAGGCATTGCCTGGGGATAATAAAGCCATCGCGGGCGAAGGCGTTCGTAAAGAGGGCGCGCCGCTGGCGCGGGATGTCGCACCTGCCCGAATGGCCGCAGCCAATACCGGTAAGCCTGAAGATAAAGATCATAAAAAGGTTAAAGATGTTTCTCAGCTTCCGCTGCAACCAACCACTATCGCCGATCTTAGCCAATTAACCGGCGGCGATGAAAAAATGCCTTTAGCGGCGCAATCAAAGCCGATGATGACTATTTTTCCCACTGCCGATGGCGTGAAAGGAGAGGATAGCTCGCTGACTTACCGTTTTCAGCGCTGGGGAAATGACTATTCCGTCAATATTCAGGCGCGGCAAGCAGGGGAGTTTTCGTTAATACCGTCAAATACGCAGGTTGAACATCGTTTGCATGATCAATGGCAAAACGGTAATCCCCAGCGCTGGCACCTGACGCGAGACGATCAACAAAATCCGCAGCAGCAACAGCACAGACAGCAATCTGGCGAGGAGGATGACGCCTGATGTCATTGCGTGTGAGACAGATTGATCGTCGCGAATGGCTATTGGCGCAAACCGCGACAGAATGCCAGCGCCATGGCCGGGAAGCGACGCTGGAATATCCGACGCGACAGGGAATGTGGGTTCGGTTGAGCGATGCAGAAAAACGGTGGTCGGCCTGGATTAAACCTGGGGACTGGCTTGAGCATGTCTCTCCCGCTCTGGCTGGGGCGGCGGTTTCTGCTGGCGCTGAGCACCTGGTCGTTCCCTGGCTTGCTGCAACAGAGCGACCGTTTGAGTTGCCCGTGCCGCATTTGTCCTGTCGGCGTTTATGCGTAGAGAACCCCGTACCGGGAAGCGCGCTGCCGGAAGGGAAATTGTTGCACATTATGAGCGATCGGGGCGGCCTGTGGTTTGAGCATCTTCCTGAACTGCCTGCAGTCGGGGGCGGCAGGCCGAAAATGCTGCGTTGGCCGTTGCGCTTTGTAATCGGTAGCAGTGATACGCAGCGTTCGTTGCTGGGCCGAATCGGGATCGGAGATGTACTCCTGATTCGTACTTCCCGTGCGGAAGTTTATTGCTACGCGAAAAAGTTAGGTCATTTCAACCGTGTTGAAGGGGGAATTATTGTGGAAACGTTAGATATTCAACATATCGAAGAAGAAAATAATACAACTGAAACTGCAGAAACTCTGCCTGGCTTGAATCAATTGCCCGTCAAACTGGAATTTGTTTTGTATCGTAAGAACGTTACCCTCGCCGAACTCGAAGCCATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAATGATATCTCATTAATTGCCTTACTGGCATTTTCCACCCTGTTGCCATTTATTATTGCGTCAGGAACCTGTTTCGTTAAATTTTCTATTGTATTTGTCATGGTGCGTAACGCCCTGGGATTACAGCAGATACCTTCAAATATGACGCTTAACGGCGTCGCATTGCTGCTTTCTATGTTTGTTATGTGGCCCATAATGCATGATGCCTACGTCTATTTTGAGGACGAAGATGTCACCTTTAATGATATTTCATCATTAAGTAAACACGTTGATGAAGGTCTGGATGGTTATCGCGATTATCTGATCAAATATTCAGATCGCGAGTTAGTTCAGTTTTTTGAAAACGCGCAACTGAAGCGTCAGTATGGAGAAGAGACCGAGACGGTAAAGCGTGACAAAGATGAAATTGAAAAACCTTCAATATTTGCGTTATTACCTGCTTATGCGCTGAGCGAAATAAAAAGCGCGTTTAAAATTGGTTTTTATCTTTATTTGCCCTTTGTCGTCGTCGACCTGGTGGTATCCAGCGTGCTACTGGCGCTGGGGATGATGATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACATGACATCATTACGAGACGGGATAGTTAAATGGATGATTTAGTGTTTGCAGGTAATAAGGCGCTCTATCTTGTTTTGATCCTGTCAGGGTGGCCGACGATTGTCGCAACGATTATCGGCCTCCTGGTAGGGTTATTCCAGACGGTAACGCAATTACAGGAACAGACGCTGCCTTTTGGCATTAAATTACTTGGCGTGTGTTTATGCTTGTTTTTACTGTCTGGCTGGTATGGCGAAGTTTTACTCTCTTACGGGCGTCAGGTGATATTCCTGGCGTTGGCTAAGGGGTAAAAAATGTTTTACGCGTTGTACTTTGAAATTCATCACCTGGTTGCGTCTGCGGCGCTAGGGTTTGCTCGCGTGGCGCCGATTTTTTTCTTCCTGCCGTTTTTGAATAGCGGGGTATTAAGCGGTGCGCCGAGAAACGCCATTATCATCCTGGTGGCATTGGGAGTATGGCCGCATGCATTGAACGAGGCGCCGCCGTTTTTATCGGTGGCGATGATCCCGTTAGTTCTGCAAGAAGCGGCGGTAGGCGTCATGCTGGGCTGTCTGCTGTCATGGCCTTTTTGGGTTATGCATGCGCTGGGTTGTATTATCGATAACCAGCGAGGGGCAACGCTAAGTAGTAGTATCGATCCGGCAAACGGTATTGATACCTCGGAAATGGCTAATTTCCTGAATATGTTTGCCGCTGTCGTTTATTTACAAAACGGCGGTCTGGTCACGATGGTTGACGTGTTAAATAAAAGCTATCAGCTATGCGATCCGATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTATTAATCAGGTGGCTCAAAACGCCTTGGTTCTGGCCAGTCCGGTGGTATTAGTGCTGTTGCTGTCAGAAGTATTCCTGGGTTTATTGTCGCGCTTTGCTCCGCAAATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAATAAAACAGAAAAACCGACTAAAAAACGGCTGGAAGACTCCGCTAAAAAAGGCCAGTCATTTAAAAGTAAAGATCTCATTATCGCCTGCCTGACGCTGGGAGGAATTGCCTATCTGGTGTCGTATGGCTCATTTAATGAGTTTATGGGGATAATTAAGATCATTATTGCGGATAATTTTGATCAGAGCATGGCTGACTACAGTTTGGCCGTTTTTGGGATAGGGTTAAAATATCTGATTCCATTTATGCTGCTCTGCTTAGTGTGTTCCGCATTACCGGCGTTATTACAGGCCGGTTTTGTGCTGGCGACAGAAGCATTAAAGCCTAATTTATCGGCGTTAAACCCGGTAGAAGGGGCAAAAAAACTTTTTAGTATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCATTGAGCCGCGTAAGGCAGTAGCGATGTATTCATTGGGCGTTTTTTGAATGTTCACTAACCACCGTCGGGGTTTAATAACTGCATCAGATAAACGCAGTCGTTAAGTTCTACAAAGTCGGTGACAGATAACAGGAGTAAGTAATGGATTATCAAAATAATGTCAGCGAAGAACGTGTTGCGGAAATGATTTGGGATGCCGTTAGTGAAGGCGCCACGCTAAAAGACGTTCATGGGATCC'\n\n\n\npwd\n\n'/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna'\n\n\n\nget_reverse_complement(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\nNameError: name 'get_reverse_complement' is not defined\n\n\n\ndna='ATGGCTAGCGATGCGAGCCCTACCGTGACCGATCCATGAGAGATGCTCTAGGCTATGAATGACGTAGCG'\ndef get_reverse_complement(dna):\n    complement = dna.replace(\"A\",\"t\")\n    complement = complement.replace(\"T\",\"a\")\n    complement = complement.replace(\"C\",\"g\")#replace nucleotides with small letters and later make them uppercase\n    complement = complement.replace(\"G\",\"c\")\n    complement = complement.upper()\n    reverse = complement[::-1]#reverse using index\n    return reverse\n    \"\"\" Computes the reverse complementary sequence of DNA for the specfied DNA\n        sequence|\n\n        dna: a DNA sequence represented as a string\n        returns: the reverse complementary DNA sequence represented as a string\n    >>> get_reverse_complement(\"ATGCCCGCTTT\")\n    'AAAGCGGGCAT'\n    >>> get_reverse_complement(\"CCGCGTTCA\")\n    'TGAACGCGG'\n   \"\"\"\n#get_reverse_complement(dna)\n\n\nget_reverse_complement(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\n'GGATCCCATGAACGTCTTTTAGCGTGGCGCCTTCACTAACGGCATCCCAAATCATTTCCGCAACACGTTCTTCGCTGACATTATTTTGATAATCCATTACTTACTCCTGTTATCTGTCACCGACTTTGTAGAACTTAACGACTGCGTTTATCTGATGCAGTTATTAAACCCCGACGGTGGTTAGTGAACATTCAAAAAACGCCCAATGAATACATCGCTACTGCCTTACGCGGCTCAATGCCGTACCTCGTTTTCTTGTGGCTGAATAACGTCTTTGCCCGCGTTTTCTACCTCTTCCAGCCAAACCAGAAGACGTAAAACTTCATCAATTTCTTCCAGACTCACCAGATCATAACGGCGATGGGTTTTGAAAAGACTGCGCGCCAGTTTGATATCGACGATCACAGGTACGCCAACCTTCTCCGCATAGGCGCGGACGGCCAGTGCGCGCTGATTCGTTTCATACACCGAGATCATCGGAATCGGCATCAATTCGGGTTTAAAATAAATCCCGATCGTAATATGCGTGGGGTTGGCAACAATCAGGCGTGAGTTTTCAATATCAGATTTCACCTGTTCAGACAGAATTTCCATATGAACTTCACGTCTTTTAGATTTAACCTCTGGGTTCCCTTCCTGCTCCTTCATTTCACGCTTCACTTCTTCCTTATCCATTTTCATATCTTTCATGGTCAGGAAATATTCCGCAATAGCATCCAATAATAAGACAATCAATGCGCAAGCAAGGCAAGTTAATACCAATGCGAGGAGAAGTTCACGCCAAATGACGGCAATACCTACAATATTGCCATTTAGCTGAGAAAAGATTTCAACCTTATATTTCTTCCAGCAAATGATGGCGGCCACCACAAAGGATGAGAGATACAGTAGGGTTTTGACCGTATCTTTAACCGTGCGCATACTAAAAAGTTTTTTTGCCCCTTCTACCGGGTTTAACGCCGATAAATTAGGCTTTAATGCTTCTGTCGCCAGCACAAAACCGGCCTGTAATAACGCCGGTAATGCGGAACACACTAAGCAGAGCAGCATAAATGGAATCAGATATTTTAACCCTATCCCAAAAACGGCCAAACTGTAGTCAGCCATGCTCTGATCAAAATTATCCGCAATAATGATCTTAATTATCCCCATAAACTCATTAAATGAGCCATACGACACCAGATAGGCAATTCCTCCCAGCGTCAGGCAGGCGATAATGAGATCTTTACTTTTAAATGACTGGCCTTTTTTAGCGGAGTCTTCCAGCCGTTTTTTAGTCGGTTTTTCTGTTTTATTCGAGGACATGCGTCGCCCCTCGCTCGTAAAACCAACTGCTTAACCCTGTGGCCTGGAAAGAGAGTCGCAGTACATTGTCCGGTAGTACCGGAGAGAAATAAAGCAGCATAATTAAAACGGCAATACCGCTTTTTACCGTCAGTGAAATCGCAAAAGCGTTCATTTGCGGAGCAAAGCGCGACAATAAACCCAGGAATACTTCTGACAGCAACAGCACTAATACCACCGGACTGGCCAGAACCAAGGCGTTTTGAGCCACCTGATTAATAAACGTTAATAGCGGCGGTAATGAAGGCGTGCACTCGTTCATCGGATCGCATAGCTGATAGCTTTTATTTAACACGTCAACCATCGTGACCAGACCGCCGTTTTGTAAATAAACGACAGCGGCAAACATATTCAGGAAATTAGCCATTTCCGAGGTATCAATACCGTTTGCCGGATCGATACTACTACTTAGCGTTGCCCCTCGCTGGTTATCGATAATACAACCCAGCGCATGCATAACCCAAAAAGGCCATGACAGCAGACAGCCCAGCATGACGCCTACCGCCGCTTCTTGCAGAACTAACGGGATCATCGCCACCGATAAAAACGGCGGCGCCTCGTTCAATGCATGCGGCCATACTCCCAATGCCACCAGGATGATAATGGCGTTTCTCGGCGCACCGCTTAATACCCCGCTATTCAAAAACGGCAGGAAGAAAAAAATCGGCGCCACGCGAGCAAACCCTAGCGCCGCAGACGCAACCAGGTGATGAATTTCAAAGTACAACGCGTAAAACATTTTTTACCCCTTAGCCAACGCCAGGAATATCACCTGACGCCCGTAAGAGAGTAAAACTTCGCCATACCAGCCAGACAGTAAAAACAAGCATAAACACACGCCAAGTAATTTAATGCCAAAAGGCAGCGTCTGTTCCTGTAATTGCGTTACCGTCTGGAATAACCCTACCAGGAGGCCGATAATCGTTGCGACAATCGTCGGCCACCCTGACAGGATCAAAACAAGATAGAGCGCCTTATTACCTGCAAACACTAAATCATCCATTTAACTATCCCGTCTCGTAATGATGTCATGTTGCAATGTCCATATACTGTAATATCAATCCCTTAGACAGTAAGGTCCAGCCATCAAGCGCGACAAAAAGCACCAGCTTAATAGGTGTAGATATCGTCACCGGACTCATCATCATCATCCCCAGCGCCAGTAGCACGCTGGATACCACCAGGTCGACGACGACAAAGGGCAAATAAAGATAAAAACCAATTTTAAACGCGCTTTTTATTTCGCTCAGCGCATAAGCAGGTAATAACGCAAATATTGAAGGTTTTTCAATTTCATCTTTGTCACGCTTTACCGTCTCGGTCTCTTCTCCATACTGACGCTTCAGTTGCGCGTTTTCAAAAAACTGAACTAACTCGCGATCTGAATATTTGATCAGATAATCGCGATAACCATCCAGACCTTCATCAACGTGTTTACTTAATGATGAAATATCATTAAAGGTGACATCTTCGTCCTCAAAATAGACGTAGGCATCATGCATTATGGGCCACATAACAAACATAGAAAGCAGCAATGCGACGCCGTTAAGCGTCATATTTGAAGGTATCTGCTGTAATCCCAGGGCGTTACGCACCATGACAAATACAATAGAAAATTTAACGAAACAGGTTCCTGACGCAATAATAAATGGCAACAGGGTGGAAAATGCCAGTAAGGCAATTAATGAGATATCATTCCCCATTACCAGACTCGCTCAGCCATTCATGGATCTCAACGCCTAAGGTGTCATTCATCTGTACCAGTTCGCCATTACCCAGCAAAACACCATTCGCCATAATTTCAACGTTAAGTTCAGCATTGGTCGGCAGTGATAATAGCTGTTGCTGCCCCATGGCTTCGAGTTCGGCGAGGGTAACGTTCTTACGATACAAAACAAATTCCAGTTTGACGGGCAATTGATTCAAGCCAGGCAGAGTTTCTGCAGTTTCAGTTGTATTATTTTCTTCTTCGATATGTTGAATATCTAACGTTTCCACAATAATTCCCCCTTCAACACGGTTGAAATGACCTAACTTTTTCGCGTAGCAATAAACTTCCGCACGGGAAGTACGAATCAGGAGTACATCTCCGATCCCGATTCGGCCCAGCAACGAACGCTGCGTATCACTGCTACCGATTACAAAGCGCAACGGCCAACGCAGCATTTTCGGCCTGCCGCCCCCGACTGCAGGCAGTTCAGGAAGATGCTCAAACCACAGGCCGCCCCGATCGCTCATAATGTGCAACAATTTCCCTTCCGGCAGCGCGCTTCCCGGTACGGGGTTCTCTACGCATAAACGCCGACAGGACAAATGCGGCACGGGCAACTCAAACGGTCGCTCTGTTGCAGCAAGCCAGGGAACGACCAGGTGCTCAGCGCCAGCAGAAACCGCCGCCCCAGCCAGAGCGGGAGAGACATGCTCAAGCCAGTCCCCAGGTTTAATCCAGGCCGACCACCGTTTTTCTGCATCGCTCAACCGAACCCACATTCCCTGTCGCGTCGGATATTCCAGCGTCGCTTCCCGGCCATGGCGCTGGCATTCTGTCGCGGTTTGCGCCAATAGCCATTCGCGACGATCAATCTGTCTCACACGCAATGACATCAGGCGTCATCCTCCTCGCCAGATTGCTGTCTGTGCTGTTGCTGCTGCGGATTTTGTTGATCGTCTCGCGTCAGGTGCCAGCGCTGGGGATTACCGTTTTGCCATTGATCATGCAAACGATGTTCAACCTGCGTATTTGACGGTATTAACGAAAACTCCCCTGCTTGCCGCGCCTGAATATTGACGGAATAGTCATTTCCCCAGCGCTGAAAACGGTAAGTCAGCGAGCTATCCTCTCCTTTCACGCCATCGGCAGTGGGAAAAATAGTCATCATCGGCTTTGATTGCGCCGCTAAAGGCATTTTTTCATCGCCGCCGGTTAATTGGCTAAGATCGGCGATAGTGGTTGGTTGCAGCGGAAGCTGAGAAACATCTTTAACCTTTTTATGATCTTTATCTTCAGGCTTACCGGTATTGGCTGCGGCCATTCGGGCAGGTGCGACATCCCGCGCCAGCGGCGCGCCCTCTTTACGAACGCCTTCGCCCGCGATGGCTTTATTATCCCCAGGCAATGCCTTGATATTATCGTCAGAGATTCCCGTGGCGTTATCGGTTACTTCACTAACGGATTCCACAGCTTTTAAATCAGCAGATAATTTTTTACCGCTTACGCTTTCTAACGGCCTATTTTTAGACGATAGCAACGCTGCGGATTTATCTACTTTGGCCTCCGCAGAGATCAAACCGACAGATTTTTCAGCAGTGACTTTCAACAGTTTTTCAGCAATCCTGAGTTCGCTTTTTCCGTTATGATGCAGACCAGAAACGTTGCCATTGTGATGTTCTGATTTCGCTGGCGCGCCATGTCGCCATGCCGCCAGTAATAACGGTAAAGCCGTTTCTTTATGCATTACGAAAGCATCGCCATAGTCGCGATCTTTTTTATCACCGGAATATTCTGTCTTATGTTTTTCCACCGCTTTTTTTAATGCTTCTGATAAACCGCCAACCTCATCCTGCTGCGGCAGTAAAATGTTCCCGGATGAACTGACAGCTGACACATCGCCCATTAAATTATCTCCTCTGACTCGGCCTCTTCCTGCTGTATCTCTCGCTGGATATAGAATCTTTTCTGACGGATTATCCAGCGTTGATAGTTCCCTTCTTTGCGCAACCAATATTTACTTTTTTTCTGAAACTCTTCCCTTTTCTTTTCCAGCTCGCTCCGTTTTTCCTGAATTTGTATAATCTGGAGTTCTAAATCTTTTATCTGCCGGCGAACAATAGACTGCTTACGTAATAACGTATAAATTTCCTCACGACTGAGCTGTCTGTTTTCTGCACGCAGCGTATCTAATAACAATTTCAGACCCGCTATTTGTTCAAGGATCGCCTCCTCCTCGGCCTGCAGCCCGCGGTCCTCATCCTGATAGCGAAGTAATATCGACTCACACTGTGAATGAAATACCGTACAGCGCCGCTGCAATACTTTAATTCTGGTCAGCGAATGCATTCATACCGCTCAACGTGTCATCAAAGGATGAATACTGCGCTACCGGCTGGCATAACCAGGCTTTCAGGCTATCCCGCATCTGCATCGCCCGATCGTTATCGATATTTTCGCCAGGACGATATTCTCCCAAGTCAATGAAAAGCTGGAGCTCTTCCAAACGCGTCATTAATTTACGCACGGCAGATGCCTGTTCAGCATGTGTCGGCGTCGTGACTTGTCCAAAAACGCGGCTTACGCTTTTCAGTACATCGATTGCCGGGTAATGTCCCTGCCCGGCCAGCTTTCTGCTCAGATACAGGTGACCGTCAAGGATAGAGCGAATTTCATCCGCCATCGGGTCCGCCTCTTCCTCGCTTTCCAGCAGTACCGTATAAAAGGCAGTAATGCTTCCCTCGCTGGTCGCCCCTGGGCGTTCCAGCAAGCGGGGCAAATTATCGAATACGGAGGCGGGATAACCTCGACGAGCCGGACGCTCTCCCGACGCCAGTGCCACGTCTCGCAAAGCACGCGCATAACGGGTCATGGAATCGATAAAAAGCACGACCCGTTTTCCCTGGTCGCGAAAATATTCCGCTACGGTTGTCGCCAGTTGCGCCGCATTGCAGCGATCGACCGAGGGGAAATCGGAAGTGGCAAAAACCAGCACGCATTTTTCTTTCTTATGCGAAGCGCGCAACATATCCACGAATTCAGTGACCTCACGGCCTCGTTCACCGATAAGACCGATAACAAAGACATCCGCCTCCGTTTGCTCGATCAGCATATGCATCAGCATGGTCTTACCGCATCCTGCGGAGGCAAAAATGCCCATTCGCTGGCCTACGCCACAGGTCAATAACCCGTCAATCGCGCGCACACCGGTAATCAGCGGTTCACGGACGCCAACGCGTGAAGCGTAAGACGGCGGTGCGACATCAATAACGCGTTCTTCGCTAATCGGCGCCACTTCAGGGGTAAAACGCTCAACGATTTTCCCTGTCGGATCC'\n\n\n\ndef rest_of_ORF(dna):\n   \n    \"\"\" Takes a DNA sequence that is assumed to begin with a start\n        codon and returns the sequence up to but not including the\n        first in frame stop codon.  If there is no in frame stop codon,\n        returns the whole string.\n\n        dna: a DNA sequence\n        returns: the open reading frame represented as a string\n    >>> rest_of_ORF(\"ATGTGAA\")\n    'ATG'\n    >>> rest_of_ORF(\"ATGAGATAGG\")\n    'ATGAGA'\n    \"\"\"\n    #dna = 'ATGGCTAGCGATGCGAGCCCTACCGTGACCGATCCATGAGAGATGCTCTAGGCTATGAATGACGTAGCG'\n    Orf1 = []\n    for i in range(0, len(dna), 3 ): \n        Orf1.append( dna[0+int(i):3+int(i)] ) #APPend the 3 nucleotide code to the empty list\n    #print(\"ORF IS ---->\", Orf1, \"\\n\")\n    lis = []\n    stops = ['TAG', 'TAA', 'TGA']\n    for i in stops:\n        if i in Orf1:\n            lis.append(i)#list the stop condons in the orf\n           # print(\"All stop codons in this orf ---->\", lis, \"\\n\")\n    indexs_stops = []\n    for i in lis:\n        indexs_stops.append(int(Orf1.index(i)))     #append the index of stop codes and sort threm\n    sorted_indexs_stops = sorted(indexs_stops)\n    start = (Orf1.index(\"ATG\")) #start index\n    #print(start)\n    if len(sorted_indexs_stops) >= 1: \n        stop = sorted_indexs_stops[0]#stop index\n        return (\"\".join(Orf1[start:stop]))#prints orf\n        pass\n    else:\n        return (\"\".join(Orf1[start:]))\n#rest_of_ORF(dna)\n\n\nrest_of_ORF(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\n''\n\n\n\nrest_of_ORF(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\n''\n\n\n\ndef find_all_ORFs_oneframe(dna):\n    \"\"\" Finds all non-nested open reading frames in the given DNA\n        sequence and returns them as a list.  This function should\n        only find ORFs that are in the default frame of the sequence\n        (i.e. they start on indices that are multiples of 3).\n        By non-nested we mean that if an ORF occurs entirely within\n        another ORF, it should not be included in the returned list of ORFs.\n\n        dna: a DNA sequence\n        returns: a list of non-nested ORFs\n    >>> find_all_ORFs_oneframe(\"ATGCATGAATGTAGATAGATGTGCCC\")\n    ['ATGCATGAATGTAGA', 'ATGTGCCC']\n    \"\"\"\n    listOfOrf = list()\n   # dna='ATGGCTAGCGATGCGAGCCCTACCGTGACCGATCCATGAGAGATGCTCTAGGCTATGAATGACGTAGCG'\n    frames = [] # storing the default reading frame\n    frames.append([dna[i:i + 3] for i in range(0, len(dna), 3)])\n    #print(frames)\n    for i in range(0,len(frames),1): #looping  the dna frame\n        start=0\n        while start <len(frames[i]): #looping the frame for start and stop codons\n            if frames[i][start]==\"ATG\":\n                for stop in range(start+1,len(frames[i]),1):\n                    if frames[i][stop]==\"TAA\" or  frames[i][stop]==\"TAG\" or  frames[i][stop]==\"TGA\" :\n                        listOfOrf.append(' '.join(frames[i][start:stop])) # retrieve the orf\n                        break\n                else:\n                     listOfOrf.append(' '.join(frames[i][start:]))\n            start+=1\n    one_f_orf =(\",\".join(listOfOrf).replace(\" \",\"\"))\n    one_f_orf = one_f_orf.split(\",\")\n    return one_f_orf\n#find_all_ORFs_oneframe(dna)\n    # TODO: implement this\n\n\ndef find_all_ORFs(dna):\n    \"\"\" Finds all non-nested open reading frames in the given DNA sequence in\n        all 3 possible frames and returns them as a list.  By non-nested we\n        mean that if an ORF occurs entirely within another ORF and they are\n        both in the same frame, it should not be included in the returned list\n        of ORFs.\n\n        dna: a DNA sequence\n        returns: a list of non-nested ORFs\n\n    >>> find_all_ORFs(\"ATGCATGAATGTAG\")\n    ['ATGCATGAATGTAG', 'ATGAATGTAG', 'ATG']\n    \"\"\"\n    dna='ATGGCTAGCGATGCGAGCCCTACCGTGACCGATCCATGAGAGATGCTCTAGGCTATGAATGACGTAGCG'\n    listOfOrf = list()\n    frames = [] # storing the three reading frames create the positive frames\n    frames.append([dna[i:i + 3] for i in range(0, len(dna), 3)])\n    frames.append([dna[i:i + 3] for i in range(1, len(dna), 3)])\n    frames.append([dna[i:i + 3] for i in range(2, len(dna), 3)])\n    #print(frames)\n    for i in range(0,len(frames),1): #looping all the frames\n        start=0\n        while start <len(frames[i]): #looping each frame for start and stop codons\n            if frames[i][start]==\"ATG\":\n                for stop in range(start+1,len(frames[i]),1):\n                    if frames[i][stop]==\"TAA\" or  frames[i][stop]==\"TAG\" or  frames[i][stop]==\"TGA\" :\n                        listOfOrf.append(' '.join(frames[i][start:stop])) # retrieve the orf\n                        break\n                else:\n                     listOfOrf.append(' '.join(frames[i][start:]))\n            start+=1\n    all_orf= \",\".join(listOfOrf).replace(\" \",\"\")\n    all_orf = all_orf.split(\",\")\n    return all_orf\nfind_all_ORFs(dna)\n\n['ATGGCTAGCGATGCGAGCCCTACCGTGACCGATCCA',\n 'ATGCTC',\n 'ATGAATGACGTAGCG',\n 'ATGCGAGCCCTACCG',\n 'ATGACG',\n 'ATGAGAGATGCTCTAGGCTATGAA']\n\n\n\nfind_all_ORFs_oneframe(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\n['ATGCGG',\n 'ATGCGTGCTGGTTTTTGCCACTTCCGATTTCCCCTCGGTCGATCGCTGCAATGCGGCGCAACTGGCGACAACCGTAGCGGAATATTTTCGCGACCAGGGAAAACGGGTCGTGCTTTTTATCGATTCCATGACCCGTTATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGA',\n 'ATGCCAGCCGGTAGCGCAGTATTCATCCTT',\n 'ATGAGGACCGCGGGCTGCAGGCCGAGGAGGAGGCGATCCTTGAACAAA',\n 'ATGGCGACATGGCGCGCCAGCGAAATCAGAACATCACAATGGCAACGTTTCTGGTCTGCATCA',\n 'ATGGCAAAACGG',\n 'ATGTCATTGCGTGTGAGACAGATTGATCGTCGCGAATGGCTATTGGCGCAAACCGCGACAGAATGCCAGCGCCATGGCCGGGAAGCGACGCTGGAATATCCGACGCGACAGGGAATGTGGGTTCGGTTGAGCGATGCAGAAAAACGGTGGTCGGCCTGGATTAAACCTGGGGACTGGCTTGAGCATGTCTCTCCCGCTCTGGCTGGGGCGGCGGTTTCTGCTGGCGCTGAGCACCTGGTCGTTCCCTGGCTTGCTGCAACAGAGCGACCGTTTGAGTTGCCCGTGCCGCATTTGTCCTGTCGGCGTTTATGCGTAGAGAACCCCGTACCGGGAAGCGCGCTGCCGGAAGGGAAATTGTTGCACATTATGAGCGATCGGGGCGGCCTGTGGTTTGAGCATCTTCCTGAACTGCCTGCAGTCGGGGGCGGCAGGCCGAAAATGCTGCGTTGGCCGTTGCGCTTTGTAATCGGTAGCAGTGATACGCAGCGTTCGTTGCTGGGCCGAATCGGGATCGGAGATGTACTCCTGATTCGTACTTCCCGTGCGGAAGTTTATTGCTACGCGAAAAAGTTAGGTCATTTCAACCGTGTTGAAGGGGGAATTATTGTGGAAACGTTAGATATTCAACATATCGAAGAAGAAAATAATACAACTGAAACTGCAGAAACTCTGCCTGGCTTGAATCAATTGCCCGTCAAACTGGAATTTGTTTTGTATCGTAAGAACGTTACCCTCGCCGAACTCGAAGCCATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGTGGGTTCGGTTGAGCGATGCAGAAAAACGGTGGTCGGCCTGGATTAAACCTGGGGACTGGCTTGAGCATGTCTCTCCCGCTCTGGCTGGGGCGGCGGTTTCTGCTGGCGCTGAGCACCTGGTCGTTCCCTGGCTTGCTGCAACAGAGCGACCGTTTGAGTTGCCCGTGCCGCATTTGTCCTGTCGGCGTTTATGCGTAGAGAACCCCGTACCGGGAAGCGCGCTGCCGGAAGGGAAATTGTTGCACATTATGAGCGATCGGGGCGGCCTGTGGTTTGAGCATCTTCCTGAACTGCCTGCAGTCGGGGGCGGCAGGCCGAAAATGCTGCGTTGGCCGTTGCGCTTTGTAATCGGTAGCAGTGATACGCAGCGTTCGTTGCTGGGCCGAATCGGGATCGGAGATGTACTCCTGATTCGTACTTCCCGTGCGGAAGTTTATTGCTACGCGAAAAAGTTAGGTCATTTCAACCGTGTTGAAGGGGGAATTATTGTGGAAACGTTAGATATTCAACATATCGAAGAAGAAAATAATACAACTGAAACTGCAGAAACTCTGCCTGGCTTGAATCAATTGCCCGTCAAACTGGAATTTGTTTTGTATCGTAAGAACGTTACCCTCGCCGAACTCGAAGCCATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGAGCGATCGGGGCGGCCTGTGGTTTGAGCATCTTCCTGAACTGCCTGCAGTCGGGGGCGGCAGGCCGAAAATGCTGCGTTGGCCGTTGCGCTTTGTAATCGGTAGCAGTGATACGCAGCGTTCGTTGCTGGGCCGAATCGGGATCGGAGATGTACTCCTGATTCGTACTTCCCGTGCGGAAGTTTATTGCTACGCGAAAAAGTTAGGTCATTTCAACCGTGTTGAAGGGGGAATTATTGTGGAAACGTTAGATATTCAACATATCGAAGAAGAAAATAATACAACTGAAACTGCAGAAACTCTGCCTGGCTTGAATCAATTGCCCGTCAAACTGGAATTTGTTTTGTATCGTAAGAACGTTACCCTCGCCGAACTCGAAGCCATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGCTGCGTTGGCCGTTGCGCTTTGTAATCGGTAGCAGTGATACGCAGCGTTCGTTGCTGGGCCGAATCGGGATCGGAGATGTACTCCTGATTCGTACTTCCCGTGCGGAAGTTTATTGCTACGCGAAAAAGTTAGGTCATTTCAACCGTGTTGAAGGGGGAATTATTGTGGAAACGTTAGATATTCAACATATCGAAGAAGAAAATAATACAACTGAAACTGCAGAAACTCTGCCTGGCTTGAATCAATTGCCCGTCAAACTGGAATTTGTTTTGTATCGTAAGAACGTTACCCTCGCCGAACTCGAAGCCATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGACATCATTACGAGACGGGATAGTTAAATGGATGATT',\n 'ATGATT',\n 'ATGGCGAAGTTTTACTCTCTTACGGGCGTCAGG',\n 'ATGCAT',\n 'ATGCGCTGGGTTGTATTATCGATAACCAGCGAGGGGCAACGC',\n 'ATGTACTGCGACTCTCTTTCCAGGCCACAGGGT',\n 'ATGTCCTCGAATAAAACAGAAAAACCGACTAAAAAACGGCTGGAAGACTCCGCTAAAAAAGGCCAGTCATTTAAAAGTAAAGATCTCATTATCGCCTGCCTGACGCTGGGAGGAATTGCCTATCTGGTGTCGTATGGCTCATTTAATGAGTTTATGGGGATAATTAAGATCATTATTGCGGATAATTTTGATCAGAGCATGGCTGACTACAGTTTGGCCGTTTTTGGGATAGGGTTAAAATATCTGATTCCATTTATGCTGCTCTGCTTAGTGTGTTCCGCATTACCGGCGTTATTACAGGCCGGTTTTGTGCTGGCGACAGAAGCATTAAAGCCTAATTTATCGGCGTTAAACCCGGTAGAAGGGGCAAAAAAACTTTTTAGTATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGGGGATAATTAAGATCATTATTGCGGATAATTTTGATCAGAGCATGGCTGACTACAGTTTGGCCGTTTTTGGGATAGGGTTAAAATATCTGATTCCATTTATGCTGCTCTGCTTAGTGTGTTCCGCATTACCGGCGTTATTACAGGCCGGTTTTGTGCTGGCGACAGAAGCATTAAAGCCTAATTTATCGGCGTTAAACCCGGTAGAAGGGGCAAAAAAACTTTTTAGTATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGGCTGACTACAGTTTGGCCGTTTTTGGGATAGGGTTAAAATATCTGATTCCATTTATGCTGCTCTGCTTAGTGTGTTCCGCATTACCGGCGTTATTACAGGCCGGTTTTGTGCTGGCGACAGAAGCATTAAAGCCTAATTTATCGGCGTTAAACCCGGTAGAAGGGGCAAAAAAACTTTTTAGTATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGCTGCTCTGCTTAGTGTGTTCCGCATTACCGGCGTTATTACAGGCCGGTTTTGTGCTGGCGACAGAAGCATTAAAGCCTAATTTATCGGCGTTAAACCCGGTAGAAGGGGCAAAAAAACTTTTTAGTATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGTCAGCGAAGAACGTGTTGCGGAAA',\n 'ATGCCGTTAGTGAAGGCGCCACGC',\n 'ATGGGATCC']\n\n\n\ndef find_all_ORFs_both_strands(dna):\n    \"\"\" Finds all non-nested open reading frames in the given DNA sequence on both\n        strands.\nATGAGGCTCAGGGATGATCTTGGGTTTTGTAATGGTCGCTGTACGATTATGATCG\n\n\n        dna: a DNA sequence\n        returns: a list of non-nested ORFs\n    >>> find_all_ORFs_both_strands(\"ATGCGAATGTAGCATCAAA\")\n    ['ATGCGAATG', 'ATGCTACATTCGCAT']\n    \"\"\"\n    #dna='ATGGCTAGCGATGCGAGCCCTACCGTGACCGATCCATGAGAGATGCTCTAGGCTATGAATGACGTAGCG'\n    listOfOrf = list()\n    frames = [] # storing the six frames that would be extacted from the fragments\n    reverseCdna = [] # storing the reverse compliments\n    frames.append([dna[i:i + 3] for i in range(0, len(dna), 3)])\n    frames.append([dna[i:i + 3] for i in range(1, len(dna), 3)])\n    frames.append([dna[i:i + 3] for i in range(2, len(dna), 3)])\n    # reverse compliment of the fragment\n    reverse = {\"A\": \"T\", \"C\": \"G\", \"T\": \"A\", \"G\": \"C\"}\n    for i in range(len(dna)):\n        reverseCdna.append(reverse[dna[-i - 1]]) if dna[-i - 1] in reverse.keys() else reverseCdna.append(dna[-i - 1])  # if any  contamination found we keep it for further more check\n    reverseCdna = ''.join(reverseCdna) # joining\n    # create the reverse complement  frames\n    frames.append([reverseCdna[i:i + 3] for i in range(0, len(reverseCdna), 3)])\n    frames.append([reverseCdna[i:i + 3] for i in range(1, len(reverseCdna), 3)])\n    frames.append([reverseCdna[i:i + 3] for i in range(2, len(reverseCdna), 3)])\n    #print(frames)\n    #print(reverseCdna)\n    for i in range(0,len(frames),1): #looping all the frames\n        start=0\n        while start <len(frames[i]): #looping each frame for start and stop codons\n            if frames[i][start]==\"ATG\":\n                for stop in range(start+1,len(frames[i]),1):\n                    if frames[i][stop]==\"TAA\" or  frames[i][stop]==\"TAG\" or  frames[i][stop]==\"TGA\" :\n                        listOfOrf.append(' '.join(frames[i][start:stop])) # retrieve the orf\n                        break\n                else:\n                     listOfOrf.append(' '.join(frames[i][start:]))\n            start+=1\n    my_string =\",\".join(listOfOrf).replace(\" \",\"\")\n    my_list = my_string.split(\",\")\n    return my_list\n#find_all_ORFs_both_strands(dna)\n\n\nfind_all_ORFs_both_strands(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\n['ATGCGG',\n 'ATGCGTGCTGGTTTTTGCCACTTCCGATTTCCCCTCGGTCGATCGCTGCAATGCGGCGCAACTGGCGACAACCGTAGCGGAATATTTTCGCGACCAGGGAAAACGGGTCGTGCTTTTTATCGATTCCATGACCCGTTATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGA',\n 'ATGCCAGCCGGTAGCGCAGTATTCATCCTT',\n 'ATGAGGACCGCGGGCTGCAGGCCGAGGAGGAGGCGATCCTTGAACAAA',\n 'ATGGCGACATGGCGCGCCAGCGAAATCAGAACATCACAATGGCAACGTTTCTGGTCTGCATCA',\n 'ATGGCAAAACGG',\n 'ATGTCATTGCGTGTGAGACAGATTGATCGTCGCGAATGGCTATTGGCGCAAACCGCGACAGAATGCCAGCGCCATGGCCGGGAAGCGACGCTGGAATATCCGACGCGACAGGGAATGTGGGTTCGGTTGAGCGATGCAGAAAAACGGTGGTCGGCCTGGATTAAACCTGGGGACTGGCTTGAGCATGTCTCTCCCGCTCTGGCTGGGGCGGCGGTTTCTGCTGGCGCTGAGCACCTGGTCGTTCCCTGGCTTGCTGCAACAGAGCGACCGTTTGAGTTGCCCGTGCCGCATTTGTCCTGTCGGCGTTTATGCGTAGAGAACCCCGTACCGGGAAGCGCGCTGCCGGAAGGGAAATTGTTGCACATTATGAGCGATCGGGGCGGCCTGTGGTTTGAGCATCTTCCTGAACTGCCTGCAGTCGGGGGCGGCAGGCCGAAAATGCTGCGTTGGCCGTTGCGCTTTGTAATCGGTAGCAGTGATACGCAGCGTTCGTTGCTGGGCCGAATCGGGATCGGAGATGTACTCCTGATTCGTACTTCCCGTGCGGAAGTTTATTGCTACGCGAAAAAGTTAGGTCATTTCAACCGTGTTGAAGGGGGAATTATTGTGGAAACGTTAGATATTCAACATATCGAAGAAGAAAATAATACAACTGAAACTGCAGAAACTCTGCCTGGCTTGAATCAATTGCCCGTCAAACTGGAATTTGTTTTGTATCGTAAGAACGTTACCCTCGCCGAACTCGAAGCCATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGTGGGTTCGGTTGAGCGATGCAGAAAAACGGTGGTCGGCCTGGATTAAACCTGGGGACTGGCTTGAGCATGTCTCTCCCGCTCTGGCTGGGGCGGCGGTTTCTGCTGGCGCTGAGCACCTGGTCGTTCCCTGGCTTGCTGCAACAGAGCGACCGTTTGAGTTGCCCGTGCCGCATTTGTCCTGTCGGCGTTTATGCGTAGAGAACCCCGTACCGGGAAGCGCGCTGCCGGAAGGGAAATTGTTGCACATTATGAGCGATCGGGGCGGCCTGTGGTTTGAGCATCTTCCTGAACTGCCTGCAGTCGGGGGCGGCAGGCCGAAAATGCTGCGTTGGCCGTTGCGCTTTGTAATCGGTAGCAGTGATACGCAGCGTTCGTTGCTGGGCCGAATCGGGATCGGAGATGTACTCCTGATTCGTACTTCCCGTGCGGAAGTTTATTGCTACGCGAAAAAGTTAGGTCATTTCAACCGTGTTGAAGGGGGAATTATTGTGGAAACGTTAGATATTCAACATATCGAAGAAGAAAATAATACAACTGAAACTGCAGAAACTCTGCCTGGCTTGAATCAATTGCCCGTCAAACTGGAATTTGTTTTGTATCGTAAGAACGTTACCCTCGCCGAACTCGAAGCCATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGAGCGATCGGGGCGGCCTGTGGTTTGAGCATCTTCCTGAACTGCCTGCAGTCGGGGGCGGCAGGCCGAAAATGCTGCGTTGGCCGTTGCGCTTTGTAATCGGTAGCAGTGATACGCAGCGTTCGTTGCTGGGCCGAATCGGGATCGGAGATGTACTCCTGATTCGTACTTCCCGTGCGGAAGTTTATTGCTACGCGAAAAAGTTAGGTCATTTCAACCGTGTTGAAGGGGGAATTATTGTGGAAACGTTAGATATTCAACATATCGAAGAAGAAAATAATACAACTGAAACTGCAGAAACTCTGCCTGGCTTGAATCAATTGCCCGTCAAACTGGAATTTGTTTTGTATCGTAAGAACGTTACCCTCGCCGAACTCGAAGCCATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGCTGCGTTGGCCGTTGCGCTTTGTAATCGGTAGCAGTGATACGCAGCGTTCGTTGCTGGGCCGAATCGGGATCGGAGATGTACTCCTGATTCGTACTTCCCGTGCGGAAGTTTATTGCTACGCGAAAAAGTTAGGTCATTTCAACCGTGTTGAAGGGGGAATTATTGTGGAAACGTTAGATATTCAACATATCGAAGAAGAAAATAATACAACTGAAACTGCAGAAACTCTGCCTGGCTTGAATCAATTGCCCGTCAAACTGGAATTTGTTTTGTATCGTAAGAACGTTACCCTCGCCGAACTCGAAGCCATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGACATCATTACGAGACGGGATAGTTAAATGGATGATT',\n 'ATGATT',\n 'ATGGCGAAGTTTTACTCTCTTACGGGCGTCAGG',\n 'ATGCAT',\n 'ATGCGCTGGGTTGTATTATCGATAACCAGCGAGGGGCAACGC',\n 'ATGTACTGCGACTCTCTTTCCAGGCCACAGGGT',\n 'ATGTCCTCGAATAAAACAGAAAAACCGACTAAAAAACGGCTGGAAGACTCCGCTAAAAAAGGCCAGTCATTTAAAAGTAAAGATCTCATTATCGCCTGCCTGACGCTGGGAGGAATTGCCTATCTGGTGTCGTATGGCTCATTTAATGAGTTTATGGGGATAATTAAGATCATTATTGCGGATAATTTTGATCAGAGCATGGCTGACTACAGTTTGGCCGTTTTTGGGATAGGGTTAAAATATCTGATTCCATTTATGCTGCTCTGCTTAGTGTGTTCCGCATTACCGGCGTTATTACAGGCCGGTTTTGTGCTGGCGACAGAAGCATTAAAGCCTAATTTATCGGCGTTAAACCCGGTAGAAGGGGCAAAAAAACTTTTTAGTATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGGGGATAATTAAGATCATTATTGCGGATAATTTTGATCAGAGCATGGCTGACTACAGTTTGGCCGTTTTTGGGATAGGGTTAAAATATCTGATTCCATTTATGCTGCTCTGCTTAGTGTGTTCCGCATTACCGGCGTTATTACAGGCCGGTTTTGTGCTGGCGACAGAAGCATTAAAGCCTAATTTATCGGCGTTAAACCCGGTAGAAGGGGCAAAAAAACTTTTTAGTATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGGCTGACTACAGTTTGGCCGTTTTTGGGATAGGGTTAAAATATCTGATTCCATTTATGCTGCTCTGCTTAGTGTGTTCCGCATTACCGGCGTTATTACAGGCCGGTTTTGTGCTGGCGACAGAAGCATTAAAGCCTAATTTATCGGCGTTAAACCCGGTAGAAGGGGCAAAAAAACTTTTTAGTATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGCTGCTCTGCTTAGTGTGTTCCGCATTACCGGCGTTATTACAGGCCGGTTTTGTGCTGGCGACAGAAGCATTAAAGCCTAATTTATCGGCGTTAAACCCGGTAGAAGGGGCAAAAAAACTTTTTAGTATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGTCAGCGAAGAACGTGTTGCGGAAA',\n 'ATGCCGTTAGTGAAGGCGCCACGC',\n 'ATGGGATCC',\n 'ATGGGCATTTTTGCCTCCGCAGGATGCGGTAAGACCATGCTGATGCATATGCTGATCGAGCAAACGGAGGCGGATGTCTTTGTTATCGGTCTTATCGGTGAACGAGGCCGTGAGGTCACTGAATTCGTGGATATGTTGCGCGCTTCGCATAAGAAAGAAAAATGCGTGCTGGTTTTTGCCACTTCCGATTTCCCCTCGGTCGATCGCTGCAATGCGGCGCAACTGGCGACAACCGTAGCGGAATATTTTCGCGACCAGGGAAAACGGGTCGTGCTTTTTATCGATTCCATGACCCGTTATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGATAATTTGCCCCGCTTGCTGGAACGCCCAGGGGCGACCAGCGAGGGAAGCATTACTGCCTTTTATACGGTACTGCTGGAAAGCGAGGAAGAGGCGGACCCGATGGCGGATGAAATTCGCTCTATCCTTGACGGTCACCTGTATCTGAGCAGAAAGCTGGCCGGGCAGGGACATTACCCGGCAATCGATGTACTGAAAAGCGTAAGCCGCGTTTTTGGACAAGTCACGACGCCGACACATGCTGAACAGGCATCTGCCGTGCGTAAATTAATGACGCGTTTGGAAGAGCTCCAGCTTTTCATTGACTTGGGAGAATATCGTCCTGGCGAAAATATCGATAACGATCGGGCGATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGCTGATGCATATGCTGATCGAGCAAACGGAGGCGGATGTCTTTGTTATCGGTCTTATCGGTGAACGAGGCCGTGAGGTCACTGAATTCGTGGATATGTTGCGCGCTTCGCATAAGAAAGAAAAATGCGTGCTGGTTTTTGCCACTTCCGATTTCCCCTCGGTCGATCGCTGCAATGCGGCGCAACTGGCGACAACCGTAGCGGAATATTTTCGCGACCAGGGAAAACGGGTCGTGCTTTTTATCGATTCCATGACCCGTTATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGATAATTTGCCCCGCTTGCTGGAACGCCCAGGGGCGACCAGCGAGGGAAGCATTACTGCCTTTTATACGGTACTGCTGGAAAGCGAGGAAGAGGCGGACCCGATGGCGGATGAAATTCGCTCTATCCTTGACGGTCACCTGTATCTGAGCAGAAAGCTGGCCGGGCAGGGACATTACCCGGCAATCGATGTACTGAAAAGCGTAAGCCGCGTTTTTGGACAAGTCACGACGCCGACACATGCTGAACAGGCATCTGCCGTGCGTAAATTAATGACGCGTTTGGAAGAGCTCCAGCTTTTCATTGACTTGGGAGAATATCGTCCTGGCGAAAATATCGATAACGATCGGGCGATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGCATATGCTGATCGAGCAAACGGAGGCGGATGTCTTTGTTATCGGTCTTATCGGTGAACGAGGCCGTGAGGTCACTGAATTCGTGGATATGTTGCGCGCTTCGCATAAGAAAGAAAAATGCGTGCTGGTTTTTGCCACTTCCGATTTCCCCTCGGTCGATCGCTGCAATGCGGCGCAACTGGCGACAACCGTAGCGGAATATTTTCGCGACCAGGGAAAACGGGTCGTGCTTTTTATCGATTCCATGACCCGTTATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGATAATTTGCCCCGCTTGCTGGAACGCCCAGGGGCGACCAGCGAGGGAAGCATTACTGCCTTTTATACGGTACTGCTGGAAAGCGAGGAAGAGGCGGACCCGATGGCGGATGAAATTCGCTCTATCCTTGACGGTCACCTGTATCTGAGCAGAAAGCTGGCCGGGCAGGGACATTACCCGGCAATCGATGTACTGAAAAGCGTAAGCCGCGTTTTTGGACAAGTCACGACGCCGACACATGCTGAACAGGCATCTGCCGTGCGTAAATTAATGACGCGTTTGGAAGAGCTCCAGCTTTTCATTGACTTGGGAGAATATCGTCCTGGCGAAAATATCGATAACGATCGGGCGATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGCTGATCGAGCAAACGGAGGCGGATGTCTTTGTTATCGGTCTTATCGGTGAACGAGGCCGTGAGGTCACTGAATTCGTGGATATGTTGCGCGCTTCGCATAAGAAAGAAAAATGCGTGCTGGTTTTTGCCACTTCCGATTTCCCCTCGGTCGATCGCTGCAATGCGGCGCAACTGGCGACAACCGTAGCGGAATATTTTCGCGACCAGGGAAAACGGGTCGTGCTTTTTATCGATTCCATGACCCGTTATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGATAATTTGCCCCGCTTGCTGGAACGCCCAGGGGCGACCAGCGAGGGAAGCATTACTGCCTTTTATACGGTACTGCTGGAAAGCGAGGAAGAGGCGGACCCGATGGCGGATGAAATTCGCTCTATCCTTGACGGTCACCTGTATCTGAGCAGAAAGCTGGCCGGGCAGGGACATTACCCGGCAATCGATGTACTGAAAAGCGTAAGCCGCGTTTTTGGACAAGTCACGACGCCGACACATGCTGAACAGGCATCTGCCGTGCGTAAATTAATGACGCGTTTGGAAGAGCTCCAGCTTTTCATTGACTTGGGAGAATATCGTCCTGGCGAAAATATCGATAACGATCGGGCGATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGTTGCGCGCTTCGCATAAGAAAGAAAAATGCGTGCTGGTTTTTGCCACTTCCGATTTCCCCTCGGTCGATCGCTGCAATGCGGCGCAACTGGCGACAACCGTAGCGGAATATTTTCGCGACCAGGGAAAACGGGTCGTGCTTTTTATCGATTCCATGACCCGTTATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGATAATTTGCCCCGCTTGCTGGAACGCCCAGGGGCGACCAGCGAGGGAAGCATTACTGCCTTTTATACGGTACTGCTGGAAAGCGAGGAAGAGGCGGACCCGATGGCGGATGAAATTCGCTCTATCCTTGACGGTCACCTGTATCTGAGCAGAAAGCTGGCCGGGCAGGGACATTACCCGGCAATCGATGTACTGAAAAGCGTAAGCCGCGTTTTTGGACAAGTCACGACGCCGACACATGCTGAACAGGCATCTGCCGTGCGTAAATTAATGACGCGTTTGGAAGAGCTCCAGCTTTTCATTGACTTGGGAGAATATCGTCCTGGCGAAAATATCGATAACGATCGGGCGATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGACCCGTTATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGATAATTTGCCCCGCTTGCTGGAACGCCCAGGGGCGACCAGCGAGGGAAGCATTACTGCCTTTTATACGGTACTGCTGGAAAGCGAGGAAGAGGCGGACCCGATGGCGGATGAAATTCGCTCTATCCTTGACGGTCACCTGTATCTGAGCAGAAAGCTGGCCGGGCAGGGACATTACCCGGCAATCGATGTACTGAAAAGCGTAAGCCGCGTTTTTGGACAAGTCACGACGCCGACACATGCTGAACAGGCATCTGCCGTGCGTAAATTAATGACGCGTTTGGAAGAGCTCCAGCTTTTCATTGACTTGGGAGAATATCGTCCTGGCGAAAATATCGATAACGATCGGGCGATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGGCGGATGAAATTCGCTCTATCCTTGACGGTCACCTGTATCTGAGCAGAAAGCTGGCCGGGCAGGGACATTACCCGGCAATCGATGTACTGAAAAGCGTAAGCCGCGTTTTTGGACAAGTCACGACGCCGACACATGCTGAACAGGCATCTGCCGTGCGTAAATTAATGACGCGTTTGGAAGAGCTCCAGCTTTTCATTGACTTGGGAGAATATCGTCCTGGCGAAAATATCGATAACGATCGGGCGATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGACGCGTTTGGAAGAGCTCCAGCTTTTCATTGACTTGGGAGAATATCGTCCTGGCGAAAATATCGATAACGATCGGGCGATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGAATGCATTCGCTGACCAGAAT',\n 'ATGGGCGATGTGTCAGCTGTCAGTTCATCCGGGAACATTTTACTGCCGCAGCAGGATGAGGTTGGCGGTTTATCAGAAGCATTAAAAAAAGCGGTGGAAAAACATAAGACAGAATATTCCGGTGATAAAAAAGATCGCGACTATGGCGATGCTTTCGTAATGCATAAAGAAACGGCTTTACCGTTATTACTGGCGGCATGGCGACATGGCGCGCCAGCGAAATCAGAACATCACAATGGCAACGTTTCTGGTCTGCATCATAACGGAAAAAGCGAACTCAGGATTGCTGAAAAACTGTTGAAAGTCACTGCTGAAAAATCTGTCGGTTTGATCTCTGCGGAGGCCAAAGTAGATAAATCCGCAGCGTTGCTATCGTCTAAAAATAGGCCGTTAGAAAGCGTAAGCGGTAAAAAATTATCTGCTGATTTAAAAGCTGTGGAATCCGTTAGTGAAGTAACCGATAACGCCACGGGAATCTCTGACGATAATATCAAGGCATTGCCTGGGGATAATAAAGCCATCGCGGGCGAAGGCGTTCGTAAAGAGGGCGCGCCGCTGGCGCGGGATGTCGCACCTGCCCGAATGGCCGCAGCCAATACCGGTAAGCCTGAAGATAAAGATCATAAAAAGGTTAAAGATGTTTCTCAGCTTCCGCTGCAACCAACCACTATCGCCGATCTTAGCCAATTAACCGGCGGCGATGAAAAAATGCCTTTAGCGGCGCAATCAAAGCCGATGATGACTATTTTTCCCACTGCCGATGGCGTGAAAGGAGAGGATAGCTCGCTGACTTACCGTTTTCAGCGCTGGGGAAATGACTATTCCGTCAATATTCAGGCGCGGCAAGCAGGGGAGTTTTCGTTAATACCGTCAAATACGCAGGTTGAACATCGTTTGCATGATCAATGGCAAAACGGTAATCCCCAGCGCTGGCACCTGACGCGAGACGATCAACAAAATCCGCAGCAGCAACAGCACAGACAGCAATCTGGCGAGGAGGATGACGCC',\n 'ATGCATAAAGAAACGGCTTTACCGTTATTACTGGCGGCATGGCGACATGGCGCGCCAGCGAAATCAGAACATCACAATGGCAACGTTTCTGGTCTGCATCATAACGGAAAAAGCGAACTCAGGATTGCTGAAAAACTGTTGAAAGTCACTGCTGAAAAATCTGTCGGTTTGATCTCTGCGGAGGCCAAAGTAGATAAATCCGCAGCGTTGCTATCGTCTAAAAATAGGCCGTTAGAAAGCGTAAGCGGTAAAAAATTATCTGCTGATTTAAAAGCTGTGGAATCCGTTAGTGAAGTAACCGATAACGCCACGGGAATCTCTGACGATAATATCAAGGCATTGCCTGGGGATAATAAAGCCATCGCGGGCGAAGGCGTTCGTAAAGAGGGCGCGCCGCTGGCGCGGGATGTCGCACCTGCCCGAATGGCCGCAGCCAATACCGGTAAGCCTGAAGATAAAGATCATAAAAAGGTTAAAGATGTTTCTCAGCTTCCGCTGCAACCAACCACTATCGCCGATCTTAGCCAATTAACCGGCGGCGATGAAAAAATGCCTTTAGCGGCGCAATCAAAGCCGATGATGACTATTTTTCCCACTGCCGATGGCGTGAAAGGAGAGGATAGCTCGCTGACTTACCGTTTTCAGCGCTGGGGAAATGACTATTCCGTCAATATTCAGGCGCGGCAAGCAGGGGAGTTTTCGTTAATACCGTCAAATACGCAGGTTGAACATCGTTTGCATGATCAATGGCAAAACGGTAATCCCCAGCGCTGGCACCTGACGCGAGACGATCAACAAAATCCGCAGCAGCAACAGCACAGACAGCAATCTGGCGAGGAGGATGACGCC',\n 'ATGGCCGCAGCCAATACCGGTAAGCCTGAAGATAAAGATCATAAAAAGGTTAAAGATGTTTCTCAGCTTCCGCTGCAACCAACCACTATCGCCGATCTTAGCCAATTAACCGGCGGCGATGAAAAAATGCCTTTAGCGGCGCAATCAAAGCCGATGATGACTATTTTTCCCACTGCCGATGGCGTGAAAGGAGAGGATAGCTCGCTGACTTACCGTTTTCAGCGCTGGGGAAATGACTATTCCGTCAATATTCAGGCGCGGCAAGCAGGGGAGTTTTCGTTAATACCGTCAAATACGCAGGTTGAACATCGTTTGCATGATCAATGGCAAAACGGTAATCCCCAGCGCTGGCACCTGACGCGAGACGATCAACAAAATCCGCAGCAGCAACAGCACAGACAGCAATCTGGCGAGGAGGATGACGCC',\n 'ATGCCTTTAGCGGCGCAATCAAAGCCGATGATGACTATTTTTCCCACTGCCGATGGCGTGAAAGGAGAGGATAGCTCGCTGACTTACCGTTTTCAGCGCTGGGGAAATGACTATTCCGTCAATATTCAGGCGCGGCAAGCAGGGGAGTTTTCGTTAATACCGTCAAATACGCAGGTTGAACATCGTTTGCATGATCAATGGCAAAACGGTAATCCCCAGCGCTGGCACCTGACGCGAGACGATCAACAAAATCCGCAGCAGCAACAGCACAGACAGCAATCTGGCGAGGAGGATGACGCC',\n 'ATGATGACTATTTTTCCCACTGCCGATGGCGTGAAAGGAGAGGATAGCTCGCTGACTTACCGTTTTCAGCGCTGGGGAAATGACTATTCCGTCAATATTCAGGCGCGGCAAGCAGGGGAGTTTTCGTTAATACCGTCAAATACGCAGGTTGAACATCGTTTGCATGATCAATGGCAAAACGGTAATCCCCAGCGCTGGCACCTGACGCGAGACGATCAACAAAATCCGCAGCAGCAACAGCACAGACAGCAATCTGGCGAGGAGGATGACGCC',\n 'ATGACTATTTTTCCCACTGCCGATGGCGTGAAAGGAGAGGATAGCTCGCTGACTTACCGTTTTCAGCGCTGGGGAAATGACTATTCCGTCAATATTCAGGCGCGGCAAGCAGGGGAGTTTTCGTTAATACCGTCAAATACGCAGGTTGAACATCGTTTGCATGATCAATGGCAAAACGGTAATCCCCAGCGCTGGCACCTGACGCGAGACGATCAACAAAATCCGCAGCAGCAACAGCACAGACAGCAATCTGGCGAGGAGGATGACGCC',\n 'ATGGCCGGGAAGCGACGCTGGAATATCCGACGCGACAGGGAATGTGGGTTCGGT',\n 'ATGCAGAAAAACGGTGGTCGGCCTGGATTAAACCTGGGGACTGGCTTGAGCATGTCTCTCCCGCTCTGGCTGGGGCGGCGGTTTCTGCTGGCGCTGAGCACCTGGTCGTTCCCTGGCTTGCTGCAACAGAGCGACCGTTTGAGTTGCCCGTGCCGCATTTGTCCTGTCGGCGTTTATGCG',\n 'ATGTCTCTCCCGCTCTGGCTGGGGCGGCGGTTTCTGCTGGCGCTGAGCACCTGGTCGTTCCCTGGCTTGCTGCAACAGAGCGACCGTTTGAGTTGCCCGTGCCGCATTTGTCCTGTCGGCGTTTATGCG',\n 'ATGTACTCC',\n 'ATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGA',\n 'ATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGA',\n 'ATGGCGAACTGGTACAGA',\n 'ATGACACCT',\n 'ATGAATGGC',\n 'ATGGGGAATGATATCTCATTAATTGCCTTACTGGCATTTTCCACCCTGTTGCCATTTATTATTGCGTCAGGAACCTGTTTCGTTAAATTTTCTATTGTATTTGTCATGGTGCGTAACGCCCTGGGATTACAGCAGATACCTTCAAATATGACGCTTAACGGCGTCGCATTGCTGCTTTCTATGTTTGTTATGTGGCCCATAATGCATGATGCCTACGTCTATTTTGAGGACGAAGATGTCACCTTTAATGATATTTCATCATTAAGTAAACACGTTGATGAAGGTCTGGATGGTTATCGCGATTATCTGATCAAATATTCAGATCGCGAGTTAGTTCAGTTTTTTGAAAACGCGCAACTGAAGCGTCAGTATGGAGAAGAGACCGAGACGGTAAAGCGTGACAAAGATGAAATTGAAAAACCTTCAATATTTGCGTTATTACCTGCTTATGCGCTGAGCGAAATAAAAAGCGCGTTTAAAATTGGTTTTTATCTTTATTTGCCCTTTGTCGTCGTCGACCTGGTGGTATCCAGCGTGCTACTGGCGCTGGGGATGATGATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGGTGCGTAACGCCCTGGGATTACAGCAGATACCTTCAAATATGACGCTTAACGGCGTCGCATTGCTGCTTTCTATGTTTGTTATGTGGCCCATAATGCATGATGCCTACGTCTATTTTGAGGACGAAGATGTCACCTTTAATGATATTTCATCATTAAGTAAACACGTTGATGAAGGTCTGGATGGTTATCGCGATTATCTGATCAAATATTCAGATCGCGAGTTAGTTCAGTTTTTTGAAAACGCGCAACTGAAGCGTCAGTATGGAGAAGAGACCGAGACGGTAAAGCGTGACAAAGATGAAATTGAAAAACCTTCAATATTTGCGTTATTACCTGCTTATGCGCTGAGCGAAATAAAAAGCGCGTTTAAAATTGGTTTTTATCTTTATTTGCCCTTTGTCGTCGTCGACCTGGTGGTATCCAGCGTGCTACTGGCGCTGGGGATGATGATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGACGCTTAACGGCGTCGCATTGCTGCTTTCTATGTTTGTTATGTGGCCCATAATGCATGATGCCTACGTCTATTTTGAGGACGAAGATGTCACCTTTAATGATATTTCATCATTAAGTAAACACGTTGATGAAGGTCTGGATGGTTATCGCGATTATCTGATCAAATATTCAGATCGCGAGTTAGTTCAGTTTTTTGAAAACGCGCAACTGAAGCGTCAGTATGGAGAAGAGACCGAGACGGTAAAGCGTGACAAAGATGAAATTGAAAAACCTTCAATATTTGCGTTATTACCTGCTTATGCGCTGAGCGAAATAAAAAGCGCGTTTAAAATTGGTTTTTATCTTTATTTGCCCTTTGTCGTCGTCGACCTGGTGGTATCCAGCGTGCTACTGGCGCTGGGGATGATGATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGTTTGTTATGTGGCCCATAATGCATGATGCCTACGTCTATTTTGAGGACGAAGATGTCACCTTTAATGATATTTCATCATTAAGTAAACACGTTGATGAAGGTCTGGATGGTTATCGCGATTATCTGATCAAATATTCAGATCGCGAGTTAGTTCAGTTTTTTGAAAACGCGCAACTGAAGCGTCAGTATGGAGAAGAGACCGAGACGGTAAAGCGTGACAAAGATGAAATTGAAAAACCTTCAATATTTGCGTTATTACCTGCTTATGCGCTGAGCGAAATAAAAAGCGCGTTTAAAATTGGTTTTTATCTTTATTTGCCCTTTGTCGTCGTCGACCTGGTGGTATCCAGCGTGCTACTGGCGCTGGGGATGATGATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGTGGCCCATAATGCATGATGCCTACGTCTATTTTGAGGACGAAGATGTCACCTTTAATGATATTTCATCATTAAGTAAACACGTTGATGAAGGTCTGGATGGTTATCGCGATTATCTGATCAAATATTCAGATCGCGAGTTAGTTCAGTTTTTTGAAAACGCGCAACTGAAGCGTCAGTATGGAGAAGAGACCGAGACGGTAAAGCGTGACAAAGATGAAATTGAAAAACCTTCAATATTTGCGTTATTACCTGCTTATGCGCTGAGCGAAATAAAAAGCGCGTTTAAAATTGGTTTTTATCTTTATTTGCCCTTTGTCGTCGTCGACCTGGTGGTATCCAGCGTGCTACTGGCGCTGGGGATGATGATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGCATGATGCCTACGTCTATTTTGAGGACGAAGATGTCACCTTTAATGATATTTCATCATTAAGTAAACACGTTGATGAAGGTCTGGATGGTTATCGCGATTATCTGATCAAATATTCAGATCGCGAGTTAGTTCAGTTTTTTGAAAACGCGCAACTGAAGCGTCAGTATGGAGAAGAGACCGAGACGGTAAAGCGTGACAAAGATGAAATTGAAAAACCTTCAATATTTGCGTTATTACCTGCTTATGCGCTGAGCGAAATAAAAAGCGCGTTTAAAATTGGTTTTTATCTTTATTTGCCCTTTGTCGTCGTCGACCTGGTGGTATCCAGCGTGCTACTGGCGCTGGGGATGATGATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGATGATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGGACATTGCAACA',\n 'ATGCTTGTTTTTACTGTCTGGCTGGTATGGCGAAGTTTTACTCTCTTACGGGCGTCAGGTGATATTCCTGGCGTTGGC',\n 'ATGGCCGCATGCATTGAACGAGGCGCCGCCGTTTTTATCGGTGGCGATGATCCCGTTAGTTCTGCAAGAAGCGGCGGTAGGCGTCATGCTGGGCTGTCTGCTGTCATGGCCTTTTTGGGTTATGCATGCGCTGGGTTGTATTATCGA',\n 'ATGGCCTTTTTGGGTTATGCATGCGCTGGGTTGTATTATCGA',\n 'ATGCGATCCGATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTAT',\n 'ATGGCTCATTTAATGAGTTTATGGGGA',\n 'ATGAGTTTATGGGGA',\n 'ATGGCAATATTG',\n 'ATGCTATTGCGGAATATTTCC',\n 'ATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTG',\n 'ATGCGGAGAAGGTTGGCGTACCTG',\n 'ATGATCTGG',\n 'ATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGG',\n 'ATGTATTCATTGGGCGTTTTT',\n 'ATGTTCACTAACCACCGTCGGGGTTTAATAACTGCATCAGATAAACGCAGTCGT',\n 'ATGTCGCACCGCCGTCTTACGCTTCACGCGTTGGCGTCCGTGAACCGC',\n 'ATGTCTTTGTTATCGGTCTTATCGGTGAACGAGGCCGTGAGGTCACTGAATTCGTGGATATGTTGCGCGCTTCGCATAAGAAAGAAAAATGCGTGCTGGTTTTTGCCACTTCCGATTTCCCCTCGGTCGATCGCTGCAATGCGGCGCAACTGGCGACAACCG',\n 'ATGCGGCGCAACTGGCGACAACCG',\n 'ATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGATAATTTGCCCCGCTTGCTGGAACGCCCAGGGGCGACCAGCGAGGGAAGCATTACTGCCTTTTATACGGTACTGCTGGAAAGCGAGGAAGAGGCGGACCCGATGGCGGATGAAATTCGCTCTATCCTTGACGGTCACCTGTATC',\n 'ATGAAATTCGCTCTATCCTTGACGGTCACCTGTATC',\n 'ATGTAC',\n 'ATGCTGAACAGGCATCTGCCGTGCGTAAAT',\n 'ATGACACGT',\n 'ATGCATTCGCTGACCAGAATTAAAGTATTGCAGCGGCGCTGTACGGTATTTCATTCACAGTGTGAGTCGATATTACTTCGCTATCAGGATGAGGACCGCGGGCTGCAGGCCGAGGAGGAGGCGATCCTTGAACAAATAGCGGGTCTGAAATTGTTATTAGATACGCTGCGTGCAGAAAACAGACAGCTCAGTCGTGAGGAAATTTATACGTTATTACGTAAGCAGTCTATTGTTCGCCGGCAGATAAAAGATTTAGAACTCCAGATTATACAAATTCAGGAAAAACGGAGCGAGCTGGAAAAGAAAAGGGAAGAGTTTCAGAAAAAAAGTAAATATTGGTTGCGCAAAGAAGGGAACTATCAACGCTGGATAATCCGTCAGAAAAGATTCTATATCCAGCGAGAGATACAGCAGGAAGAGGCCGAGTCAGAGGAGATAATT',\n 'ATGTGTCAGCTGTCAGTTCATCCGGGAACATTTTACTGCCGCAGCAGGATGAGGTTGGCGGTTTATCAGAAGCAT',\n 'ATGAGGTTGGCGGTTTATCAGAAGCAT',\n 'ATGGCGATGCTTTCG',\n 'ATGCTTTCG',\n 'ATGGCGCGCCAGCGAAATCAGAACATCACAATGGCAACGTTTCTGGTCTGCATCATAACGGAAAAAGCGAACTCAGGATTGCTGAAAAACTGT',\n 'ATGGCAACGTTTCTGGTCTGCATCATAACGGAAAAAGCGAACTCAGGATTGCTGAAAAACTGT',\n 'ATGTCGCACCTGCCCGAATGGCCGCAGCCAATACCGGTAAGCCTGAAGATAAAGATCATAAAAAGGTTAAAGATGTTTCTCAGCTTCCGCTGCAACCAACCACTATCGCCGATCTTAGCCAAT',\n 'ATGTTTCTCAGCTTCCGCTGCAACCAACCACTATCGCCGATCTTAGCCAAT',\n 'ATGAAAAAATGCCTT',\n 'ATGGCG',\n 'ATGACTATTCCGTCAATATTCAGGCGCGGCAAGCAGGGGAGTTTTCGT',\n 'ATGATCAATGGCAAAACGGTAATCCCCAGCGCTGGCACC',\n 'ATGACGCCTGATGTCATTGCGTGTGAGACAGAT',\n 'ATGGCTATTGGCGCAAACCGCGACAGAATGCCAGCGCCATGGCCGGGAAGCGACGCTGGAATATCCGACGCGACAGGGAATGTGGGTTCGGTTGAGCGATGCAGAAAAACGGTGGTCGGCCTGGAT',\n 'ATGCCAGCGCCATGGCCGGGAAGCGACGCTGGAATATCCGACGCGACAGGGAATGTGGGTTCGGTTGAGCGATGCAGAAAAACGGTGGTCGGCCTGGAT',\n 'ATGCGTAGAGAACCCCGTACCGGGAAGCGCGCTGCCGGAAGGGAAATTGTTGCACATTATGAGCGATCGGGGCGGCCTGTGGTT',\n 'ATGGCTGAGCGAGTCTGG',\n 'ATGATATCTCAT',\n 'ATGATGCCTACGTCTATTTTGAGGACGAAGATGTCACCTTTAATGATATTTCATCAT',\n 'ATGCCTACGTCTATTTTGAGGACGAAGATGTCACCTTTAATGATATTTCATCAT',\n 'ATGTCACCTTTAATGATATTTCATCAT',\n 'ATGATATTTCATCAT',\n 'ATGAAGGTCTGGATGGTTATCGCGATTATC',\n 'ATGGTTATCGCGATTATC',\n 'ATGGAGAAGAGACCGAGACGG',\n 'ATGAAATTGAAAAACCTTCAATATTTGCGTTATTACCTGCTTATGCGC',\n 'ATGCGC',\n 'ATGGCTGGACCTTACTGTCTAAGGGAT',\n 'ATGGATGATTTAGTGTTTGCAGGTAATAAGGCGCTCTATCTTGTTTTGATCCTGTCAGGGTGGCCGACGATTGTCGCAACGATTATCGGCCTCCTGGTAGGGTTATTCCAGACGGTAACGCAATTACAGGAACAGACGCTGCCTTTTGGCATTAAATTACTTGGCGTGTGTTTATGCTTGTTTTTACTGTCTGGCTGGTATGGCGAAGTTTTACTCTCTTACGGGCGTCAGGTGATATTCCTGGCGTTGGCTAAGGGG',\n 'ATGTTTTACGCGTTGTACTTTGAAATTCATCACCTGGTTGCGTCTGCGGCGCTAGGGTTTGCTCGCGTGGCGCCGATTTTTTTCTTCCTGCCGTTTTTGAATAGCGGGGTATTAAGCGGTGCGCCGAGAAACGCCATTATCATCCTGGTGGCATTGGGAGTATGGCCGCATGCATTGAACGAGGCGCCGCCGTTTTTATCGGTGGCGATGATCCCGTTAGTTCTGCAAGAAGCGGCGGTAGGCGTCATGCTGGGCTGTCTGCTGTCATGGCCTTTTTGGGTTATGCATGCGCTGGGTTGTATTATCGATAACCAGCGAGGGGCAACGCTAAGTAGTAGTATCGATCCGGCAAACGGTATTGATACCTCGGAAATGGCTAATTTCCTGAATATGTTTGCCGCTGTCGTTTATTTACAAAACGGCGGTCTGGTCACGATGGTTGACGTGTTAAATAAAAGCTATCAGCTATGCGATCCGATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTATTAATCAGGTGGCTCAAAACGCCTTGGTTCTGGCCAGTCCGGTGGTATTAGTGCTGTTGCTGTCAGAAGTATTCCTGGGTTTATTGTCGCGCTTTGCTCCGCAAATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGATCCCGTTAGTTCTGCAAGAAGCGGCGGTAGGCGTCATGCTGGGCTGTCTGCTGTCATGGCCTTTTTGGGTTATGCATGCGCTGGGTTGTATTATCGATAACCAGCGAGGGGCAACGCTAAGTAGTAGTATCGATCCGGCAAACGGTATTGATACCTCGGAAATGGCTAATTTCCTGAATATGTTTGCCGCTGTCGTTTATTTACAAAACGGCGGTCTGGTCACGATGGTTGACGTGTTAAATAAAAGCTATCAGCTATGCGATCCGATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTATTAATCAGGTGGCTCAAAACGCCTTGGTTCTGGCCAGTCCGGTGGTATTAGTGCTGTTGCTGTCAGAAGTATTCCTGGGTTTATTGTCGCGCTTTGCTCCGCAAATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGCTGGGCTGTCTGCTGTCATGGCCTTTTTGGGTTATGCATGCGCTGGGTTGTATTATCGATAACCAGCGAGGGGCAACGCTAAGTAGTAGTATCGATCCGGCAAACGGTATTGATACCTCGGAAATGGCTAATTTCCTGAATATGTTTGCCGCTGTCGTTTATTTACAAAACGGCGGTCTGGTCACGATGGTTGACGTGTTAAATAAAAGCTATCAGCTATGCGATCCGATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTATTAATCAGGTGGCTCAAAACGCCTTGGTTCTGGCCAGTCCGGTGGTATTAGTGCTGTTGCTGTCAGAAGTATTCCTGGGTTTATTGTCGCGCTTTGCTCCGCAAATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGCATGCGCTGGGTTGTATTATCGATAACCAGCGAGGGGCAACGCTAAGTAGTAGTATCGATCCGGCAAACGGTATTGATACCTCGGAAATGGCTAATTTCCTGAATATGTTTGCCGCTGTCGTTTATTTACAAAACGGCGGTCTGGTCACGATGGTTGACGTGTTAAATAAAAGCTATCAGCTATGCGATCCGATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTATTAATCAGGTGGCTCAAAACGCCTTGGTTCTGGCCAGTCCGGTGGTATTAGTGCTGTTGCTGTCAGAAGTATTCCTGGGTTTATTGTCGCGCTTTGCTCCGCAAATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGGCTAATTTCCTGAATATGTTTGCCGCTGTCGTTTATTTACAAAACGGCGGTCTGGTCACGATGGTTGACGTGTTAAATAAAAGCTATCAGCTATGCGATCCGATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTATTAATCAGGTGGCTCAAAACGCCTTGGTTCTGGCCAGTCCGGTGGTATTAGTGCTGTTGCTGTCAGAAGTATTCCTGGGTTTATTGTCGCGCTTTGCTCCGCAAATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGTTTGCCGCTGTCGTTTATTTACAAAACGGCGGTCTGGTCACGATGGTTGACGTGTTAAATAAAAGCTATCAGCTATGCGATCCGATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTATTAATCAGGTGGCTCAAAACGCCTTGGTTCTGGCCAGTCCGGTGGTATTAGTGCTGTTGCTGTCAGAAGTATTCCTGGGTTTATTGTCGCGCTTTGCTCCGCAAATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGGTTGACGTGTTAAATAAAAGCTATCAGCTATGCGATCCGATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTATTAATCAGGTGGCTCAAAACGCCTTGGTTCTGGCCAGTCCGGTGGTATTAGTGCTGTTGCTGTCAGAAGTATTCCTGGGTTTATTGTCGCGCTTTGCTCCGCAAATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTATTAATCAGGTGGCTCAAAACGCCTTGGTTCTGGCCAGTCCGGTGGTATTAGTGCTGTTGCTGTCAGAAGTATTCCTGGGTTTATTGTCGCGCTTTGCTCCGCAAATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGGATTATCAAAATAATGTCAGCGAAGAACGTGTTGCGGAAATGATTTGGGATGCCGTTAGTGAAGGCGCCACGCTAAAAGACGTTCATGGGATCC',\n 'ATGATTTGGGATGCCGTTAGTGAAGGCGCCACGCTAAAAGACGTTCATGGGATCC',\n 'ATGCCGTACCTCGTTTTCTTGTGGCTGAATAACGTCTTTGCCCGCGTTTTCTACCTCTTCCAGCCAAACCAGAAGACG',\n 'ATGGGTTTTGAAAAGACTGCGCGCCAGTTTGATATCGACGATCACAGGTACGCCAACCTTCTCCGCATAGGCGCGGACGGCCAGTGCGCGCTGATTCGTTTCATACACCGAGATCATCGGAATCGGCATCAATTCGGGTTTAAAATAAATCCCGATCGTAATATGCGTGGGGTTGGCAACAATCAGGCG',\n 'ATGCGTGGGGTTGGCAACAATCAGGCG',\n 'ATGAACTTCACGTCTTTTAGATTTAACCTCTGGGTTCCCTTCCTGCTCCTTCATTTCACGCTTCACTTCTTCCTTATCCATTTTCATATCTTTCATGGTCAGGAAATATTCCGCAATAGCATCCAA',\n 'ATGAAGGCGTGCACTCGTTCATCGGATCGCATAGCTGATAGCTTTTATTTAACACGTCAACCATCG',\n 'ATGACAGCAGACAGCCCAGCA',\n 'ATGCATGCGGCCATACTCCCAATGCCACCAGGA',\n 'ATGCCACCAGGA',\n 'ATGTCCATATACTGTAATATCAATCCCTTAGACAGTAAGGTCCAGCCATCAAGCGCGACAAAAAGCACCAGCTTAATAGGTGTAGATATCGTCACCGGACTCATCATCATCATCCCCAGCGCCAGTAGCACGCTGGATACCACCAGGTCGACGACGACAAAGGGCAAA',\n 'ATGGGCCACATAACAAACATAGAAAGCAGCAATGCGACGCCGTTAAGCGTCATATTTGAAGGTATCTGCTGTAATCCCAGGGCGTTACGCACCATGACAAATACAATAGAAAATTTAACGAAACAGGTTCCTGACGCAATAATAAATGGCAACAGGGTGGAAAATGCCAGTAAGGCAATTAATGAGATATCATTCCCCATTACCAGACTCGCTCAGCCATTCATGGATCTCAACGCC',\n 'ATGACAAATACAATAGAAAATTTAACGAAACAGGTTCCTGACGCAATAATAAATGGCAACAGGGTGGAAAATGCCAGTAAGGCAATTAATGAGATATCATTCCCCATTACCAGACTCGCTCAGCCATTCATGGATCTCAACGCC',\n 'ATGGATCTCAACGCC',\n 'ATGTTGAATATC',\n 'ATGACC',\n 'ATGCTCAAACCACAGGCCGCCCCGATCGCTCATAATGTGCAACAATTTCCCTTCCGGCAGCGCGCTTCCCGGTACGGGGTTCTCTACGCA',\n 'ATGCGGCACGGGCAACTCAAACGGTCGCTCTGTTGCAGCAAGCCAGGGAACGACCAGGTGCTCAGCGCCAGCAGAAACCGCCGCCCCAGCCAGAGCGGGAGAGACATGCTCAAGCCAGTCCCCAGGTTTAATCCAGGCCGACCACCGTTTTTCTGCATCGCTCAACCGAACCCACATTCCCTGTCGCGTCGGATATTCCAGCGTCGCTTCCCGGCCATGGCGCTGGCATTCTGTCGCGGTTTGCGCCAA',\n 'ATGCTCAAGCCAGTCCCCAGGTTTAATCCAGGCCGACCACCGTTTTTCTGCATCGCTCAACCGAACCCACATTCCCTGTCGCGTCGGATATTCCAGCGTCGCTTCCCGGCCATGGCGCTGGCATTCTGTCGCGGTTTGCGCCAA',\n 'ATGGCGCTGGCATTCTGTCGCGGTTTGCGCCAA',\n 'ATGGCTTTATTATCCCCAGGCAATGCCTTGATATTATCGTCAGAGATTCCCGTGGCGTTATCGGTTACTTCACTAACGGATTCCACAGCTTTTAAATCAGCAGATAATTTTTTACCGCTTACGCTTTCTAACGGCCTATTTTTAGACGATAGCAACGCTGCGGATTTATCTACTTTGGCCTCCGCAGAGATCAAACCGACAGATTTTTCAGCAGTGACTTTCAACAGTTTTTCAGCAATCCTGAGTTCGCTTTTTCCGTTA',\n 'ATGTTCCCGGATGAACTGACAGCTGACACATCGCCCATTAAATTATCTCCTCTGACTCGGCCTCTTCCTGCTGTATCTCTCGCTGGATATAGAATCTTTTCTGACGGATTATCCAGCGTTGATAGTTCCCTTCTTTGCGCAACCAATATTTACTTTTTTTCTGAAACTCTTCCCTTTTCTTTTCCAGCTCGCTCCGTTTTTCCTGAATTTGTA',\n 'ATGAAAAGCTGGAGCTCTTCCAAACGCGTCATTAATTTACGCACGGCAGATGCCTGTTCAGCATGTGTCGGCGTCGTGACTTGTCCAAAAACGCGGCTTACGCTTTTCAGTACATCGATTGCCGGG',\n 'ATGCTTCCCTCGCTGGTCGCCCCTGGGCGTTCCAGCAAGCGGGGCAAATTATCGAATACGGAGGCGGGA',\n 'ATGGAATCGATAAAAAGCACGACCCGTTTTCCCTGGTCGCGAAAATATTCCGCTACGGTTGTCGCCAGTTGCGCCGCATTGCAGCGATCGACCGAGGGGAAATCGGAAGTGGCAAAAACCAGCACGCATTTTTCTTTCTTATGCGAAGCGCGCAACATATCCACGAATTCAGTGACCTCACGGCCTCGTTCACCGATAAGACCGATAACAAAGACATCCGCCTCCGTTTGCTCGATCAGCATATGCATCAGCATGGTCTTACCGCATCCTGCGGAGGCAAAAATGCCCATTCGCTGGCCTACGCCACAGGTCAATAACCCGTCAATCGCGCGCACACCGGTAATCAGCGGTTCACGGACGCCAACGCGTGAAGCG',\n 'ATGGTCTTACCGCATCCTGCGGAGGCAAAAATGCCCATTCGCTGGCCTACGCCACAGGTCAATAACCCGTCAATCGCGCGCACACCGGTAATCAGCGGTTCACGGACGCCAACGCGTGAAGCG',\n 'ATGCCCATTCGCTGGCCTACGCCACAGGTCAATAACCCGTCAATCGCGCGCACACCGGTAATCAGCGGTTCACGGACGCCAACGCGTGAAGCG',\n 'ATGAACGTCTTT',\n 'ATGCAGTTATTAAACCCCGACGGTGGT',\n 'ATGAATACATCGCTACTGCCTTACGCGGCTCAATGCCGTACCTCGTTTTCTTGTGGC',\n 'ATGGTCAGGAAATATTCCGCAATAGCATCCAATAATAAGACAATCAATGCGCAAGCAAGGCAAGTTAATACCAATGCGAGGAGAAGTTCACGCCAAATGACGGCAATACCTACAATATTGCCATTTAGC',\n 'ATGACGGCAATACCTACAATATTGCCATTTAGC',\n 'ATGATGGCGGCCACCACAAAGGATGAGAGATACAGTAGGGTTTTGACCGTATCTTTAACCGTGCGCATACTAAAAAGTTTTTTTGCCCCTTCTACCGGGTTTAACGCCGATAAATTAGGCTTTAATGCTTCTGTCGCCAGCACAAAACCGGCCTGTAATAACGCCGGTAATGCGGAACACACTAAGCAGAGCAGCATAAATGGAATCAGATATTTTAACCCTATCCCAAAAACGGCCAAACTG',\n 'ATGGCGGCCACCACAAAGGATGAGAGATACAGTAGGGTTTTGACCGTATCTTTAACCGTGCGCATACTAAAAAGTTTTTTTGCCCCTTCTACCGGGTTTAACGCCGATAAATTAGGCTTTAATGCTTCTGTCGCCAGCACAAAACCGGCCTGTAATAACGCCGGTAATGCGGAACACACTAAGCAGAGCAGCATAAATGGAATCAGATATTTTAACCCTATCCCAAAAACGGCCAAACTG',\n 'ATGCTC',\n 'ATGATCTTAATTATCCCCATAAACTCATTAAATGAGCCATACGACACCAGA',\n 'ATGAGATCTTTACTTTTAAATGACTGGCCTTTTTTAGCGGAGTCTTCCAGCCGTTTTTTAGTCGGTTTTTCTGTTTTATTCGAGGACATGCGTCGCCCCTCGCTCGTAAAACCAACTGCT',\n 'ATGCGTCGCCCCTCGCTCGTAAAACCAACTGCT',\n 'ATGCATAACCCAAAAAGGCCA',\n 'ATGCGGCCATACTCCCAATGCCACCAGGATGATAATGGCGTTTCTCGGCGCACCGCT',\n 'ATGAATTTCAAAGTACAACGCGTAAAACATTTTTTACCCCTTAGCCAACGCCAGGAATATCACCTGACGCCCGTAAGAGAG',\n 'ATGTTGCAATGTCCATATACTGTAATATCAATCCCT',\n 'ATGATGAAATATCAT',\n 'ATGAAATATCAT',\n 'ATGCGACGCCGT',\n 'ATGGCAACAGGGTGGAAAATGCCAGTAAGGCAATTAATGAGATATCATTCCCCATTACCAGACTCGCTCAGCCATTCATGGATCTCAACGCCTAAGGTGTCATTCATCTGTACCAGTTCGCCATTACCCAGCAAAACACCATTCGCCATAATTTCAACGTTAAGTTCAGCATTGGTCGGCAGTGATAATAGCTGTTGCTGCCCCATGGCTTCGAGTTCGGCGAGGGTAACGTTCTTACGATACAAAACAAATTCCAGTTTGACGGGCAAT',\n 'ATGCCAGTAAGGCAATTAATGAGATATCATTCCCCATTACCAGACTCGCTCAGCCATTCATGGATCTCAACGCCTAAGGTGTCATTCATCTGTACCAGTTCGCCATTACCCAGCAAAACACCATTCGCCATAATTTCAACGTTAAGTTCAGCATTGGTCGGCAGTGATAATAGCTGTTGCTGCCCCATGGCTTCGAGTTCGGCGAGGGTAACGTTCTTACGATACAAAACAAATTCCAGTTTGACGGGCAAT',\n 'ATGAGATATCATTCCCCATTACCAGACTCGCTCAGCCATTCATGGATCTCAACGCCTAAGGTGTCATTCATCTGTACCAGTTCGCCATTACCCAGCAAAACACCATTCGCCATAATTTCAACGTTAAGTTCAGCATTGGTCGGCAGTGATAATAGCTGTTGCTGCCCCATGGCTTCGAGTTCGGCGAGGGTAACGTTCTTACGATACAAAACAAATTCCAGTTTGACGGGCAAT',\n 'ATGGCTTCGAGTTCGGCGAGGGTAACGTTCTTACGATACAAAACAAATTCCAGTTTGACGGGCAAT',\n 'ATGTGCAACAATTTCCCTTCCGGCAGCGCGCTTCCCGGTACGGGGTTCTCTACGCATAAACGCCGACAGGACAAATGCGGCACGGGCAACTCAAACGGTCGCTCTGTTGCAGCAAGCCAGGGAACGACCAGGTGCTCAGCGCCAGCAGAAACCGCCGCCCCAGCCAGAGCGGGAGAGACATGCTCAAGCCAGTCCCCAGGTTTAATCCAGGCCGACCACCGTTTTTCTGCATCGCTCAACCGAACCCACATTCCCTGTCGCGTCGGATATTCCAGCGTCGCTTCCCGGCCATGGCGCTGGCATTCTGTCGCGGTTTGCGCCAATAGCCATTCGCGACGATCAATCTGTCTCACACGCAATGACATCAGGCGTCATCCTCCTCGCCAGATTGCTGTCTGTGCTGTTGCTGCTGCGGATTTTGTTGATCGTCTCGCGTCAGGTGCCAGCGCTGGGGATTACCGTTTTGCCATTGATCATGCAAACGATGTTCAACCTGCGTATTTGACGGTATTAACGAAAACTCCCCTGCTTGCCGCGCCTGAATAT',\n 'ATGCCT',\n 'ATGCCGCCAGTAATAACGGTAAAGCCGTTTCTTTATGCATTACGAAAGCATCGCCATAGTCGCGATCTTTTTTATCACCGGAATATTCTGTCTTATGTTTTTCCACCGCTTTTTTTAATGCTTCTGATAAACCGCCAACCTCATCCTGCTGCGGCAGTAAAATGTTCCCGGATGAAC',\n 'ATGCTTCTGATAAACCGCCAACCTCATCCTGCTGCGGCAGTAAAATGTTCCCGGATGAAC',\n 'ATGAAC',\n 'ATGAAATACCGTACAGCGCCGCTGCAATACTTTAATTCTGGTCAGCGAATGCATTCATACCGCTCAACGTGTCATCAAAGGATGAATACTGCGCTACCGGCTGGCATAACCAGGCTTTCAGGCTATCCCGCATCTGCATCGCCCGATCGTTATCGATATTTTCGCCAGGACGATATTCTCCCAAGTCAA',\n 'ATGCATTCATACCGCTCAACGTGTCATCAAAGGATGAATACTGCGCTACCGGCTGGCATAACCAGGCTTTCAGGCTATCCCGCATCTGCATCGCCCGATCGTTATCGATATTTTCGCCAGGACGATATTCTCCCAAGTCAA',\n 'ATGAATACTGCGCTACCGGCTGGCATAACCAGGCTTTCAGGCTATCCCGCATCTGCATCGCCCGATCGTTATCGATATTTTCGCCAGGACGATATTCTCCCAAGTCAA',\n 'ATGCCTGTTCAGCATGTGTCGGCGTCG',\n 'ATGCGCAAGCAAGGCAAGTTAATACCAATGCGAGGAGAAGTTCACGCCAAA',\n 'ATGCGAGGAGAAGTTCACGCCAAA',\n 'ATGAGAGATACAGTAGGGTTT',\n 'ATGCTTCTGTCGCCAGCACAAAACCGGCCTGTAATAACGCCGGTAATGCGGAACACACTAAGCAGAGCAGCA',\n 'ATGCGGAACACACTAAGCAGAGCAGCA',\n 'ATGGAATCAGATATTTTAACCCTATCCCAAAAACGGCCAAACTGTAGTCAGCCATGCTCTGATCAAAATTATCCGCAA',\n 'ATGAGCCATACGACACCAGATAGGCAATTCCTCCCAGCGTCAGGCAGGCGA',\n 'ATGACTGGCCTTTTT',\n 'ATGACGCCTACCGCCGCTTCTTGCAGAACTAACGGGATCATCGCCACCGATAAAAACGGCGGCGCCTCGTTCAATGCATGCGGCCATACTCCCAATGCCACCAGGATGATAATGGCGTTTCTCGGCGCACCGCTTAATACCCCGCTATTCAAAAACGGCAGGAAGAAAAAAATCGGCGCCACGCGAGCAAACCCTAGCGCCGCAGACGCAACCAGG',\n 'ATGATAATGGCGTTTCTCGGCGCACCGCTTAATACCCCGCTATTCAAAAACGGCAGGAAGAAAAAAATCGGCGCCACGCGAGCAAACCCTAGCGCCGCAGACGCAACCAGG',\n 'ATGGCGTTTCTCGGCGCACCGCTTAATACCCCGCTATTCAAAAACGGCAGGAAGAAAAAAATCGGCGCCACGCGAGCAAACCCTAGCGCCGCAGACGCAACCAGG',\n 'ATGCCAAAAGGCAGCGTCTGTTCCTGTAATTGCGTTACCGTCTGGAATAACCCTACCAGGAGGCCGATAATCGTTGCGACAATCGTCGGCCACCCTGACAGGATCAAAACAAGA',\n 'ATGATGTCATGTTGCAATGTCCATATACTG',\n 'ATGTCATGTTGCAATGTCCATATACTG',\n 'ATGCATTATGGGCCACATAACAAACATAGAAAGCAGCAATGCGACGCCGTTAAGCGTCATATT',\n 'ATGACATCAGGCGTCATCCTCCTCGCCAGATTGCTGTCTGTGCTGTTGCTGCTGCGGATTTTGTTGATCGTCTCGCGTCAGGTGCCAGCGCTGGGGATTACCGTTTTGCCATTGATCATGCAAACGATGTTCAACCTGCGTATT',\n 'ATGCAAACGATGTTCAACCTGCGTATT',\n 'ATGTTCAACCTGCGTATT',\n 'ATGATCTTTATCTTCAGGCTTACCGGTATTGGCTGCGGCCATTCGGGCAGGTGCGACATCCCGCGCCAGCGGCGCGCCCTCTTTACGAACGCCTTCGCCCGCGATGGCTTTATTATCCCCAGGCAATGCCTTGATATTATCGTCAGAGATTCCCGTGGCGTTATCGGTTACTTCACTAACGGATTCCACAGCTTT',\n 'ATGATGCAGACCAGAAACGTTGCCATTGTGATGTTC',\n 'ATGCAGACCAGAAACGTTGCCATTGTGATGTTC',\n 'ATGTTC',\n 'ATGTCGCCATGCCGCCAG',\n 'ATGCATTACGAAAGCATCGCCATAGTCGCGATCTTTTTTATCACCGGAATATTCTGTCTTATGTTTTTCCACCGCTTTTTT',\n 'ATGTTTTTCCACCGCTTTTTT',\n 'ATGTGTCGGCGTCGTGACTTGTCCAAAAACGCGGCTTACGCTTTTCAGTACATCGATTGCCGGGTAATGTCCCTGCCCGGCCAGCTTTCTGCTCAGATACAGGTGACCGTCAAGGATAGAGCGAATTTCATCCGCCATCGGGTCCGCCTCTTCCTCGCTTTCCAGCAGTACCGTATAAAAGGCAGTAATGCTTCCCTCGCTGGTCGCCCCTGGGCGTTCCAGCAAGCGGGGCAAATTATCGAATACGGAGGCGGGATAACCTCGACGAGCCGGACGCTCTCCCGACGCCAGTGCCACGTCTCGCAAAGCACGCGCATAACGGGTCATGGAATCGATAAAAAGCACGACCCGTTTTCCCTGGTCGCGAAAATATTCCGCTACGGTTGTCGCCAGTTGCGCCGCATTGCAGCGATCGACCGAGGGGAAATCGGAAGTGGCAAAAACCAGCACGCATTTTTCTTTCTTATGCGAAGCGCGCAACATATCCACGAATTCAGTGACCTCACGGCCTCGTTCACCGATAAGACCGATAACAAAGACATCCGCCTCCGTTTGCTCGATCAGCATATGCATCAGCATGGTCTTACCGCATCCTGCGGAGGCAAAAATGCCCATTCGCTGGCCTACGCCACAGGTCAA',\n 'ATGTCCCTGCCCGGCCAGCTTTCTGCTCAGATACAGGTGACCGTCAAGGATAGAGCGAATTTCATCCGCCATCGGGTCCGCCTCTTCCTCGCTTTCCAGCAGTACCGTATAAAAGGCAGTAATGCTTCCCTCGCTGGTCGCCCCTGGGCGTTCCAGCAAGCGGGGCAAATTATCGAATACGGAGGCGGGATAACCTCGACGAGCCGGACGCTCTCCCGACGCCAGTGCCACGTCTCGCAAAGCACGCGCATAACGGGTCATGGAATCGATAAAAAGCACGACCCGTTTTCCCTGGTCGCGAAAATATTCCGCTACGGTTGTCGCCAGTTGCGCCGCATTGCAGCGATCGACCGAGGGGAAATCGGAAGTGGCAAAAACCAGCACGCATTTTTCTTTCTTATGCGAAGCGCGCAACATATCCACGAATTCAGTGACCTCACGGCCTCGTTCACCGATAAGACCGATAACAAAGACATCCGCCTCCGTTTGCTCGATCAGCATATGCATCAGCATGGTCTTACCGCATCCTGCGGAGGCAAAAATGCCCATTCGCTGGCCTACGCCACAGGTCAA',\n 'ATGCGAAGCGCGCAACATATCCACGAATTCAGTGACCTCACGGCCTCGTTCACCGATAAGACCGATAACAAAGACATCCGCCTCCGTTTGCTCGATCAGCATATGCATCAGCATGGTCTTACCGCATCCTGCGGAGGCAAAAATGCCCATTCGCTGGCCTACGCCACAGGTCAA',\n 'ATGCATCAGCATGGTCTTACCGCATCCTGCGGAGGCAAAAATGCCCATTCGCTGGCCTACGCCACAGGTCAA']\n\n\n\ndef longest_ORF(dna):\n    \"\"\" Finds the longest ORF on both strands of the specified DNA and returns it\n        as a string\n    >>> longest_ORF(\"ATGCGAATGTAGCATCAAA\")\n    'ATGCTACATTCGCAT'\n    \"\"\"\n    #dna='ATGGCTAGCGATGCGAGCCCTACCGTGACCGATCCATGAGAGATGCTCTAGGCTATGAATGACGTAGCG'\n    ourorf2=find_all_ORFs_both_strands(dna)\n    thelength=[]\n    for i in ourorf2:\n        thelength.append(len(i))\n    seqlen=dict((j,i) for j,i in zip(thelength,ourorf2))\n    orderedlength=sorted(thelength)\n    return seqlen[thelength[-1]]\n#longest_ORF(dna)\n\n\n longest_ORF(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\n'ATGCATCAGCATGGTCTTACCGCATCCTGCGGAGGCAAAAATGCCCATTCGCTGGCCTACGCCACAGGTCAA'\n\n\n\nimport random\n\n\ndef shuffle_string(s):\n    \"\"\"Shuffles the characters in the input string\n        NOTE: this is a helper function, you do not\n        have to modify th|s in any way \"\"\"\n    return ''.join(random.sample(s, len(s)))\n\n\nnum_trials=1000\ndef longest_ORF_noncoding(dna, num_trials):\n    \"\"\" Computes the maximum length of the longest ORF over num_trials shuffles\n        of the specfied DNA sequence\n\n        dna: a DNA sequence\n        num_trials: the number of random shuffles\n        returns: the maximum length longest ORF \n    \"\"\"\n    num_trials=1000\n    import random\n    i=0\n    frames3 =[]\n    listofshuffled=[]\n    while i <num_trials:\n        listofshuffled.append(shuffle_string(dna))\n        i+=1\n    for i in listofshuffled:\n        frames3.append(longest_ORF(i))\n    return (max(frames3,key=len))\n#longest_ORF_noncoding(dna, num_trials) \n\n\n\nlongest_ORF_noncoding(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'), 1000)\n\n'ATGGGGTCTTGCCATTTCCTCCTTCTCAGCGTCTGTTTACAAGATGAAGTACACGCCGGTCCGTATCTGTCGGCCGAGCTACTTGTTGGCAAATGCGTGGATTTTGCAGAAGGTTACCAAAGCTCTGTTAGTCCGTCGACTCCACTAATTGCCAATTGCTCAGCGCAACTCACAGTCAGCACTCAAGCTCACGCTGCAGCTGCTCATTCTCTTCCAGGTAGTTGGGAGAGATGCACCTTCACTCAGCCTCTCGCTGAGAAAGCTCCTCCGCAAAGGGCCCATCTGATTAGACATTACTGTCTGGTCCTCCTCTTTTCTGATCAA'\n\n\n\ndef coding_strand_to_AA(dna):\n    \"\"\" Computes the Protein encoded by a sequence of DNA.  This function\n        does not check for start and stop codons (it assumes that the input\n        DNA sequence represents an protein coding region).\n\n        dna: a DNA sequence represented as a string\n        returns: a string containing the sequence of amino acids encoded by the\n                 the input DNA fragment\n\n        >>> coding_strand_to_AA(\"ATGCGA\")\n        'MR'\n        >>> coding_strand_to_AA(\"ATGCCCGCTTT\")\n        'MPA'\n    \"\"\"\n    aa = ['F', 'L', 'I', 'M', 'V', 'S', 'P', 'T', 'A', 'Y',\n      '|', 'H', 'Q', 'N', 'K', 'D', 'E', 'C', 'W', 'R',\n      'G']\n\n    codons = [['TTT', 'TTC'],\n              ['TTA', 'TTG', 'CTT', 'CTC', 'CTA', 'CTG'],\n              ['ATT', 'ATC', 'ATA'],\n              ['ATG'],\n              ['GTT', 'GTC', 'GTA', 'GTG'],\n              ['TCT', 'TCC', 'TCA', 'TCG', 'AGT', 'AGC'],\n              ['CCT', 'CCC', 'CCA', 'CCG'],\n              ['ACT', 'ACC', 'ACA', 'ACG'],\n              ['GCT', 'GCC', 'GCA', 'GCG'],\n              ['TAT', 'TAC'],\n              ['TAA', 'TAG', 'TGA'],\n              ['CAT', 'CAC'],\n              ['CAA', 'CAG'],\n              ['AAT', 'AAC'],\n              ['AAA', 'AAG'],\n              ['GAT', 'GAC'],\n              ['GAA', 'GAG'],\n              ['TGT', 'TGC'],\n              ['TGG'],\n              ['CGT', 'CGC', 'CGA', 'CGG', 'AGA', 'AGG'],\n              ['GGT', 'GGC', 'GGA', 'GGG']]\n\n    # create a dictionary lookup table for mapping codons into amino acids\n    aa_table = {}\n    for i in range(len(aa)):\n        for codon in codons[i]:\n            aa_table[codon] = aa[i]\n    init_pos = 0\n    return''.join([aa_table[dna[pos:pos + 3]] for pos in range (init_pos, len(dna) -2, 3)])\n    pass\n#coding_strand_to_AA(dna)\n\n\ncoding_strand_to_AA(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\n'GSDRENR|AFYP|SGAD|RRTRY|CRTAVLRFTRWRP|TADYRCARD|RVIDLWRRPANGHFCLRRMR|DHADAYADRANGGGCLCYRSYR|TRP|GH|IRGYVARFA|ERKMRAGFCHFRFPLGRSLQCGATGDNRSGIFSRPGKTGRAFYRFHDPLCACFARRGTGVGRASGSSRLSRLRIR|FAPLAGTPRGDQRGKHYCLLYGTAGKRGRGGPDGG|NSLYP|RSPVSEQKAGRAGTLPGNRCTEKRKPRFWTSHDADTC|TGICRA|INDAFGRAPAFH|LGRISSWRKYR|RSGDADAG|PESLVMPAGSAVFIL||HVERYECIR|PELKYCSGAVRYFIHSVSRYYFAIRMRTAGCRPRRRRSLNK|RV|NCY|IRCVQKTDSSVVRKFIRYYVSSLLFAGR|KI|NSRLYKFRKNGASWKRKGKSFRKKVNIGCAKKGTINAG|SVRKDSISSERYSRKRPSQRR|FNGRCVSCQFIREHFTAAAG|GWRFIRSIKKSGGKT|DRIFR||KRSRLWRCFRNA|RNGFTVITGGMATWRASEIRTSQWQRFWSAS|RKKRTQDC|KTVESHC|KICRFDLCGGQSR|IRSVAIV|K|AVRKRKR|KIIC|FKSCGIR||SNR|RHGNL|R|YQGIAWG||SHRGRRRS|RGRAAGAGCRTCPNGRSQYR|A|R|RS|KG|RCFSASAATNHYRRS|PINRRR|KNAFSGAIKADDDYFSHCRWRERRG|LADLPFSALGK|LFRQYSGAASRGVFVNTVKYAG|TSFA|SMAKR|SPALAPDARRSTKSAAATAQTAIWRGG|RLMSLRVRQIDRREWLLAQTATECQRHGREATLEYPTRQGMWVRLSDAEKRWSAWIKPGDWLEHVSPALAGAAVSAGAEHLVVPWLAATERPFELPVPHLSCRRLCVENPVPGSALPEGKLLHIMSDRGGLWFEHLPELPAVGGGRPKMLRWPLRFVIGSSDTQRSLLGRIGIGDVLLIRTSRAEVYCYAKKLGHFNRVEGGIIVETLDIQHIEEENNTTETAETLPGLNQLPVKLEFVLYRKNVTLAELEAMGQQQLLSLPTNAELNVEIMANGVLLGNGELVQMNDTLGVEIHEWLSESGNGE|YLINCLTGIFHPVAIYYCVRNLFR|IFYCICHGA|RPGITADTFKYDA|RRRIAAFYVCYVAHNA|CLRLF|GRRCHL||YFIIK|TR||RSGWLSRLSDQIFRSRVSSVF|KRATEASVWRRDRDGKA|QR|N|KTFNICVITCLCAERNKKRV|NWFLSLFALCRRRPGGIQRATGAGDDDDESGDDIYTY|AGAFCRA|WLDLTV|GIDITVYGHCNMTSLRDGIVKWMI|CLQVIRRSILF|SCQGGRRLSQRLSASW|GYSRR|RNYRNRRCLLALNYLACVYACFYCLAGMAKFYSLTGVR|YSWRWLRGKKCFTRCTLKFITWLRLRR|GLLAWRRFFSSCRF|IAGY|AVRRETPLSSWWHWEYGRMH|TRRRRFYRWR|SR|FCKKRR|ASCWAVCCHGLFGLCMRWVVLSITSEGQR|VVVSIRQTVLIPRKWLIS|ICLPLSFIYKTAVWSRWLTC|IKAISYAIR|TSARLHYRRY|RLLIRWLKTPWFWPVRWY|CCCCQKYSWVYCRALLRK|TLLRFH|R|KAVLPF|LCCFISLRYYRTMYCDSLSRPQG|AVGFTSEGRRMSSNKTEKPTKKRLEDSAKKGQSFKSKDLIIACLTLGGIAYLVSYGSFNEFMGIIKIIIADNFDQSMADYSLAVFGIGLKYLIPFMLLCLVCSALPALLQAGFVLATEALKPNLSALNPVEGAKKLFSMRTVKDTVKTLLYLSSFVVAAIICWKKYKVEIFSQLNGNIVGIAVIWRELLLALVLTCLACALIVLLLDAIAEYFLTMKDMKMDKEEVKREMKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH|AA|GSSDVFIGRFLNVH|PPSGFNNCIR|TQSLSSTKSVTDNRSK|WIIKIMSAKNVLRK|FGMPLVKAPR|KTFMGS'\n\n\n\ndef gene_finder(dna):\n    \"\"\" Returns the amino acid sequences that are likely coded by the specified dna\n\n        dna: a DNA sequence\n        returns: a list of all amino acid sequences coded by the sequence dna.\n    \"\"\"\n    my_genes=[]\n    for i in find_all_ORFs_both_strands(dna):\n        my_genes.append(coding_strand_to_AA(i))\n    return my_genes\n#gene_finder(dna)\n\n\ngene_finder(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\n['MR',\n 'MRAGFCHFRFPLGRSLQCGATGDNRSGIFSRPGKTGRAFYRFHDPLCACFARRGTGVGRASGSSRLSRLRIR',\n 'MPAGSAVFIL',\n 'MRTAGCRPRRRRSLNK',\n 'MATWRASEIRTSQWQRFWSAS',\n 'MAKR',\n 'MSLRVRQIDRREWLLAQTATECQRHGREATLEYPTRQGMWVRLSDAEKRWSAWIKPGDWLEHVSPALAGAAVSAGAEHLVVPWLAATERPFELPVPHLSCRRLCVENPVPGSALPEGKLLHIMSDRGGLWFEHLPELPAVGGGRPKMLRWPLRFVIGSSDTQRSLLGRIGIGDVLLIRTSRAEVYCYAKKLGHFNRVEGGIIVETLDIQHIEEENNTTETAETLPGLNQLPVKLEFVLYRKNVTLAELEAMGQQQLLSLPTNAELNVEIMANGVLLGNGELVQMNDTLGVEIHEWLSESGNGE',\n 'MWVRLSDAEKRWSAWIKPGDWLEHVSPALAGAAVSAGAEHLVVPWLAATERPFELPVPHLSCRRLCVENPVPGSALPEGKLLHIMSDRGGLWFEHLPELPAVGGGRPKMLRWPLRFVIGSSDTQRSLLGRIGIGDVLLIRTSRAEVYCYAKKLGHFNRVEGGIIVETLDIQHIEEENNTTETAETLPGLNQLPVKLEFVLYRKNVTLAELEAMGQQQLLSLPTNAELNVEIMANGVLLGNGELVQMNDTLGVEIHEWLSESGNGE',\n 'MSDRGGLWFEHLPELPAVGGGRPKMLRWPLRFVIGSSDTQRSLLGRIGIGDVLLIRTSRAEVYCYAKKLGHFNRVEGGIIVETLDIQHIEEENNTTETAETLPGLNQLPVKLEFVLYRKNVTLAELEAMGQQQLLSLPTNAELNVEIMANGVLLGNGELVQMNDTLGVEIHEWLSESGNGE',\n 'MLRWPLRFVIGSSDTQRSLLGRIGIGDVLLIRTSRAEVYCYAKKLGHFNRVEGGIIVETLDIQHIEEENNTTETAETLPGLNQLPVKLEFVLYRKNVTLAELEAMGQQQLLSLPTNAELNVEIMANGVLLGNGELVQMNDTLGVEIHEWLSESGNGE',\n 'MGQQQLLSLPTNAELNVEIMANGVLLGNGELVQMNDTLGVEIHEWLSESGNGE',\n 'MANGVLLGNGELVQMNDTLGVEIHEWLSESGNGE',\n 'MNDTLGVEIHEWLSESGNGE',\n 'MTSLRDGIVKWMI',\n 'MI',\n 'MAKFYSLTGVR',\n 'MH',\n 'MRWVVLSITSEGQR',\n 'MYCDSLSRPQG',\n 'MSSNKTEKPTKKRLEDSAKKGQSFKSKDLIIACLTLGGIAYLVSYGSFNEFMGIIKIIIADNFDQSMADYSLAVFGIGLKYLIPFMLLCLVCSALPALLQAGFVLATEALKPNLSALNPVEGAKKLFSMRTVKDTVKTLLYLSSFVVAAIICWKKYKVEIFSQLNGNIVGIAVIWRELLLALVLTCLACALIVLLLDAIAEYFLTMKDMKMDKEEVKREMKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MGIIKIIIADNFDQSMADYSLAVFGIGLKYLIPFMLLCLVCSALPALLQAGFVLATEALKPNLSALNPVEGAKKLFSMRTVKDTVKTLLYLSSFVVAAIICWKKYKVEIFSQLNGNIVGIAVIWRELLLALVLTCLACALIVLLLDAIAEYFLTMKDMKMDKEEVKREMKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MADYSLAVFGIGLKYLIPFMLLCLVCSALPALLQAGFVLATEALKPNLSALNPVEGAKKLFSMRTVKDTVKTLLYLSSFVVAAIICWKKYKVEIFSQLNGNIVGIAVIWRELLLALVLTCLACALIVLLLDAIAEYFLTMKDMKMDKEEVKREMKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MLLCLVCSALPALLQAGFVLATEALKPNLSALNPVEGAKKLFSMRTVKDTVKTLLYLSSFVVAAIICWKKYKVEIFSQLNGNIVGIAVIWRELLLALVLTCLACALIVLLLDAIAEYFLTMKDMKMDKEEVKREMKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MRTVKDTVKTLLYLSSFVVAAIICWKKYKVEIFSQLNGNIVGIAVIWRELLLALVLTCLACALIVLLLDAIAEYFLTMKDMKMDKEEVKREMKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MKDMKMDKEEVKREMKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MKMDKEEVKREMKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MDKEEVKREMKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MSAKNVLRK',\n 'MPLVKAPR',\n 'MGS',\n 'MGIFASAGCGKTMLMHMLIEQTEADVFVIGLIGERGREVTEFVDMLRASHKKEKCVLVFATSDFPSVDRCNAAQLATTVAEYFRDQGKRVVLFIDSMTRYARALRDVALASGERPARRGYPASVFDNLPRLLERPGATSEGSITAFYTVLLESEEEADPMADEIRSILDGHLYLSRKLAGQGHYPAIDVLKSVSRVFGQVTTPTHAEQASAVRKLMTRLEELQLFIDLGEYRPGENIDNDRAMQMRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MLMHMLIEQTEADVFVIGLIGERGREVTEFVDMLRASHKKEKCVLVFATSDFPSVDRCNAAQLATTVAEYFRDQGKRVVLFIDSMTRYARALRDVALASGERPARRGYPASVFDNLPRLLERPGATSEGSITAFYTVLLESEEEADPMADEIRSILDGHLYLSRKLAGQGHYPAIDVLKSVSRVFGQVTTPTHAEQASAVRKLMTRLEELQLFIDLGEYRPGENIDNDRAMQMRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MHMLIEQTEADVFVIGLIGERGREVTEFVDMLRASHKKEKCVLVFATSDFPSVDRCNAAQLATTVAEYFRDQGKRVVLFIDSMTRYARALRDVALASGERPARRGYPASVFDNLPRLLERPGATSEGSITAFYTVLLESEEEADPMADEIRSILDGHLYLSRKLAGQGHYPAIDVLKSVSRVFGQVTTPTHAEQASAVRKLMTRLEELQLFIDLGEYRPGENIDNDRAMQMRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MLIEQTEADVFVIGLIGERGREVTEFVDMLRASHKKEKCVLVFATSDFPSVDRCNAAQLATTVAEYFRDQGKRVVLFIDSMTRYARALRDVALASGERPARRGYPASVFDNLPRLLERPGATSEGSITAFYTVLLESEEEADPMADEIRSILDGHLYLSRKLAGQGHYPAIDVLKSVSRVFGQVTTPTHAEQASAVRKLMTRLEELQLFIDLGEYRPGENIDNDRAMQMRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MLRASHKKEKCVLVFATSDFPSVDRCNAAQLATTVAEYFRDQGKRVVLFIDSMTRYARALRDVALASGERPARRGYPASVFDNLPRLLERPGATSEGSITAFYTVLLESEEEADPMADEIRSILDGHLYLSRKLAGQGHYPAIDVLKSVSRVFGQVTTPTHAEQASAVRKLMTRLEELQLFIDLGEYRPGENIDNDRAMQMRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MTRYARALRDVALASGERPARRGYPASVFDNLPRLLERPGATSEGSITAFYTVLLESEEEADPMADEIRSILDGHLYLSRKLAGQGHYPAIDVLKSVSRVFGQVTTPTHAEQASAVRKLMTRLEELQLFIDLGEYRPGENIDNDRAMQMRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MADEIRSILDGHLYLSRKLAGQGHYPAIDVLKSVSRVFGQVTTPTHAEQASAVRKLMTRLEELQLFIDLGEYRPGENIDNDRAMQMRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MTRLEELQLFIDLGEYRPGENIDNDRAMQMRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MQMRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MNAFADQN',\n 'MGDVSAVSSSGNILLPQQDEVGGLSEALKKAVEKHKTEYSGDKKDRDYGDAFVMHKETALPLLLAAWRHGAPAKSEHHNGNVSGLHHNGKSELRIAEKLLKVTAEKSVGLISAEAKVDKSAALLSSKNRPLESVSGKKLSADLKAVESVSEVTDNATGISDDNIKALPGDNKAIAGEGVRKEGAPLARDVAPARMAAANTGKPEDKDHKKVKDVSQLPLQPTTIADLSQLTGGDEKMPLAAQSKPMMTIFPTADGVKGEDSSLTYRFQRWGNDYSVNIQARQAGEFSLIPSNTQVEHRLHDQWQNGNPQRWHLTRDDQQNPQQQQHRQQSGEEDDA',\n 'MHKETALPLLLAAWRHGAPAKSEHHNGNVSGLHHNGKSELRIAEKLLKVTAEKSVGLISAEAKVDKSAALLSSKNRPLESVSGKKLSADLKAVESVSEVTDNATGISDDNIKALPGDNKAIAGEGVRKEGAPLARDVAPARMAAANTGKPEDKDHKKVKDVSQLPLQPTTIADLSQLTGGDEKMPLAAQSKPMMTIFPTADGVKGEDSSLTYRFQRWGNDYSVNIQARQAGEFSLIPSNTQVEHRLHDQWQNGNPQRWHLTRDDQQNPQQQQHRQQSGEEDDA',\n 'MAAANTGKPEDKDHKKVKDVSQLPLQPTTIADLSQLTGGDEKMPLAAQSKPMMTIFPTADGVKGEDSSLTYRFQRWGNDYSVNIQARQAGEFSLIPSNTQVEHRLHDQWQNGNPQRWHLTRDDQQNPQQQQHRQQSGEEDDA',\n 'MPLAAQSKPMMTIFPTADGVKGEDSSLTYRFQRWGNDYSVNIQARQAGEFSLIPSNTQVEHRLHDQWQNGNPQRWHLTRDDQQNPQQQQHRQQSGEEDDA',\n 'MMTIFPTADGVKGEDSSLTYRFQRWGNDYSVNIQARQAGEFSLIPSNTQVEHRLHDQWQNGNPQRWHLTRDDQQNPQQQQHRQQSGEEDDA',\n 'MTIFPTADGVKGEDSSLTYRFQRWGNDYSVNIQARQAGEFSLIPSNTQVEHRLHDQWQNGNPQRWHLTRDDQQNPQQQQHRQQSGEEDDA',\n 'MAGKRRWNIRRDRECGFG',\n 'MQKNGGRPGLNLGTGLSMSLPLWLGRRFLLALSTWSFPGLLQQSDRLSCPCRICPVGVYA',\n 'MSLPLWLGRRFLLALSTWSFPGLLQQSDRLSCPCRICPVGVYA',\n 'MYS',\n 'MLNLTLKLWRMVFCWVMANWYR',\n 'MVFCWVMANWYR',\n 'MANWYR',\n 'MTP',\n 'MNG',\n 'MGNDISLIALLAFSTLLPFIIASGTCFVKFSIVFVMVRNALGLQQIPSNMTLNGVALLLSMFVMWPIMHDAYVYFEDEDVTFNDISSLSKHVDEGLDGYRDYLIKYSDRELVQFFENAQLKRQYGEETETVKRDKDEIEKPSIFALLPAYALSEIKSAFKIGFYLYLPFVVVDLVVSSVLLALGMMMMSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MVRNALGLQQIPSNMTLNGVALLLSMFVMWPIMHDAYVYFEDEDVTFNDISSLSKHVDEGLDGYRDYLIKYSDRELVQFFENAQLKRQYGEETETVKRDKDEIEKPSIFALLPAYALSEIKSAFKIGFYLYLPFVVVDLVVSSVLLALGMMMMSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MTLNGVALLLSMFVMWPIMHDAYVYFEDEDVTFNDISSLSKHVDEGLDGYRDYLIKYSDRELVQFFENAQLKRQYGEETETVKRDKDEIEKPSIFALLPAYALSEIKSAFKIGFYLYLPFVVVDLVVSSVLLALGMMMMSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MFVMWPIMHDAYVYFEDEDVTFNDISSLSKHVDEGLDGYRDYLIKYSDRELVQFFENAQLKRQYGEETETVKRDKDEIEKPSIFALLPAYALSEIKSAFKIGFYLYLPFVVVDLVVSSVLLALGMMMMSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MWPIMHDAYVYFEDEDVTFNDISSLSKHVDEGLDGYRDYLIKYSDRELVQFFENAQLKRQYGEETETVKRDKDEIEKPSIFALLPAYALSEIKSAFKIGFYLYLPFVVVDLVVSSVLLALGMMMMSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MHDAYVYFEDEDVTFNDISSLSKHVDEGLDGYRDYLIKYSDRELVQFFENAQLKRQYGEETETVKRDKDEIEKPSIFALLPAYALSEIKSAFKIGFYLYLPFVVVDLVVSSVLLALGMMMMSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MMMMSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MMMSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MMSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MDIAT',\n 'MLVFTVWLVWRSFTLLRASGDIPGVG',\n 'MAACIERGAAVFIGGDDPVSSARSGGRRHAGLSAVMAFLGYACAGLYYR',\n 'MAFLGYACAGLYYR',\n 'MRSDERVHAFITAAINVY',\n 'MAHLMSLWG',\n 'MSLWG',\n 'MAIL',\n 'MLLRNIS',\n 'MKRISAHWPSAPMRRRLAYL',\n 'MRRRLAYL',\n 'MIW',\n 'MKFYVFWFGWKR',\n 'MYSLGVF',\n 'MFTNHRRGLITASDKRSR',\n 'MSHRRLTLHALASVNR',\n 'MSLLSVLSVNEAVRSLNSWICCALRIRKKNACWFLPLPISPRSIAAMRRNWRQP',\n 'MRRNWRQP',\n 'MRVLCETWHWRRESVRLVEVIPPPYSIICPACWNAQGRPAREALLPFIRYCWKARKRRTRWRMKFALSLTVTCI',\n 'MKFALSLTVTCI',\n 'MY',\n 'MLNRHLPCVN',\n 'MTR',\n 'MHSLTRIKVLQRRCTVFHSQCESILLRYQDEDRGLQAEEEAILEQIAGLKLLLDTLRAENRQLSREEIYTLLRKQSIVRRQIKDLELQIIQIQEKRSELEKKREEFQKKSKYWLRKEGNYQRWIIRQKRFYIQREIQQEEAESEEII',\n 'MCQLSVHPGTFYCRSRMRLAVYQKH',\n 'MRLAVYQKH',\n 'MAMLS',\n 'MLS',\n 'MARQRNQNITMATFLVCIITEKANSGLLKNC',\n 'MATFLVCIITEKANSGLLKNC',\n 'MSHLPEWPQPIPVSLKIKIIKRLKMFLSFRCNQPLSPILAN',\n 'MFLSFRCNQPLSPILAN',\n 'MKKCL',\n 'MA',\n 'MTIPSIFRRGKQGSFR',\n 'MINGKTVIPSAGT',\n 'MTPDVIACETD',\n 'MAIGANRDRMPAPWPGSDAGISDATGNVGSVERCRKTVVGLD',\n 'MPAPWPGSDAGISDATGNVGSVERCRKTVVGLD',\n 'MRREPRTGKRAAGREIVAHYERSGRPVV',\n 'MAERVW',\n 'MISH',\n 'MMPTSILRTKMSPLMIFHH',\n 'MPTSILRTKMSPLMIFHH',\n 'MSPLMIFHH',\n 'MIFHH',\n 'MKVWMVIAII',\n 'MVIAII',\n 'MEKRPRR',\n 'MKLKNLQYLRYYLLMR',\n 'MR',\n 'MAGPYCLRD',\n 'MDDLVFAGNKALYLVLILSGWPTIVATIIGLLVGLFQTVTQLQEQTLPFGIKLLGVCLCLFLLSGWYGEVLLSYGRQVIFLALAKG',\n 'MFYALYFEIHHLVASAALGFARVAPIFFFLPFLNSGVLSGAPRNAIIILVALGVWPHALNEAPPFLSVAMIPLVLQEAAVGVMLGCLLSWPFWVMHALGCIIDNQRGATLSSSIDPANGIDTSEMANFLNMFAAVVYLQNGGLVTMVDVLNKSYQLCDPMNECTPSLPPLLTFINQVAQNALVLASPVVLVLLLSEVFLGLLSRFAPQMNAFAISLTVKSGIAVLIMLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MIPLVLQEAAVGVMLGCLLSWPFWVMHALGCIIDNQRGATLSSSIDPANGIDTSEMANFLNMFAAVVYLQNGGLVTMVDVLNKSYQLCDPMNECTPSLPPLLTFINQVAQNALVLASPVVLVLLLSEVFLGLLSRFAPQMNAFAISLTVKSGIAVLIMLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MLGCLLSWPFWVMHALGCIIDNQRGATLSSSIDPANGIDTSEMANFLNMFAAVVYLQNGGLVTMVDVLNKSYQLCDPMNECTPSLPPLLTFINQVAQNALVLASPVVLVLLLSEVFLGLLSRFAPQMNAFAISLTVKSGIAVLIMLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MHALGCIIDNQRGATLSSSIDPANGIDTSEMANFLNMFAAVVYLQNGGLVTMVDVLNKSYQLCDPMNECTPSLPPLLTFINQVAQNALVLASPVVLVLLLSEVFLGLLSRFAPQMNAFAISLTVKSGIAVLIMLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MANFLNMFAAVVYLQNGGLVTMVDVLNKSYQLCDPMNECTPSLPPLLTFINQVAQNALVLASPVVLVLLLSEVFLGLLSRFAPQMNAFAISLTVKSGIAVLIMLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MFAAVVYLQNGGLVTMVDVLNKSYQLCDPMNECTPSLPPLLTFINQVAQNALVLASPVVLVLLLSEVFLGLLSRFAPQMNAFAISLTVKSGIAVLIMLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MVDVLNKSYQLCDPMNECTPSLPPLLTFINQVAQNALVLASPVVLVLLLSEVFLGLLSRFAPQMNAFAISLTVKSGIAVLIMLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MNECTPSLPPLLTFINQVAQNALVLASPVVLVLLLSEVFLGLLSRFAPQMNAFAISLTVKSGIAVLIMLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MNAFAISLTVKSGIAVLIMLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MDYQNNVSEERVAEMIWDAVSEGATLKDVHGI',\n 'MIWDAVSEGATLKDVHGI',\n 'MPYLVFLWLNNVFARVFYLFQPNQKT',\n 'MGFEKTARQFDIDDHRYANLLRIGADGQCALIRFIHRDHRNRHQFGFKINPDRNMRGVGNNQA',\n 'MRGVGNNQA',\n 'MNFTSFRFNLWVPFLLLHFTLHFFLIHFHIFHGQEIFRNSIQ',\n 'MKACTRSSDRIADSFYLTRQPS',\n 'MTADSPA',\n 'MHAAILPMPPG',\n 'MPPG',\n 'MSIYCNINPLDSKVQPSSATKSTSLIGVDIVTGLIIIIPSASSTLDTTRSTTTKGK',\n 'MGHITNIESSNATPLSVIFEGICCNPRALRTMTNTIENLTKQVPDAIINGNRVENASKAINEISFPITRLAQPFMDLNA',\n 'MTNTIENLTKQVPDAIINGNRVENASKAINEISFPITRLAQPFMDLNA',\n 'MDLNA',\n 'MLNI',\n 'MT',\n 'MLKPQAAPIAHNVQQFPFRQRASRYGVLYA',\n 'MRHGQLKRSLCCSKPGNDQVLSASRNRRPSQSGRDMLKPVPRFNPGRPPFFCIAQPNPHSLSRRIFQRRFPAMALAFCRGLRQ',\n 'MLKPVPRFNPGRPPFFCIAQPNPHSLSRRIFQRRFPAMALAFCRGLRQ',\n 'MALAFCRGLRQ',\n 'MALLSPGNALILSSEIPVALSVTSLTDSTAFKSADNFLPLTLSNGLFLDDSNAADLSTLASAEIKPTDFSAVTFNSFSAILSSLFPL',\n 'MFPDELTADTSPIKLSPLTRPLPAVSLAGYRIFSDGLSSVDSSLLCATNIYFFSETLPFSFPARSVFPEFV',\n 'MKSWSSSKRVINLRTADACSACVGVVTCPKTRLTLFSTSIAG',\n 'MLPSLVAPGRSSKRGKLSNTEAG',\n 'MESIKSTTRFPWSRKYSATVVASCAALQRSTEGKSEVAKTSTHFSFLCEARNISTNSVTSRPRSPIRPITKTSASVCSISICISMVLPHPAEAKMPIRWPTPQVNNPSIARTPVISGSRTPTREA',\n 'MVLPHPAEAKMPIRWPTPQVNNPSIARTPVISGSRTPTREA',\n 'MPIRWPTPQVNNPSIARTPVISGSRTPTREA',\n 'MNVF',\n 'MQLLNPDGG',\n 'MNTSLLPYAAQCRTSFSCG',\n 'MVRKYSAIASNNKTINAQARQVNTNARRSSRQMTAIPTILPFS',\n 'MTAIPTILPFS',\n 'MMAATTKDERYSRVLTVSLTVRILKSFFAPSTGFNADKLGFNASVASTKPACNNAGNAEHTKQSSINGIRYFNPIPKTAKL',\n 'MAATTKDERYSRVLTVSLTVRILKSFFAPSTGFNADKLGFNASVASTKPACNNAGNAEHTKQSSINGIRYFNPIPKTAKL',\n 'ML',\n 'MILIIPINSLNEPYDTR',\n 'MRSLLLNDWPFLAESSSRFLVGFSVLFEDMRRPSLVKPTA',\n 'MRRPSLVKPTA',\n 'MHNPKRP',\n 'MRPYSQCHQDDNGVSRRTA',\n 'MNFKVQRVKHFLPLSQRQEYHLTPVRE',\n 'MLQCPYTVISIP',\n 'MMKYH',\n 'MKYH',\n 'MRRR',\n 'MATGWKMPVRQLMRYHSPLPDSLSHSWISTPKVSFICTSSPLPSKTPFAIISTLSSALVGSDNSCCCPMASSSARVTFLRYKTNSSLTGN',\n 'MPVRQLMRYHSPLPDSLSHSWISTPKVSFICTSSPLPSKTPFAIISTLSSALVGSDNSCCCPMASSSARVTFLRYKTNSSLTGN',\n 'MRYHSPLPDSLSHSWISTPKVSFICTSSPLPSKTPFAIISTLSSALVGSDNSCCCPMASSSARVTFLRYKTNSSLTGN',\n 'MASSSARVTFLRYKTNSSLTGN',\n 'MCNNFPSGSALPGTGFSTHKRRQDKCGTGNSNGRSVAASQGTTRCSAPAETAAPARAGETCSSQSPGLIQADHRFSASLNRTHIPCRVGYSSVASRPWRWHSVAVCANSHSRRSICLTRNDIRRHPPRQIAVCAVAAADFVDRLASGASAGDYRFAIDHANDVQPAYLTVLTKTPLLAAPEY',\n 'MP',\n 'MPPVITVKPFLYALRKHRHSRDLFYHRNILSYVFPPLFLMLLINRQPHPAAAVKCSRMN',\n 'MLLINRQPHPAAAVKCSRMN',\n 'MN',\n 'MKYRTAPLQYFNSGQRMHSYRSTCHQRMNTALPAGITRLSGYPASASPDRYRYFRQDDILPSQ',\n 'MHSYRSTCHQRMNTALPAGITRLSGYPASASPDRYRYFRQDDILPSQ',\n 'MNTALPAGITRLSGYPASASPDRYRYFRQDDILPSQ',\n 'MPVQHVSAS',\n 'MRKQGKLIPMRGEVHAK',\n 'MRGEVHAK',\n 'MRDTVGF',\n 'MLLSPAQNRPVITPVMRNTLSRAA',\n 'MRNTLSRAA',\n 'MESDILTLSQKRPNCSQPCSDQNYPQ',\n 'MSHTTPDRQFLPASGRR',\n 'MTGLF',\n 'MTPTAASCRTNGIIATDKNGGASFNACGHTPNATRMIMAFLGAPLNTPLFKNGRKKKIGATRANPSAADATR',\n 'MIMAFLGAPLNTPLFKNGRKKKIGATRANPSAADATR',\n 'MAFLGAPLNTPLFKNGRKKKIGATRANPSAADATR',\n 'MPKGSVCSCNCVTVWNNPTRRPIIVATIVGHPDRIKTR',\n 'MMSCCNVHIL',\n 'MSCCNVHIL',\n 'MHYGPHNKHRKQQCDAVKRHI',\n 'MTSGVILLARLLSVLLLLRILLIVSRQVPALGITVLPLIMQTMFNLRI',\n 'MQTMFNLRI',\n 'MFNLRI',\n 'MIFIFRLTGIGCGHSGRCDIPRQRRALFTNAFARDGFIIPRQCLDIIVRDSRGVIGYFTNGFHSF',\n 'MMQTRNVAIVMF',\n 'MQTRNVAIVMF',\n 'MF',\n 'MSPCRQ',\n 'MHYESIAIVAIFFITGIFCLMFFHRFF',\n 'MFFHRFF',\n 'MCRRRDLSKNAAYAFQYIDCRVMSLPGQLSAQIQVTVKDRANFIRHRVRLFLAFQQYRIKGSNASLAGRPWAFQQAGQIIEYGGGITSTSRTLSRRQCHVSQSTRITGHGIDKKHDPFSLVAKIFRYGCRQLRRIAAIDRGEIGSGKNQHAFFFLMRSAQHIHEFSDLTASFTDKTDNKDIRLRLLDQHMHQHGLTASCGGKNAHSLAYATGQ',\n 'MSLPGQLSAQIQVTVKDRANFIRHRVRLFLAFQQYRIKGSNASLAGRPWAFQQAGQIIEYGGGITSTSRTLSRRQCHVSQSTRITGHGIDKKHDPFSLVAKIFRYGCRQLRRIAAIDRGEIGSGKNQHAFFFLMRSAQHIHEFSDLTASFTDKTDNKDIRLRLLDQHMHQHGLTASCGGKNAHSLAYATGQ',\n 'MRSAQHIHEFSDLTASFTDKTDNKDIRLRLLDQHMHQHGLTASCGGKNAHSLAYATGQ',\n 'MHQHGLTASCGGKNAHSLAYATGQ']\n\n\n\nif __name__ == \"__main__\":\n    import doctest\n    doctest.testmod()\n\n\nfrom load import load_nitrogenase_seq\n>>> nitrogenase = load_nitrogenase_seq()\n>>> print(nitrogenase)\n\nATGGGAAAACTCCGGCAGATCGCTTTCTACGGCAAGGGCGGGATCGGCAAGTCGACGACC\nTCGCAGAACACCCTCGCGGCACTGGTCGAGATGGGTCAGAAGATCCTCATCGTCGGCTGC\nGATCCCAAGGCCGACTCGACCCGCCTGATCCTGAACACCAAGCTGCAGGACACCGTGCTT\nCACCTCGCCGCCGAAGCGGGCTCCGTCGAGGATCTCGAACTCGAGGATGTGGTCAAGATC\nGGCTACAAGGGCATCAAATGCACCGAAGCCGGCGGGCCGGAGCCGGGCGTGGGCTGCGCG\nGGCCGCGGCGTCATCACCGCCATCAACTTCCTGGAAGAGAACGGCGCCTATGACGACGTC\nGACTACGTCTCCTACGACGTGCTGGGCGACGTGGTCTGCGGCGGCTTCGCCATGCCGATC\nCGCGAGAACAAGGCGCAGGAAATCTACATCGTCATGTCGGGCGAGATGATGGCGCTCTAT\nGCGGCCAACAACATCGCCAAGGGCATCCTGAAATACGCGAACTCGGGCGGCGTGCGCCTC\nGGCGGCCTGATCTGCAACGAGCGCAAGACCGACCGCGAGCTGGAACTGGCCGAGGCCCTC\nGCCGCGCGTCTGGGCTGCAAGATGATCCACTTCGTTCCGCGCGACAATATCGTGCAGCAC\nGCCGAGCTCCGCCGCGAGACGGTCATCCAGTATGCGCCCGAGAGCAAGCAGGCGCAGGAA\nTATCGCGAACTGGCCCGCAAGATCCACGAGAACTCGGGCAAGGGCGTGATCCCGACCCCG\nATCACCATGGAAGAGCTGGAAGAGATGCTGATGGATTTCGGCATCATGCAGTCCGAGGAA\nGACCGGCTCGCCGCCATCGCCGCCGCCGAGGCCTGA"
  },
  {
    "objectID": "datacamp/introduction_deeplearning/introduction-to-deep-learning-in-python.html",
    "href": "datacamp/introduction_deeplearning/introduction-to-deep-learning-in-python.html",
    "title": "Mburu Blog",
    "section": "",
    "text": "In this exercise, you’ll write code to do forward propagation (prediction) for your first neural network:\n\n\n\n1_4.png\n\n\nEach data point is a customer. The first input is how many accounts they have, and the second input is how many children they have. The model will predict how many transactions the user makes in the next year. You will use this data throughout the first 2 chapters of this course.\nThe input data has been pre-loaded as input_data, and the weights are available in a dictionary called weights. The array of weights for the first node in the hidden layer are in weights[‘node_0’], and the array of weights for the second node in the hidden layer are in weights[‘node_1’].\nThe weights feeding into the output node are available in weights[‘output’].\nNumPy will be pre-imported for you as np in all exercises.\n\nimport numpy as np\n\nweights = {'node_0': np.array([2, 4]), \n           'node_1': np.array([ 4, -5]), \n           'output': np.array([2, 7])}\n\ninput_data = np.array([3, 5])\n\n# Calculate node 0 value: node_0_value\nnode_0_value = (input_data * weights['node_0']).sum()\n\n# Calculate node 1 value: node_1_value\nnode_1_value = (input_data * weights['node_1']).sum()\n\n# Put node values into array: hidden_layer_outputs\nhidden_layer_outputs = np.array([node_0_value, node_1_value])\n\n# Calculate output: output\noutput = (hidden_layer_outputs * weights['output']).sum()\n\n# Print output\nprint(output)\n\n-39\n\n\n\n\nAs Dan explained to you in the video, an “activation function” is a function applied at each node. It converts the node’s input into some output.\nThe rectified linear activation function (called ReLU) has been shown to lead to very high-performance networks. This function takes a single number as an input, returning 0 if the input is negative, and the input if the input is positive.\nHere are some examples: relu(3) = 3 relu(-3) = 0\n\ndef relu(input):\n    '''Define your relu activation function here'''\n    # Calculate the value for the output of the relu function: output\n    output = max(input,0)\n    \n    # Return the value just calculated\n    return(output)\n\n# Calculate node 0 value: node_0_output\nnode_0_input = (input_data * weights['node_0']).sum()\nnode_0_output = relu(node_0_input)\n\n# Calculate node 1 value: node_1_output\nnode_1_input = (input_data * weights['node_1']).sum()\nnode_1_output = relu(node_1_input)\n\n# Put node values into array: hidden_layer_outputs\nhidden_layer_outputs = np.array([node_0_output, node_1_output])\n\n# Calculate model output (do not apply relu)\nmodel_output = (hidden_layer_outputs * weights['output']).sum()\n\n# Print model output\nprint(model_output)\n\n52\n\n\n\n\n\nYou’ll now define a function called predict_with_network() which will generate predictions for multiple data observations, which are pre-loaded as input_data. As before, weights are also pre-loaded. In addition, the relu() function you defined in the previous exercise has been pre-loaded.\n\ninput_data = [np.array([3, 5]), np.array([ 1, -1]), np.array([0, 0]), np.array([8, 4])]\n# Define predict_with_network()\ndef predict_with_network(input_data_row, weights):\n\n    # Calculate node 0 value\n    node_0_input = (input_data_row * weights['node_0']).sum()\n    node_0_output = relu(node_0_input)\n\n    # Calculate node 1 value\n    node_1_input = (input_data_row * weights['node_1']).sum()\n    node_1_output = relu(node_1_input)\n\n    # Put node values into array: hidden_layer_outputs\n    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n    \n    # Calculate model output\n    input_to_final_layer =  (hidden_layer_outputs * weights['output']).sum()\n    model_output = relu(input_to_final_layer)\n    \n    # Return model output\n    return(model_output)\n\n\n# Create empty list to store prediction results\nresults = []\nfor input_data_row in input_data:\n    # Append prediction to results\n    results.append(predict_with_network(input_data_row,weights))\n\n# Print results\nprint(results)\n        \n\n[52, 63, 0, 148]\n\n\n\n\n\nYou now have a model with 2 hidden layers. The values for an input data point are shown inside the input nodes. The weights are shown on the edges/lines. What prediction would this model make on this data point?\nAssume the activation function at each node is the identity function. That is, each node’s output will be the same as its input. So the value of the bottom node in the first hidden layer is -1, and not 0, as it would be if the ReLU activation function was used.\n\n\n\nch1ex9.png\n\n\nThe answer is 0\n\n\n\nIn this exercise, you’ll write code to do forward propagation for a neural network with 2 hidden layers. Each hidden layer has two nodes. The input data has been preloaded as input_data. The nodes in the first hidden layer are called node_0_0 and node_0_1. Their weights are pre-loaded as weights[‘node_0_0’] and weights[‘node_0_1’] respectively.\nThe nodes in the second hidden layer are called node_1_0 and node_1_1. Their weights are pre-loaded as weights[‘node_1_0’] and weights[‘node_1_1’] respectively.\nWe then create a model output from the hidden nodes using weights pre-loaded as weights[‘output’]. \n\nweights = {'node_0_0': np.array([2, 4]),\n           'node_0_1': np.array([ 4, -5]),\n           'node_1_0': np.array([-1,  2]),\n           'node_1_1': np.array([1, 2]),\n           'output': np.array([2, 7])}\n\ninput_data =  np.array([3, 5])\n\ndef predict_with_network(input_data):\n    # Calculate node 0 in the first hidden layer\n    node_0_0_input = (input_data * weights['node_0_0']).sum()\n    node_0_0_output = relu(node_0_0_input)\n\n    # Calculate node 1 in the first hidden layer\n    node_0_1_input =  (input_data * weights['node_0_1']).sum()\n    node_0_1_output = relu(node_0_1_input)\n\n    # Put node values into array: hidden_0_outputs\n    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n    \n    # Calculate node 0 in the second hidden layer\n    node_1_0_input =  (hidden_0_outputs * weights['node_1_0']).sum()\n    node_1_0_output = relu(node_1_0_input)\n\n    # Calculate node 1 in the second hidden layer\n    node_1_1_input = (hidden_0_outputs * weights['node_1_1']).sum()\n    node_1_1_output = relu(node_1_1_input)\n\n    # Put node values into array: hidden_1_outputs\n    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n\n    # Calculate model output: model_output\n    model_output = ( hidden_1_outputs *weights['output']).sum()\n    \n    # Return model_output\n    return(model_output)\n\noutput = predict_with_network(input_data)\nprint(output)\n\n182\n\n\n\n\n\nHow are the weights that determine the features/interactions in Neural Networks created? - The model training process sets them to optimize predictive accuracy.\n\n\n\nWhich layers of a model capture more complex or “higher level” interactions? - The last layers capture the most complex interactions.\n\n\n\nFor the exercises in this chapter, you’ll continue working with the network to predict transactions for a bank.\nWhat is the error (predicted - actual) for the following network using the ReLU activation function when the input data is [3, 2] and the actual value of the target (what you are trying to predict) is 5? It may be helpful to get out a pen and piece of paper to calculate these values. \n\nThe network generates a prediction of 16, which results in an error of 11.\nIncreasing the weight to 2.01 would increase the resulting error from 9 to 9.08, making the predictions less accurate.\n\nCoding how weight changes affect accuracy Now you’ll get to change weights in a real network and see how they affect model accuracy!\nHave a look at the following neural network:\n Its weights have been pre-loaded as weights_0. Your task in this exercise is to update a single weight in weights_0 to create weights_1, which gives a perfect prediction (in which the predicted value is equal to target_actual: 3).\nUse a pen and paper if necessary to experiment with different combinations. You’ll use the predict_with_network() function, which takes an array of data as the first argument, and weights as the second argument.\n\n# Define predict_with_network()\ndef predict_with_network(input_data_row, weights):\n\n    # Calculate node 0 value\n    node_0_input = (input_data_row * weights['node_0']).sum()\n    node_0_output = node_0_input\n\n    # Calculate node 1 value\n    node_1_input = (input_data_row * weights['node_1']).sum()\n    node_1_output = node_1_input\n\n    # Put node values into array: hidden_layer_outputs\n    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n    \n    # Calculate model output\n    input_to_final_layer =  (hidden_layer_outputs * weights['output']).sum()\n    model_output = input_to_final_layer\n    \n    # Return model output\n    return(model_output)\n# The data point you will make a prediction for\ninput_data = np.array([0, 3])\n\n# Sample weights\nweights_0 = {'node_0': [2, 1],\n             'node_1': [1, 2],\n             'output': [1, 1]\n            }\n\n# The actual target value, used to calculate the error\ntarget_actual = 3\n\n# Make prediction using original weights\nmodel_output_0 = predict_with_network(input_data, weights_0)\n\n# Calculate error: error_0\nerror_0 = model_output_0 - target_actual\n\n# Create weights that cause the network to make perfect prediction (3): weights_1\nweights_1 = {'node_0': [2, 1],\n             'node_1': [1, 0],\n             'output': [1, 1]\n            }\n\n# Make prediction using new weights: model_output_1\nmodel_output_1 = predict_with_network(input_data, weights_1)\n\n# Calculate error: error_1\nerror_1 = model_output_1 - target_actual\n\n# Print error_0 and error_1\nprint(error_0)\nprint(error_1)\n\n6\n0\n\n\n\n\n\nYou’ve seen how different weights will have different accuracies on a single prediction. But usually, you’ll want to measure model accuracy on many points. You’ll now write code to compare model accuracies for two different sets of weights, which have been stored as weights_0 and weights_1.\ninput_data is a list of arrays. Each item in that list contains the data to make a single prediction. target_actuals is a list of numbers. Each item in that list is the actual value we are trying to predict.\nIn this exercise, you’ll use the mean_squared_error() function from sklearn.metrics. It takes the true values and the predicted values as arguments.\nYou’ll also use the preloaded predict_with_network() function, which takes an array of data as the first argument, and weights as the second argument.\n\ninput_data = [np.array([0, 3]), np.array([1, 2]), np.array([-1, -2]), np.array([4, 0])]\ntarget_actuals = [1, 3, 5, 7]\nweights_0 = {'node_0': np.array([2, 1]),\n             'node_1': np.array([1, 2]), \n             'output': np.array([1, 1])}\n\nweights_1 = {'node_0': np.array([2, 1]),\n             'node_1': np.array([1. , 1.5]),\n             'output': np.array([1. , 1.5])}\n\nfrom sklearn.metrics import mean_squared_error\n\n# Create model_output_0 \nmodel_output_0 = []\n# Create model_output_1\nmodel_output_1 = []\n\n# Loop over input_data\nfor row in input_data:\n    # Append prediction to model_output_0\n    model_output_0.append(predict_with_network(row, weights_0))\n    \n    # Append prediction to model_output_1\n    model_output_1.append(predict_with_network(row, weights_1))\n\n# Calculate the mean squared error for model_output_0: mse_0\nmse_0 = mean_squared_error(target_actuals, model_output_0)\n\n# Calculate the mean squared error for model_output_1: mse_1\nmse_1 = mean_squared_error(target_actuals, model_output_1)\n\n# Print mse_0 and mse_1\nprint(\"Mean squared error with weights_0: %f\" %mse_0)\nprint(\"Mean squared error with weights_1: %f\" %mse_1)\n\nMean squared error with weights_0: 80.250000\nMean squared error with weights_1: 99.890625\n\n\n\n\n\nYou’re now going to practice calculating slopes. When plotting the mean-squared error loss function against predictions, the slope is 2 * x * (xb-y), or 2 * input_data * error. Note that x and b may have multiple numbers (x is a vector for each data point, and b is a vector). In this case, the output will also be a vector, which is exactly what you want.\nYou’re ready to write the code to calculate this slope while using a single data point. You’ll use pre-defined weights called weights as well as data for a single point called input_data. The actual value of the target you want to predict is stored in target.\n\ninput_data = np.array([1, 2, 3])\nweights =  np.array([0, 2, 1])\ntarget = [0]\n# Calculate the predictions: preds\npreds = (weights * input_data).sum()\n\n# Calculate the error: error\nerror = preds - target\n\n# Calculate the slope: slope\nslope = input_data * error * 2\n\n# Print the slope\nprint(slope)\n\n[14 28 42]\n\n\n\n\n\nHurray! You’ve just calculated the slopes you need. Now it’s time to use those slopes to improve your model. If you add the slopes to your weights, you will move in the right direction. However, it’s possible to move too far in that direction. So you will want to take a small step in that direction first, using a lower learning rate, and verify that the model is improving.\nThe weights have been pre-loaded as weights, the actual value of the target as target, and the input data as input_data. The predictions from the initial weights are stored as preds.\n\n# Set the learning rate: learning_rate\nlearning_rate = 0.01\n\n# Calculate the predictions: preds\npreds = (weights * input_data).sum()\n\n# Calculate the error: error\nerror = preds - target\n\n# Calculate the slope: slope\nslope = 2 * input_data * error\n\n# Update the weights: weights_updated\nweights_updated = weights - (learning_rate * slope) \n\n# Get updated predictions: preds_updated\npreds_updated = (weights_updated * input_data).sum()\n\n# Calculate updated error: error_updated\nerror_updated = preds_updated - target\n\n# Print the original error\nprint(error)\n\n# Print the updated error\nprint(error_updated)\n\n[7]\n[5.04]\n\n\n\n\n\nYou’re now going to make multiple updates so you can dramatically improve your model weights, and see how the predictions improve with each update.\nTo keep your code clean, there is a pre-loaded get_slope() function that takes input_data, target, and weights as arguments. There is also a get_mse() function that takes the same arguments. The input_data, target, and weights have been pre-loaded.\nThis network does not have any hidden layers, and it goes directly from the input (with 3 nodes) to an output node. Note that weights is a single array.\nWe have also pre-loaded matplotlib.pyplot, and the error history will be plotted after you have done your gradient descent steps.\n\nExercise functions\n\n\ndef get_error(input_data, target, weights):\n    preds = (weights * input_data).sum()\n    error = preds - target\n    return(error)\n\n\n# get slope function\ndef get_slope(input_data, target, weights):\n    error = get_error(input_data, target, weights)   \n    slope = 2 * input_data * error\n    return(slope)\n\ndef get_mse(input_data, target, weights):\n    errors = get_error(input_data, target, weights)\n    mse = np.mean(errors**2)\n    return(mse)\n\n\nfrom matplotlib import pyplot as plt\nn_updates = 20\nmse_hist = []\n\n# Iterate over the number of updates\nfor i in range(n_updates):\n    # Calculate the slope: slope\n    slope = get_slope(input_data, target, weights)\n    \n    # Update the weights: weights\n    weights = weights - 0.01 * slope\n    \n    # Calculate mse with new weights: mse\n    mse = get_mse(input_data, target, weights)\n    \n    # Append the mse to mse_hist\n    mse_hist.append(mse)\n\n# Plot the mse history\nplt.plot(mse_hist)\nplt.xlabel('Iterations')\nplt.ylabel('Mean Squared Error')\nplt.show()\n\n\n\n\n\n\n\nIf your predictions were all exactly right, and your errors were all exactly 0, the slope of the loss function with respect to your predictions would also be 0. In that circumstance, which of the following statements would be correct?\n\nthe updates to all weights in the network would indeed also be 0\n\n\n\n\nIn the network shown below, we have done forward propagation, and node values calculated as part of forward propagation are shown in white. The weights are shown in black. Layers after the question mark show the slopes calculated as part of back-prop, rather than the forward-prop values. Those slope values are shown in purple.\nThis network again uses the ReLU activation function, so the slope of the activation function is 1 for any node receiving a positive value as input. Assume the node being examined had a positive value (so the activation function’s slope is 1).\n\n\n\nch2ex14_1.png\n\n\nWhat is the slope needed to update the weight with the question mark?\n\n\n\nch2ex14_2.png\n\n\n\nThe slope needed to update this weight is indeed 6\n\n\n\n\nYou will soon start building models in Keras to predict wages based on various professional and demographic factors. Before you start building a model, it’s good to understand your data by performing some exploratory analysis.\nThe data is pre-loaded into a pandas DataFrame called df. Use the .head() and .describe() methods in the IPython Shell for a quick overview of the DataFrame.\nThe target variable you’ll be predicting is wage_per_hour. Some of the predictor variables are binary indicators, where a value of 1 represents True, and 0 represents False.\nOf the 9 predictor variables in the DataFrame, how many are binary indicators? The min and max values as shown by .describe() will be informative here. How many binary indicator predictors are there? - 6\n\nimport pandas as pd\n\ndf = pd.read_csv(\"hourly_wages.csv\")\n\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      wage_per_hour\n      union\n      education_yrs\n      experience_yrs\n      age\n      female\n      marr\n      south\n      manufacturing\n      construction\n    \n  \n  \n    \n      0\n      5.10\n      0\n      8\n      21\n      35\n      1\n      1\n      0\n      1\n      0\n    \n    \n      1\n      4.95\n      0\n      9\n      42\n      57\n      1\n      1\n      0\n      1\n      0\n    \n    \n      2\n      6.67\n      0\n      12\n      1\n      19\n      0\n      0\n      0\n      1\n      0\n    \n    \n      3\n      4.00\n      0\n      12\n      4\n      22\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      7.50\n      0\n      12\n      17\n      35\n      0\n      1\n      0\n      0\n      0\n    \n  \n\n\n\n\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      wage_per_hour\n      union\n      education_yrs\n      experience_yrs\n      age\n      female\n      marr\n      south\n      manufacturing\n      construction\n    \n  \n  \n    \n      count\n      534.000000\n      534.000000\n      534.000000\n      534.000000\n      534.000000\n      534.000000\n      534.000000\n      534.000000\n      534.000000\n      534.000000\n    \n    \n      mean\n      9.024064\n      0.179775\n      13.018727\n      17.822097\n      36.833333\n      0.458801\n      0.655431\n      0.292135\n      0.185393\n      0.044944\n    \n    \n      std\n      5.139097\n      0.384360\n      2.615373\n      12.379710\n      11.726573\n      0.498767\n      0.475673\n      0.455170\n      0.388981\n      0.207375\n    \n    \n      min\n      1.000000\n      0.000000\n      2.000000\n      0.000000\n      18.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      5.250000\n      0.000000\n      12.000000\n      8.000000\n      28.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      50%\n      7.780000\n      0.000000\n      12.000000\n      15.000000\n      35.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      75%\n      11.250000\n      0.000000\n      15.000000\n      26.000000\n      44.000000\n      1.000000\n      1.000000\n      1.000000\n      0.000000\n      0.000000\n    \n    \n      max\n      44.500000\n      1.000000\n      18.000000\n      55.000000\n      64.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n    \n  \n\n\n\n\n\n\n\nNow you’ll get to work with your first model in Keras, and will immediately be able to run more complex neural network models on larger datasets compared to the first two chapters.\nTo start, you’ll take the skeleton of a neural network and add a hidden layer and an output layer. You’ll then fit that model and see Keras do the optimization so your model continually gets better.\nAs a start, you’ll predict workers wages based on characteristics like their industry, education and level of experience. You can find the dataset in a pandas dataframe called df. For convenience, everything in df except for the target has been converted to a NumPy matrix called predictors. The target, wage_per_hour, is available as a NumPy matrix called target.\nFor all exercises in this chapter, we’ve imported the Sequential model constructor, the Dense layer constructor, and pandas.\n\ntarget = df['wage_per_hour'].values\n\npredictors  = df.drop('wage_per_hour', axis = 1).values\n\n# Import necessary modules\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\n\n# Save the number of columns in predictors: n_cols\nn_cols = predictors.shape[1]\n\n# Set up the model: model\nmodel = Sequential()\n\n# Add the first layer\nmodel.add(Dense(50, activation = \"relu\", input_shape =(n_cols, )))\n\n# Add the second layer\nmodel.add(Dense(32, activation= \"relu\"))\n\n# Add the output layer\nmodel.add(Dense(1))\n\n\n\n\nYou’re now going to compile the model you specified earlier. To compile the model, you need to specify the optimizer and loss function to use. In the video, Dan mentioned that the Adam optimizer is an excellent choice. You can read more about it as well as other keras optimizers here, and if you are really curious to learn more, you can read the original paper that introduced the Adam optimizer.\nIn this exercise, you’ll use the Adam optimizer and the mean squared error loss function. Go for it!\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n\n\n# Verify that model contains information from compiling\nprint(\"Loss function: \" + model.loss)\n\nLoss function: mean_squared_error\n\n\n\n\n\nYou’re at the most fun part. You’ll now fit the model. Recall that the data to be used as predictive features is loaded in a NumPy matrix called predictors and the data to be predicted is stored in a NumPy matrix called target. Your model is pre-written and it has been compiled with the code from the previous exercise.\n\nmodel.fit(predictors, target, epochs = 10)\n\nEpoch 1/10\n534/534 [==============================] - 0s 50us/step - loss: 33.0082\nEpoch 2/10\n534/534 [==============================] - 0s 39us/step - loss: 26.2000\nEpoch 3/10\n534/534 [==============================] - 0s 43us/step - loss: 24.1919\nEpoch 4/10\n534/534 [==============================] - 0s 37us/step - loss: 23.1762\nEpoch 5/10\n534/534 [==============================] - 0s 32us/step - loss: 22.8131\nEpoch 6/10\n534/534 [==============================] - 0s 41us/step - loss: 22.1156\nEpoch 7/10\n534/534 [==============================] - 0s 43us/step - loss: 21.9082\nEpoch 8/10\n534/534 [==============================] - 0s 32us/step - loss: 21.8637\nEpoch 9/10\n534/534 [==============================] - 0s 41us/step - loss: 21.5395\nEpoch 10/10\n534/534 [==============================] - 0s 41us/step - loss: 21.7754\n\n\n<keras.callbacks.callbacks.History at 0x15038471e10>\n\n\n\n\n\nNow you will start modeling with a new dataset for a classification problem. This data includes information about passengers on the Titanic. You will use predictors such as age, fare and where each passenger embarked from to predict who will survive. This data is from a tutorial on data science competitions. Look here for descriptions of the features.\nThe data is pre-loaded in a pandas DataFrame called df.\nIt’s smart to review the maximum and minimum values of each variable to ensure the data isn’t misformatted or corrupted. What was the maximum age of passengers on the Titanic? Use the .describe() method in the IPython Shell to answer this question.\n\ntitanic = pd.read_csv(\"titanic_all_numeric.csv\")\n\ntitanic.describe()\n\n\n\n\n\n  \n    \n      \n      survived\n      pclass\n      age\n      sibsp\n      parch\n      fare\n      male\n      embarked_from_cherbourg\n      embarked_from_queenstown\n      embarked_from_southampton\n    \n  \n  \n    \n      count\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n    \n    \n      mean\n      0.383838\n      2.308642\n      29.699118\n      0.523008\n      0.381594\n      32.204208\n      0.647587\n      0.188552\n      0.086420\n      0.722783\n    \n    \n      std\n      0.486592\n      0.836071\n      13.002015\n      1.102743\n      0.806057\n      49.693429\n      0.477990\n      0.391372\n      0.281141\n      0.447876\n    \n    \n      min\n      0.000000\n      1.000000\n      0.420000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.000000\n      2.000000\n      22.000000\n      0.000000\n      0.000000\n      7.910400\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      50%\n      0.000000\n      3.000000\n      29.699118\n      0.000000\n      0.000000\n      14.454200\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      75%\n      1.000000\n      3.000000\n      35.000000\n      1.000000\n      0.000000\n      31.000000\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      max\n      1.000000\n      3.000000\n      80.000000\n      8.000000\n      6.000000\n      512.329200\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n    \n  \n\n\n\n\n\n\n\nYou’ll now create a classification model using the titanic dataset, which has been pre-loaded into a DataFrame called df. You’ll take information about the passengers and predict which ones survived.\nThe predictive variables are stored in a NumPy array predictors. The target to predict is in df.survived, though you’ll have to manipulate it for keras. The number of predictive features is stored in n_cols.\nHere, you’ll use the ‘sgd’ optimizer, which stands for Stochastic Gradient Descent. You’ll learn more about this in the next chapter!\n\n# Import necessary modules\n\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n# Convert the target to categorical: target\ntitanic_target = titanic.survived.values\n\n\npredictors_titanic = titanic.drop('survived', axis = 1).values\n\nX_train, X_test, y_train, y_test = train_test_split(predictors_titanic, titanic_target, test_size=0.20, random_state=42)\n\nn_cols_tita = X_train.shape[1]\n\ntitanic_train_target = to_categorical(y_train)\n\n# Set up the model\nmodel = Sequential()\n\n# Add the first layer\nmodel.add(Dense(32, activation=\"relu\", input_shape = (n_cols_tita, )))\n\n# Add the output layer\nmodel.add(Dense(2, activation=\"softmax\"))\n\n# Compile the model\nmodel.compile(optimizer='sgd',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Fit the model\n\nmodel.fit(X_train, titanic_train_target, epochs =10)\n\n\nEpoch 1/10\n712/712 [==============================] - 0s 179us/step - loss: 3.4747 - accuracy: 0.5730\nEpoch 2/10\n712/712 [==============================] - 0s 38us/step - loss: 1.0107 - accuracy: 0.6180\nEpoch 3/10\n712/712 [==============================] - 0s 48us/step - loss: 0.6336 - accuracy: 0.6994\nEpoch 4/10\n712/712 [==============================] - 0s 43us/step - loss: 0.6382 - accuracy: 0.6531\nEpoch 5/10\n712/712 [==============================] - 0s 43us/step - loss: 0.6030 - accuracy: 0.6840\nEpoch 6/10\n712/712 [==============================] - 0s 32us/step - loss: 0.6132 - accuracy: 0.6770\nEpoch 7/10\n712/712 [==============================] - 0s 32us/step - loss: 0.6026 - accuracy: 0.6924\nEpoch 8/10\n712/712 [==============================] - 0s 36us/step - loss: 0.5848 - accuracy: 0.7008\nEpoch 9/10\n712/712 [==============================] - 0s 28us/step - loss: 0.5762 - accuracy: 0.7177\nEpoch 10/10\n712/712 [==============================] - 0s 36us/step - loss: 0.5721 - accuracy: 0.7107\n\n\n<keras.callbacks.callbacks.History at 0x1503f6f42e8>\n\n\n\n\n\nThe trained network from your previous coding exercise is now stored as model. New data to make predictions is stored in a NumPy array as pred_data. Use model to make predictions on your new data.\nIn this exercise, your predictions will be probabilities, which is the most common way for data scientists to communicate their predictions to colleagues.\n\n# Calculate predictions: predictions\npredictions = model.predict(X_test)\n\n# Calculate predicted probability of survival: predicted_prob_true\npredicted_prob_true = predictions[:,1]\n\n# print predicted_prob_true\nprint(predicted_prob_true)\n\n[0.4393523  0.48866495 0.39731222 0.61088556 0.45835057 0.74529445\n 0.55348486 0.4456181  0.4904442  0.6187477  0.716584   0.37187845\n 0.63650054 0.37413827 0.541665   0.61706865 0.78558517 0.5543863\n 0.51765    0.8420926  0.4106096  0.6868524  0.50741607 0.41173482\n 0.520634   0.4718528  0.6947375  0.5317584  0.4826215  0.5232366\n 0.39360932 0.529245   0.6563818  0.5493375  0.39192986 0.44160265\n 0.68162596 0.55348486 0.7492457  0.43249092 0.6784965  0.36272633\n 0.43647745 0.4258504  0.60461956 0.5229629  0.39649883 0.42204925\n 0.39490002 0.8264776  0.53941685 0.812945   0.5041746  0.81333935\n 0.32684872 0.73319507 0.48613372 0.97091943 0.7334454  0.5025388\n 0.39904588 0.57022554 0.707388   0.6702328  0.4258504  0.5491688\n 0.7179668  0.40357038 0.31448063 0.8217575  0.6639794  0.9792342\n 0.61968404 0.75069606 0.4067286  0.527727   0.5332341  0.6472109\n 0.6576547  0.650716   0.4980961  0.61798155 0.8576531  0.42570966\n 0.7872377  0.8979904  0.8258033  0.7168161  0.2451397  0.41589525\n 0.48707423 0.49481165 0.831457   0.42561615 0.432491   0.4054837\n 0.838141   0.36951503 0.662202   0.40898585 0.7925822  0.37044364\n 0.77948564 0.3615601  0.4043448  0.39024127 0.7965787  0.80498594\n 0.44281185 0.71056294 0.70790493 0.39157522 0.7721747  0.6972393\n 0.95430315 0.41867766 0.84590524 0.5125226  0.6374015  0.7336344\n 0.41982025 0.7423645  0.9299448  0.5468816  0.3975107  0.6588786\n 0.83827174 0.7418929  0.5992507  0.4020873  0.5549551  0.4393523\n 0.26635876 0.53534836 0.83954513 0.4868858  0.8362998  0.40104023\n 0.3643543  0.5572469  0.40633902 0.78937674 0.52118057 0.6527681\n 0.59238505 0.74086416 0.5282352  0.40132585 0.88646907 0.3742481\n 0.41920006 0.40615824 0.38914305 0.67594177 0.43647745 0.41079226\n 0.6831359  0.55391175 0.75062567 0.5808794  0.36788794 0.6730173\n 0.53411037 0.753962   0.4338457  0.8958585  0.47474706 0.7077925\n 0.40378138 0.36860514 0.68651134 0.51760703 0.6977267  0.4944778\n 0.37849128 0.40261698 0.5987059  0.5686516  0.5323318 ]\n\n\n\n\n\nWhich of the following could prevent a model from showing an improved loss in its first few epochs? * Learning rate too low.\n\nLearning rate too high.\nPoor choice of activation function.\nAll of the above.\nAll of the above.\n\n\n\n\nIt’s time to get your hands dirty with optimization. You’ll now try optimizing a model at a very low learning rate, a very high learning rate, and a “just right” learning rate. You’ll want to look at the results after running this exercise, remembering that a low value for the loss function is good.\nFor these exercises, we’ve pre-loaded the predictors and target values from your previous classification models (predicting who would survive on the Titanic). You’ll want the optimization to start from scratch every time you change the learning rate, to give a fair comparison of how each learning rate did in your results. So we have created a function get_new_model() that creates an unoptimized model to optimize.\n\ninput_shape = (n_cols_tita,)\ndef get_new_model(input_shape = input_shape):\n    model = Sequential()\n    model.add(Dense(32, activation='relu', input_shape = input_shape))\n    model.add(Dense(4, activation='relu'))\n    model.add(Dense(2, activation='softmax'))\n    return(model)\n\n# Import the SGD optimizer\nfrom keras.optimizers import SGD\n\n# Create list of learning rates: lr_to_test\nlr_to_test = [.000001, 0.01, 1]\n\n# Loop over learning rates\nfor lr in lr_to_test:\n    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n    \n    # Build new model to test, unaffected by previous models\n    model = get_new_model()\n    \n    # Create SGD optimizer with specified learning rate: my_optimizer\n    my_optimizer = SGD(lr = lr)\n    \n    # Compile the model\n    model.compile(optimizer = my_optimizer, loss = 'categorical_crossentropy')   \n    \n    # Fit the model\n    model.fit(X_train, titanic_train_target, epochs =10)\n\n    \n\n\n\nTesting model with learning rate: 0.000001\n\nEpoch 1/10\n712/712 [==============================] - 0s 301us/step - loss: 1.8486\nEpoch 2/10\n712/712 [==============================] - 0s 38us/step - loss: 1.8446\nEpoch 3/10\n712/712 [==============================] - 0s 38us/step - loss: 1.8405\nEpoch 4/10\n712/712 [==============================] - 0s 36us/step - loss: 1.8365\nEpoch 5/10\n712/712 [==============================] - 0s 38us/step - loss: 1.8325\nEpoch 6/10\n712/712 [==============================] - 0s 38us/step - loss: 1.8284\nEpoch 7/10\n712/712 [==============================] - 0s 36us/step - loss: 1.8243\nEpoch 8/10\n712/712 [==============================] - 0s 31us/step - loss: 1.8202\nEpoch 9/10\n712/712 [==============================] - 0s 46us/step - loss: 1.8161\nEpoch 10/10\n712/712 [==============================] - 0s 43us/step - loss: 1.8121\n\n\nTesting model with learning rate: 0.010000\n\nEpoch 1/10\n712/712 [==============================] - 0s 296us/step - loss: 0.9847\nEpoch 2/10\n712/712 [==============================] - 0s 35us/step - loss: 0.6400\nEpoch 3/10\n712/712 [==============================] - 0s 38us/step - loss: 0.6324\nEpoch 4/10\n712/712 [==============================] - 0s 28us/step - loss: 0.6304\nEpoch 5/10\n712/712 [==============================] - 0s 42us/step - loss: 0.6224\nEpoch 6/10\n712/712 [==============================] - 0s 32us/step - loss: 0.6261\nEpoch 7/10\n712/712 [==============================] - 0s 38us/step - loss: 0.6325\nEpoch 8/10\n712/712 [==============================] - 0s 32us/step - loss: 0.6248\nEpoch 9/10\n712/712 [==============================] - 0s 35us/step - loss: 0.6300\nEpoch 10/10\n712/712 [==============================] - 0s 35us/step - loss: 0.6293\n\n\nTesting model with learning rate: 1.000000\n\nEpoch 1/10\n712/712 [==============================] - 0s 305us/step - loss: 1.7169\nEpoch 2/10\n712/712 [==============================] - 0s 41us/step - loss: 0.6688\nEpoch 3/10\n712/712 [==============================] - 0s 38us/step - loss: 0.6729\nEpoch 4/10\n712/712 [==============================] - 0s 36us/step - loss: 0.6676\nEpoch 5/10\n712/712 [==============================] - 0s 32us/step - loss: 0.6665\nEpoch 6/10\n712/712 [==============================] - 0s 42us/step - loss: 0.6681\nEpoch 7/10\n712/712 [==============================] - 0s 36us/step - loss: 0.6702\nEpoch 8/10\n712/712 [==============================] - 0s 39us/step - loss: 0.6648\nEpoch 9/10\n712/712 [==============================] - 0s 42us/step - loss: 0.6669\nEpoch 10/10\n712/712 [==============================] - 0s 41us/step - loss: 0.6684\n\n\n\n\n\nNow it’s your turn to monitor model accuracy with a validation data set. A model definition has been provided as model. Your job is to add the code to compile it and then fit it. You’ll check the validation score in each epoch.\n\n# Save the number of columns in predictors: n_cols\nn_cols = predictors.shape[1]\ninput_shape = (n_cols_tita,)\n\n# Specify the model\nmodel = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape = input_shape))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n\n\n# Fit the model\nhist = model.fit(X_train, titanic_train_target,validation_split = 0.3, epochs = 10)\n\n\nTrain on 498 samples, validate on 214 samples\nEpoch 1/10\n498/498 [==============================] - 0s 830us/step - loss: 0.8264 - accuracy: 0.6265 - val_loss: 0.9846 - val_accuracy: 0.6449\nEpoch 2/10\n498/498 [==============================] - 0s 114us/step - loss: 0.8584 - accuracy: 0.6165 - val_loss: 1.0564 - val_accuracy: 0.6495\nEpoch 3/10\n498/498 [==============================] - 0s 90us/step - loss: 0.7803 - accuracy: 0.6928 - val_loss: 0.7301 - val_accuracy: 0.6729\nEpoch 4/10\n498/498 [==============================] - 0s 79us/step - loss: 0.6068 - accuracy: 0.6968 - val_loss: 0.5833 - val_accuracy: 0.7243\nEpoch 5/10\n498/498 [==============================] - 0s 80us/step - loss: 0.6750 - accuracy: 0.6747 - val_loss: 0.5960 - val_accuracy: 0.6449\nEpoch 6/10\n498/498 [==============================] - 0s 102us/step - loss: 0.5876 - accuracy: 0.6847 - val_loss: 0.7054 - val_accuracy: 0.6869\nEpoch 7/10\n498/498 [==============================] - 0s 72us/step - loss: 0.6315 - accuracy: 0.7149 - val_loss: 0.5770 - val_accuracy: 0.6822\nEpoch 8/10\n498/498 [==============================] - 0s 82us/step - loss: 0.5804 - accuracy: 0.7068 - val_loss: 0.6251 - val_accuracy: 0.6402\nEpoch 9/10\n498/498 [==============================] - 0s 81us/step - loss: 0.6225 - accuracy: 0.6928 - val_loss: 0.5864 - val_accuracy: 0.7336\nEpoch 10/10\n498/498 [==============================] - 0s 90us/step - loss: 0.5744 - accuracy: 0.7329 - val_loss: 0.5157 - val_accuracy: 0.7430\n\n\n\n\n\nNow that you know how to monitor your model performance throughout optimization, you can use early stopping to stop optimization when it isn’t helping any more. Since the optimization stops automatically when it isn’t helping, you can also set a high value for epochs in your call to .fit(), as Dan showed in the video.\nThe model you’ll optimize has been specified as model. As before, the data is pre-loaded as predictors and target.\n\n# Import EarlyStopping\nfrom keras.callbacks import EarlyStopping\n\n# Save the number of columns in predictors: n_cols\nn_cols = predictors.shape[1]\ninput_shape = (n_cols_tita,)\n\n# Specify the model\nmodel = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape = input_shape))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n\n# Define early_stopping_monitor\nearly_stopping_monitor = EarlyStopping(patience=2)\n\n# Fit the model\nmodel.fit(X_train, titanic_train_target, validation_split=0.3, epochs = 30,callbacks = [early_stopping_monitor])\n\n\nTrain on 498 samples, validate on 214 samples\nEpoch 1/30\n498/498 [==============================] - 0s 857us/step - loss: 0.9300 - accuracy: 0.6165 - val_loss: 0.8886 - val_accuracy: 0.6495\nEpoch 2/30\n498/498 [==============================] - 0s 80us/step - loss: 0.8286 - accuracy: 0.6566 - val_loss: 0.8388 - val_accuracy: 0.5888\nEpoch 3/30\n498/498 [==============================] - 0s 78us/step - loss: 0.7250 - accuracy: 0.6426 - val_loss: 0.6632 - val_accuracy: 0.7056\nEpoch 4/30\n498/498 [==============================] - 0s 102us/step - loss: 0.6263 - accuracy: 0.6767 - val_loss: 0.6021 - val_accuracy: 0.6916\nEpoch 5/30\n498/498 [==============================] - 0s 90us/step - loss: 0.6294 - accuracy: 0.6847 - val_loss: 0.6558 - val_accuracy: 0.6869\nEpoch 6/30\n498/498 [==============================] - 0s 86us/step - loss: 0.6445 - accuracy: 0.7269 - val_loss: 0.5836 - val_accuracy: 0.7383\nEpoch 7/30\n498/498 [==============================] - 0s 78us/step - loss: 0.6068 - accuracy: 0.7470 - val_loss: 0.6457 - val_accuracy: 0.6308\nEpoch 8/30\n498/498 [==============================] - 0s 121us/step - loss: 0.5972 - accuracy: 0.7309 - val_loss: 0.6628 - val_accuracy: 0.6682\n\n\n<keras.callbacks.callbacks.History at 0x150464ec550>\n\n\n\n\n\nNow you know everything you need to begin experimenting with different models!\nA model called model_1 has been pre-loaded. You can see a summary of this model printed in the IPython Shell. This is a relatively small network, with only 10 units in each hidden layer.\nIn this exercise you’ll create a new model called model_2 which is similar to model_1, except it has 100 units in each hidden layer.\nAfter you create model_2, both models will be fitted, and a graph showing both models loss score at each epoch will be shown. We added the argument verbose=False in the fitting commands to print out fewer updates, since you will look at these graphically instead of as text.\nBecause you are fitting two models, it will take a moment to see the outputs after you hit run, so be patient.\n\n# Define early_stopping_monitor\nearly_stopping_monitor = EarlyStopping(patience=2)\n\n## model 1\n# Create the new model: model_2\nmodel_1 = Sequential()\n\n# Add the first and second layers\nmodel_1.add(Dense(10, activation = \"relu\", input_shape=input_shape))\nmodel_1.add(Dense(10, activation = \"relu\"))\n\n# Add the output layer\nmodel_1.add(Dense(2, activation = \"softmax\"))\n\n\n# Compile model_2\nmodel_1.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n\n# Create the new model: model_2\nmodel_2 = Sequential()\n\n# Add the first and second layers\nmodel_2.add(Dense(100, activation = \"relu\", input_shape=input_shape))\nmodel_2.add(Dense(100, activation = \"relu\"))\n\n# Add the output layer\nmodel_2.add(Dense(2, activation = \"softmax\"))\n\n\n# Compile model_2\nmodel_2.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n\n# Fit model_1\nmodel_1_training = model_1.fit(X_train, titanic_train_target,\n                               epochs=15, validation_split=0.2, \n                               callbacks=[early_stopping_monitor], verbose=False)\n\n# Fit model_2\nmodel_2_training = model_2.fit(X_train, titanic_train_target,\n                               epochs=15, validation_split=0.2,\n                               callbacks=[early_stopping_monitor], verbose=False)\n\nimport matplotlib.pyplot as plt\n# Create the plot\nplt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\nplt.xlabel('Epochs')\nplt.ylabel('Validation score')\nplt.show()\n\n\n\n\n\n\n\nYou’ve seen how to experiment with wider networks. In this exercise, you’ll try a deeper network (more hidden layers).\nOnce again, you have a baseline model called model_1 as a starting point. It has 1 hidden layer, with 50 units. You can see a summary of that model’s structure printed out. You will create a similar network with 3 hidden layers (still keeping 50 units in each layer).\nThis will again take a moment to fit both models, so you’ll need to wait a few seconds to see the results after you run your code.\n\n# The input shape to use in the first hidden layer\ninput_shape = (n_cols_tita,)\n\n# Create the new model: model_2\nmodel_2 = Sequential()\n\n# Add the first, second, and third hidden layers\nmodel_2.add(Dense(50, activation = \"relu\", input_shape=input_shape))\nmodel_2.add(Dense(50, activation = \"relu\"))\nmodel_2.add(Dense(50, activation = \"relu\"))\n\n\n# Add the output layer\nmodel_2.add(Dense(2, activation = \"softmax\"))\n\n\n# Compile model_2\nmodel_2.compile(optimizer = 'adam',\n                loss = 'categorical_crossentropy', \n                metrics=['accuracy'])\n\n\n\n# Fit model 2\nmodel_2_training = model_2.fit(X_train, titanic_train_target,\n                               epochs=20, validation_split=0.4,\n                               callbacks=[early_stopping_monitor], verbose=False)\n\n# Create the plot\nplt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\nplt.xlabel('Epochs')\nplt.ylabel('Validation score')\nplt.show()\n\n\n\n\n\nThe blue model is the one you made and the red is the original model. The model with the lower loss value is the better model.\n\n\n\n\nYou’ve just run an experiment where you compared two networks that were identical except that the 2nd network had an extra hidden layer. You see that this 2nd network (the deeper network) had better performance. Given that, which of the following would be a good experiment to run next for even better performance?\n\nIncreasing the number of units in each hidden layer would be a good next step to try achieving even better performance.\n\n\n\n\nYou’ve reached the final exercise of the course - you now know everything you need to build an accurate model to recognize handwritten digits!\nWe’ve already done the basic manipulation of the MNIST dataset shown in the video, so you have X and y loaded and ready to model with. Sequential and Dense from keras are also pre-imported.\nTo add an extra challenge, we’ve loaded only 2500 images, rather than 60000 which you will see in some published results. Deep learning models perform better with more data, however, they also take longer to train, especially when they start becoming more complex.\nIf you have a computer with a CUDA compatible GPU, you can take advantage of it to improve computation time. If you don’t have a GPU, no problem! You can set up a deep learning environment in the cloud that can run your models on a GPU. Here is a blog post by Dan that explains how to do this - check it out after completing this exercise! It is a great next step as you continue your deep learning journey.\nReady to take your deep learning to the next level? Check out Advanced Deep Learning with Keras in Python to see how the Keras functional API lets you build domain knowledge to solve new types of problems. Once you know how to use the functional API, take a look at “Convolutional Neural Networks for Image Processing” to learn image-specific applications of Keras.\n\nmnist = pd.read_csv(\"mnist.csv\")\nmnist.head()\n\n\n\n\n\n  \n    \n      \n      label\n      pixel0\n      pixel1\n      pixel2\n      pixel3\n      pixel4\n      pixel5\n      pixel6\n      pixel7\n      pixel8\n      ...\n      pixel774\n      pixel775\n      pixel776\n      pixel777\n      pixel778\n      pixel779\n      pixel780\n      pixel781\n      pixel782\n      pixel783\n    \n  \n  \n    \n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n5 rows × 785 columns\n\n\n\n\n# Convert the target to categorical: target\nmnist_target = mnist.label.values\n\n\nmnist_predictors = mnist.drop('label', axis = 1).values\n\nX_train, X_test, y_train, y_test = train_test_split(mnist_predictors, mnist_target,\n                                                    test_size=0.20, random_state=42)\n\nn_cols_mnist = X_train.shape[1]\n\nmnist_train_target = to_categorical(y_train)\n\n\n# Create the model: model\nmodel = Sequential()\n\n# Add the first hidden layer\nmodel.add(Dense(784, activation= \"relu\", input_shape = (784,)))\n\n# Add the second hidden layer\nmodel.add(Dense(784, activation= \"relu\"))\n\n# Add the output layer\nmodel.add(Dense(10, activation= \"softmax\"))\n\n\n# Compile the model\nmodel.compile(optimizer = 'adam',\n                loss = 'categorical_crossentropy', \n                metrics=['accuracy'])\n\nearly_stopping_monitor = EarlyStopping(patience=5)\n\n# Fit the model\nhist = model.fit(X_train, mnist_train_target,\n          epochs=20, validation_split=0.3,\n         callbacks=[early_stopping_monitor])\n                               \n\nTrain on 23520 samples, validate on 10080 samples\nEpoch 1/20\n23520/23520 [==============================] - 13s 537us/step - loss: 3.0947 - accuracy: 0.8814 - val_loss: 0.4898 - val_accuracy: 0.9318\nEpoch 2/20\n23520/23520 [==============================] - 14s 574us/step - loss: 0.3139 - accuracy: 0.9447 - val_loss: 0.3341 - val_accuracy: 0.9422\nEpoch 3/20\n23520/23520 [==============================] - 12s 518us/step - loss: 0.2072 - accuracy: 0.9548 - val_loss: 0.2999 - val_accuracy: 0.9432\nEpoch 4/20\n23520/23520 [==============================] - 12s 515us/step - loss: 0.1948 - accuracy: 0.9574 - val_loss: 0.3780 - val_accuracy: 0.9422\nEpoch 5/20\n23520/23520 [==============================] - 13s 546us/step - loss: 0.2242 - accuracy: 0.9537 - val_loss: 0.2870 - val_accuracy: 0.9459\nEpoch 6/20\n23520/23520 [==============================] - 12s 526us/step - loss: 0.1955 - accuracy: 0.9591 - val_loss: 0.4099 - val_accuracy: 0.9320\nEpoch 7/20\n23520/23520 [==============================] - 12s 492us/step - loss: 0.2014 - accuracy: 0.9546 - val_loss: 0.3483 - val_accuracy: 0.9427\nEpoch 8/20\n23520/23520 [==============================] - 12s 524us/step - loss: 0.1532 - accuracy: 0.9652 - val_loss: 0.2823 - val_accuracy: 0.9504\nEpoch 9/20\n23520/23520 [==============================] - 17s 705us/step - loss: 0.1633 - accuracy: 0.9633 - val_loss: 0.3615 - val_accuracy: 0.9444\nEpoch 10/20\n23520/23520 [==============================] - 14s 615us/step - loss: 0.1365 - accuracy: 0.9675 - val_loss: 0.3071 - val_accuracy: 0.9497\nEpoch 11/20\n23520/23520 [==============================] - 19s 803us/step - loss: 0.1499 - accuracy: 0.9665 - val_loss: 0.3288 - val_accuracy: 0.9531\nEpoch 12/20\n23520/23520 [==============================] - 15s 637us/step - loss: 0.1298 - accuracy: 0.9704 - val_loss: 0.3034 - val_accuracy: 0.9473\nEpoch 13/20\n23520/23520 [==============================] - 15s 632us/step - loss: 0.1144 - accuracy: 0.9740 - val_loss: 0.2939 - val_accuracy: 0.9519"
  },
  {
    "objectID": "datacamp/foundation_prob_R/eg.html",
    "href": "datacamp/foundation_prob_R/eg.html",
    "title": "Untitled",
    "section": "",
    "text": "This Quarto document is made interactive using Shiny. Interactive documents allow readers to modify parameters and see the results immediately. Learn more about Shiny interactive documents at https://quarto.org/docs/interactive/shiny/."
  },
  {
    "objectID": "datacamp/foundation_prob_R/eg.html#inputs-and-outputs",
    "href": "datacamp/foundation_prob_R/eg.html#inputs-and-outputs",
    "title": "Untitled",
    "section": "Inputs and Outputs",
    "text": "Inputs and Outputs\nYou can embed Shiny inputs and outputs in your document. Outputs are automatically updated whenever inputs change. This demonstrates how a standard R plot can be made interactive:\n\n\n\nNumber of bins:"
  },
  {
    "objectID": "datacamp/Intro_Python_Data/Intro_datascience.html",
    "href": "datacamp/Intro_Python_Data/Intro_datascience.html",
    "title": "Mburu Blog",
    "section": "",
    "text": "Modules (sometimes called packages or libraries) help group together related sets of tools in Python. In this exercise, we’ll examine two modules that are frequently used by Data Scientists:\nstatsmodels: used in machine learning; usually aliased as sm seaborn: a visualization library; usually aliased as sns Note that each module has a standard alias, which allows you to access the tools inside of the module without typing as many characters. For example, aliasing lets us shorten seaborn.scatterplot() to sns.scatterplot().\n\nimport statsmodels as sm\nimport seaborn as sns\nimport numpy as np\n\n\n\n\nBefore we start looking for Bayes’ kidnapper, we need to fill out a Missing Puppy Report with details of the case. Each piece of information will be stored as a variable.\nWe define a variable using an equals sign (=). For instance, we would define the variable height:\nheight = 24 In this exercise, we’ll be defining bayes_age to be 4.0 months old. The data type for this variable will be float, meaning that it is a number.\n\nbayes_age = 4.0\nbayes_age\n\n4.0\n\n\n\n\n\nLet’s continue to fill out the Missing Puppy Report for Bayes. In the previous exercise, we defined bayes_age, which was a float, which represents a number.\nIn this exercise, we’ll define favorite_toy and owner, which will both be strings. A string represents text. A string is surrounded by quotation marks (’ or “) and can contain letters, numbers, and special characters. It doesn’t matter if you use single (’) or double (”) quotes, but it’s important to be consistent throughout your code.\n\nfavorite_toy = \"Mr. Squeaky\"\nowner = \"DataCamp\"\n# Display variables\nprint(favorite_toy)\nprint(owner)\n\nMr. Squeaky\nDataCamp\n\n\n\n\n\nIt’s easy to make errors when you’re trying to type strings quickly.\nDon’t forget to use quotes! Without quotes, you’ll get a name error. owner = DataCamp Use the same type of quotation mark. If you start with a single quote, and end with a double quote, you’ll get a syntax error. fur_color = “blonde’ Someone at the police station made an error when filling out the final lines of Bayes’ Missing Puppy Report. In this exercise, you will correct the errors.\n\n# One or more of the following lines contains an error\n# Correct it so that it runs without producing syntax errors\nbirthday = '2017-07-14'\ncase_id = 'DATACAMP!123-456?'"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html",
    "href": "datacamp/imputations_in_R/imputations_in_R.html",
    "title": "Handling Missing Data with Imputations in R",
    "section": "",
    "text": "knitr::opts_chunk$set(\n    echo = TRUE,\n    message = FALSE,\n    warning = FALSE\n)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(janitor)\nlibrary(ggthemes)\nlibrary(here)\nlibrary(lubridate)\nlibrary(knitr)\nlibrary(broom)\n\n\n\nMissing data is a common problem and dealing with it appropriately is extremely important. Ignoring the missing data points or filling them incorrectly may cause the models to work in unexpected ways and cause the predictions and inferences to be biased.\nIn this chapter, you will be working with the biopics dataset. It contains information on a number of biographical movies, including their earnings, subject characteristics and some other variables. Some of the data points are, however, missing. The original data comes with the fivethirtyeight R package, but in this course, you will work with a slightly preprocessed version.\nIn this exercise, you will get to know the dataset and fit a linear regression model to explain a movie’s earnings. Let’s begin!\n\n\n\nbiopics <- read_csv(\"data/biopics.csv\")\n# Print first 10 observations\nhead(biopics, 10) %>%\n    kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\nyear\nearnings\nsub_num\nsub_type\nsub_race\nnon_white\nsub_sex\n\n\n\n\nUK\n1971\nNA\n1\nCriminal\nNA\n0\nMale\n\n\nUS/UK\n2013\n56.700\n1\nOther\nAfrican\n1\nMale\n\n\nUS/UK\n2010\n18.300\n1\nAthlete\nNA\n0\nMale\n\n\nCanada\n2014\nNA\n1\nOther\nWhite\n0\nMale\n\n\nUS\n1998\n0.537\n1\nOther\nNA\n0\nMale\n\n\nUS\n2008\n81.200\n1\nOther\nother\n1\nMale\n\n\nUK\n2002\n1.130\n1\nMusician\nWhite\n0\nMale\n\n\nUS\n2013\n95.000\n1\nAthlete\nAfrican\n1\nMale\n\n\nUS\n1994\n19.600\n1\nAthlete\nNA\n0\nMale\n\n\nUS/UK\n1987\n1.080\n2\nAuthor\nNA\n0\nMale\n\n\n\n\n\n\n\n\n\n# Get the number of missing values per variable\nbiopics %>%\n    is.na() %>% \n    colSums()\n\n  country      year  earnings   sub_num  sub_type  sub_race non_white   sub_sex \n        0         0       324         0         0       197         0         0 \n\n\n\n\n\n\n# Fit linear regression to predict earnings\nmodel_1 <- lm(earnings ~ country + year + sub_type, \n              data = biopics)\n\nsummary(model_1)\n\n\nCall:\nlm(formula = earnings ~ country + year + sub_type, data = biopics)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.283 -20.466  -5.251   6.871 285.210 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(>|t|)   \n(Intercept)                 -743.2411   273.2831  -2.720  0.00682 **\ncountryCanada/UK              -6.9648    19.5228  -0.357  0.72146   \ncountryUK                      7.0207    15.4945   0.453  0.65071   \ncountryUS                     30.9079    15.0039   2.060  0.04004 * \ncountryUS/Canada              31.6905    18.8308   1.683  0.09316 . \ncountryUS/UK                  23.7589    15.4580   1.537  0.12508   \ncountryUS/UK/Canada           -4.8187    29.6967  -0.162  0.87118   \nyear                           0.3783     0.1359   2.784  0.00562 **\nsub_typeActivist             -21.7103    13.0520  -1.663  0.09701 . \nsub_typeActor                -41.6236    16.8004  -2.478  0.01364 * \nsub_typeActress              -34.9628    17.5264  -1.995  0.04673 * \nsub_typeActress / activist     7.1816    37.6378   0.191  0.84877   \nsub_typeArtist               -25.2620    13.8543  -1.823  0.06898 . \nsub_typeAthlete              -10.7316    12.1242  -0.885  0.37661   \nsub_typeAthlete / military    66.3717    37.6682   1.762  0.07882 . \nsub_typeAuthor               -25.9330    12.6080  -2.057  0.04034 * \nsub_typeAuthor (poet)        -17.1963    17.1851  -1.001  0.31759   \nsub_typeComedian             -29.3344    18.3419  -1.599  0.11053   \nsub_typeCriminal              -7.3534    12.2475  -0.600  0.54857   \nsub_typeGovernment           -16.9917    23.5048  -0.723  0.47016   \nsub_typeHistorical            -4.0166    12.6665  -0.317  0.75133   \nsub_typeJournalist           -30.6610    28.0016  -1.095  0.27418   \nsub_typeMedia                -15.7588    16.7744  -0.939  0.34806   \nsub_typeMedicine               5.0987    21.0749   0.242  0.80895   \nsub_typeMilitary              15.1616    14.0730   1.077  0.28196   \nsub_typeMilitary / activist   29.8300    37.6688   0.792  0.42888   \nsub_typeMusician             -21.1765    12.1482  -1.743  0.08206 . \nsub_typeOther                -17.5989    11.4405  -1.538  0.12476   \nsub_typePolitician           -21.0700    37.6688  -0.559  0.57623   \nsub_typeSinger                 1.0769    14.9161   0.072  0.94248   \nsub_typeTeacher               42.4600    37.6407   1.128  0.25997   \nsub_typeWorld leader           0.5964    16.2407   0.037  0.97072   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 36 on 405 degrees of freedom\n  (324 observations deleted due to missingness)\nMultiple R-squared:  0.1799,    Adjusted R-squared:  0.1171 \nF-statistic: 2.865 on 31 and 405 DF,  p-value: 1.189e-06\n\n\n\n\n\n\nYou are interested in how well the model you’ve just built fits the data. To measure this, you want to calculate the median absolute difference between the true and predicted earnings. You run the following line of code:\nAs some observations were removed from the model, the two vectors inside abs() have different lengths, and so the entries of the shorter one get replicated to enable the subtraction. Consequently, the resulting number has no meaning. Analyzing models fit to incomplete data can be treacherous\n\n\nmedian(abs(biopics$earnings - model_1$fitted.values), na.rm = TRUE)\n\n[1] 21.66698\n\n\n\n\n\n\nChoosing the best of multiple competing models can be tricky if these models are built on incomplete data. In this exercise, you will extend the model you have built previously by adding one more explanatory variable: the race of the movie’s subject. Then, you will try to compare it to the previous model.\nAs a reminder, this is how you have fitted the first model:\n\nmodel_1 <- lm(earnings ~ country + year + sub_type, data = biopics) Let’s see if we can judge whether adding the race variable improves the model!\n\n\n\n\n# Fit linear regression to predict earnings\nmodel_2 <- lm(earnings ~ country + year + sub_type + sub_race, \n              data = biopics)\n\n# Print summaries of both models\n\nsummary(model_2)\n\n\nCall:\nlm(formula = earnings ~ country + year + sub_type + sub_race, \n    data = biopics)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-58.323 -16.237  -4.018   5.614 200.234 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                -139.27034  287.97218  -0.484 0.629031    \ncountryCanada/UK              4.00206   18.25641   0.219 0.826643    \ncountryUK                    13.84774   14.91395   0.929 0.353943    \ncountryUS                    31.42015   14.32201   2.194 0.029069 *  \ncountryUS/Canada             18.29811   18.65109   0.981 0.327403    \ncountryUS/UK                 29.40669   14.79424   1.988 0.047817 *  \ncountryUS/UK/Canada           5.28487   34.26999   0.154 0.877553    \nyear                          0.08053    0.14277   0.564 0.573156    \nsub_typeActivist            -22.70696   13.91011  -1.632 0.103718    \nsub_typeActor               -37.18944   16.80696  -2.213 0.027722 *  \nsub_typeActress             -29.08213   17.54697  -1.657 0.098561 .  \nsub_typeActress / activist   22.74806   34.10892   0.667 0.505370    \nsub_typeArtist              -16.16366   14.44232  -1.119 0.264019    \nsub_typeAthlete               1.82705   13.21810   0.138 0.890163    \nsub_typeAthlete / military   81.76200   33.27768   2.457 0.014619 *  \nsub_typeAuthor              -16.89061   13.34913  -1.265 0.206817    \nsub_typeAuthor (poet)       -10.46216   17.81790  -0.587 0.557562    \nsub_typeComedian            -29.04858   19.58703  -1.483 0.139185    \nsub_typeCriminal             -3.63899   13.49577  -0.270 0.787636    \nsub_typeGovernment           -3.98375   21.53144  -0.185 0.853347    \nsub_typeHistorical           -1.84026   13.64400  -0.135 0.892806    \nsub_typeJournalist          -19.52435   25.70076  -0.760 0.448085    \nsub_typeMedia               -23.58188   18.39661  -1.282 0.200952    \nsub_typeMedicine             19.79476   33.28029   0.595 0.552465    \nsub_typeMilitary            -11.90055   15.58559  -0.764 0.445772    \nsub_typeMusician            -11.87866   12.76816  -0.930 0.352999    \nsub_typeOther                -8.26334   12.46291  -0.663 0.507854    \nsub_typePolitician          -13.12470   33.28805  -0.394 0.693677    \nsub_typeSinger               12.59513   15.42311   0.817 0.414829    \nsub_typeTeacher              52.19210   33.25064   1.570 0.117624    \nsub_typeWorld leader          5.70258   15.84955   0.360 0.719272    \nsub_raceAsian               -33.21461   17.04703  -1.948 0.052365 .  \nsub_raceHispanic            -25.63976    9.37824  -2.734 0.006657 ** \nsub_raceMid Eastern          -0.75224   11.54403  -0.065 0.948091    \nsub_raceMulti racial        -26.03619    9.67832  -2.690 0.007571 ** \nsub_raceother               -23.90532   12.36017  -1.934 0.054113 .  \nsub_raceWhite               -20.10327    5.90967  -3.402 0.000767 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31.01 on 280 degrees of freedom\n  (444 observations deleted due to missingness)\nMultiple R-squared:  0.2566,    Adjusted R-squared:  0.161 \nF-statistic: 2.684 on 36 and 280 DF,  p-value: 3.145e-06\n\n\n\nThe two models are not comparable, because each of them is based on a different data sample.\nWith incomplete datasets, changing the model’s architecture can impact the set of observations that are actually used by the model. This might prevent us from comparing different models.\n\n\n\n\n\nIn this exercise, you will face six different scenarios in which some data are missing. Try assigning each of them to the most likely missing data mechanism. As a refresher, here are some general guidelines:\n\nIf the reason for missingness is purely random, it’s MCAR.\nIf the reason for missingness can be explained by another variable, it’s MAR.\nIf the reason for missingness depends on the missing value itself, it’s MNAR.\n\nFurther explanation from Missing data mechanisms\n\nMissing completely at random (MCAR). When data are MCAR, the fact that the data are missing is independent of the observed and unobserved data. In other words, no systematic differences exist between participants with missing data and those with complete data. For example, some participants may have missing laboratory values because a batch of lab samples was processed improperly. In these instances, the missing data reduce the analyzable population of the study and consequently, the statistical power, but do not introduce bias: when data are MCAR, the data which remain can be considered a simple random sample of the full data set of interest. MCAR is generally regarded as a strong and often unrealistic assumption.\nMissing at random (MAR). When data are MAR, the fact that the data are missing is systematically related to the observed but not the unobserved data.15 For example, a registry examining depression may encounter data that are MAR if male participants are less likely to complete a survey about depression severity than female participants. That is, if probability of completion of the survey is related to their sex (which is fully observed) but not the severity of their depression, then the data may be regarded as MAR. Complete case analyses, which are based on only observations for which all relevant data are present and no fields are missing, of a data set containing MAR data may or may not result in bias. If the complete case analysis is biased, however, proper accounting for the known factors (in the above example, sex) can produce unbiased results in analysis.\nMissing not at random (MNAR). When data are MNAR, the fact that the data are missing is systematically related to the unobserved data, that is, the missingness is related to events or factors which are not measured by the researcher. To extend the previous example, the depression registry may encounter data that are MNAR if participants with severe depression are more likely to refuse to complete the survey about depression severity. As with MAR data, complete case analysis of a data set containing MNAR data may or may not result in bias; if the complete case analysis is biased, however, the fact that the sources of missing data are themselves unmeasured means that (in general) this issue cannot be addressed in analysis and the estimate of effect will likely be biased.\n\n ## t-test for MAR: data preparation Great work on classifying the missing data mechanisms in the last exercise! Of all three, MAR is arguably the most important one to detect, as many imputation methods assume the data are MAR. This exercise will, therefore, focus on testing for MAR.\nYou will be working with the familiar biopics data. The goal is to test whether the number of missing values in earnings differs per subject’s gender. In this exercise, you will only prepare the data for the t-test. First, you will create a dummy variable indicating missingness in earnings. Then, you will split it per gender by first filtering the data to keep one of the genders, and then pulling the dummy variable. For filtering, it might be helpful to print biopics’s head() in the console and examine the gender variable.\n\n# Create a dummy variable for missing earnings\nbiopics <- biopics %>% \n  mutate(missing_earnings = ifelse(is.na(earnings), TRUE, FALSE))\n\n# Pull the missing earnings dummy for males\nmissing_earnings_males <- biopics %>% \n  filter(sub_sex == \"Male\") %>% \n  pull(missing_earnings)\n\n# Pull the missing earnings dummy for females\nmissing_earnings_females <- biopics %>% \n  filter(sub_sex == \"Female\") %>% \n  pull(missing_earnings)\n\n# Run the t-test\nt.test(missing_earnings_males, missing_earnings_females)\n\n\n    Welch Two Sample t-test\n\ndata:  missing_earnings_males and missing_earnings_females\nt = 1.1116, df = 294.39, p-value = 0.2672\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.03606549  0.12969214\nsample estimates:\nmean of x mean of y \n0.4366438 0.3898305 \n\n\n\nNotice how the missing earnings percentage is not significantly different for both genders, even though the sample values (at the bottom of the test’s output) differ by almost 5 percentage points. Also, keep in mind that the conclusion that the data are not MAR is only valid for the specific variables we have tested.\n\n\n\n\nThe aggregation plot provides the answer to the basic question one may ask about an incomplete dataset: in which combinations of variables the data are missing, and how often? It is very useful for gaining a high-level overview of the missingness patterns. For example, it makes it immediately visible if there is some combination of variables that are often missing together, which might suggest some relation between them.\nIn this exercise, you will first draw the aggregation plot for the biopics data and then practice making conclusions based on it. Let’s do some plotting!\n\n# Load the VIM package\nlibrary(VIM)\n\n# Draw an aggregation plot of biopics\nbiopics %>% \n    aggr(combined = TRUE, numbers = TRUE)\n\n\n\n\n\n10% of the observations have missing values in both earnings and sub_race.\nThere are more missing values in sub_race than in earnings. This is false\n42% of the observations have no missing entries.\nThere are exactly two variables in the biopics data that have missing values.\nThis one is false! It is actually the other way round, there are more missing values in earnings. You can see it from the bars above the plot. Now that you have a high-level overview of the missingness in the data, let’s look more closely at specific variables!\n\n\n\n\nThe aggregation plot you have drawn in the previous exercise gave you some high-level overview of the missing data. If you are interested in the interaction between specific variables, a spine plot is the way to go. It allows you to study the percentage of missing values in one variable for different values of the other, which is conceptually very similar to the t-tests you have been running in the previous lesson.\nIn this exercise, you will draw a spine plot to investigate the percentage of missing data in earnings for different categories of sub_race. Is there more missing data on earnings for some specific races of the movie’s main character? Let’s find out! The VIM package has already been loaded for you.\n\n# Draw a spine plot to analyse missing values in earnings by sub_race\nbiopics %>% \n    select(sub_race, earnings) %>% as.data.frame() %>%\n    spineMiss()\n\n\n\n\n\n\n\nIn the vast majority of movies, the main character is white.\nWhen the main subject is African, we are the most likely to have complete earnings information.\nAs far as earnings and sub_race are concerned, the data seem to be MAR.\nThe race that appears most rarely in the data has around 40% of earnings missing.\n\n\nThis one is false! The scarcest race is Asian, as this bar is the thinnest. The missing earnings, however, amount to around 20%, not 40%. Let’s build upon the idea of a spine plot to create one more visualization in the next exercise!\n\n\n\n\n\nThe spine plot you have created in the previous exercise allows you to study missing data patterns between two variables at a time. This idea is generalized to more variables in the form of a mosaic plot.\nIn this exercise, you will start by creating a dummy variable indicating whether the United States was involved in the production of each movie. To do this, you will use the grepl() function, which checks if the string passed as its first argument is present in the object passed as its second argument. Then, you will draw a mosaic plot to see if the subject’s gender correlates with the amount of missing data on earnings for both US and non-US movies.\nThe biopics data as well as the VIM package are already loaded for you. Let’s do some exploratory plotting!\nNote that a propriety display_image()function has been created to return the output from the latest VIMpackage version. Make sure to expand the HTML Viewer section.\n\n# Prepare data for plotting and draw a mosaic plot\nbiopics %>%\n    # Create a dummy variable for US-produced movies\n    mutate(is_US_movie = grepl(\"US\", country)) %>%\n    # Draw mosaic plot\n    mosaicMiss(highlight = \"earnings\", \n             plotvars = c(\"is_US_movie\", \"sub_sex\"))\n\n\n\n# Return plot from latest VIM package - expand the HTML viewer section\n#display_image()\n\n\nBefore you expand the output, notice how, for non-US movies, there is less missing data on earnings for movies featuring females. This doesn’t look MCAR! You are now done with Chapter 1 and ready to take a deep dive into imputation methods."
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#smelling-the-danger-of-mean-imputation",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#smelling-the-danger-of-mean-imputation",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Smelling the danger of mean imputation",
    "text": "Smelling the danger of mean imputation\nOne of the most popular imputation methods is the mean imputation, in which missing values in a variable are replaced with the mean of the observed values in this variable. However, in many cases this simple approach is a poor choice. Sometimes a quick look at the data can already alert you to the dangers of mean-imputing.\nIn this chapter, you will be working with a subsample of the Tropical Atmosphere Ocean (tao) project data. The dataset consists of atmospheric measurements taken in two different time periods at five different locations. The data comes with the VIM package.\nIn this exercise you will familiarize yourself with the data and perform a simple analysis that will indicate what the consequences of mean imputation could be. Let’s take a look at the tao data!\n\nPrint first 10 observations\n\ntao <- read.csv(\"data/tao.csv\")\n# Print first 10 observations\nhead(tao, 10)\n\n   year latitude longitude sea_surface_temp air_temp humidity uwind vwind\n1  1997        0      -110            27.59    27.15     79.6  -6.4   5.4\n2  1997        0      -110            27.55    27.02     75.8  -5.3   5.3\n3  1997        0      -110            27.57    27.00     76.5  -5.1   4.5\n4  1997        0      -110            27.62    26.93     76.2  -4.9   2.5\n5  1997        0      -110            27.65    26.84     76.4  -3.5   4.1\n6  1997        0      -110            27.83    26.94     76.7  -4.4   1.6\n7  1997        0      -110            28.01    27.04     76.5  -2.0   3.5\n8  1997        0      -110            28.04    27.11     78.3  -3.7   4.5\n9  1997        0      -110            28.02    27.21     78.6  -4.2   5.0\n10 1997        0      -110            28.05    27.25     76.9  -3.6   3.5\n\n\n\n\nGet the number of missing values per column\n\n# Get the number of missing values per column\ntao %>%\n  is.na() %>% \n  colSums()\n\n            year         latitude        longitude sea_surface_temp \n               0                0                0                3 \n        air_temp         humidity            uwind            vwind \n              81               93                0                0 \n\n\n\n\nCalculate the number of missing values in air_temp per year\n\n# Calculate the number of missing values in air_temp per year\ntao %>% \n  group_by(year) %>% \n  summarize(num_miss = sum(is.na(air_temp))) %>%\n    kable()\n\n\n\n\nyear\nnum_miss\n\n\n\n\n1993\n4\n\n\n1997\n77"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#mean-imputing-the-temperature",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#mean-imputing-the-temperature",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Mean-imputing the temperature",
    "text": "Mean-imputing the temperature\nMean imputation can be a risky business. If the variable you are mean-imputing is correlated with other variables, this correlation might be destroyed by the imputed values. You saw it looming in the previous exercise when you analyzed the air_temp variable.\nTo find out whether these concerns are valid, in this exercise you will perform mean imputation on air_temp, while also creating a binary indicator for where the values are imputed. It will come in handy in the next exercise, when you will be assessing your imputation’s performance. Let’s fill in those missing values!\n\ntao_imp <- tao %>% \n  # Create a binary indicator for missing values in air_temp\n  mutate(air_temp_imp = ifelse(is.na(air_temp), TRUE, FALSE)) %>% \n  # Impute air_temp with its mean\n  mutate(air_temp = ifelse(is.na(air_temp), mean(air_temp, na.rm = TRUE), air_temp))\n\n# Print the first 10 rows of tao_imp\nhead(tao_imp, 10) %>%\n    head() %>%\n    kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nlatitude\nlongitude\nsea_surface_temp\nair_temp\nhumidity\nuwind\nvwind\nair_temp_imp\n\n\n\n\n1997\n0\n-110\n27.59\n27.15\n79.6\n-6.4\n5.4\nFALSE\n\n\n1997\n0\n-110\n27.55\n27.02\n75.8\n-5.3\n5.3\nFALSE\n\n\n1997\n0\n-110\n27.57\n27.00\n76.5\n-5.1\n4.5\nFALSE\n\n\n1997\n0\n-110\n27.62\n26.93\n76.2\n-4.9\n2.5\nFALSE\n\n\n1997\n0\n-110\n27.65\n26.84\n76.4\n-3.5\n4.1\nFALSE\n\n\n1997\n0\n-110\n27.83\n26.94\n76.7\n-4.4\n1.6\nFALSE\n\n\n\n\n\n\nAssessing imputation quality with margin plot\nIn the last exercise, you have mean-imputed air_temp and added an indicator variable to denote which values were imputed, called air_temp_imp. Time to see how well this works.\nUpon examining the tao data, you might have noticed that it also contains a variable called sea_surface_temp, which could reasonably be expected to be positively correlated with air_temp. If that’s the case, you would expect these two temperatures to be both high or both low at the same time. Imputing mean air temperature when the sea temperature is high or low would break this relation.\nTo find out, in this exercise you will select the two temperature variables and the indicator variable and use them to draw a margin plot. Let’s assess the mean imputation!\n\n# Draw a margin plot of air_temp vs sea_surface_temp\ntao_imp %>% \n  select(air_temp, sea_surface_temp, air_temp_imp) %>%\n  marginplot(delimiter = \"imp\")\n\n\n\n\nQuestion\n\nJudging by the margin plot you have drawn, what’s wrong with this mean imputation?\nPossible Answers\n\n\nAll the imputed air_temp values are the same, no matter the sea_surface_temp. This breaks the correlation between these two variables.\nThe imputed values are located in the space where there is no observed data, which makes them outliers.\nThe variance of the imputed data differs from the one of observed data.\nAll three above answers are correct. correct\n\n\nNotice how air and sea surface temperatures correlate. Imputing average air temperature in the observations where sea surface temperature is high creates clearly outlying data points and destroys the relation between these two variables. If the sea surface temperature is high, we would like to impute air temperature values that are also high. Head over to the upcoming video to learn a method that is able to do that!\n\n\n\nThe problem of mean imputation\n\nggplot(tao_imp, aes(air_temp, sea_surface_temp, color = air_temp_imp))+\n    geom_point()+\n    scale_color_brewer(name = \"Imputed\", type = \"qual\", palette = \"Dark2\")+\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\nVanilla hot-deck\nHot-deck imputation is a simple method that replaces every missing value in a variable by the last observed value in this variable. It’s very fast, as only one pass through the data is needed, but in its simplest form, hot-deck may sometimes break relations between the variables.\nIn this exercise, you will try it out on the tao dataset. You will hot-deck-impute missing values in the air temperature column air_temp and then draw a margin plot to analyze the relation between the imputed values with the sea surface temperature column sea_surface_temp. Let’s see how it works!\n\n# Load VIM package\nlibrary(VIM)\n\n# Impute air_temp in tao with hot-deck imputation\ntao_imp <- hotdeck(tao, variable = \"air_temp\")\n\n# Check the number of missing values in each variable\ntao_imp %>% \n    is.na() %>% \n    colSums()\n\n            year         latitude        longitude sea_surface_temp \n               0                0                0                3 \n        air_temp         humidity            uwind            vwind \n               0               93                0                0 \n    air_temp_imp \n               0 \n\n# Draw a margin plot of air_temp vs sea_surface_temp\ntao_imp %>% \n    select(air_temp, sea_surface_temp, air_temp_imp) %>% \n    marginplot(delimiter = \"imp\")\n\n\n\n\n\nDoes the imputation look good? Notice the observations in the top left part of the plot with imputed air_temp and high sea_surface_temp. These observations must have been preceded by ones with low air_temp in the data frame, and so after hot-deck imputation, they ended up being outliers with low air_temp and high sea_surface_temp.\n\n\n\nHot-deck tricks & tips I: imputing within domains\nOne trick that may help when hot-deck imputation breaks the relations between the variables is imputing within domains. What this means is that if the variable to be imputed is correlated with another, categorical variable, one can simply run hot-deck separately for each of its categories.\nFor instance, you might expect air temperature to depend on time, as we are seeing the average temperatures rising due to global warming. The time indicator you have available in the tao data is a categorical variable, year. Let’s first check if the average air temperature is different in each of the two studied years and then run hot-deck within year domains. Finally, you will draw the margin plot again to assess the imputation performance.\n\n# Calculate mean air_temp per year\ntao %>% \n    group_by(year) %>% \n    summarize(average_air_temp = mean(air_temp, na.rm = TRUE)) %>%\n    kable()\n\n\n\n\nyear\naverage_air_temp\n\n\n\n\n1993\n23.36596\n\n\n1997\n27.10979\n\n\n\n\n# Hot-deck-impute air_temp in tao by year domain\ntao_imp <- hotdeck(tao, variable = \"air_temp\", domain_var = \"year\")\n\n# Draw a margin plot of air_temp vs sea_surface_temp\ntao_imp %>% \n    select(air_temp, sea_surface_temp, air_temp_imp) %>% \n    marginplot(delimiter = \"imp\")\n\n\n\n\n\nThe results look much better this time. However, if you look at the top right corner of the plot, you will see that the variance in the imputed (orange) values is somewhat larger than among the observed (blue) values. Let’s see if we can improve even further in the next exercise\n\n\n\nHot-deck tricks & tips II: sorting by correlated variables\nAnother trick that can boost the performance of hot-deck imputation is sorting the data by variables correlated to the one we want to impute.\nFor instance, in all the margin plots you have been drawing recently, you have seen that air temperature is strongly correlated with sea surface temperature, which makes a lot of sense. You can exploit this knowledge to improve your hot-deck imputation. If you first order the data by sea_surface_temp, then every imputed air_temp value will come from a donor with a similar sea_surface_temp. Let’s see how this will work!\n\n# Hot-deck-impute air_temp in tao ordering by sea_surface_temp\ntao_imp <- hotdeck(tao, \n                   variable = \"air_temp\", \n                   ord_var = \"sea_surface_temp\")\n\n# Draw a margin plot of air_temp vs sea_surface_temp\ntao_imp %>% \n    select(air_temp, sea_surface_temp, air_temp_imp) %>% \n    marginplot(delimiter = \"imp\")\n\n\n\n\n\nThis time the imputation seems not to impact the relation between air and sea temperatures: if not for the colors, you likely wouldn’t know which ones are the imputed values. Hot-deck imputation, possibly enhanced with domain-imputing or sorting, is a fast and simple method that can serve you well in many situations. However, sometimes you may need a more complex approach. Head over to the next video to learn about k-Nearest-Neighbors imputation!\n\n\n\nJust a little experiment\n\n# Hot-deck-impute air_temp in tao ordering by sea_surface_temp\ntao_imp <- hotdeck(tao, \n                   variable = \"air_temp\", \n                   ord_var = \"sea_surface_temp\",\n                   domain_var = \"year\")\n\n# Draw a margin plot of air_temp vs sea_surface_temp\ntao_imp %>% \n    select(air_temp, sea_surface_temp, air_temp_imp) %>% \n    marginplot(delimiter = \"imp\")\n\n\n\n\n\n\nChoosing the number of neighbors\nk-Nearest-Neighbors (or kNN) imputation fills the missing values in an observation based on the values coming from the k other observations that are most similar to it. The number of these similar observations, called neighbors, that are considered is a parameter that has to be chosen beforehand.\nHow to choose k? One way is to try different values and see how they impact the relations between the imputed and observed data.\nLet’s try imputing humidity in the tao data using three different values of k and see how the imputed values fit the relation between humidity and sea_surface_temp.\n\nImpute humidity with kNN imputation using 30 neighbors and draw a marginplot() of sea_surface_temp vs humidity.\n\n\n# Impute humidity using 30 neighbors\ntao_imp <- kNN(tao, k = 30, variable = \"humidity\")\n\n# Draw a margin plot of sea_surface_temp vs humidity\ntao_imp %>% \n    select(sea_surface_temp, humidity, humidity_imp) %>% \n    marginplot(delimiter = \"imp\", main = \"k = 30\")\n\n\n\n\n\nImpute humidity with kNN imputation using 15 neighbors and draw a margin plot of sea_surface_temp vs humidity.\n\n\n# Impute humidity using 15 neighbors\ntao_imp <- kNN(tao, k = 15, variable = \"humidity\")\n\n# Draw a margin plot of sea_surface_temp vs humidity\ntao_imp %>% \n    select(sea_surface_temp, humidity, humidity_imp) %>% \n    marginplot(delimiter = \"imp\", main = \"k = 15\")\n\n\n\n\n\nImpute humidity with kNN imputation using 5 neighbors and draw a margin plot of sea_surface_temp vs humidity.\n\n\n# Impute humidity using 5 neighbors\ntao_imp <- kNN(tao, k = 5, variable = \"humidity\")\n\n# Draw a margin plot of sea_surface_temp vs humidity\ntao_imp %>% \n    select(sea_surface_temp, humidity, humidity_imp) %>% \n    marginplot(delimiter = \"imp\", main = \"k = 5\")\n\n\n\n\n\nYou can browse through the three plots you have just drawn. The last one seems to capture the most variation in the data, so you should be good to use k = 5 in this case. Let’s look at how we can improve on this default kNN imputation with some tricks!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#knn-tricks-tips-i-weighting-donors",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#knn-tricks-tips-i-weighting-donors",
    "title": "Handling Missing Data with Imputations in R",
    "section": "kNN tricks & tips I: weighting donors",
    "text": "kNN tricks & tips I: weighting donors\nA variation of kNN imputation that is frequently applied uses the so-called distance-weighted aggregation. What this means is that when we aggregate the values from the neighbors to obtain a replacement for a missing value, we do so using the weighted mean and the weights are inverted distances from each neighbor. As a result, closer neighbors have more impact on the imputed value.\nIn this exercise, you will apply the distance-weighted aggregation while imputing the tao data. This will only require passing two additional arguments to the kNN() function. Let’s try it out!\n\n# Load the VIM package\nlibrary(VIM)\n\n# Impute humidity with kNN using distance-weighted mean\ntao_imp <- kNN(tao, \n               k = 5, \n               variable = \"humidity\", \n               numFun = weighted.mean,\n               weightDist = TRUE)\n\ntao_imp %>% \n    select(sea_surface_temp, humidity, humidity_imp) %>% \n    marginplot(delimiter = \"imp\")\n\n\n\n\n\nDistance-weighted aggregation makes the kNN imputation more robust to situations where an observation is unique in some way and doesn’t have many very similar neighbors. In such cases, the least similar neighbors get assigned a small weight and contribute less to the imputed values. Head over to the last exercise of this chapter to learn one more trick that makes kNN more robust and accurate!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#knn-tricks-tips-ii-sorting-variables",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#knn-tricks-tips-ii-sorting-variables",
    "title": "Handling Missing Data with Imputations in R",
    "section": "kNN tricks & tips II: sorting variables",
    "text": "kNN tricks & tips II: sorting variables\nAs the k-Nearest Neighbors algorithm loops over the variables in the data to impute them, it computes distances between observations using other variables, some of which have already been imputed in the previous steps. This means that if the variables located earlier in the data have a lot of missing values, then the subsequent distance calculation is based on a lot of imputed values. This introduces noise to the distance calculation.\nFor this reason, it is a good practice to sort the variables increasingly by the number of missing values before performing kNN imputation. This way, each distance calculation is based on as much observed data and as little imputed data as possible.\nLet’s try this out on the tao data!\n\n# Get tao variable names sorted by number of NAs\nvars_by_NAs <- tao %>%\n  is.na() %>%\n  colSums() %>%\n  sort(decreasing = FALSE) %>% \n  names()\nvars_by_NAs\n\n[1] \"year\"             \"latitude\"         \"longitude\"        \"uwind\"           \n[5] \"vwind\"            \"sea_surface_temp\" \"air_temp\"         \"humidity\"        \n\n# Sort tao variables and feed it to kNN imputation\ntao_imp <- tao %>% \n  select(vars_by_NAs) %>% \n  kNN(k = 5)\n\ntao_imp %>% \n    select(sea_surface_temp, humidity, humidity_imp) %>% \n    marginplot(delimiter = \"imp\")\n\n\n\n\n\nThe kNN you have just coded should be more accurate and robust against faulty imputations, so remember to sort your variables first before performing kNN imputation! This brings us to the end of this chapter. Keep it up! See you in Chapter 3, where you will learn to use statistical and machine learning models to impute missing values!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#linear-regression-imputation",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#linear-regression-imputation",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Linear regression imputation",
    "text": "Linear regression imputation\nSometimes, you can use domain knowledge, previous research or simply your common sense to describe the relations between the variables in your data. In such cases, model-based imputation is a great solution, as it allows you to impute each variable according to a statistical model that you can specify yourself, taking into account any assumptions you might have about how the variables impact each other.\nFor continuous variables, a popular model choice is linear regression. It doesn’t restrict you to linear relations though! You can always include a square or a logarithm of a variable in the predictors. In this exercise, you will work with the simputation package to run a single linear regression imputation on the tao data and analyze the results. Let’s give it a try!\n\n# Load the simputation package\nlibrary(simputation)\n\n# Impute air_temp and humidity with linear regression\nformula <- air_temp + humidity ~ year + latitude + sea_surface_temp\ntao_imp <- impute_lm(tao, formula)\n\n# Check the number of missing values per column\ntao_imp %>% \n  is.na() %>% \n  colSums()\n\n            year         latitude        longitude sea_surface_temp \n               0                0                0                3 \n        air_temp         humidity            uwind            vwind \n               3                2                0                0 \n\n# Print rows of tao_imp in which air_temp or humidity are still missing \ntao_imp %>% \n  filter(is.na(air_temp) | is.na(humidity)) %>%\n    kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nlatitude\nlongitude\nsea_surface_temp\nair_temp\nhumidity\nuwind\nvwind\n\n\n\n\n1993\n0\n-95\nNA\nNA\nNA\n-5.6\n3.1\n\n\n1993\n0\n-95\nNA\nNA\nNA\n-6.3\n0.5\n\n\n1993\n-2\n-95\nNA\nNA\n89.9\n-3.4\n2.4\n\n\n\n\n\n\nLinear regression fails when at least one of the predictors is missing. In this case, it was sea_surface_temp. In the next exercise, you will fix it by initializing the missing values before running impute_lm()"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#initializing-missing-values-iterating-over-variables",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#initializing-missing-values-iterating-over-variables",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Initializing missing values & iterating over variables",
    "text": "Initializing missing values & iterating over variables\nAs you have just seen, running impute_lm() might not fill-in all the missing values. To ensure you impute all of them, you should initialize the missing values with a simple method, such as the hot-deck imputation you learned about in the previous chapter, which simply feeds forward the last observed value.\nMoreover, a single imputation is usually not enough. It is based on the basic initialized values and could be biased. A proper approach is to iterate over the variables, imputing them one at a time in the locations where they were originally missing.\nIn this exercise, you will first initialize the missing values with hot-deck imputation and then loop five times over air_temp and humidity from the tao data to impute them with linear regression. Let’s get to it!\n\n# Initialize missing values with hot-deck\ntao_imp <- hotdeck(tao)\n\n# Create boolean masks for where air_temp and humidity are missing\nmissing_air_temp <- tao_imp$air_temp_imp\nmissing_humidity <-  tao_imp$humidity_imp\n\nfor (i in 1:5) {\n  # Set air_temp to NA in places where it was originally missing and re-impute it\n  tao_imp$air_temp[missing_air_temp] <- NA\n  tao_imp <- impute_lm(tao_imp, air_temp ~ year + latitude + sea_surface_temp + humidity)\n  # Set humidity to NA in places where it was originally missing and re-impute it\n  tao_imp$humidity[missing_humidity] <- NA\n  tao_imp <- impute_lm(tao_imp, humidity ~ year + latitude + sea_surface_temp + air_temp)\n}\n\n\nThat’s a professional approach to model-based imputation you have just coded! But how do we know that 5 is the proper number of iterations to run? Let’s look at the convergence in the next exercise!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#detecting-convergence",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#detecting-convergence",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Detecting convergence",
    "text": "Detecting convergence\nGreat job iterating over the variables in the last exercise! But how many iterations are needed? When the imputed values don’t change with the new iteration, we can stop.\nYou will now extend your code to compute the differences between the imputed variables in subsequent iterations. To do this, you will use the Mean Absolute Percentage Change function, defined for you as follows:\nmapc <- function(a, b) { mean(abs(b - a) / a, na.rm = TRUE) } mapc() outputs a single number that tells you how much b differs from a. You will use it to check how much the imputed variables change across iterations. Based on this, you will decide how many of them are needed!\nThe boolean masks missing_air_temp and missing_humidity are available for you, as is the hotdeck-initialized tao_imp data.\n\nmapc <- function(a, b) {\n  mean(abs(b - a) / a, na.rm = TRUE)\n}\n\ndiff_air_temp <- c()\ndiff_humidity <- c()\n\nfor (i in 1:5) {\n  # Assign the outcome of the previous iteration (or initialization) to prev_iter\n  prev_iter <- tao_imp\n  # Impute air_temp and humidity at originally missing locations\n  tao_imp$air_temp[missing_air_temp] <- NA\n  tao_imp <- impute_lm(tao_imp, air_temp ~ year + latitude + sea_surface_temp + humidity)\n  tao_imp$humidity[missing_humidity] <- NA\n  tao_imp <- impute_lm(tao_imp, humidity ~ year + latitude + sea_surface_temp + air_temp)\n  # Calculate MAPC for air_temp and humidity and append them to previous iteration's MAPCs\n  diff_air_temp <- c(diff_air_temp, mapc(prev_iter$air_temp, tao_imp$air_temp))\n  diff_humidity <- c(diff_humidity, mapc(prev_iter$humidity, tao_imp$humidity))\n}\n\ndf_diff  <- data.frame(diff_air_temp, diff_humidity)\nplot_diffs <- function(a, b) {\n  data.frame(\"mapc\" = c(a, b),\n             \"Variable\" = c(rep(\"air_temp\", length(a)),\n                            rep(\"humidity\", length(b))),\n             \"Iterations\" = c(1:length(a), 1:length(b))) %>% \n    ggplot(aes(Iterations, mapc, color = Variable)) +\n    geom_line(size = 1.5) +\n    ylab(\"Mean absolute percentage change\") +\n    ggtitle(\"Changes in imputed variables' values across iterations\") +\n    theme(legend.position = \"bottom\")\n}\n\nplot_diffs(diff_air_temp, diff_humidity)\n\n\n\n\n\nTwo are enough, as the third one brings virtually no change anymore!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#logistic-regression-imputation",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#logistic-regression-imputation",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Logistic regression imputation",
    "text": "Logistic regression imputation\nA popular choice for imputing binary variables is logistic regression. Unfortunately, there is no function similar to impute_lm() that would do it. That’s why you’ll write such a function yourself!\nLet’s call the function impute_logreg(). Its first argument will be a data frame df, whose missing values have been initialized and only containing missing values in the column to be imputed. The second argument will be a formula for the logistic regression model.\nThe function will do the following:\nKeep the locations of missing values. Build the model. Make predictions. Replace missing values with predictions. Don’t worry about the line creating imp_var - this is just a way to extract the name of the column to impute from the formula. Let’s do some functional programming!\n\nimpute_logreg <- function(df, formula) {\n  # Extract name of response variable\n  imp_var <- as.character(formula[2])\n  # Save locations where the response is missing\n  missing_imp_var <- is.na(df[imp_var])\n  # Fit logistic regression mode\n  logreg_model <- glm(formula, data = df, family = binomial)\n  # Predict the response and convert it to 0s and 1s\n  preds <- predict(logreg_model, type = \"response\")\n  preds <- ifelse(preds >= 0.5, 1, 0)\n  # Impute missing values with predictions\n  df[missing_imp_var, imp_var] <-preds[missing_imp_var]\n  return(df)\n}"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#drawing-from-conditional-distribution",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#drawing-from-conditional-distribution",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Drawing from conditional distribution",
    "text": "Drawing from conditional distribution\nSimply calling predict() on a model will always return the same value for the same values of the predictors. This results in a small variability in imputed data. In order to increase it, so that the imputation replicates the variability from the original data, we can draw from the conditional distribution. What this means is that instead of always predicting 1 whenever the model outputs a probability larger than 0.5, we can draw the prediction from a binomial distribution described by the probability returned by the model.\nYou will work on the code you have written in the previous exercise. The following line was removed:\npreds <- ifelse(preds >= 0.5, 1, 0) Your task is to fill its place with drawing from a binomial distribution. That’s just one line of code!\n\n impute_logreg <- function(df, formula) {\n  # Extract name of response variable\n  imp_var <- as.character(formula[2])\n  # Save locations where the response is missing\n  missing_imp_var <- is.na(df[imp_var])\n  # Fit logistic regression mode\n  logreg_model <- glm(formula, data = df, family = binomial)\n  # Predict the response\n  preds <- predict(logreg_model, type = \"response\")\n  # Sample the predictions from binomial distribution\n  preds <- rbinom(length(preds), size = 1, prob = preds)\n  # Impute missing values with predictions\n  df[missing_imp_var, imp_var] <- preds[missing_imp_var]\n  return(df)\n}\n\n\nDrawing from the conditional distribution will make the imputed data’s variability more similar to the one of original, observed data. With this powerful function at hand, you are now ready to design a model-based imputation flow that takes care of both continuous and binary variables. Let’s do it in the next exercise!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#model-based-imputation-with-multiple-variable-types",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#model-based-imputation-with-multiple-variable-types",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Model-based imputation with multiple variable types",
    "text": "Model-based imputation with multiple variable types\nGreat job on writing the function to implement logistic regression imputation with drawing from conditional distribution. That’s pretty advanced statistics you have coded! In this exercise, you will combine what you learned so far about model-based imputation to impute different types of variables in the tao data.\nYour task is to iterate over variables just like you have done in the previous chapter and impute two variables:\nis_hot, a new binary variable that was created out of air_temp, which is 1 if air_temp is at or above 26 degrees and is 0 otherwise; humidity, a continuous variable you are already familiar with. You will have to use the linear regression function you have learned before, as well as your own function for logistic regression. Let’s get to it!\n\n# Initialize missing values with hot-deck\ntao <- tao %>% \n    mutate(is_hot = ifelse(air_temp > 26, 1, 0))\ntao_imp <- hotdeck(tao)\n\n# Create boolean masks for where is_hota and humidity are missing\nmissing_is_hot <- tao_imp$is_hot_imp\nmissing_humidity <- tao_imp$humidity_imp\n\nfor (i in 1:3) {\n  # Set is_hot to NA in places where it was originally missing and re-impute it\n  tao_imp$is_hot[missing_is_hot] <- NA\n  tao_imp <- impute_logreg(tao_imp, is_hot ~ sea_surface_temp)\n  # Set humidity to NA in places where it was originally missing and re-impute it\n  tao_imp$humidity[missing_humidity] <- NA\n  tao_imp <- impute_lm(tao_imp, \n  humidity ~ sea_surface_temp + air_temp)\n}\n\n\nYou have used the simputation package where possible, filling the gaps with your own programming, in order to run a model-based imputation that takes care of both continuous and binary variables, additionally inreasing variability in imputed data in the latter case. Well done! Let’s continue to the final lesson of this chapter, where you will learn how to use tree-based machine learning models for imputation."
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#imputing-with-random-forests",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#imputing-with-random-forests",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Imputing with random forests",
    "text": "Imputing with random forests\nA machine learning approach to imputation might be both more accurate and easier to implement compared to traditional statistical models. First, it doesn’t require you to specify relationships between variables. Moreover, machine learning models such as random forests are able to discover highly complex, non-linear relations and exploit them to predict missing values.\nIn this exercise, you will get acquainted with the missForest package, which builds a separate random forest to predict missing values for each variable, one by one. You will call the imputing function on the biographic movies data, biopics, which you have worked with earlier in the course and then extract the filled-in data as well as the estimated imputation errors.\nLet’s plant some random forests!\n\n# Load the missForest package\nbiopics <- read_csv(\"data/biopics.csv\")\nlibrary(missForest)\n\ncont_lev <- c(\"UK\", \"US/UK\", \"Canada US\", \n           \"Canada/UK\", \"US/Canada\", \"US/UK/Canada\")\n\nbiopics <- biopics %>%\n    mutate(country = factor(country, levels = cont_lev))\nbiopics <- biopics %>%\n    mutate_if(is.character, factor)\n# Impute biopics data using missForest\nbiopics <- as.data.frame(biopics)\nimp_res <- missForest(biopics)\n\n# Extract imputed data and check for missing values\nimp_data <- imp_res$ximpnhanes_imp\nprint(sum(is.na(imp_data)))\n\n[1] 0\n\n# Extract and print imputation errors\nimp_err <- imp_res$OOBerror\nprint(imp_err)\n\n     NRMSE        PFC \n0.02015264 0.15126417 \n\n\nNote that missForest() outputs a list and you have to manually extract the imputed data - it’s a common mistake to overlook it when building a data processing pipeline. Also, take a look at the errors. Can you tell which variables have been imputed particularly well? Let’s look at it more closely in the next exercise!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#variable-wise-imputation-errors",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#variable-wise-imputation-errors",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Variable-wise imputation errors",
    "text": "Variable-wise imputation errors\nIn the previous exercise you have extracted the estimated imputation errors from missForest’s output. This gave you two numbers:\nthe normalized root mean squared error (NRMSE) for all continuous variables; the proportion of falsely classified entries (PFC) for all categorical variables. However, it could well be that the imputation model performs great for one continuous variable and poor for another! To diagnose such cases, it is enough to tell missForest to produce variable-wise error estimates. This is done by setting the variablewise argument to TRUE.\nThe biopics data and missForest package have already been loaded for you, so let’s take a closer look at the errors!\n\n# Impute biopics data with missForest computing per-variable errors\nimp_res <- missForest(biopics, variablewise = TRUE)\n\n# Extract and print imputation errors\nper_variable_errors <- imp_res$OOBerror\nprint(per_variable_errors)\n\n         PFC          MSE          MSE          MSE          PFC          PFC \n   0.3543307    0.0000000 1141.7047096    0.0000000    0.0000000    0.1790780 \n         MSE          PFC \n   0.0000000    0.0000000 \n\n# Rename errors' columns to include variable names\nnames(per_variable_errors) <- paste(names(biopics), \n                                    names(per_variable_errors),\n                                    sep = \"_\")\n\n# Print the renamed errors\nprint(per_variable_errors)\n\n  country_PFC      year_MSE  earnings_MSE   sub_num_MSE  sub_type_PFC \n    0.3543307     0.0000000  1141.7047096     0.0000000     0.0000000 \n sub_race_PFC non_white_MSE   sub_sex_PFC \n    0.1790780     0.0000000     0.0000000"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#speed-accuracy-trade-off",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#speed-accuracy-trade-off",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Speed-accuracy trade-off",
    "text": "Speed-accuracy trade-off\nIn the last video, you have seen there are two knobs you can tune to influence the performance of the random forests:\nNumber of decision trees in each forest. Number of variables used for splitting within decision trees. Increasing each of them might improve the accuracy of the imputation model, but it will also require more time to run. In this exercise, you will explore these ideas yourself by fitting missForest() to the biopics data twice with different settings. As you follow the instructions, pay attention to the errors you will be printing, and to the time the code takes to run.\n\n# Set number of trees to 50 and number of variables used for splitting to 6\nimp_res <- missForest(biopics, ntree = 5, mtry = 2)\n\n# Print the resulting imputation errors\nprint(imp_res$OOBerror)\n\n     NRMSE        PFC \n0.02468137 0.19389523 \n\n# Set number of trees to 50 and number of variables used for splitting to 6\nimp_res <- missForest(biopics, ntree = 50, mtry = 6)\n\n# Print the resulting imputation errors\nprint(imp_res$OOBerror)\n\n     NRMSE        PFC \n0.02018355 0.13507288 \n\n\n\nCompare the errors and the run times of the two imputation models. Can you see a relation? There ain’t no such thing as a free lunch, they say. To get a more precise imputation, you had to spend more in computing time! Congratulations on finishing the chapter! See you in the final chapter, where you will learn to incorporate uncertainty from imputation into your analyses and predictions."
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#wrapping-imputation-modeling-in-a-function",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#wrapping-imputation-modeling-in-a-function",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Wrapping imputation & modeling in a function",
    "text": "Wrapping imputation & modeling in a function\nWhenever you perform any analysis or modeling on imputed data, you should account for the uncertainty from imputation. Running a model on a dataset imputed only once ignores the fact that imputation estimates the missing values with uncertainty. Standard errors from such a model tend to be too small. The solution to this is multiple imputation and one way to implement it is by bootstrapping.\nIn the upcoming exercises, you will work with the familiar biopics data. The goal is to use multiple imputation by bootstrapping and linear regression to see if, based on the data at hand, biographical movies featuring females earn less than those about males.\nLet’s start with writing a function that creates a bootstrap sample, imputes it, and fits a linear regression model.\n\ncalc_gender_coef <- function(data, indices) {\n  # Get bootstrap sample\n  data_boot <- data[indices, ]\n  # Impute with kNN imputation\n  data_imp <- kNN(data_boot, k = 5)\n  # Fit linear regression\n  linear_model <- lm(earnings ~ sub_sex + sub_type + year,data = data_imp)\n  # Extract and return gender coefficient\n  gender_coefficient <- coef(linear_model)[2]\n  return(gender_coefficient)\n}\n\nThe calc_gender_coef() function you have just coded takes the data and bootstrap indices as inputs, and outputs our statistic of interest - the impact of gender on earnings from linear regression. You can now feed this function to the bootstrapping algorithm!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#running-the-bootstrap",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#running-the-bootstrap",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Running the bootstrap",
    "text": "Running the bootstrap\nGood job writing calc_gender_coef() in the last exercise! This function creates a bootstrap sample, imputes it and, outputs the linear regression coefficient describing the impact of movie subject’s being a female on the movie’s earnings.\nIn this exercise, you will use the boot package in order to obtain a bootstrapped distribution of such coefficients. The spread of this distribution will capture the uncertainty from imputation. You will also look at how the bootstrapped distribution differs from a single-time imputation and regression. Let’s do some bootstrapping!\n\n# Load the boot library\nlibrary(boot)\n\n# Run bootstrapping on biopics data\nboot_results <- boot(biopics, statistic = calc_gender_coef, R = 50)\n\n# Print and plot bootstrapping results\nprint(boot_results)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = biopics, statistic = calc_gender_coef, R = 50)\n\n\nBootstrap Statistics :\n    original     bias    std. error\nt1* 2.552249 -0.9320901    3.550253\n\nplot(boot_results)\n\n\n\n# Calculate and print confidence interval\nboot_ci <- boot.ci(boot_results, conf = .95, type = \"norm\")\nprint(boot_ci)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 50 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_results, conf = 0.95, type = \"norm\")\n\nIntervals : \nLevel      Normal        \n95%   (-3.474, 10.443 )  \nCalculations and Intervals on Original Scale\n\n\n\nIf you had run the kNN imputation and the regression analysis on biopics data only once, you would have obtained the female-coefficient of -1.45 (called ‘original’ in the console output), suggesting that movies about females indeed earn less. However, correcting for the uncertainty from imputation, you have obtained the distribution that covers both negative and postive values!\n\n\nBootstrapping confidence intervals\nHaving bootstrapped the distribution of the female-effect coefficient in the last exercise, you can now use it to estimate a confidence interval. It will allow you to make the following assessment about your data: “Given the uncertainty from imputation, we are 95% sure that the female-effect on earnings is between a and b”, where a and b are the lower and upper bounds of the interval.\nIn the last exercise, you have run bootstrapping with R = 50 replicates. In most applications, however, this is not enough. In this exercise, you can use boot_results that were prepared for you using 1000 replicates. First, you will look at the bootstrapped distribution to see if it looks normal. If so, you can then rely on the normal distribution to calculate the confidence interval.\n\n# Run bootstrapping on biopics data\nboot_results <- boot(biopics, statistic = calc_gender_coef, R = 1000)\n\n# Print and plot bootstrapping results\nprint(boot_results)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = biopics, statistic = calc_gender_coef, R = 1000)\n\n\nBootstrap Statistics :\n    original     bias    std. error\nt1* 2.339167 -0.8716358    3.905079\n\nplot(boot_results)\n\n\n\n# Calculate and print confidence interval\nboot_ci <- boot.ci(boot_results, conf = .95, type = \"norm\")\nprint(boot_ci)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_results, conf = 0.95, type = \"norm\")\n\nIntervals : \nLevel      Normal        \n95%   (-4.443, 10.865 )  \nCalculations and Intervals on Original Scale\n\n\n\nDespite the coefficient leaning to be a negative relationship, bootstrap replicates show that some movies with female leads actually earn more! Accounting for the uncertainty from imputation, you cannot be 100% sure about the direction of this relation, even though a single analysis suggests otherwise."
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#the-mice-flow-mice---with---pool",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#the-mice-flow-mice---with---pool",
    "title": "Handling Missing Data with Imputations in R",
    "section": "The mice flow: mice - with - pool",
    "text": "The mice flow: mice - with - pool\nMultiple imputation by chained equations, or MICE, allows us to estimate the uncertainty from imputation by imputing a data set multiple times with model-based imputation, while drawing from conditional distributions. This way, each imputed data set is slightly different. Then, an analysis is conducted on each of them and the results are pooled together, yielding the quantities of interest, alongside their confidence intervals that reflect the imputation uncertainty.\nIn this exercise, you will practice the typical MICE flow: mice() - with() - pool(). You will perform a regression analysis on the biopics data to see which subject occupation, sub_type, is associated with highest movie earnings. Let’s play with mice!\n\n# Load mice package\nlibrary(mice)\n\n# Impute biopics with mice using 5 imputations\nbiopics_multiimp <- mice(biopics, m = 5, seed = 3108)\n\n\n iter imp variable\n  1   1  country  earnings  sub_race\n  1   2  country  earnings  sub_race\n  1   3  country  earnings  sub_race\n  1   4  country  earnings  sub_race\n  1   5  country  earnings  sub_race\n  2   1  country  earnings  sub_race\n  2   2  country  earnings  sub_race\n  2   3  country  earnings  sub_race\n  2   4  country  earnings  sub_race\n  2   5  country  earnings  sub_race\n  3   1  country  earnings  sub_race\n  3   2  country  earnings  sub_race\n  3   3  country  earnings  sub_race\n  3   4  country  earnings  sub_race\n  3   5  country  earnings  sub_race\n  4   1  country  earnings  sub_race\n  4   2  country  earnings  sub_race\n  4   3  country  earnings  sub_race\n  4   4  country  earnings  sub_race\n  4   5  country  earnings  sub_race\n  5   1  country  earnings  sub_race\n  5   2  country  earnings  sub_race\n  5   3  country  earnings  sub_race\n  5   4  country  earnings  sub_race\n  5   5  country  earnings  sub_race\n\n# Fit linear regression to each imputed data set \nlm_multiimp <- with(biopics_multiimp,  lm(earnings~year+sub_type ))\n\n# Pool and summarize regression results\nlm_pooled <- pool(lm_multiimp)\nsummary(lm_pooled, conf.int = TRUE, conf.level = 0.95)\n\n                             term     estimate  std.error   statistic\n1                     (Intercept) -408.5476080 188.003023 -2.17309063\n2                            year    0.2195188   0.091547  2.39788059\n3  sub_typeAcademic (Philosopher)  -12.0393151  39.386794 -0.30566883\n4                sub_typeActivist  -12.5649024  12.493282 -1.00573272\n5                   sub_typeActor  -23.5461708  15.296101 -1.53935771\n6                 sub_typeActress  -17.5662047  12.764488 -1.37617778\n7      sub_typeActress / activist   20.3881286  35.764655  0.57006362\n8                  sub_typeArtist  -22.4452374  13.451761 -1.66857238\n9                 sub_typeAthlete   -0.8963086  13.009378 -0.06889712\n10     sub_typeAthlete / military   82.4367906  35.569407  2.31763188\n11                 sub_typeAuthor  -19.1635360  12.262989 -1.56271336\n12          sub_typeAuthor (poet)  -20.0620382  14.444056 -1.38894771\n13               sub_typeComedian  -18.1686840  16.980795 -1.06995483\n14               sub_typeCriminal   -5.8479945  13.420469 -0.43575187\n15             sub_typeGovernment   16.0450423  46.390726  0.34586746\n16             sub_typeHistorical   -3.7623435  11.294642 -0.33310871\n17             sub_typeJournalist  -22.8565780  26.181388 -0.87300866\n18                  sub_typeMedia   -7.0627461  18.145706 -0.38922410\n19               sub_typeMedicine   19.1339721  20.966662  0.91259031\n20               sub_typeMilitary   27.6733819  17.764776  1.55776701\n21    sub_typeMilitary / activist   41.9247600  35.860752  1.16909873\n22               sub_typeMusician  -14.1096056  11.123075 -1.26849868\n23                  sub_typeOther  -12.5957619  11.127539 -1.13194499\n24             sub_typePolitician   -8.9752400  35.860752 -0.25028030\n25                 sub_typeSinger   -2.3189932  14.863480 -0.15601953\n26                sub_typeTeacher   55.5076474  35.777696  1.55145953\n27           sub_typeWorld leader    1.9363047  14.102239  0.13730477\n           df    p.value         2.5 %      97.5 %\n1    8.441788 0.05977554 -838.16570234  21.0704863\n2    8.986358 0.04007631    0.01237714   0.4266604\n3   78.086760 0.76067020  -90.45102629  66.3723961\n4   23.732194 0.32468917  -38.36517557  13.2353708\n5   23.125611 0.13728938  -55.17905857   8.0867171\n6   63.616800 0.17359176  -43.06915926   7.9367499\n7  533.362518 0.56887459  -49.86873468  90.6449919\n8   17.334211 0.11316311  -50.78434701   5.8938723\n9   11.560021 0.94624889  -29.36142352  27.5688062\n10 610.794724 0.02079891   12.58361683 152.2899644\n11  18.269586 0.13527579  -44.89989577   6.5728238\n12  40.490599 0.17244132  -49.24354989   9.1194734\n13  68.008128 0.28842244  -52.05326005  15.7158921\n14  10.341265 0.67197238  -35.61744245  23.9214534\n15   5.438799 0.74242360 -100.36959617 132.4596809\n16  19.622883 0.74258502  -27.35161361  19.8269266\n17 401.472498 0.38318021  -74.32631797  28.6131621\n18  10.959555 0.70456670  -47.01916501  32.8936729\n19  11.569949 0.38008106  -26.73749557  65.0054398\n20   7.029288 0.16307001  -14.29818398  69.6449479\n21 499.260259 0.24292173  -28.53182460 112.3813447\n22  20.977532 0.21851670  -37.24281435   9.0236032\n23  15.905797 0.27443480  -36.19645010  11.0049264\n24 499.260259 0.80247354  -79.43182460  61.4813447\n25  16.388102 0.87792336  -33.76762529  29.1296390\n26 528.585147 0.12139010  -14.77627920 125.7915739\n27  27.018773 0.89180799  -26.99815808  30.8707674\n\n\nYou have followed the mice - with - pool flow to impute, model and pool the results. Now take a look at the console output: a couple of sub_types have a positive impact on earnings. However, accounting for imputation uncertainty with 95% confidence, we are never sure of these effects, as the lower bounds are negative! With one exception: for sub_typeAthlete / military, both upper and lower bounds are positive. What we can say for sure is thus that movies about military athletes are popular!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#choosing-default-models",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#choosing-default-models",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Choosing default models",
    "text": "Choosing default models\nMICE creates a separate imputation model for each variable in the data. What kind of model it is depends on the type of the variable in question. A popular way to specify the kinds of models we want to use is set a default model for each of the four variable types.\nYou can do this by passing the defaultMethod argument to mice(), which should be a vector of length 4 containing the default imputation methods for:\nContinuous variables, Binary variables, Categorical variables (unordered factors), Factor variables (ordered factors). In this exercise, you will take advantage of mice’s documentation to view the list of available methods and to pick the desired ones for the algorithm to use. Let’s do some model selection!\n\n# Impute biopics using the methods specified in the instruction\nbiopics_multiimp <- mice(biopics, m = 20, \n                         defaultMethod = c(\"cart\", \"lda\", \"pmm\", \"polr\"))\n\n\n iter imp variable\n  1   1  country  earnings  sub_race\n  1   2  country  earnings  sub_race\n  1   3  country  earnings  sub_race\n  1   4  country  earnings  sub_race\n  1   5  country  earnings  sub_race\n  1   6  country  earnings  sub_race\n  1   7  country  earnings  sub_race\n  1   8  country  earnings  sub_race\n  1   9  country  earnings  sub_race\n  1   10  country  earnings  sub_race\n  1   11  country  earnings  sub_race\n  1   12  country  earnings  sub_race\n  1   13  country  earnings  sub_race\n  1   14  country  earnings  sub_race\n  1   15  country  earnings  sub_race\n  1   16  country  earnings  sub_race\n  1   17  country  earnings  sub_race\n  1   18  country  earnings  sub_race\n  1   19  country  earnings  sub_race\n  1   20  country  earnings  sub_race\n  2   1  country  earnings  sub_race\n  2   2  country  earnings  sub_race\n  2   3  country  earnings  sub_race\n  2   4  country  earnings  sub_race\n  2   5  country  earnings  sub_race\n  2   6  country  earnings  sub_race\n  2   7  country  earnings  sub_race\n  2   8  country  earnings  sub_race\n  2   9  country  earnings  sub_race\n  2   10  country  earnings  sub_race\n  2   11  country  earnings  sub_race\n  2   12  country  earnings  sub_race\n  2   13  country  earnings  sub_race\n  2   14  country  earnings  sub_race\n  2   15  country  earnings  sub_race\n  2   16  country  earnings  sub_race\n  2   17  country  earnings  sub_race\n  2   18  country  earnings  sub_race\n  2   19  country  earnings  sub_race\n  2   20  country  earnings  sub_race\n  3   1  country  earnings  sub_race\n  3   2  country  earnings  sub_race\n  3   3  country  earnings  sub_race\n  3   4  country  earnings  sub_race\n  3   5  country  earnings  sub_race\n  3   6  country  earnings  sub_race\n  3   7  country  earnings  sub_race\n  3   8  country  earnings  sub_race\n  3   9  country  earnings  sub_race\n  3   10  country  earnings  sub_race\n  3   11  country  earnings  sub_race\n  3   12  country  earnings  sub_race\n  3   13  country  earnings  sub_race\n  3   14  country  earnings  sub_race\n  3   15  country  earnings  sub_race\n  3   16  country  earnings  sub_race\n  3   17  country  earnings  sub_race\n  3   18  country  earnings  sub_race\n  3   19  country  earnings  sub_race\n  3   20  country  earnings  sub_race\n  4   1  country  earnings  sub_race\n  4   2  country  earnings  sub_race\n  4   3  country  earnings  sub_race\n  4   4  country  earnings  sub_race\n  4   5  country  earnings  sub_race\n  4   6  country  earnings  sub_race\n  4   7  country  earnings  sub_race\n  4   8  country  earnings  sub_race\n  4   9  country  earnings  sub_race\n  4   10  country  earnings  sub_race\n  4   11  country  earnings  sub_race\n  4   12  country  earnings  sub_race\n  4   13  country  earnings  sub_race\n  4   14  country  earnings  sub_race\n  4   15  country  earnings  sub_race\n  4   16  country  earnings  sub_race\n  4   17  country  earnings  sub_race\n  4   18  country  earnings  sub_race\n  4   19  country  earnings  sub_race\n  4   20  country  earnings  sub_race\n  5   1  country  earnings  sub_race\n  5   2  country  earnings  sub_race\n  5   3  country  earnings  sub_race\n  5   4  country  earnings  sub_race\n  5   5  country  earnings  sub_race\n  5   6  country  earnings  sub_race\n  5   7  country  earnings  sub_race\n  5   8  country  earnings  sub_race\n  5   9  country  earnings  sub_race\n  5   10  country  earnings  sub_race\n  5   11  country  earnings  sub_race\n  5   12  country  earnings  sub_race\n  5   13  country  earnings  sub_race\n  5   14  country  earnings  sub_race\n  5   15  country  earnings  sub_race\n  5   16  country  earnings  sub_race\n  5   17  country  earnings  sub_race\n  5   18  country  earnings  sub_race\n  5   19  country  earnings  sub_race\n  5   20  country  earnings  sub_race\n\n# Print biopics_multiimp\nprint(biopics_multiimp)\n\nClass: mids\nNumber of multiple imputations:  20 \nImputation methods:\n  country      year  earnings   sub_num  sub_type  sub_race non_white   sub_sex \n    \"pmm\"        \"\"    \"cart\"        \"\"        \"\"     \"pmm\"        \"\"        \"\" \nPredictorMatrix:\n         country year earnings sub_num sub_type sub_race non_white sub_sex\ncountry        0    1        1       1        1        1         1       1\nyear           1    0        1       1        1        1         1       1\nearnings       1    1        0       1        1        1         1       1\nsub_num        1    1        1       0        1        1         1       1\nsub_type       1    1        1       1        0        1         1       1\nsub_race       1    1        1       1        1        0         1       1\nNumber of logged events:  300 \n  it im      dep meth\n1  1  1  country  pmm\n2  1  1 earnings cart\n3  1  1 sub_race  pmm\n4  1  2  country  pmm\n5  1  2 earnings cart\n6  1  2 sub_race  pmm\n                                                                                                                                           out\n1 sub_typeActress / activist, sub_typeAthlete / military, sub_typeGovernment, sub_typeMilitary / activist, sub_typePolitician, sub_typeTeacher\n2                                                                                             countryCanada US, sub_typeAcademic (Philosopher)\n3                                                                                                countryCanada US, sub_typeMilitary / activist\n4 sub_typeActress / activist, sub_typeAthlete / military, sub_typeGovernment, sub_typeMilitary / activist, sub_typePolitician, sub_typeTeacher\n5                                                                                             countryCanada US, sub_typeAcademic (Philosopher)\n6                                                                                                countryCanada US, sub_typeMilitary / activist\n\n\n\nThe ability to specify imputation models might come in handy when you see some specific methods underperforming. Another factor infuencing how the imputation methods work is the set of predictors they use. Let’s look at how to set these in the next exercise."
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#using-predictor-matrix",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#using-predictor-matrix",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Using predictor matrix",
    "text": "Using predictor matrix\nAn important decision that needs to be taken when using model-based imputation is which variables should be included as predictors, and in which models. In mice(), this is governed by the predictor matrix and by default, all variables are used to impute all others.\nIn case of many variables in the data or little time to do a proper model selection, you can use mice’s functionality to create a predictor matrix based on the correlations between the variables. This matrix can then be passed to mice(). In this exercise, you will practice exactly this: you will first build a predictor matrix such that each variable will be imputed using variables most correlated to it; then, you will feed your predictor matrix to the imputing function. Let’s try this simple model selection!\n\n# Create predictor matrix with minimum correlation of 0.1\npred_mat <- quickpred(biopics, mincor = 0.1)\n\n# Impute biopics with mice\nbiopics_multiimp <- mice(biopics, \n                         m = 10, \n                         predictorMatrix = pred_mat,\n                         seed = 3108)\n\n\n iter imp variable\n  1   1  country  earnings  sub_race\n  1   2  country  earnings  sub_race\n  1   3  country  earnings  sub_race\n  1   4  country  earnings  sub_race\n  1   5  country  earnings  sub_race\n  1   6  country  earnings  sub_race\n  1   7  country  earnings  sub_race\n  1   8  country  earnings  sub_race\n  1   9  country  earnings  sub_race\n  1   10  country  earnings  sub_race\n  2   1  country  earnings  sub_race\n  2   2  country  earnings  sub_race\n  2   3  country  earnings  sub_race\n  2   4  country  earnings  sub_race\n  2   5  country  earnings  sub_race\n  2   6  country  earnings  sub_race\n  2   7  country  earnings  sub_race\n  2   8  country  earnings  sub_race\n  2   9  country  earnings  sub_race\n  2   10  country  earnings  sub_race\n  3   1  country  earnings  sub_race\n  3   2  country  earnings  sub_race\n  3   3  country  earnings  sub_race\n  3   4  country  earnings  sub_race\n  3   5  country  earnings  sub_race\n  3   6  country  earnings  sub_race\n  3   7  country  earnings  sub_race\n  3   8  country  earnings  sub_race\n  3   9  country  earnings  sub_race\n  3   10  country  earnings  sub_race\n  4   1  country  earnings  sub_race\n  4   2  country  earnings  sub_race\n  4   3  country  earnings  sub_race\n  4   4  country  earnings  sub_race\n  4   5  country  earnings  sub_race\n  4   6  country  earnings  sub_race\n  4   7  country  earnings  sub_race\n  4   8  country  earnings  sub_race\n  4   9  country  earnings  sub_race\n  4   10  country  earnings  sub_race\n  5   1  country  earnings  sub_race\n  5   2  country  earnings  sub_race\n  5   3  country  earnings  sub_race\n  5   4  country  earnings  sub_race\n  5   5  country  earnings  sub_race\n  5   6  country  earnings  sub_race\n  5   7  country  earnings  sub_race\n  5   8  country  earnings  sub_race\n  5   9  country  earnings  sub_race\n  5   10  country  earnings  sub_race\n\n# Print biopics_multiimp\nprint(biopics_multiimp)\n\nClass: mids\nNumber of multiple imputations:  10 \nImputation methods:\n  country      year  earnings   sub_num  sub_type  sub_race non_white   sub_sex \n\"polyreg\"        \"\"     \"pmm\"        \"\"        \"\" \"polyreg\"        \"\"        \"\" \nPredictorMatrix:\n         country year earnings sub_num sub_type sub_race non_white sub_sex\ncountry        0    1        1       0        0        0         0       0\nyear           0    0        0       0        0        0         0       0\nearnings       1    1        0       0        0        1         1       0\nsub_num        0    0        0       0        0        0         0       0\nsub_type       0    0        0       0        0        0         0       0\nsub_race       0    1        1       1        1        0         1       0\nNumber of logged events:  100 \n  it im      dep    meth                         out\n1  1  1 earnings     pmm            countryCanada US\n2  1  1 sub_race polyreg sub_typeMilitary / activist\n3  1  2 earnings     pmm            countryCanada US\n4  1  2 sub_race polyreg sub_typeMilitary / activist\n5  1  3 earnings     pmm            countryCanada US\n6  1  3 sub_race polyreg sub_typeMilitary / activist\n\n\n\nLook at the predictor matrix you’ve used that is printed in the console. Which variables have been used as predictors to impute earnings?\n\n\ncountry, year and non_white\nProvides an easy way to set up predictor matrices, but if you can afford it, you should try to choose the predictors based on the insights from data analytics or on domain knowledge."
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#analyzing-missing-data-patterns",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#analyzing-missing-data-patterns",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Analyzing missing data patterns",
    "text": "Analyzing missing data patterns\nThe first step in working with incomplete data is to gain some insights into the missingness patterns, and a good way to do it is with visualizations. You will start your analysis of the africa data with employing the VIM package to create two visualizations: the aggregation plot and the spine plot. They will tell you how many data are missing, in which variables and configurations, and whether we can say something about the missing data mechanism. Let’s kick off with some plotting!\n\nafrica <- read.csv( \"data/africa_clean.csv\")\n# Load VIM\nlibrary(VIM)\n\n# Draw a combined aggregation plot of africa\nafrica %>%\n  aggr(combined = TRUE, numbers = TRUE)\n\n\n\n\n\nQuestion\n\nBased on the aggregation plot you have just created, which of the following statements is TRUE?\nans Whenever gdp_pc is missing, trade is missing too\n\n\n# Draw a spine plot of country vs trade\nafrica %>% \n  select(country ,trade) %>%\n  spineMiss()\n\n\n\n\n\n\nQuestion\n\nBased on the spine plot you have just created, which of the following statements is TRUE?\nCorrect, there are not that many missing values! Also, notice from the spine plot that the africa data seem to be MAR - at least with respect to the GDP and country, which means it can be imputed. Let’s try multiple imputation to fill-in the missing values in the next exercise!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#imputing-and-inspecting-outcomes",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#imputing-and-inspecting-outcomes",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Imputing and inspecting outcomes",
    "text": "Imputing and inspecting outcomes\nGood job on visualizing missing data in the previous exercise! You have discovered there are some missing entries in GDP, gdp_pc, and trade as percentage of GDP, trade. Also, you suspect the data are MAR, and thus imputable. In this exercise, you will make use of multiple imputation from the mice package to impute the africa data. Then, you will draw a strip plot of gdp_pc vs trade to see if the imputed data do not break the relation between these variables. Let mice do the job!\n\n# Load mice\nlibrary(mice)\n\n# Impute africa with mice\nafrica_multiimp <- mice(africa, m = 5, defaultMethod = \"cart\", seed = 3108)\n\n\n iter imp variable\n  1   1  gdp_pc  trade\n  1   2  gdp_pc  trade\n  1   3  gdp_pc  trade\n  1   4  gdp_pc  trade\n  1   5  gdp_pc  trade\n  2   1  gdp_pc  trade\n  2   2  gdp_pc  trade\n  2   3  gdp_pc  trade\n  2   4  gdp_pc  trade\n  2   5  gdp_pc  trade\n  3   1  gdp_pc  trade\n  3   2  gdp_pc  trade\n  3   3  gdp_pc  trade\n  3   4  gdp_pc  trade\n  3   5  gdp_pc  trade\n  4   1  gdp_pc  trade\n  4   2  gdp_pc  trade\n  4   3  gdp_pc  trade\n  4   4  gdp_pc  trade\n  4   5  gdp_pc  trade\n  5   1  gdp_pc  trade\n  5   2  gdp_pc  trade\n  5   3  gdp_pc  trade\n  5   4  gdp_pc  trade\n  5   5  gdp_pc  trade\n\n# Draw a stripplot of gdp_pc versus trade\nstripplot(africa_multiimp, gdp_pc ~ trade | .imp, pch = 20, cex = 2)\n\n\n\n\n\nt seems the imputation works well: there are small clusters in the scatter plots, likely corresponding to different countries. Each imputed data point fits into one of the clusters, instead of being an outlier somewhere between the clusters. Having done the imputation, you can now proceed to modeling!"
  },
  {
    "objectID": "datacamp/imputations_in_R/imputations_in_R.html#inference-with-imputed-data",
    "href": "datacamp/imputations_in_R/imputations_in_R.html#inference-with-imputed-data",
    "title": "Handling Missing Data with Imputations in R",
    "section": "Inference with imputed data",
    "text": "Inference with imputed data\nIn the last exercise, you have run mice to multiply impute the africa data. In this one, you will implement the other two steps of the mice - with - pool flow you’ve learned about earlier in the course. The model of interest is a linear regression that explains the GDP, gdp_pc, with other variables. You are particularly interested in the coefficient of civil liberties, civlib. Is more liberty associated with more economic growth once we incorporate the uncertainty from imputation? Let’s find out!\n\n# Fit linear regression to each imputed data set\nlm_multiimp <- with(africa_multiimp, lm(gdp_pc ~ country + year + trade + infl + civlib))\n\n# Pool regression results\nlm_pooled <- pool(lm_multiimp)\n\n# Summarize pooled results\nsummary(lm_pooled, conf.int = TRUE, conf.level = 0.90)\n\n              term      estimate    std.error  statistic       df      p.value\n1      (Intercept) -30068.441810 5618.7067367 -5.3514880 107.7805 4.954407e-07\n2   countryBurundi     67.199848   61.9274336  1.0851386 107.8599 2.802797e-01\n3  countryCameroon    650.257144   58.5640793 11.1033444 107.1503 1.591166e-19\n4     countryCongo   1343.288063  113.7299489 11.8112078 106.8653 4.209726e-21\n5   countrySenegal    527.686999   78.7955032  6.6969177 106.7578 1.027035e-09\n6    countryZambia    415.136577   83.9727225  4.9437075 107.0560 2.853378e-06\n7             year     15.335776    2.8263293  5.4260402 107.7640 3.574950e-07\n8            trade      4.941659    1.5893889  3.1091568 105.7726 2.410907e-03\n9             infl     -4.347124    0.9876079 -4.4016696 108.0009 2.533732e-05\n10          civlib    -49.411852  132.1837617 -0.3738118 107.4442 7.092809e-01\n             5 %          95 %\n1  -39390.518971 -20746.364650\n2     -35.544192    169.943888\n3     553.087683    747.426605\n4    1154.583058   1531.993068\n5     396.945387    658.428610\n6     275.808050    554.465103\n7      10.646566     20.024986\n8       2.304247      7.579071\n9      -5.985649     -2.708598\n10   -268.725783    169.902078\n\n\n\nQuestion\n\nBased on the summary of the pooled regression results that you have just printed to the console, which of the following statements about the civil liberties in Africa is false?\nCorrect, this one is false! Since the lower and upper bounds have different signs, we cannot be sure of the direction of the effect. Congratulations, you have come a long way and learned a lot. Well done! Let’s sum it all up in the final video of the course."
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html",
    "href": "datacamp/functions_in_R/functions_R.html",
    "title": "Functions in R",
    "section": "",
    "text": "One way to make your code more readable is to be careful about the order you pass arguments when you call functions, and whether you pass the arguments by position or by name.\ngold_medals, a numeric vector of the number of gold medals won by each country in the 2016 Summer Olympics, is provided.\nFor convenience, the arguments of median() and rank() are displayed using args(). Setting rank()’s na.last argument to “keep” means “keep the rank of NA values as NA”.\nBest practice for calling functions is to include them in the order shown by args(), and to only name rare arguments.\n\ngd_nms <-  c(\"USA\", \"GBR\", \"CHN\", \"RUS\", \"GER\", \"JPN\", \"FRA\", \"KOR\", \"ITA\", \n            \"AUS\", \"NED\", \"HUN\", \"BRA\", \"ESP\", \"KEN\", \"JAM\", \"CRO\", \"CUB\", \n            \"NZL\", \"CAN\", \"UZB\", \"KAZ\", \"COL\", \"SUI\", \"IRI\", \"GRE\", \"ARG\", \n            \"DEN\", \"SWE\", \"RSA\", \"UKR\", \"SRB\", \"POL\", \"PRK\", \"BEL\", \"THA\", \n            \"SVK\", \"GEO\", \"AZE\", \"BLR\", \"TUR\", \"ARM\", \"CZE\", \"ETH\", \"SLO\", \n            \"INA\", \"ROU\", \"BRN\", \"VIE\", \"TPE\", \"BAH\", \"IOA\", \"CIV\", \"FIJ\", \n            \"JOR\", \"KOS\", \"PUR\", \"SIN\", \"TJK\", \"MAS\", \"MEX\", \"VEN\", \"ALG\", \n            \"IRL\", \"LTU\", \"BUL\", \"IND\", \"MGL\", \"BDI\", \"GRN\", \"NIG\", \"PHI\", \n            \"QAT\", \"NOR\", \"EGY\", \"TUN\", \"ISR\", \"AUT\", \"DOM\", \"EST\", \"FIN\", \n            \"MAR\", \"NGR\", \"POR\", \"TTO\", \"UAE\", \"IOC\")\n\ngold_medals <- c(46L, 27L, 26L, 19L, 17L, 12L, 10L, 9L, 8L, 8L, 8L, 8L, 7L, \n                 7L, 6L, 6L, 5L, 5L, 4L, 4L,4L, 3L, 3L, 3L, 3L, 3L, 3L, 2L, 2L, \n                 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L,\n                 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,\n                 1L, 1L, 1L, 1L, 0L,0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, \n                 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,\n                 0L, 0L, 0L, 0L, 0L, 0L, NA)\n\nnames(gold_medals) <- gd_nms\n\ngold_medals\n\nUSA GBR CHN RUS GER JPN FRA KOR ITA AUS NED HUN BRA ESP KEN JAM CRO CUB NZL CAN \n 46  27  26  19  17  12  10   9   8   8   8   8   7   7   6   6   5   5   4   4 \nUZB KAZ COL SUI IRI GRE ARG DEN SWE RSA UKR SRB POL PRK BEL THA SVK GEO AZE BLR \n  4   3   3   3   3   3   3   2   2   2   2   2   2   2   2   2   2   2   1   1 \nTUR ARM CZE ETH SLO INA ROU BRN VIE TPE BAH IOA CIV FIJ JOR KOS PUR SIN TJK MAS \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   0 \nMEX VEN ALG IRL LTU BUL IND MGL BDI GRN NIG PHI QAT NOR EGY TUN ISR AUT DOM EST \n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \nFIN MAR NGR POR TTO UAE IOC \n  0   0   0   0   0   0  NA \n\n# Note the arguments to median()\nargs(median)\n\nfunction (x, na.rm = FALSE, ...) \nNULL\n\n# Rewrite this function call, following best practices\nmedian(gold_medals, na.rm = TRUE)\n\n[1] 1\n\n# Note the arguments to rank()\nargs(rank)\n\nfunction (x, na.last = TRUE, ties.method = c(\"average\", \"first\", \n    \"last\", \"random\", \"max\", \"min\")) \nNULL\n\n# Rewrite this function call, following best practices\n\nrank(-gold_medals, na.last = \"keep\", ties.method=  \"min\")\n\nUSA GBR CHN RUS GER JPN FRA KOR ITA AUS NED HUN BRA ESP KEN JAM CRO CUB NZL CAN \n  1   2   3   4   5   6   7   8   9   9   9   9  13  13  15  15  17  17  19  19 \nUZB KAZ COL SUI IRI GRE ARG DEN SWE RSA UKR SRB POL PRK BEL THA SVK GEO AZE BLR \n 19  22  22  22  22  22  22  28  28  28  28  28  28  28  28  28  28  28  39  39 \nTUR ARM CZE ETH SLO INA ROU BRN VIE TPE BAH IOA CIV FIJ JOR KOS PUR SIN TJK MAS \n 39  39  39  39  39  39  39  39  39  39  39  39  39  39  39  39  39  39  39  60 \nMEX VEN ALG IRL LTU BUL IND MGL BDI GRN NIG PHI QAT NOR EGY TUN ISR AUT DOM EST \n 60  60  60  60  60  60  60  60  60  60  60  60  60  60  60  60  60  60  60  60 \nFIN MAR NGR POR TTO UAE IOC \n 60  60  60  60  60  60  NA"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#your-first-function-tossing-a-coin",
    "href": "datacamp/functions_in_R/functions_R.html#your-first-function-tossing-a-coin",
    "title": "Functions in R",
    "section": "Your first function: tossing a coin",
    "text": "Your first function: tossing a coin\nTime to write your first function! It’s a really good idea when writing functions to start simple. You can always make a function more complicated later if it’s really necessary, so let’s not worry about arguments for now.\n\ncoin_sides <- c(\"head\", \"tail\")\n\n# Sample from coin_sides once\nsample(coin_sides, 1)\n\n[1] \"tail\"\n\n# Your functions, from previous steps\ntoss_coin <- function() {\n  coin_sides <- c(\"head\", \"tail\")\n  sample(coin_sides, 1)\n}\n\n# Call your function\ntoss_coin()\n\n[1] \"tail\""
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#inputs-to-functions",
    "href": "datacamp/functions_in_R/functions_R.html#inputs-to-functions",
    "title": "Functions in R",
    "section": "Inputs to functions",
    "text": "Inputs to functions\nMost functions require some sort of input to determine what to compute. The inputs to functions are called arguments. You specify them inside the parentheses after the word “function.”\nAs mentioned in the video, the following exercises assume that you are using sample() to do random sampling.\n\n# Update the function to return n coin tosses\ntoss_coin <- function(n_flips) {\n  coin_sides <- c(\"head\", \"tail\")\n  sample(coin_sides, n_flips, replace = TRUE)\n}\n\n# Generate 10 coin tosses\ntoss_coin(1000) %>% table() %>% prop.table()\n\n.\n head  tail \n0.499 0.501"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#multiple-inputs-to-functions",
    "href": "datacamp/functions_in_R/functions_R.html#multiple-inputs-to-functions",
    "title": "Functions in R",
    "section": "Multiple inputs to functions",
    "text": "Multiple inputs to functions\nIf a function should have more than one argument, list them in the function signature, separated by commas. To solve this exercise, you need to know how to specify sampling weights to sample(). Set the prob argument to a numeric vector with the same length as x. Each value of prob is the probability of sampling the corresponding element of x, so their values add up to one. In the following example, each sample has a 20% chance of “bat”, a 30% chance of “cat” and a 50% chance of “rat”.\n\n# Update the function so heads have probability p_head\ntoss_coin <- function(n_flips, p_head) {\n  coin_sides <- c(\"head\", \"tail\")\n  # Define a vector of weights\n  weights <- c(p_head,1- p_head)\n  # Modify the sampling to be weighted\n  sample(coin_sides, n_flips, replace = TRUE, prob = weights)\n}\n\n# Generate 10 coin tosses\ntoss_coin(10,p_head=.8)\n\n [1] \"head\" \"tail\" \"tail\" \"head\" \"head\" \"head\" \"head\" \"tail\" \"tail\" \"head\""
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#data-or-detail",
    "href": "datacamp/functions_in_R/functions_R.html#data-or-detail",
    "title": "Functions in R",
    "section": "Data or detail?",
    "text": "Data or detail?\nRecall that data arguments are what a function computes on, and detail arguments advise on how the computation should be performed. Each of the arguments to t.test() is shown, along with a brief description of it."
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#renaming-glm",
    "href": "datacamp/functions_in_R/functions_R.html#renaming-glm",
    "title": "Functions in R",
    "section": "Renaming GLM",
    "text": "Renaming GLM\nR’s generalized linear regression function, glm(), suffers the same usability problems as lm(): its name is an acronym, and its formula and data arguments are in the wrong order. To solve this exercise, you need to know two things about generalized linear regression: glm() formulas are specified like lm() formulas: response is on the left, and explanatory variables are added on the right. To model count data, set glm()’s family argument to poisson, making it a Poisson regression. Here you’ll use data on the number of yearly visits to Snake River at Jackson Hole, Wyoming, snake_river_visits.\n\nsnake_river_visits <-read_rds(\"snake_river_visits.rds\")\n# From previous step\nrun_poisson_regression <- function(data, formula) {\n  glm(formula, data, family = poisson)\n}\n\n# Re-run the Poisson regression, using your function\nmodel <- snake_river_visits %>%\n  run_poisson_regression(n_visits ~ gender + income + travel)\n\n# Run this to see the predictions\nsnake_river_visits %>%\n  mutate(predicted_n_visits = predict(model, ., type = \"response\"))%>%\n  arrange(desc(predicted_n_visits)) %>%\n    head() %>% kable()\n\n\n\n\nn_visits\ngender\nincome\ntravel\npredicted_n_visits\n\n\n\n\n80\nfemale\n[$0,$25k]\n[0h,0.25h]\n86.5186\n\n\n35\nfemale\n[$0,$25k]\n[0h,0.25h]\n86.5186\n\n\n50\nfemale\n[$0,$25k]\n[0h,0.25h]\n86.5186\n\n\n125\nfemale\n[$0,$25k]\n[0h,0.25h]\n86.5186\n\n\n24\nfemale\n[$0,$25k]\n[0h,0.25h]\n86.5186\n\n\n100\nfemale\n[$0,$25k]\n[0h,0.25h]\n86.5186"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#numeric-defaults",
    "href": "datacamp/functions_in_R/functions_R.html#numeric-defaults",
    "title": "Functions in R",
    "section": "Numeric defaults",
    "text": "Numeric defaults\ncut_by_quantile() converts a numeric vector into a categorical variable where quantiles define the cut points. This is a useful function, but at the moment you have to specify five arguments to make it work. This is too much thinking and typing. By specifying default arguments, you can make it easier to use. Let’s start with n, which specifies how many categories to cut x into.A numeric vector of the number of visits to Snake River is provided as n_visits.\n\n# Set the default for n to 5\nn_visits = snake_river_visits$n_visits\ncut_by_quantile <- function(x, n=5, na.rm, labels, interval_type) {\n  probs <- seq(0, 1, length.out = n + 1)\n  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)\n  right <- switch(interval_type, \"(lo, hi]\" = TRUE, \"[lo, hi)\" = FALSE)\n  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)\n}\n\n# Remove the n argument from the call\ncut_by_quantile(\n  n_visits, \n  na.rm = FALSE, \n  labels = c(\"very low\", \"low\", \"medium\", \"high\", \"very high\"),\n  interval_type = \"(lo, hi]\"\n) %>% head()\n\n[1] very low very low very low very low very low very low\nLevels: very low low medium high very high"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#logical-defaults",
    "href": "datacamp/functions_in_R/functions_R.html#logical-defaults",
    "title": "Functions in R",
    "section": "Logical defaults",
    "text": "Logical defaults\ncut_by_quantile() is now slightly easier to use, but you still always have to specify the na.rm argument. This removes missing values – it behaves the same as the na.rm argument to mean() or sd().\nWhere functions have an argument for removing missing values, the best practice is to not remove them by default (in case you hadn’t spotted that you had missing values). That means that the default for na.rm should be FALSE.\n\n# Set the default for na.rm to FALSE\ncut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels, interval_type) {\n  probs <- seq(0, 1, length.out = n + 1)\n  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)\n  right <- switch(interval_type, \"(lo, hi]\" = TRUE, \"[lo, hi)\" = FALSE)\n  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)\n}\n\n# Remove the na.rm argument from the call\ncut_by_quantile(\n  n_visits, \n  labels = c(\"very low\", \"low\", \"medium\", \"high\", \"very high\"),\n  interval_type = \"(lo, hi]\"\n) %>% head()\n\n[1] very low very low very low very low very low very low\nLevels: very low low medium high very high"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#null-defaults",
    "href": "datacamp/functions_in_R/functions_R.html#null-defaults",
    "title": "Functions in R",
    "section": "NULL defaults",
    "text": "NULL defaults\nThe cut() function used by cut_by_quantile() can automatically provide sensible labels for each category. The code to generate these labels is pretty complicated, so rather than appearing in the function signature directly, its labels argument defaults to NULL, and the calculation details are shown on the ?cut help page.\n\n# Set the default for labels to NULL\ncut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels=NULL, interval_type) {\n  probs <- seq(0, 1, length.out = n + 1)\n  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)\n  right <- switch(interval_type, \"(lo, hi]\" = TRUE, \"[lo, hi)\" = FALSE)\n  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)\n}\n\n# Remove the labels argument from the call\ncut_by_quantile(\n  n_visits,\n  #labels = c(\"very low\", \"low\", \"medium\", \"high\", \"very high\"),\n  interval_type = \"(lo, hi]\"\n) %>% head()\n\n[1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1]\nLevels: [0,1] (1,2] (2,10] (10,35] (35,350]"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#categorical-defaults",
    "href": "datacamp/functions_in_R/functions_R.html#categorical-defaults",
    "title": "Functions in R",
    "section": "Categorical defaults",
    "text": "Categorical defaults\nWhen cutting up a numeric vector, you need to worry about what happens if a value lands exactly on a boundary. You can either put this value into a category of the lower interval or the higher interval. That is, you can choose your intervals to include values at the top boundary but not the bottom (in mathematical terminology, “open on the left, closed on the right”, or (lo, hi]). Or you can choose the opposite (“closed on the left, open on the right”, or [lo, hi)). cut_by_quantile() should allow these two choices.\nThe pattern for categorical defaults is:\nfunction(cat_arg = c(“choice1”, “choice2”)) { cat_arg <- match.arg(cat_arg) }\nFree hint: In the console, type head(rank) to see the start of rank()’s definition, and look at the ties.method argument.\n\n# Set the categories for interval_type to \"(lo, hi]\" and \"[lo, hi)\"\ncut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels = NULL, \n                            interval_type= c(\"(lo, hi]\", \"[lo, hi)\")) {\n  # Match the interval_type argument\n  interval_type <- match.arg(interval_type)\n  probs <- seq(0, 1, length.out = n + 1)\n  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)\n  right <- switch(interval_type, \"(lo, hi]\" = TRUE, \"[lo, hi)\" = FALSE)\n  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)\n}\n\n# Remove the interval_type argument from the call\ncut_by_quantile(n_visits) %>% head()\n\n[1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1]\nLevels: [0,1] (1,2] (2,10] (10,35] (35,350]"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#harmonic-mean",
    "href": "datacamp/functions_in_R/functions_R.html#harmonic-mean",
    "title": "Functions in R",
    "section": "Harmonic mean",
    "text": "Harmonic mean\nThe harmonic mean is the reciprocal of the arithmetic mean of the reciprocal of the data. That is\nThe harmonic mean is often used to average ratio data. You’ll be using it on the price/earnings ratio of stocks in the Standard and Poor’s 500 index, provided as std_and_poor500. Price/earnings ratio is a measure of how expensive a stock is.\nThe dplyr package is loaded.\n\nstd_and_poor500 <- read_rds(\"std_and_poor500_with_pe_2019-06-21.rds\")\n# Look at the Standard and Poor 500 data\nglimpse(std_and_poor500)\n\nRows: 505\nColumns: 5\n$ symbol   <chr> \"MMM\", \"ABT\", \"ABBV\", \"ABMD\", \"ACN\", \"ATVI\", \"ADBE\", \"AMD\", \"…\n$ company  <chr> \"3M Company\", \"Abbott Laboratories\", \"AbbVie Inc.\", \"ABIOMED …\n$ sector   <chr> \"Industrials\", \"Health Care\", \"Health Care\", \"Health Care\", \"…\n$ industry <chr> \"Industrial Conglomerates\", \"Health Care Equipment\", \"Pharmac…\n$ pe_ratio <dbl> 18.31678, 57.66621, 22.43805, 45.63993, 27.00233, 20.13596, 5…\n\n# Write a function to calculate the reciprocal\n# From previous steps\nget_reciprocal <- function(x) {\n  1 / x\n}\ncalc_harmonic_mean <- function(x) {\n  x %>%\n    get_reciprocal() %>%\n    mean() %>%\n    get_reciprocal()\n}\n\nstd_and_poor500 %>% \n  # Group by sector\n  group_by(sector) %>% \n  # Summarize, calculating harmonic mean of P/E ratio\n  summarise(hmean_pe_ratio = calc_harmonic_mean(pe_ratio))\n\n# A tibble: 11 × 2\n   sector                 hmean_pe_ratio\n   <chr>                           <dbl>\n 1 Communication Services           NA  \n 2 Consumer Discretionary           NA  \n 3 Consumer Staples                 NA  \n 4 Energy                           NA  \n 5 Financials                       NA  \n 6 Health Care                      NA  \n 7 Industrials                      NA  \n 8 Information Technology           NA  \n 9 Materials                        NA  \n10 Real Estate                      32.5\n11 Utilities                        NA"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#dealing-with-missing-values",
    "href": "datacamp/functions_in_R/functions_R.html#dealing-with-missing-values",
    "title": "Functions in R",
    "section": "Dealing with missing values",
    "text": "Dealing with missing values\nIn the last exercise, many sectors had an NA value for the harmonic mean. It would be useful for your function to be able to remove missing values before calculating.\nRather than writing your own code for this, you can outsource this functionality to mean().\nThe dplyr package is loaded.\n\n# From previous step\ncalc_harmonic_mean <- function(x, na.rm = FALSE) {\n  x %>%\n    get_reciprocal() %>%\n    mean(na.rm = na.rm) %>%\n    get_reciprocal()\n}\n\nstd_and_poor500 %>% \n  # Group by sector\n  group_by(sector) %>% \n  # Summarize, calculating harmonic mean of P/E ratio\n  summarise(hmean_pe_ratio = calc_harmonic_mean(pe_ratio, na.rm = TRUE))\n\n# A tibble: 11 × 2\n   sector                 hmean_pe_ratio\n   <chr>                           <dbl>\n 1 Communication Services           17.5\n 2 Consumer Discretionary           15.2\n 3 Consumer Staples                 19.8\n 4 Energy                           13.7\n 5 Financials                       12.9\n 6 Health Care                      26.6\n 7 Industrials                      18.2\n 8 Information Technology           21.6\n 9 Materials                        16.3\n10 Real Estate                      32.5\n11 Utilities                        23.9"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#passing-arguments-with",
    "href": "datacamp/functions_in_R/functions_R.html#passing-arguments-with",
    "title": "Functions in R",
    "section": "Passing arguments with …",
    "text": "Passing arguments with …\nRather than explicitly giving calc_harmonic_mean() and na.rm argument, you can use … to simply “pass other arguments” to mean().\nThe dplyr package is loaded.\n\ncalc_harmonic_mean <- function(x, ...) {\n  x %>%\n    get_reciprocal() %>%\n    mean(...) %>%\n    get_reciprocal()\n}\n\n\nstd_and_poor500 %>% \n  # Group by sector\n  group_by(sector) %>% \n  # Summarize, calculating harmonic mean of P/E ratio\n  summarise(hmean_pe_ratio = calc_harmonic_mean(pe_ratio, na.rm = TRUE))\n\n# A tibble: 11 × 2\n   sector                 hmean_pe_ratio\n   <chr>                           <dbl>\n 1 Communication Services           17.5\n 2 Consumer Discretionary           15.2\n 3 Consumer Staples                 19.8\n 4 Energy                           13.7\n 5 Financials                       12.9\n 6 Health Care                      26.6\n 7 Industrials                      18.2\n 8 Information Technology           21.6\n 9 Materials                        16.3\n10 Real Estate                      32.5\n11 Utilities                        23.9"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#throwing-errors-with-bad-arguments",
    "href": "datacamp/functions_in_R/functions_R.html#throwing-errors-with-bad-arguments",
    "title": "Functions in R",
    "section": "Throwing errors with bad arguments",
    "text": "Throwing errors with bad arguments\nIf a user provides a bad input to a function, the best course of action is to throw an error letting them know. The two rules are\nThrow the error message as soon as you realize there is a problem (typically at the start of the function). Make the error message easily understandable. You can use the assert_*() functions from assertive to check inputs and throw errors when they fail.\n\nlibrary(assertive)\ncalc_harmonic_mean <- function(x, na.rm = FALSE) {\n  # Assert that x is numeric\n  assert_is_numeric(x)\n  x %>%\n    get_reciprocal() %>%\n    mean(na.rm = na.rm) %>%\n    get_reciprocal()\n}\n\n# See what happens when you pass it strings\n#calc_harmonic_mean(std_and_poor500$sector)"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#custom-error-logic",
    "href": "datacamp/functions_in_R/functions_R.html#custom-error-logic",
    "title": "Functions in R",
    "section": "Custom error logic",
    "text": "Custom error logic\nSometimes the assert_*() functions in assertive don’t give the most informative error message. For example, the assertions that check if a number is in a numeric range will tell the user that a value is out of range, but the won’t say why that’s a problem. In that case, you can use the is_*() functions in conjunction with messages, warnings, or errors to define custom feedback.\nThe harmonic mean only makes sense when x has all positive values. (Try calculating the harmonic mean of one and minus one to see why.) Make sure your users know this!\n\ncalc_harmonic_mean <- function(x, na.rm = FALSE) {\n  assert_is_numeric(x)\n  # Check if any values of x are non-positive\n  if(any(is_non_positive(x), na.rm = TRUE)) {\n    # Throw an error\n    stop(\"x contains non-positive values, so the harmonic mean makes no sense.\")\n  }\n  x %>%\n    get_reciprocal() %>%\n    mean(na.rm = na.rm) %>%\n    get_reciprocal()\n}\n\n# See what happens when you pass it negative numbers\n#calc_harmonic_mean(std_and_poor500$pe_ratio - 20)"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#fixing-function-arguments",
    "href": "datacamp/functions_in_R/functions_R.html#fixing-function-arguments",
    "title": "Functions in R",
    "section": "Fixing function arguments",
    "text": "Fixing function arguments\nThe harmonic mean function is almost complete. However, you still need to provide some checks on the na.rm argument. This time, rather than throwing errors when the input is in an incorrect form, you are going to try to fix it.\nna.rm should be a logical vector with one element (that is, TRUE, or FALSE).\nThe assertive package is loaded for you.\n\n# Update the function definition to fix the na.rm argument\ncalc_harmonic_mean <- function(x, na.rm = FALSE) {\n  assert_is_numeric(x)\n  if(any(is_non_positive(x), na.rm = TRUE)) {\n    stop(\"x contains non-positive values, so the harmonic mean makes no sense.\")\n  }\n  # Use the first value of na.rm, and coerce to logical\n  na.rm <- coerce_to(use_first(na.rm), target_class = \"logical\")\n  x %>%\n    get_reciprocal() %>%\n    mean(na.rm = na.rm) %>%\n    get_reciprocal()\n}\n\n# See what happens when you pass it malformed na.rm\ncalc_harmonic_mean(std_and_poor500$pe_ratio, na.rm = 1:5)\n\n[1] 18.23871"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#returning-early",
    "href": "datacamp/functions_in_R/functions_R.html#returning-early",
    "title": "Functions in R",
    "section": "Returning early",
    "text": "Returning early\nSometimes, you don’t need to run through the whole body of a function to get the answer. In that case you can return early from that function using return().\nTo check if x is divisible by n, you can use is_divisible_by(x, n) from assertive.\nAlternatively, use the modulo operator, %%. x %% n gives the remainder when dividing x by n, so x %% n == 0 determines whether x is divisible by n. Try 1:10 %% 3 == 0 in the console.\nTo solve this exercise, you need to know that a leap year is every 400th year (like the year 2000) or every 4th year that isn’t a century (like 1904 but not 1900 or 1905).\nassertive is loaded.\n\nis_leap_year <- function(year) {\n  # If year is div. by 400 return TRUE\n  if(is_divisible_by(year, 400)) {\n    return(TRUE)\n  }\n  # If year is div. by 100 return FALSE\n  if(is_divisible_by(year, 100)) {\n    return(FALSE)\n  }  \n  # If year is div. by 4 return TRUE\n   if(is_divisible_by(year, 4)) {\n    return(TRUE)\n  \n  \n  \n  # Otherwise return FALSE\n   } else return(FALSE)\n}\nis_leap_year(year = 1900)\n\n[1] FALSE"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#returning-invisibly",
    "href": "datacamp/functions_in_R/functions_R.html#returning-invisibly",
    "title": "Functions in R",
    "section": "Returning invisibly",
    "text": "Returning invisibly\nWhen the main purpose of a function is to generate output, like drawing a plot or printing something in the console, you may not want a return value to be printed as well. In that case, the value should be invisibly returned.\nThe base R plot function returns NULL, since its main purpose is to draw a plot. This isn’t helpful if you want to use it in piped code: instead it should invisibly return the plot data to be piped on to the next step.\nRecall that plot() has a formula interface: instead of giving it vectors for x and y, you can specify a formula describing which columns of a data frame go on the x and y axes, and a data argument for the data frame. Note that just like lm(), the arguments are the wrong way round because the detail argument, formula, comes before the data argument.\n\n# Using cars, draw a scatter plot of dist vs. speed\nplt_dist_vs_speed <- plot(dist ~ speed, data = cars)\n\n# Oh no! The plot object is NULL\nplt_dist_vs_speed\n\nNULL\n\n# Define a pipeable plot fn with data and formula args\npipeable_plot <- function(data, formula) {\n  # Call plot() with the formula interface\n  plot(formula, data)\n  # Invisibly return the input dataset\n  invisible(head(data))\n}\n\n# Draw the scatter plot of dist vs. speed again\nplt_dist_vs_speed <- cars %>% \n  pipeable_plot(dist ~ speed)\n\n\n\n# Now the plot object has a value\nplt_dist_vs_speed\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#returning-many-things",
    "href": "datacamp/functions_in_R/functions_R.html#returning-many-things",
    "title": "Functions in R",
    "section": "Returning many things",
    "text": "Returning many things\nFunctions can only return one value. If you want to return multiple things, then you can store them all in a list.\nIf users want to have the list items as separate variables, they can assign each list element to its own variable using zeallot’s multi-assignment operator, %<-%.\nglance(), tidy(), and augment() each take the model object as their only argument.\nThe Poisson regression model of Snake River visits is available as model. broom and zeallot are loaded.\n\nlibrary(zeallot)\nlibrary(broom)\nmodel <- glm(n_visits ~ gender + income + travel, \n             data =snake_river_visits )\n# From previous step\ngroom_model <- function(model) {\n  list(\n    model = glance(model),\n    coefficients = tidy(model),\n    observations = augment(model)\n  )\n}\n\n# Call groom_model on model, assigning to 3 variables\n\nc(mdl, cff,  obs) %<-% groom_model(model)\n\n# See these individual variables\nmdl; cff; obs\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1       820296.     345 -1791. 3599. 3630.  636485.         339   346\n\n\n# A tibble: 7 × 5\n  term              estimate std.error statistic  p.value\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)          61.5       7.83     7.86  5.06e-14\n2 genderfemale         10.8       4.94     2.19  2.88e- 2\n3 income($25k,$55k]    -1.95      7.74    -0.251 8.02e- 1\n4 income($55k,$95k]   -19.3       8.01    -2.42  1.63e- 2\n5 income($95k,$Inf)   -18.6       7.47    -2.49  1.32e- 2\n6 travel(0.25h,4h]    -26.6       6.00    -4.44  1.24e- 5\n7 travel(4h,Infh)     -45.1       6.30    -7.16  5.06e-12\n\n\n# A tibble: 346 × 11\n   .rownames n_visits gender income  travel .fitted .resid .std.…¹   .hat .sigma\n   <chr>        <dbl> <fct>  <fct>   <fct>    <dbl>  <dbl>   <dbl>  <dbl>  <dbl>\n 1 25               2 female ($95k,… (4h,I…    8.67  -6.67 -0.155  0.0179   43.4\n 2 26               1 female ($95k,… (4h,I…    8.67  -7.67 -0.179  0.0179   43.4\n 3 27               1 male   ($95k,… (0.25…   16.3  -15.3  -0.357  0.0153   43.4\n 4 29               1 male   ($95k,… (4h,I…   -2.18   3.18  0.0738 0.0122   43.4\n 5 30               1 female ($55k,… (4h,I…    7.95  -6.95 -0.162  0.0229   43.4\n 6 31               1 male   [$0,$2… [0h,0…   61.5  -60.5  -1.42   0.0326   43.3\n 7 33              80 female [$0,$2… [0h,0…   72.4    7.61  0.178  0.0291   43.4\n 8 34             104 female ($95k,… [0h,0…   53.8   50.2   1.17   0.0215   43.3\n 9 35              55 male   ($25k,… (0.25…   33.0   22.0   0.512  0.0165   43.4\n10 36             350 female ($25k,… [0h,0…   70.4  280.    6.52   0.0215   40.6\n# … with 336 more rows, 1 more variable: .cooksd <dbl>, and abbreviated\n#   variable name ¹​.std.resid"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#returning-metadata",
    "href": "datacamp/functions_in_R/functions_R.html#returning-metadata",
    "title": "Functions in R",
    "section": "Returning metadata",
    "text": "Returning metadata\nSometimes you want the return multiple things from a function, but you want the result to have a particular class (for example, a data frame or a numeric vector), so returning a list isn’t appropriate. This is common when you have a result plus metadata about the result. (Metadata is “data about the data”. For example, it could be the file a dataset was loaded from, or the username of the person who created the variable, or the number of iterations for an algorithm to converge.)\nIn that case, you can store the metadata in attributes. Recall the syntax for assigning attributes is as follows.\nattr(object, “attribute_name”) <- attribute_value\n\npipeable_plot <- function(data, formula) {\n  plot(formula, data)\n  # Add a \"formula\" attribute to data\n  attr(data, \"formula\") <- formula\n  \n  invisible(data)\n}\n\n# From previous exercise\nplt_dist_vs_speed <- cars %>% \n  pipeable_plot(dist ~ speed)\n\n\n\n# Examine the structure of the result\nplt_dist_vs_speed\n\n   speed dist\n1      4    2\n2      4   10\n3      7    4\n4      7   22\n5      8   16\n6      9   10\n7     10   18\n8     10   26\n9     10   34\n10    11   17\n11    11   28\n12    12   14\n13    12   20\n14    12   24\n15    12   28\n16    13   26\n17    13   34\n18    13   34\n19    13   46\n20    14   26\n21    14   36\n22    14   60\n23    14   80\n24    15   20\n25    15   26\n26    15   54\n27    16   32\n28    16   40\n29    17   32\n30    17   40\n31    17   50\n32    18   42\n33    18   56\n34    18   76\n35    18   84\n36    19   36\n37    19   46\n38    19   68\n39    20   32\n40    20   48\n41    20   52\n42    20   56\n43    20   64\n44    22   66\n45    23   54\n46    24   70\n47    24   92\n48    24   93\n49    24  120\n50    25   85"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#creating-and-exploring-environments",
    "href": "datacamp/functions_in_R/functions_R.html#creating-and-exploring-environments",
    "title": "Functions in R",
    "section": "Creating and exploring environments",
    "text": "Creating and exploring environments\nEnvironments are used to store other variables. Mostly, you can think of them as lists, but there’s an important extra property that is relevant to writing functions. Every environment has a parent environment (except the empty environment, at the root of the environment tree). This determines which variables R know about at different places in your code.\nFacts about the Republic of South Africa are contained in capitals, national_parks, and population.\n\n# From previous steps\nrsa_lst <- list(\n  capitals = c(\"J Burg\", \"Capetown\"),\n  national_parks = c(\"Krug\", \"Iddo\"),\n  population = 30000\n)\nrsa_env <- list2env(rsa_lst)\n\n# Find the parent environment of rsa_env\nparent <- parent.env(rsa_env)\n\n# Print its name\nenvironmentName(parent)\n\n[1] \"R_GlobalEnv\"\n\n\nDo variables exist? If R cannot find a variable in the current environment, it will look in the parent environment, then the grandparent environment, and so on until it finds it.\nrsa_env has been modified so it includes capitals and national_parks, but not population.\n\n# Compare the contents of the global environment and rsa_env\nls.str(globalenv())\n\ncalc_harmonic_mean : function (x, na.rm = FALSE)  \ncff : tibble [7 × 5] (S3: tbl_df/tbl/data.frame)\ncoin_sides :  chr [1:2] \"head\" \"tail\"\ncut_by_quantile : function (x, n = 5, na.rm = FALSE, labels = NULL, interval_type = c(\"(lo, hi]\", \n    \"[lo, hi)\"))  \ngd_nms :  chr [1:87] \"USA\" \"GBR\" \"CHN\" \"RUS\" \"GER\" \"JPN\" \"FRA\" \"KOR\" \"ITA\" \"AUS\" ...\nget_reciprocal : function (x)  \ngold_medals :  Named int [1:87] 46 27 26 19 17 12 10 9 8 8 ...\ngroom_model : function (model)  \nis_leap_year : function (year)  \nmdl : tibble [1 × 8] (S3: tbl_df/tbl/data.frame)\nmodel : List of 31\n $ coefficients     : Named num [1:7] 61.54 10.85 -1.95 -19.33 -18.62 ...\n $ residuals        : Named num [1:346] -6.67 -7.67 -15.33 3.18 -6.95 ...\n $ fitted.values    : Named num [1:346] 8.67 8.67 16.33 -2.18 7.95 ...\n $ effects          : Named num [1:346] -509.9 -174.8 167.5 -46.6 162.4 ...\n $ R                : num [1:7, 1:7] -18.6 0 0 0 0 ...\n $ rank             : int 7\n $ qr               :List of 5\n $ family           :List of 11\n $ linear.predictors: Named num [1:346] 8.67 8.67 16.33 -2.18 7.95 ...\n $ deviance         : num 636485\n $ aic              : num 3599\n $ null.deviance    : num 820296\n $ iter             : int 2\n $ weights          : Named num [1:346] 1 1 1 1 1 1 1 1 1 1 ...\n $ prior.weights    : Named num [1:346] 1 1 1 1 1 1 1 1 1 1 ...\n $ df.residual      : int 339\n $ df.null          : int 345\n $ y                : Named num [1:346] 2 1 1 1 1 1 80 104 55 350 ...\n $ converged        : logi TRUE\n $ boundary         : logi FALSE\n $ model            :'data.frame':  346 obs. of  4 variables:\n $ na.action        : 'omit' Named int [1:64] 1 2 3 4 5 6 7 8 9 10 ...\n $ call             : language glm(formula = n_visits ~ gender + income + travel, data = snake_river_visits)\n $ formula          :Class 'formula'  language n_visits ~ gender + income + travel\n $ terms            :Classes 'terms', 'formula'  language n_visits ~ gender + income + travel\n $ data             :'data.frame':  410 obs. of  4 variables:\n $ offset           : NULL\n $ control          :List of 3\n $ method           : chr \"glm.fit\"\n $ contrasts        :List of 3\n $ xlevels          :List of 3\nn_visits :  num [1:410] 0 0 0 0 0 0 0 0 0 0 ...\nobs : tibble [346 × 11] (S3: tbl_df/tbl/data.frame)\nparent : <environment: R_GlobalEnv> \npipeable_plot : function (data, formula)  \nplt_dist_vs_speed : 'data.frame':   50 obs. of  2 variables:\n $ speed: num  4 4 7 7 8 9 10 10 10 11 ...\n $ dist : num  2 10 4 22 16 10 18 26 34 17 ...\nrsa_env : <environment: 0x55da3f008db8> \nrsa_lst : List of 3\n $ capitals      : chr [1:2] \"J Burg\" \"Capetown\"\n $ national_parks: chr [1:2] \"Krug\" \"Iddo\"\n $ population    : num 30000\nrun_poisson_regression : function (data, formula)  \nsnake_river_visits : 'data.frame':  410 obs. of  4 variables:\n $ n_visits: num  0 0 0 0 0 0 0 0 0 0 ...\n $ gender  : Factor w/ 2 levels \"male\",\"female\": 1 1 1 2 1 2 2 2 1 1 ...\n $ income  : Factor w/ 4 levels \"[$0,$25k]\",\"($25k,$55k]\",..: 4 2 4 2 4 2 4 4 4 4 ...\n $ travel  : Factor w/ 3 levels \"[0h,0.25h]\",\"(0.25h,4h]\",..: NA NA NA NA NA NA NA NA NA NA ...\nstd_and_poor500 : 'data.frame': 505 obs. of  5 variables:\n $ symbol  : chr  \"MMM\" \"ABT\" \"ABBV\" \"ABMD\" ...\n $ company : chr  \"3M Company\" \"Abbott Laboratories\" \"AbbVie Inc.\" \"ABIOMED Inc\" ...\n $ sector  : chr  \"Industrials\" \"Health Care\" \"Health Care\" \"Health Care\" ...\n $ industry: chr  \"Industrial Conglomerates\" \"Health Care Equipment\" \"Pharmaceuticals\" \"Health Care Equipment\" ...\n $ pe_ratio: num  18.3 57.7 22.4 45.6 27 ...\ntoss_coin : function (n_flips, p_head)  \n\nls.str(rsa_env)\n\ncapitals :  chr [1:2] \"J Burg\" \"Capetown\"\nnational_parks :  chr [1:2] \"Krug\" \"Iddo\"\npopulation :  num 30000\n\n# Does population exist in rsa_env?\nexists(\"population\", envir = rsa_env)\n\n[1] TRUE\n\n# Does population exist in rsa_env, ignoring inheritance?\nexists(\"population\", envir = rsa_env, inherits = FALSE)\n\n[1] TRUE"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#converting-areas-to-metric-1",
    "href": "datacamp/functions_in_R/functions_R.html#converting-areas-to-metric-1",
    "title": "Functions in R",
    "section": "Converting areas to metric 1",
    "text": "Converting areas to metric 1\nIn this chapter, you’ll be working with grain yield data from the United States Department of Agriculture, National Agricultural Statistics Service. Unfortunately, they report all areas in acres. So, the first thing you need to do is write some utility functions to convert areas in acres to areas in hectares.\nTo solve this exercise, you need to know the following:\nThere are 4840 square yards in an acre. There are 36 inches in a yard and one inch is 0.0254 meters. There are 10000 square meters in a hectare.\n\n# Write a function to convert acres to sq. yards\nacres_to_sq_yards <- function(acres) {\n  acres * 4840\n}\n\n# Write a function to convert yards to meters\nyards_to_meters <- function(yards){\n    yards * 36*0.0254\n}\n\n# Write a function to convert sq. meters to hectares\n# Write a function to convert yards to meters\nsq_meters_to_hectares <- function(sq_meters){\n    sq_meters/10000\n}"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#converting-areas-to-metric-2",
    "href": "datacamp/functions_in_R/functions_R.html#converting-areas-to-metric-2",
    "title": "Functions in R",
    "section": "Converting areas to metric 2",
    "text": "Converting areas to metric 2\nYou’re almost there with creating a function to convert acres to hectares. You need another utility function to deal with getting from square yards to square meters. Then, you can bring everything together to write the overall acres-to-hectares conversion function. Finally, in the next exercise you’ll be calculating area conversions in the denominator of a ratio, so you’ll need a harmonic acre-to-hectare conversion function.\nFree hints: magrittr’s raise_to_power() will be useful here. The last step is similar to Chapter 2’s Harmonic Mean.\nThe three utility functions from the last exercise (acres_to_sq_yards(), yards_to_meters(), and sq_meters_to_hectares()) are available, as is your get_reciprocal() from Chapter 2. magrittr is loaded.\n\n# Write a function to convert sq. yards to sq. meters\nsq_yards_to_sq_meters <- function(sq_yards) {\n  sq_yards %>%\n    # Take the square root\n    sqrt() %>%\n    # Convert yards to meters\n    yards_to_meters() %>%\n    # Square it\n    raise_to_power(2)\n}\n\n# Load the function from the previous step\n#load_step2()\n\n# Write a function to convert acres to hectares\nacres_to_hectares <- function(acres) {\n  acres %>%\n    # Convert acres to sq yards\n    acres_to_sq_yards() %>%\n    # Convert sq yards to sq meters\n    sq_yards_to_sq_meters() %>%\n    # Convert sq meters to hectares\n    sq_meters_to_hectares()\n}\n\n# Define a harmonic acres to hectares function\nharmonic_acres_to_hectares <- function(acres) {\n  acres %>% \n    # Get the reciprocal\n    get_reciprocal() %>%\n    # Convert acres to hectares\n    acres_to_hectares %>% \n    # Get the reciprocal again\n    get_reciprocal()\n}"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#converting-yields-to-metric",
    "href": "datacamp/functions_in_R/functions_R.html#converting-yields-to-metric",
    "title": "Functions in R",
    "section": "Converting yields to metric",
    "text": "Converting yields to metric\nThe yields in the NASS corn data are also given in US units, namely bushels per acre. You’ll need to write some more utility functions to convert this unit to the metric unit of kg per hectare.\nBushels historically meant a volume of 8 gallons, but in the context of grain, they are now defined as masses. This mass differs for each grain! To solve this exercise, you need to know these facts.\nOne pound (lb) is 0.45359237 kilograms (kg). One bushel is 48 lbs of barley, 56 lbs of corn, or 60 lbs of wheat. magrittr is loaded.\n\nlibrary(magrittr)\n# Write a function to convert lb to kg\nlbs_to_kgs <- function(lbs){\n    lbs * 0.45359237\n}\n\n# Write a function to convert bushels to lbs\nbushels_to_lbs <- function(bushels, crop) {\n  # Define a lookup table of scale factors\n  c(barley = 48, corn = 56, wheat = 60) %>%\n    # Extract the value for the crop\n    magrittr::extract(crop) %>%\n    # Multiply by the no. of bushels\n    multiply_by(bushels)\n}\n\n\n# Write a function to convert bushels to kg\nbushels_to_kgs <- function(bushels, crop) {\n  bushels %>%\n    # Convert bushels to lbs for this crop\n    bushels_to_lbs(crop) %>%\n    # Convert lbs to kgs\n    lbs_to_kgs()\n}\n\n\n\n# Write a function to convert bushels/acre to kg/ha\nbushels_per_acre_to_kgs_per_hectare <- function(bushels_per_acre, crop = c(\"barley\", \"corn\", \"wheat\")) {\n  # Match the crop argument\n  crop <- match.arg(crop)\n  bushels_per_acre %>%\n    # Convert bushels to kgs for this crop\n    bushels_to_kgs(crop) %>%\n    # Convert harmonic acres to ha\n    harmonic_acres_to_hectares()\n}"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#applying-the-unit-conversion",
    "href": "datacamp/functions_in_R/functions_R.html#applying-the-unit-conversion",
    "title": "Functions in R",
    "section": "Applying the unit conversion",
    "text": "Applying the unit conversion\nNow that you’ve written some functions, it’s time to apply them! The NASS corn dataset is available, and you can fortify it (jargon for “adding new columns”) with metrics areas and yields.\nThis fortification process can also be turned in to a function, so you’ll define a function for this, and test it on the NASS wheat dataset.\n\ncorn <- readRDS(\"nass.corn.rds\")\nwheat <- readRDS(\"nass.wheat.rds\")\nglimpse(corn)\n\nRows: 6,381\nColumns: 4\n$ year                   <int> 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866,…\n$ state                  <chr> \"Alabama\", \"Arkansas\", \"California\", \"Connectic…\n$ farmed_area_acres      <dbl> 1050000, 280000, 42000, 57000, 200000, 125000, …\n$ yield_bushels_per_acre <dbl> 9.0, 18.0, 28.0, 34.0, 23.0, 9.0, 6.0, 29.0, 36…\n\ncorn %>%\n  # Add some columns\n  mutate(\n    # Convert farmed area from acres to ha\n    farmed_area_ha = acres_to_hectares(farmed_area_acres),\n    # Convert yield from bushels/acre to kg/ha\n    yield_kg_per_ha = bushels_per_acre_to_kgs_per_hectare(\n      yield_bushels_per_acre,\n      crop = \"corn\"\n    )\n  ) %>%\n  head()\n\n  year       state farmed_area_acres yield_bushels_per_acre farmed_area_ha\n1 1866     Alabama           1050000                      9      424919.92\n2 1866    Arkansas            280000                     18      113311.98\n3 1866  California             42000                     28       16996.80\n4 1866 Connecticut             57000                     34       23067.08\n5 1866    Delaware            200000                     23       80937.13\n6 1866     Florida            125000                      9       50585.71\n  yield_kg_per_ha\n1         564.909\n2        1129.818\n3        1757.495\n4        2134.101\n5        1443.656\n6         564.909\n\n# Wrap this code into a function\nfortify_with_metric_units <- function(data, crop){\n\n\n  data %>%\n    mutate(\n      farmed_area_ha = acres_to_hectares(farmed_area_acres),\n      yield_kg_per_ha = bushels_per_acre_to_kgs_per_hectare(\n        yield_bushels_per_acre, \n        crop = crop\n      )\n    )\n}\n\n# Try it on the wheat dataset\nwheat <- fortify_with_metric_units(wheat, crop = \"wheat\")"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#plotting-yields-over-time",
    "href": "datacamp/functions_in_R/functions_R.html#plotting-yields-over-time",
    "title": "Functions in R",
    "section": "Plotting yields over time",
    "text": "Plotting yields over time\nNow that the units have been dealt with, it’s time to explore the datasets. An obvious question to ask about each crop is, “how do the yields change over time in each US state?” Let’s draw a line plot to find out! ggplot2 is loaded, and corn and wheat datasets are available with metric units.\n\n# Wrap this plotting code into a function\nplot_yield_vs_year <- function(data){\n  ggplot(data, aes(year, yield_kg_per_ha)) +\n    geom_line(aes(group = state)) +\n    geom_smooth()\n}\n\n# Test it on the wheat dataset\nplot_yield_vs_year(wheat)"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#a-nation-divided",
    "href": "datacamp/functions_in_R/functions_R.html#a-nation-divided",
    "title": "Functions in R",
    "section": "A nation divided",
    "text": "A nation divided\nThe USA has a varied climate, so we might expect yields to differ between states. Rather than trying to reason about 50 states separately, we can use the USA Census Regions to get 9 groups.\nThe “Corn Belt”, where most US corn is grown is in the “West North Central” and “East North Central” regions. The “Wheat Belt” is in the “West South Central” region.\ndplyr is loaded, the corn and wheat datasets are available, as is usa_census_regions.\n\n# Inner join the corn dataset to usa_census_regions by state\nlibrary(data.table)\nusa_census_regions <- read_csv(\"usa_census_regions.csv\")\nsetnames(usa_census_regions, c(\"State\", \"Region\"), c(\"state\", \"census_region\"))\nView(usa_census_regions)\n# corn %>%\n#   inner_join(usa_census_regions, by= \"state\")\n# Wrap this code into a function\nfortify_with_census_region <- function(data){\n  data %>%\n    inner_join(usa_census_regions, by = \"state\")\n}\n\n# Try it on the wheat dataset\nwheat <- fortify_with_census_region(wheat)"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#plotting-yields-over-time-by-region",
    "href": "datacamp/functions_in_R/functions_R.html#plotting-yields-over-time-by-region",
    "title": "Functions in R",
    "section": "Plotting yields over time by region",
    "text": "Plotting yields over time by region\nSo far, you have a function to plot yields over time for each crop, and you’ve added a census_region column to the crop datasets. Now you are ready to look at how the yields change over time in each region of the USA.\nggplot2 is loaded. corn and wheat have been fortified with census regions. plot_yield_vs_year() is available.\n\n# Wrap this code into a function\nplot_yield_vs_year_by_region <- function(data) {\n\n  plot_yield_vs_year(data) +\n    facet_wrap(vars(census_region))\n}\n\n# Try it on the wheat dataset\n\nplot_yield_vs_year_by_region(wheat)"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#running-a-model",
    "href": "datacamp/functions_in_R/functions_R.html#running-a-model",
    "title": "Functions in R",
    "section": "Running a model",
    "text": "Running a model\nThe smooth trend line you saw in the plots of yield over time use a generalized additive model (GAM) to determine where the line should lie. This sort of model is ideal for fitting nonlinear curves. So we can make predictions about future yields, let’s explicitly run the model. The syntax for running this GAM takes the following form.\ngam(response ~ s(explanatory_var1) + explanatory_var2, data = dataset) Here, s() means “make the variable smooth”, where smooth very roughly means nonlinear.\nmgcv and dplyr are loaded; the corn and wheat datasets are available.\n\n# Wrap the model code into a function\nlibrary(mgcv)\nrun_gam_yield_vs_year_by_region <- function(data){\n\n\n  gam(yield_kg_per_ha ~ s(year) + census_region, data = data)\n\n}\n\n# Try it on the wheat dataset\nwheat_model <- run_gam_yield_vs_year_by_region(wheat)\nwheat_model\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nyield_kg_per_ha ~ s(year) + census_region\n\nEstimated degrees of freedom:\n6.93  total = 10.93 \n\nGCV score: 341585.5"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#making-yield-predictions",
    "href": "datacamp/functions_in_R/functions_R.html#making-yield-predictions",
    "title": "Functions in R",
    "section": "Making yield predictions",
    "text": "Making yield predictions\nThe fun part of modeling is using the models to make predictions. You can do this using a call to predict(), in the following form.\npredict(model, cases_to_predict, type = “response”) mgcv and dplyr are loaded; GAMs of the corn and wheat datasets are available as corn_model and wheat_model. A character vector of census regions is stored as census_regions.\n\ncensus_regions <- wheat$census_region %>% unique()\n# Wrap this prediction code into a function\npredict_yields <- function(model, year){\n\n  predict_this <- data.table(\n    year = year,\n    census_region = census_regions\n  ) \n  pred_yield_kg_per_ha <- predict(model, predict_this, type = \"response\")\n  predict_this %>%\n    mutate(pred_yield_kg_per_ha = pred_yield_kg_per_ha)\n}\n\n# Try it on the wheat dataset\npredict_yields(wheat_model, year = 2050)\n\n   year census_region pred_yield_kg_per_ha\n1: 2050         South       <multi-column>\n2: 2050          West       <multi-column>\n3: 2050     Northeast       <multi-column>\n4: 2050       Midwest       <multi-column>"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#do-it-all-over-again",
    "href": "datacamp/functions_in_R/functions_R.html#do-it-all-over-again",
    "title": "Functions in R",
    "section": "Do it all over again",
    "text": "Do it all over again\nHopefully, by now, you’ve realized that the real benefit to writing functions is that you can reuse your code easily. Now you are going to rerun the whole analysis from this chapter on a new crop, barley. Since all the infrastructure is in place, that’s less effort than it sounds!\nBarley prefers a cooler climate compared to corn and wheat and is commonly grown in the US mountain states of Idaho and Montana.\ndplyr and ggplot2, and mgcv are loaded; fortify_with_metric_units(), fortify_with_census_region(), plot_yield_vs_year_by_region(), run_gam_yield_vs_year_by_region(), and predict_yields() are available.\n\nfortified_barley <- barley %>% \n  # Fortify with metric units\n  fortify_with_metric_units() %>%\n  # Fortify with census regions\n  fortify_with_census_region()\n\n# See the result\nglimpse(fortified_barley)\n# From previous step\nfortified_barley <- barley %>% \n  fortify_with_metric_units() %>%\n  fortify_with_census_region()\n\nfortified_barley %>% \n  # Run a GAM of yield vs. year by region\n  run_gam_yield_vs_year_by_region %>% \n  # Make predictions of yields in 2050\n  predict_yields(year = 2050)"
  },
  {
    "objectID": "datacamp/tensorflow_R/tensorflow_R.html",
    "href": "datacamp/tensorflow_R/tensorflow_R.html",
    "title": "Introduction to TensorFlow in R",
    "section": "",
    "text": "library(tensorflow)\nmsg = tf$constant('Hello, TensorFlow!')\ntf$print(msg)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html",
    "href": "datacamp/us_census/us_census.html",
    "title": "Census data in r with tidycensus",
    "section": "",
    "text": "tidycensus is an R package designed to return data from the US Census Bureau ready for use within the Tidyverse.\nTo acquire data from the US Census Bureau using the tidycensus R package, you must first acquire and set a Census API key. After obtaining your key, you can install it for future use with the census_api_key() function in tidycensus.\nThis exercise uses a fake API key for purposes of illustration.\n\n# Load the tidycensus package into your R session\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(tigris)\nlibrary(here)\n\n# Define your Census API key and set it with census_api_key()\napi_key <- Sys.getenv(\"CENSUS_API_KEY\")\n\ncensus_api_key(api_key)\n\n##Setting a cache directory Spatial data from the US Census Bureau can get very big - sometimes hundreds of megabytes in size. By default, tigris functions download data from the US Census Bureau’s website - but this can get tiresome if downloading the same large datasets over and over. To resolve this, tigris includes an option to cache downloaded data on a user’s computer for future use, meaning that files only have to be downloaded from the Census website once. In this exercise, you’ll get acquainted with the caching functionality in tigris.\n\n# Set the cache directory\ntigris_cache_dir(here(\"tigris_cache\"))\n\n# Set the tigris_use_cache option\noptions(tigris_use_cache = TRUE)\n\n# Check to see that you've modified the option correctly\ngetOption(\"tigris_use_cache\")\n\n[1] TRUE"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#getting-census-data-with-tidycensus",
    "href": "datacamp/us_census/us_census.html#getting-census-data-with-tidycensus",
    "title": "Census data in r with tidycensus",
    "section": "Getting Census data with tidycensus",
    "text": "Getting Census data with tidycensus\nIn this exercise, you will load and inspect data from the 2010 US Census and 2012-2016 American Community Survey. The core functions of get_decennial() and get_acs() in tidycensus are used to obtain data from these sources; the 2010 Census and 2012-2016 ACS are the defaults for these functions, respectively.\nBy inspecting the data, you’ll get a sense of differences between decennial US Census data and data from the ACS, which is based on a sample and subject to a margin of error. Whereas get_decennial() returns a data value for each row, get_acs() returns estimate and moe columns representing the ACS estimate and margin of error.\n\n# Obtain and view state populations from the 2010 US Census\nstate_pop <- get_decennial(geography = \"state\", \n                           variables = \"P001001\")\n\nhead(state_pop)\n\n# A tibble: 6 × 4\n  GEOID NAME       variable    value\n  <chr> <chr>      <chr>       <dbl>\n1 01    Alabama    P001001   4779736\n2 02    Alaska     P001001    710231\n3 04    Arizona    P001001   6392017\n4 05    Arkansas   P001001   2915918\n5 06    California P001001  37253956\n6 22    Louisiana  P001001   4533372\n\n# Obtain and view state median household income from the 2012-2016 American Community Survey\nstate_income <- get_acs(geography = \"state\", \n                        variables = \"B19013_001\")\n\nhead(state_income)\n\n# A tibble: 6 × 5\n  GEOID NAME       variable   estimate   moe\n  <chr> <chr>      <chr>         <dbl> <dbl>\n1 01    Alabama    B19013_001    54943   377\n2 02    Alaska     B19013_001    80287  1113\n3 04    Arizona    B19013_001    65913   387\n4 05    Arkansas   B19013_001    52123   458\n5 06    California B19013_001    84097   236\n6 08    Colorado   B19013_001    80184   450"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#understanding-tidycensus-options",
    "href": "datacamp/us_census/us_census.html#understanding-tidycensus-options",
    "title": "Census data in r with tidycensus",
    "section": "Understanding tidycensus options",
    "text": "Understanding tidycensus options\nAs discussed in this lesson, Census data comprise thousands of variables available across dozens of geographies! Most of these geography-variable combinations are accessible with tidycensus; however, it helps to understand the package options.\nSome data, like Census tracts, are only available by state, and users might want to subset by county; tidycensus facilitates this with state and county parameters when appropriate. Additionally, tidycensus includes the Census variable ID in the variable column; however, a user might want to supply her own variable name, which can be accomplished with a named vector.\nYou’ll be using the Census variable B19013_001 here, which refers to median household income.\n\n# Get an ACS dataset for Census tracts in Texas by setting the state\ntx_income <- get_acs(geography = \"tract\",\n                     variables = \"B19013_001\",\n                     state = \"TX\")\n\n# Inspect the dataset\nhead(tx_income)\n\n# A tibble: 6 × 5\n  GEOID       NAME                                         varia…¹ estim…²   moe\n  <chr>       <chr>                                        <chr>     <dbl> <dbl>\n1 48001950100 Census Tract 9501, Anderson County, Texas    B19013…   61325  9171\n2 48001950401 Census Tract 9504.01, Anderson County, Texas B19013…   92813 45136\n3 48001950402 Census Tract 9504.02, Anderson County, Texas B19013…      NA    NA\n4 48001950500 Census Tract 9505, Anderson County, Texas    B19013…   41713  6650\n5 48001950600 Census Tract 9506, Anderson County, Texas    B19013…   32552 12274\n6 48001950700 Census Tract 9507, Anderson County, Texas    B19013…   35811  5573\n# … with abbreviated variable names ¹​variable, ²​estimate\n\n# Get an ACS dataset for Census tracts in Travis County, TX\ntravis_income <- get_acs(geography = \"tract\",\n                         variables = \"B19013_001\", \n                         state = \"TX\",\n                         county = \"Travis\")\n\n# Inspect the dataset\nhead(travis_income)\n\n# A tibble: 6 × 5\n  GEOID       NAME                                    variable   estimate   moe\n  <chr>       <chr>                                   <chr>         <dbl> <dbl>\n1 48453000101 Census Tract 1.01, Travis County, Texas B19013_001   121964 31935\n2 48453000102 Census Tract 1.02, Travis County, Texas B19013_001   201417 26672\n3 48453000203 Census Tract 2.03, Travis County, Texas B19013_001    81994 14344\n4 48453000204 Census Tract 2.04, Travis County, Texas B19013_001    93219 26118\n5 48453000205 Census Tract 2.05, Travis County, Texas B19013_001    75000 24198\n6 48453000206 Census Tract 2.06, Travis County, Texas B19013_001    88342 10549\n\n# Supply custom variable names\ntravis_income2 <- get_acs(geography = \"tract\", \n                          variables = c(hhincome = \"B19013_001\"), \n                          state = \"TX\",\n                          county = \"Travis\")\n\n# Inspect the dataset\nhead(travis_income2)\n\n# A tibble: 6 × 5\n  GEOID       NAME                                    variable estimate   moe\n  <chr>       <chr>                                   <chr>       <dbl> <dbl>\n1 48453000101 Census Tract 1.01, Travis County, Texas hhincome   121964 31935\n2 48453000102 Census Tract 1.02, Travis County, Texas hhincome   201417 26672\n3 48453000203 Census Tract 2.03, Travis County, Texas hhincome    81994 14344\n4 48453000204 Census Tract 2.04, Travis County, Texas hhincome    93219 26118\n5 48453000205 Census Tract 2.05, Travis County, Texas hhincome    75000 24198\n6 48453000206 Census Tract 2.06, Travis County, Texas hhincome    88342 10549"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#tidy-and-wide-data-in-tidycensus",
    "href": "datacamp/us_census/us_census.html#tidy-and-wide-data-in-tidycensus",
    "title": "Census data in r with tidycensus",
    "section": "Tidy and wide data in tidycensus",
    "text": "Tidy and wide data in tidycensus\nBy default, tidycensus functions return tidy data frames, in which each row represents a unique unit-variable combination. However, at times it is useful to have each Census variable in its own column for some methods of visualization and analysis. To accomplish this, you can set output = “wide” in your calls to get_acs() or get_decennial(), which will place estimates/values and margins of error in their own columns.\n\n# Return county data in wide format\nor_wide <- get_acs(geography = \"county\", \n                     state = \"OR\",\n                     variables = c(hhincome = \"B19013_001\", \n                            medage = \"B01002_001\"), \n                     output = \"wide\")\n\n# Compare output to the tidy format from previous exercises\nhead(or_wide)\n\n# A tibble: 6 × 6\n  GEOID NAME                     hhincomeE hhincomeM medageE medageM\n  <chr> <chr>                        <dbl>     <dbl>   <dbl>   <dbl>\n1 41001 Baker County, Oregon         46922      3271    47.7     0.9\n2 41003 Benton County, Oregon        68732      2689    33.3     0.3\n3 41005 Clackamas County, Oregon     88517      1424    41.6     0.2\n4 41007 Clatsop County, Oregon       61846      2651    44.5     0.4\n5 41009 Columbia County, Oregon      73909      3517    43.3     0.4\n6 41011 Coos County, Oregon          52548      3145    48.4     0.3\n\n# Create a scatterplot\nplot(or_wide$hhincomeE, or_wide$medageE)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#loading-variables-in-tidycensus",
    "href": "datacamp/us_census/us_census.html#loading-variables-in-tidycensus",
    "title": "Census data in r with tidycensus",
    "section": "Loading variables in tidycensus",
    "text": "Loading variables in tidycensus\nThere are hundreds of thousands of variables in the decennial Census and American Community Survey samples, which can make it difficult to know which variable codes to use! tidycensus aims to make this easier with the load_variables() function, which obtains a dataset of variables from a specified sample and loads it into R as a browsable data frame.\n\n# Load variables from the 2012-2016 ACS\nv16 <- load_variables(year = 2016,\n           dataset = \"acs5\",\n           cache = TRUE)\n\n# Get variables from the ACS Data Profile\nv16p <- load_variables(year = 2016,\n                       dataset = \"acs5/profile\",\n                       cache = TRUE)\n\n# Set year and dataset to get variables from the 2000 Census SF3\nv00 <- load_variables(year = 2000,\n                      dataset = \"sf3\",\n                      cache = TRUE)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#exploring-variables-with-tidyverse-tools",
    "href": "datacamp/us_census/us_census.html#exploring-variables-with-tidyverse-tools",
    "title": "Census data in r with tidycensus",
    "section": "Exploring variables with tidyverse tools",
    "text": "Exploring variables with tidyverse tools\nOnce loaded, your dataset of Census or ACS variables might contain thousands of rows. In RStudio, it is recommended to use the View() function to interactively search for these variables. Outside of RStudio, these datasets can be browsed using tidyverse filtering tools.\n\n# Filter for table B19001\nfilter(v16, str_detect(name, \"B19001\"))\n\n# A tibble: 170 × 4\n   name        label                               concept               geogr…¹\n   <chr>       <chr>                               <chr>                 <chr>  \n 1 B19001A_001 Estimate!!Total                     HOUSEHOLD INCOME IN … tract  \n 2 B19001A_002 Estimate!!Total!!Less than $10,000  HOUSEHOLD INCOME IN … tract  \n 3 B19001A_003 Estimate!!Total!!$10,000 to $14,999 HOUSEHOLD INCOME IN … tract  \n 4 B19001A_004 Estimate!!Total!!$15,000 to $19,999 HOUSEHOLD INCOME IN … tract  \n 5 B19001A_005 Estimate!!Total!!$20,000 to $24,999 HOUSEHOLD INCOME IN … tract  \n 6 B19001A_006 Estimate!!Total!!$25,000 to $29,999 HOUSEHOLD INCOME IN … tract  \n 7 B19001A_007 Estimate!!Total!!$30,000 to $34,999 HOUSEHOLD INCOME IN … tract  \n 8 B19001A_008 Estimate!!Total!!$35,000 to $39,999 HOUSEHOLD INCOME IN … tract  \n 9 B19001A_009 Estimate!!Total!!$40,000 to $44,999 HOUSEHOLD INCOME IN … tract  \n10 B19001A_010 Estimate!!Total!!$45,000 to $49,999 HOUSEHOLD INCOME IN … tract  \n# … with 160 more rows, and abbreviated variable name ¹​geography\n\n# Use public transportation to search for related variables\nfilter(v16p, str_detect(label, fixed(\"public transportation\", \n                                ignore_case = TRUE)))\n\n# A tibble: 2 × 3\n  name       label                                                       concept\n  <chr>      <chr>                                                       <chr>  \n1 DP03_0021  Estimate!!COMMUTING TO WORK!!Workers 16 years and over!!Pu… SELECT…\n2 DP03_0021P Percent!!COMMUTING TO WORK!!Workers 16 years and over!!Pub… SELECT…"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#comparing-geographies-with-ggplot2-visualizations",
    "href": "datacamp/us_census/us_census.html#comparing-geographies-with-ggplot2-visualizations",
    "title": "Census data in r with tidycensus",
    "section": "Comparing geographies with ggplot2 visualizations",
    "text": "Comparing geographies with ggplot2 visualizations\nWhen exploring Census or ACS data, you’ll often want to know how data varies among different geographic units. For example - which US states have higher - or lower - median household incomes? This can be accomplished through visualization using dot plots, which are particularly effective for showing ranks visually. In this exercise, you’ll use the popular ggplot2 data visualization package to accomplish this.\n\n# Access the 1-year ACS  with the survey parameter\nne_income <- get_acs(geography = \"state\",\n                     variables = \"B19013_001\", \n                     survey = \"acs1\", \n                     state = c(\"ME\", \"NH\", \"VT\", \"MA\", \n                               \"RI\", \"CT\", \"NY\"))\n\n# Create a dot plot\n\n  \n# Reorder the states in descending order of estimates\nggplot(ne_income, aes(x = estimate, y = reorder(NAME, estimate))) + \n  geom_point()"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#customizing-ggplot2-visualizations-of-acs-data",
    "href": "datacamp/us_census/us_census.html#customizing-ggplot2-visualizations-of-acs-data",
    "title": "Census data in r with tidycensus",
    "section": "Customizing ggplot2 visualizations of ACS data",
    "text": "Customizing ggplot2 visualizations of ACS data\nWhile the ggplot2 defaults are excellent for exploratory visualization of data, you’ll likely want to customize your charts before sharing them with others. In this exercise, you’ll customize your tidycensus dot plot by modifying the chart colors, tick labels, and axis labels. You’ll also learn how to format labels using the scales package, as label formatters can be imported using the :: syntax.\n\n# Set dot color and size\ng_color <- ggplot(ne_income, aes(x = estimate, y = reorder(NAME, estimate))) + \n  geom_point(color = \"navy\", size = 4)\n\n# Format the x-axis labels\ng_scale <- g_color + \n  scale_x_continuous(labels = scales::dollar) + \n  theme_minimal(base_size = 12) \n\n# Label your x-axis, y-axis, and title your chart\ng_label <- g_scale + \n  labs(x =\"2016 ACS estimate\", \n       y = \"\", \n       title = \"Median household income by state\")\n  \ng_label"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#download-and-view-a-table-of-data-from-the-acs",
    "href": "datacamp/us_census/us_census.html#download-and-view-a-table-of-data-from-the-acs",
    "title": "Census data in r with tidycensus",
    "section": "Download and view a table of data from the ACS",
    "text": "Download and view a table of data from the ACS\nVariables in the decennial Census and American Community Survey are organized into tables, within which they share a common prefix. Commonly, analysts will want to work with all variables in a given table, as these variables might represent different aspects of a common characteristic (such as race or income levels). To request data for an entire table in tidycensus, users can specify a table argument with the table prefix, and optionally cache a dataset of table codes to speed up table searching in future requests. In this exercise, you’ll acquire a table of variables representing different income bands, then filter out the denominator rows.\n\n# Download table \"B19001\"\nwa_income <- get_acs(geography = \"county\", \n                 state = \"WA\", \n                 table = \"B19001\")\n\n# Check out the first few rows of wa_income\nhead(wa_income)\n\n# A tibble: 6 × 5\n  GEOID NAME                     variable   estimate   moe\n  <chr> <chr>                    <chr>         <dbl> <dbl>\n1 53001 Adams County, Washington B19001_001     6158   123\n2 53001 Adams County, Washington B19001_002      474   171\n3 53001 Adams County, Washington B19001_003      255   107\n4 53001 Adams County, Washington B19001_004      204    90\n5 53001 Adams County, Washington B19001_005      393   134\n6 53001 Adams County, Washington B19001_006      358   159"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#get-a-summary-variable-and-calculate-percentages",
    "href": "datacamp/us_census/us_census.html#get-a-summary-variable-and-calculate-percentages",
    "title": "Census data in r with tidycensus",
    "section": "Get a summary variable and calculate percentages",
    "text": "Get a summary variable and calculate percentages\nMany variables in the Census and American Community Survey are represented as counts or estimated counts. While count data is useful for some applications, it is often good practice to normalize count data by its denominator to convert it to a proportion or percentage to make clearer comparisons. This is facilitated in tidycensus with the summary_var argument, which allows users to request that a variable is given its own column in a tidy Census dataset. This value can then be used as the denominator for subsequent calculations of percentages.\nSummary question: When the summary_var parameter is requested in get_acs(), what information is returned by the function?\n\n# Assign Census variables vector to race_vars \nrace_vars <- c(White = \"B03002_003\", Black = \"B03002_004\", Native = \"B03002_005\", \n               Asian = \"B03002_006\", HIPI = \"B03002_007\", Hispanic = \"B03002_012\")\n\n# Request a summary variable from the ACS\nca_race <- get_acs(geography = \"county\", \n                   state = \"CA\",\n                   variables = race_vars, \n                   summary_var = \"B03002_001\")\n\n# Calculate a new percentage column and check the result\nca_race_pct <- ca_race %>%\n  mutate(pct = 100 * (estimate / summary_est))\n\nhead(ca_race_pct)\n\n# A tibble: 6 × 8\n  GEOID NAME                       variable estim…¹   moe summa…² summa…³    pct\n  <chr> <chr>                      <chr>      <dbl> <dbl>   <dbl>   <dbl>  <dbl>\n1 06001 Alameda County, California White     499730   988 1673133      NA 29.9  \n2 06001 Alameda County, California Black     166017  1837 1673133      NA  9.92 \n3 06001 Alameda County, California Native      5248   318 1673133      NA  0.314\n4 06001 Alameda County, California Asian     524980  2437 1673133      NA 31.4  \n5 06001 Alameda County, California HIPI       12699   566 1673133      NA  0.759\n6 06001 Alameda County, California Hispanic  374542    NA 1673133      NA 22.4  \n# … with abbreviated variable names ¹​estimate, ²​summary_est, ³​summary_moe"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#finding-the-largest-group-by-county",
    "href": "datacamp/us_census/us_census.html#finding-the-largest-group-by-county",
    "title": "Census data in r with tidycensus",
    "section": "Finding the largest group by county",
    "text": "Finding the largest group by county\ntidyverse data wrangling tools in packages like dplyr and purrr are extremely powerful for exploring Census data. tidycensus is specifically designed with data exploration within the tidyverse in mind. For example, users might be interested in finding out the largest racial/ethnic group within each county for a given state. This can be accomplished using dplyr grouping capabilities, which allow users to identify the largest ACS group estimate and filter to retain the rows that match that group.\n\n# Group the dataset and filter the estimate\nca_largest <- ca_race %>%\n  group_by(GEOID) %>%\n  filter(estimate == max(estimate)) \n\nhead(ca_largest)\n\n# A tibble: 6 × 7\n# Groups:   GEOID [6]\n  GEOID NAME                         variable estimate   moe summary_est summa…¹\n  <chr> <chr>                        <chr>       <dbl> <dbl>       <dbl>   <dbl>\n1 06001 Alameda County, California   Asian      524980  2437     1673133      NA\n2 06003 Alpine County, California    White         730   153        1344     228\n3 06005 Amador County, California    White       30081   412       40095      NA\n4 06007 Butte County, California     White      153153   300      217884      NA\n5 06009 Calaveras County, California White       35925   129       45349      NA\n6 06011 Colusa County, California    Hispanic    13177    NA       21780      NA\n# … with abbreviated variable name ¹​summary_moe\n\n# Group the dataset and get a breakdown of the results\nca_largest %>% \n  group_by(variable) %>%\n  tally()\n\n# A tibble: 3 × 2\n  variable     n\n  <chr>    <int>\n1 Asian        2\n2 Hispanic    16\n3 White       40"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#recoding-variables-and-calculating-group-sums",
    "href": "datacamp/us_census/us_census.html#recoding-variables-and-calculating-group-sums",
    "title": "Census data in r with tidycensus",
    "section": "Recoding variables and calculating group sums",
    "text": "Recoding variables and calculating group sums\ndplyr, one of the core packages within the tidyverse, includes numerous functions for data wrangling. This functionality allows users to recode datasets, define groups within those datasets, and perform calculations over those groups. Such operations commonly take place within a pipe, denoted with the %>% operator.\nIn this exercise, you’ll work with ACS data in just such a tidyverse workflow. You’ll be identifying median household income variables in ACS table B19001 that are below $35,000; between $35,000 and $75,000; and above $75,000. You’ll then tabulate the number of households that fall into each group for counties in Washington.\n\n# Use a tidy workflow to wrangle ACS data\nwa_grouped <- wa_income %>%\n  filter(variable != \"B19001_001\") %>%\n  mutate(incgroup = case_when(\n    variable < \"B19001_008\" ~ \"below35k\", \n    variable < \"B19001_013\" ~ \"35kto75k\", \n    TRUE ~ \"above75k\"\n  )) %>%\n  group_by(NAME, incgroup) %>%\n  summarize(group_est = sum(estimate))\n\nwa_grouped\n\n# A tibble: 117 × 3\n# Groups:   NAME [39]\n   NAME                      incgroup group_est\n   <chr>                     <chr>        <dbl>\n 1 Adams County, Washington  35kto75k      2156\n 2 Adams County, Washington  above75k      2094\n 3 Adams County, Washington  below35k      1908\n 4 Asotin County, Washington 35kto75k      3215\n 5 Asotin County, Washington above75k      3537\n 6 Asotin County, Washington below35k      2535\n 7 Benton County, Washington 35kto75k     21285\n 8 Benton County, Washington above75k     37911\n 9 Benton County, Washington below35k     15094\n10 Chelan County, Washington 35kto75k      9163\n# … with 107 more rows"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#comparing-acs-estimates-for-multiple-years",
    "href": "datacamp/us_census/us_census.html#comparing-acs-estimates-for-multiple-years",
    "title": "Census data in r with tidycensus",
    "section": "Comparing ACS estimates for multiple years",
    "text": "Comparing ACS estimates for multiple years\nThe American Community Survey is updated every year, which allows researchers to use ACS datasets to study demographic changes over time.\nIn this exercise, you’ll learn how to use the tidyverse function map_df() to work with multi-year ACS data. map_df() helps analysts iterate through a sequence of values, compute a process for each of those values, then combine the results into a single data frame. You’ll be using map_df() in this way with ACS data, as you iterate through a vector of years, retrieve ACS data for each year, and combine the results. This will allow you to view how ACS estimates have changed over time.\n\n# Map through ACS1 estimates to see how they change through the years\nmi_cities <- map_df(2012:2016, function(x) {\n  get_acs(geography = \"place\", \n          variables = c(totalpop = \"B01003_001\"), \n          state = \"MI\", \n          survey = \"acs1\", \n          year = x) %>%\n    mutate(year = x)\n})\n\nmi_cities %>% arrange(NAME, year)\n\n# A tibble: 80 × 6\n   GEOID   NAME                     variable estimate   moe  year\n   <chr>   <chr>                    <chr>       <dbl> <dbl> <int>\n 1 2603000 Ann Arbor city, Michigan totalpop   116128    35  2012\n 2 2603000 Ann Arbor city, Michigan totalpop   117034    43  2013\n 3 2603000 Ann Arbor city, Michigan totalpop   117759    44  2014\n 4 2603000 Ann Arbor city, Michigan totalpop   117070    33  2015\n 5 2603000 Ann Arbor city, Michigan totalpop   120777    33  2016\n 6 2621000 Dearborn city, Michigan  totalpop    96470    28  2012\n 7 2621000 Dearborn city, Michigan  totalpop    95888    35  2013\n 8 2621000 Dearborn city, Michigan  totalpop    95546    48  2014\n 9 2621000 Dearborn city, Michigan  totalpop    95180    40  2015\n10 2621000 Dearborn city, Michigan  totalpop    94430    52  2016\n# … with 70 more rows"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#inspecting-margins-of-error",
    "href": "datacamp/us_census/us_census.html#inspecting-margins-of-error",
    "title": "Census data in r with tidycensus",
    "section": "Inspecting margins of error",
    "text": "Inspecting margins of error\nACS data are distinct from decennial Census data in that they represent estimates with an associated margin of error. ACS margins of error by default represent a 90 percent confidence level around an estimate, which means that we are 90 percent sure that the true value falls within a range of the reported estimate plus or minus the reported margin of error.\nIn this exercise, you’ll get some experience working with data that has high margins of error relative to their estimates. We’ll use the example of poverty for the population aged 75 and above for Census tracts in Vermont.\n\n# Get data on elderly poverty by Census tract in Vermont\nvt_eldpov <- get_acs(geography = \"tract\", \n                     variables = c(eldpovm = \"B17001_016\", \n                                   eldpovf = \"B17001_030\"), \n                     state = \"VT\")\n\nvt_eldpov\n\n# A tibble: 386 × 5\n   GEOID       NAME                                       variable estim…¹   moe\n   <chr>       <chr>                                      <chr>      <dbl> <dbl>\n 1 50001960100 Census Tract 9601, Addison County, Vermont eldpovm        2     5\n 2 50001960100 Census Tract 9601, Addison County, Vermont eldpovf        8     7\n 3 50001960200 Census Tract 9602, Addison County, Vermont eldpovm        4     6\n 4 50001960200 Census Tract 9602, Addison County, Vermont eldpovf        0    10\n 5 50001960300 Census Tract 9603, Addison County, Vermont eldpovm        0    10\n 6 50001960300 Census Tract 9603, Addison County, Vermont eldpovf        7     9\n 7 50001960400 Census Tract 9604, Addison County, Vermont eldpovm        7    10\n 8 50001960400 Census Tract 9604, Addison County, Vermont eldpovf       15    11\n 9 50001960500 Census Tract 9605, Addison County, Vermont eldpovm        6    10\n10 50001960500 Census Tract 9605, Addison County, Vermont eldpovf       14    16\n# … with 376 more rows, and abbreviated variable name ¹​estimate\n\n# Identify rows with greater margins of error than their estimates\nmoe_check <- filter(vt_eldpov, moe > estimate)\n\n# Check proportion of rows where the margin of error exceeds the estimate\nnrow(moe_check) / nrow(vt_eldpov)\n\n[1] 0.7927461"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#using-margin-of-error-functions-in-tidycensus",
    "href": "datacamp/us_census/us_census.html#using-margin-of-error-functions-in-tidycensus",
    "title": "Census data in r with tidycensus",
    "section": "Using margin of error functions in tidycensus",
    "text": "Using margin of error functions in tidycensus\nWhile the Census Bureau API and tidycensus return pre-computed margins of error for you, you may want to derive new estimates from downloaded ACS data and in turn understand the margins of error around these derived estimates. tidycensus includes four functions (listed below) to help you with these tasks, each of which incorporates the recommended formulas from the US Census Bureau.\nmoe_sum() moe_product() moe_ratio() moe_prop()\n\n# Calculate a margin of error for a sum\nmoe_sum(moe = c(55, 33, 44, 12, 4))\n\n[1] 78.80355\n\n# Calculate a margin of error for a product\nmoe_product(est1 = 55,\n    est2 = 33,\n    moe1 = 12,\n    moe2 = 9)\n\n[1] 633.9093\n\n# Calculate a margin of error for a ratio\nmoe_ratio(num = 1000,\n    denom = 950,\n    moe_num = 200,\n    moe_denom = 177)\n\n[1] 0.287724\n\n# Calculate a margin of error for a proportion\nmoe_prop(num = 374,\n    denom = 1200,\n    moe_num = 122,\n    moe_denom = 333)\n\n[1] 0.05344178"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#calculating-group-wise-margins-of-error",
    "href": "datacamp/us_census/us_census.html#calculating-group-wise-margins-of-error",
    "title": "Census data in r with tidycensus",
    "section": "Calculating group-wise margins of error",
    "text": "Calculating group-wise margins of error\nOne way to reduce margins of error in an ACS analysis is to combine estimates when appropriate. This can be accomplished using tidyverse group-wise data analysis tools. In this exercise, you’ll combine estimates for male and female elderly poverty in Vermont, and use the moe_sum() function as part of this group-wise analysis. While you may lose some detail with this type of approach, your estimates will be more reliable relative to their margins of error than before you combined them.\n\n# Group the dataset and calculate a derived margin of error\nvt_eldpov2 <- vt_eldpov %>%\n  group_by(GEOID) %>%\n  summarize(\n    estmf = sum(estimate), \n    moemf = moe_sum(moe = moe, estimate = estimate)\n  )\n\n# Filter rows where newly-derived margin of error exceeds newly-derived estimate\nmoe_check2 <- filter(vt_eldpov2, moemf > estmf)\n\n# Check proportion of rows where margin of error exceeds estimate\nnrow(moe_check2) / nrow(vt_eldpov2)\n\n[1] 0.626943"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#quick-visual-exploration-of-acs-margins-of-error",
    "href": "datacamp/us_census/us_census.html#quick-visual-exploration-of-acs-margins-of-error",
    "title": "Census data in r with tidycensus",
    "section": "Quick visual exploration of ACS margins of error",
    "text": "Quick visual exploration of ACS margins of error\nIn Chapter 1, you learned how to create a dot plot of ACS income estimates. In this chapter, you’ve also learned about the importance of taking margins of error into account in ACS analyses. While margins of error are likely minimal for state-level estimates, they may be more significant for sub-state estimates, like counties. In this exercise, you’ll learn how to visualize margins of error around estimates with ggplot2.\n\n# Request median household income data\nmaine_inc <- get_acs(geography = \"county\", \n                     variables = c(hhincome = \"B19013_001\"), \n                     state = \"ME\") \n\n# Generate horizontal error bars with dots\nggplot(maine_inc, aes(x = estimate, y = NAME)) + \n  geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) + \n  geom_point()"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#customizing-a-ggplot2-margin-of-error-plot",
    "href": "datacamp/us_census/us_census.html#customizing-a-ggplot2-margin-of-error-plot",
    "title": "Census data in r with tidycensus",
    "section": "Customizing a ggplot2 margin of error plot",
    "text": "Customizing a ggplot2 margin of error plot\nYou’ve hopefully identified some problems with the chart you created in the previous exercise. As the counties are not ordered, patterns in the data are difficult for a viewer to parse. Specifically, margin of error plots are much more effective when dots are ordered as the ordering allows viewers to understand the uncertainty in estimate values relative to other estimates. Additionally, the lack of plot formatting makes it difficult for chart viewers to understand the chart’s content. In this exercise, you’ll clean up your ggplot2 code to create a much more visually appealing margin of error chart.\n\n# Remove unnecessary content from the county's name\nmaine_inc2 <- maine_inc %>%\n  mutate(NAME = str_replace(NAME, \" County, Maine\", \"\"))\n\n# Build a margin of error plot incorporating your modifications\nggplot(maine_inc2, aes(x = estimate, y = reorder(NAME, estimate))) + \n  geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) + \n  geom_point(size = 3, color = \"darkgreen\") + \n  theme_grey(base_size = 14) + \n  labs(title = \"Median household income\", \n       subtitle = \"Counties in Maine\", \n       x = \"ACS estimate (bars represent margins of error)\", \n       y = \"\") + \n  scale_x_continuous(labels = scales::dollar)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#getting-census-boundary-files-with-tigris",
    "href": "datacamp/us_census/us_census.html#getting-census-boundary-files-with-tigris",
    "title": "Census data in r with tidycensus",
    "section": "Getting Census boundary files with tigris",
    "text": "Getting Census boundary files with tigris\nThe US Census Bureau’s TIGER/Line shapefiles include boundary files for the geography at which decennial Census and ACS data are aggregated. These geographies include legal entities that have legal standing in the U.S., such as states and counties, and statistical entities used for data tabulation such as Census tracts and block groups. In this exercise, you’ll use the tigris package to acquire such boundary files for counties in Colorado and Census tracts for Colorado’s Denver County, which covers the city of Denver.\n\n# Get a counties dataset for Colorado and plot it\nco_counties <- counties(state = \"CO\")\nplot(co_counties)\n\n\n\n# Get a Census tracts dataset for Denver County, Colorado and plot it\ndenver_tracts <- tracts(state = \"CO\", county = \"Denver\")\nplot(denver_tracts)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#getting-geographic-features-with-tigris",
    "href": "datacamp/us_census/us_census.html#getting-geographic-features-with-tigris",
    "title": "Census data in r with tidycensus",
    "section": "Getting geographic features with tigris",
    "text": "Getting geographic features with tigris\nIn addition to enumeration units, the TIGER/Line database produced by the Census Bureau includes geographic features. These features consist of several datasets for use in thematic mapping and spatial analysis, such as transportation infrastructure and water features. In this exercise, you’ll acquire and plot roads and water data with tigris.\n\n# Plot area water features for Lane County, Oregon\nlane_water <- area_water(state = \"OR\", county = \"Lane\")\nplot(lane_water)\n\n\n\n# Plot primary & secondary roads for the state of New Hampshire\nnh_roads <- primary_secondary_roads(state = \"NH\")\nplot(nh_roads)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#understanding-the-structure-of-tigris-objects",
    "href": "datacamp/us_census/us_census.html#understanding-the-structure-of-tigris-objects",
    "title": "Census data in r with tidycensus",
    "section": "Understanding the structure of tigris objects",
    "text": "Understanding the structure of tigris objects\nBy default, tigris returns objects of class SpatialDataFrame from the sp package. Objects of class Spatial represent components of spatial data in different slots, which include descriptions of the object’s geometry, attributes, and coordinate system. In this exercise, we’ll briefly examine the structure of objects returned by tigris functions.\n\n# Check the class of the data\nclass(co_counties)\n\n[1] \"sf\"         \"data.frame\"\n\n# Take a look at the information in the data slot\nhead(co_counties)\n\nSimple feature collection with 6 features and 17 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -108.3811 ymin: 36.99961 xmax: -102.0448 ymax: 41.0026\nGeodetic CRS:  NAD83\n    STATEFP COUNTYFP COUNTYNS GEOID     NAME        NAMELSAD LSAD CLASSFP MTFCC\n23       08      109 00198170 08109 Saguache Saguache County   06      H1 G4020\n107      08      115 00198173 08115 Sedgwick Sedgwick County   06      H1 G4020\n124      08      017 00198124 08017 Cheyenne Cheyenne County   06      H1 G4020\n163      08      027 00198129 08027   Custer   Custer County   06      H1 G4020\n200      08      067 00198148 08067 La Plata La Plata County   06      H1 G4020\n228      08      111 00198171 08111 San Juan San Juan County   06      H1 G4020\n    CSAFP CBSAFP METDIVFP FUNCSTAT      ALAND   AWATER    INTPTLAT     INTPTLON\n23   <NA>   <NA>     <NA>        A 8206547699  4454510 +38.0316514 -106.2346662\n107  <NA>   <NA>     <NA>        A 1419419015  3530746 +40.8715679 -102.3553579\n124  <NA>   <NA>     <NA>        A 4605713960  8166129 +38.8356456 -102.6017914\n163  <NA>   <NA>     <NA>        A 1913031975  3364150 +38.1019955 -105.3735123\n200  <NA>  20420     <NA>        A 4376255277 25642579 +37.2873673 -107.8397178\n228  <NA>   <NA>     <NA>        A 1003660672  2035929 +37.7810492 -107.6702567\n                          geometry\n23  MULTIPOLYGON (((-105.8093 3...\n107 MULTIPOLYGON (((-102.2091 4...\n124 MULTIPOLYGON (((-102.547 38...\n163 MULTIPOLYGON (((-105.7969 3...\n200 MULTIPOLYGON (((-107.7124 3...\n228 MULTIPOLYGON (((-107.9751 3...\n\n# Check the coordinate system of the data"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#tigerline-and-cartographic-boundary-files",
    "href": "datacamp/us_census/us_census.html#tigerline-and-cartographic-boundary-files",
    "title": "Census data in r with tidycensus",
    "section": "TIGER/Line and cartographic boundary files",
    "text": "TIGER/Line and cartographic boundary files\nIn addition to its TIGER/Line shapefiles, the US Census Bureau releases cartographic boundary shapefiles for enumeration units. TIGER/Line shapefiles correspond to legal boundaries of units, which can include water area and in turn, may not be preferable for thematic mapping. The Census Bureau’s cartographic boundary shapefiles are clipped to the US shoreline and are generalized, which can make them superior for mapping projects. In this exercise, you’ll compare the TIGER/Line and cartographic boundary representations of the US state of Michigan.\n\n# Get a counties dataset for Michigan\nmi_tiger <- counties(\"MI\")\n\n# Get the equivalent cartographic boundary shapefile\nmi_cb <- counties(\"MI\", cb = TRUE)\n\n# Overlay the two on a plot to make a comparison\nplot(mi_tiger)\nplot(mi_cb, add = TRUE, border = \"red\")"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#getting-data-as-simple-features-objects",
    "href": "datacamp/us_census/us_census.html#getting-data-as-simple-features-objects",
    "title": "Census data in r with tidycensus",
    "section": "Getting data as simple features objects",
    "text": "Getting data as simple features objects\nThe sf package, which stands for simple features, promises to revolutionize the way that vector spatial data are handled within R. sf objects represent spatial data much like regular data frames, with a list-column that contains the geometry of the geographic dataset. tigris can return spatial data as simple features objects either by declaring class = “sf” within a function call or by setting as a global option. In this exercise, you’ll get acquainted with simple features in tigris.\n\n# Get data from tigris as simple features\noptions(tigris_class = \"sf\")\n\n# Get countries from Colorado and view the first few rows\ncolorado_sf <- counties(\"CO\")\nhead(colorado_sf)\n\nSimple feature collection with 6 features and 17 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -108.3811 ymin: 36.99961 xmax: -102.0448 ymax: 41.0026\nGeodetic CRS:  NAD83\n    STATEFP COUNTYFP COUNTYNS GEOID     NAME        NAMELSAD LSAD CLASSFP MTFCC\n23       08      109 00198170 08109 Saguache Saguache County   06      H1 G4020\n107      08      115 00198173 08115 Sedgwick Sedgwick County   06      H1 G4020\n124      08      017 00198124 08017 Cheyenne Cheyenne County   06      H1 G4020\n163      08      027 00198129 08027   Custer   Custer County   06      H1 G4020\n200      08      067 00198148 08067 La Plata La Plata County   06      H1 G4020\n228      08      111 00198171 08111 San Juan San Juan County   06      H1 G4020\n    CSAFP CBSAFP METDIVFP FUNCSTAT      ALAND   AWATER    INTPTLAT     INTPTLON\n23   <NA>   <NA>     <NA>        A 8206547699  4454510 +38.0316514 -106.2346662\n107  <NA>   <NA>     <NA>        A 1419419015  3530746 +40.8715679 -102.3553579\n124  <NA>   <NA>     <NA>        A 4605713960  8166129 +38.8356456 -102.6017914\n163  <NA>   <NA>     <NA>        A 1913031975  3364150 +38.1019955 -105.3735123\n200  <NA>  20420     <NA>        A 4376255277 25642579 +37.2873673 -107.8397178\n228  <NA>   <NA>     <NA>        A 1003660672  2035929 +37.7810492 -107.6702567\n                          geometry\n23  MULTIPOLYGON (((-105.8093 3...\n107 MULTIPOLYGON (((-102.2091 4...\n124 MULTIPOLYGON (((-102.547 38...\n163 MULTIPOLYGON (((-105.7969 3...\n200 MULTIPOLYGON (((-107.7124 3...\n228 MULTIPOLYGON (((-107.9751 3...\n\n# Plot its geometry column\nplot(colorado_sf$geometry)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#working-with-historic-shapefiles",
    "href": "datacamp/us_census/us_census.html#working-with-historic-shapefiles",
    "title": "Census data in r with tidycensus",
    "section": "Working with historic shapefiles",
    "text": "Working with historic shapefiles\nTo ensure clean integration with the tidycensus package - which you’ll learn about in the next chapter - tigris defaults to returning shapefiles that correspond to the year of the most recently-released ACS data. However, you may want boundary files for other years. tigris allows R users to obtain shapefiles for 1990, 2000, and 2010 through 2017, which represent many boundary changes over time. In this exercise, you’ll use tigris to explore how Census tract boundaries have changed in Williamson County, Texas between 1990 and 2016.\n\n# Get a historic Census tract shapefile from 1990 for Williamson County, Texas\nwilliamson90 <- tracts(state = \"TX\", county = \"Williamson\", \n                       cb = TRUE, year = 1990)\n\n# Compare with a current dataset for 2016\nwilliamson16 <- tracts(state = \"TX\", county = \"Williamson\", \n                       cb = TRUE, year = 2016)\n\n# Plot the geometry to compare the results                       \npar(mfrow = c(1, 2))\nplot(williamson90$geometry)\nplot(williamson16$geometry)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#combining-datasets-of-the-same-tigris-type",
    "href": "datacamp/us_census/us_census.html#combining-datasets-of-the-same-tigris-type",
    "title": "Census data in r with tidycensus",
    "section": "Combining datasets of the same tigris type",
    "text": "Combining datasets of the same tigris type\nOften, datasets from the US Census Bureau are available by state, which means they are available by state from tigris as well. In many instances, you’ll want to combine datasets for multiple states. For example, an analysis of the Portland, Oregon metropolitan area would include areas in both Oregon and Washington north of the Columbia River; however, these areas are represented in different Census files. In this exercise, you’ll learn how to combine datasets with the rbind_tigris() function.\n\n# Get Census tract boundaries for Oregon and Washington\nor_tracts <- tracts(\"OR\", cb = TRUE)\nwa_tracts <- tracts(\"WA\", cb = TRUE)\n\n# Check the tigris attributes of each object\nattr(or_tracts, \"tigris\")\n\n[1] \"tract\"\n\nattr(wa_tracts, \"tigris\")\n\n[1] \"tract\"\n\n# Combine the datasets then plot the result\nor_wa_tracts <- rbind_tigris(or_tracts, wa_tracts)\nplot(or_wa_tracts$geometry)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#getting-data-for-multiple-states",
    "href": "datacamp/us_census/us_census.html#getting-data-for-multiple-states",
    "title": "Census data in r with tidycensus",
    "section": "Getting data for multiple states",
    "text": "Getting data for multiple states\nIn the previous exercise, you learned how to combine datasets with the rbind_tigris() function. If you need data for more than two states, however, this process can get tedious. In this exercise, you’ll learn how to generate a list of datasets for multiple states with the tidyverse map() function, and combine those datasets with rbind_tigris().\n\n# Generate a vector of state codes and assign to new_england\nnew_england <- c(\"ME\", \"NH\", \"VT\", \"MA\")\n\n# Iterate through the states and request tract data for state\nne_tracts <- map(new_england, function(x) {\n  tracts(state = x, cb = TRUE)\n}) %>%\n  rbind_tigris()\n\nplot(ne_tracts$geometry)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#joining-data-from-an-external-data-frame",
    "href": "datacamp/us_census/us_census.html#joining-data-from-an-external-data-frame",
    "title": "Census data in r with tidycensus",
    "section": "Joining data from an external data frame",
    "text": "Joining data from an external data frame\nWhen working with geographic data in R, you’ll commonly want to join attribute information from an external dataset to it for mapping and spatial analysis. The sf package enables the use of the tidyverse *_join() functions for simple features objects for this purpose. In this exercise, you’ll learn how to join data to a spatial dataset of legislative boundaries for the Texas House of Representatives that you’ve obtained using tigris.\n\n# Get boundaries for Texas and set the house parameter\ntx_house <- state_legislative_districts(state = \"TX\", house = \"lower\", cb = TRUE)\n\n# Merge data on legislators to their corresponding boundaries\ntx_joined <- left_join(tx_house, tx_members, by = c(\"NAME\" = \"District\"))\n\nhead(tx_joined)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#plotting-simple-features-with-geom_sf",
    "href": "datacamp/us_census/us_census.html#plotting-simple-features-with-geom_sf",
    "title": "Census data in r with tidycensus",
    "section": "Plotting simple features with geom_sf()",
    "text": "Plotting simple features with geom_sf()\nThe newest version of ggplot2 includes a geom_sf() function to plot simple features objects natively. This allows you to make maps using familiar ggplot2 syntax! In this exercise, you’ll walk through the process of creating a map with ggplot2 step-by-step.\n\n# Plot the legislative district boundaries\nggplot(tx_joined) + \n  geom_sf()\n\n# Set fill aesthetic to map areas represented by Republicans and Democrats\nggplot(tx_joined, aes(fill = Party)) + \n  geom_sf()\n\n# Set values so that Republican areas are red and Democratic areas are blue\nggplot(tx_joined, aes(fill = Party)) + \n  geom_sf() + \n  scale_fill_manual(values = c(\"R\" = \"red\", \"D\" = \"blue\"))"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#customizing-geom_sf-plots",
    "href": "datacamp/us_census/us_census.html#customizing-geom_sf-plots",
    "title": "Census data in r with tidycensus",
    "section": "Customizing geom_sf() plots",
    "text": "Customizing geom_sf() plots\nAs you’ve learned in previous chapters, it is a good idea to clean up and format your ggplot2 visualizations before sharing with others. In this exercise, you’ll make some modifications to your map of Texas House districts such as removing the gridlines and adding an informative title.\n\n# Draw a ggplot without gridlines and with an informative title\nggplot(tx_joined, aes(fill = Party)) + \n  geom_sf() + \n  coord_sf(crs = 3083, datum = NA) + \n  scale_fill_manual(values = c(\"R\" = \"red\", \"D\" = \"blue\")) + \n  theme_minimal(base_size = 16) + \n  labs(title = \"State House Districts in Texas\")"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#getting-simple-feature-geometry",
    "href": "datacamp/us_census/us_census.html#getting-simple-feature-geometry",
    "title": "Census data in r with tidycensus",
    "section": "Getting simple feature geometry",
    "text": "Getting simple feature geometry\ntidycensus can obtain simple feature geometry for many geographies by adding the argument geometry = TRUE. In this exercise, you’ll obtain a dataset of median housing values for owner-occupied units by Census tract in Orange County, California with simple feature geometry included.\n\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(sf)\n\n# Get dataset with geometry set to TRUE\norange_value <- get_acs(geography = \"tract\", state = \"CA\", \n                    county = \"Orange\", \n                    variables = \"B25077_001\", \n                    geometry = TRUE)\n\n# Plot the estimate to view a map of the data\nplot(orange_value[\"estimate\"])"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#joining-data-from-tigris-and-tidycensus",
    "href": "datacamp/us_census/us_census.html#joining-data-from-tigris-and-tidycensus",
    "title": "Census data in r with tidycensus",
    "section": "Joining data from tigris and tidycensus",
    "text": "Joining data from tigris and tidycensus\nGeometry is currently supported in tidycensus for geographies in the core Census hierarchy - state, county, Census tract, block group, and block - as well as zip code tabulation areas. However, you may be interested in mapping data for other geographies. In this case, you can download the equivalent boundary file from the Census Bureau using the tigris package and join your demographic data to it for mapping.\n\n# Get an income dataset for Idaho by school district\nidaho_income <- get_acs(geography = \"school district (unified)\", \n                        variables = \"B19013_001\", \n                        state = \"ID\")\n\n# Get a school district dataset for Idaho\nidaho_school <- school_districts(state = \"ID\", type = \"unified\", class = \"sf\")\n\n# Join the income dataset to the boundaries dataset\nid_school_joined <- left_join(idaho_school, idaho_income, by = \"GEOID\")\n\nplot(id_school_joined[\"estimate\"])"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#shifting-alaska-and-hawaii-geometry",
    "href": "datacamp/us_census/us_census.html#shifting-alaska-and-hawaii-geometry",
    "title": "Census data in r with tidycensus",
    "section": "Shifting Alaska and Hawaii geometry",
    "text": "Shifting Alaska and Hawaii geometry\nAnalysts will commonly want to map data for the entire United States by state or county; however, this can be difficult by default as Alaska and Hawaii are distant from the continental United States. A common solution is to rescale and shift Alaska and Hawaii for mapping purposes, which is supported in tidycensus. You’ll learn how to do this in this exercise.\n\n# Get a dataset of median home values from the 1-year ACS\nstate_value <- get_acs(geography = \"state\", \n                       variables = \"B25077_001\", \n                       survey = \"acs1\", \n                       geometry = TRUE, \n                       shift_geo = TRUE)\n\n# Plot the dataset to view the shifted geometry\nplot(state_value[\"estimate\"])"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#making-a-choropleth-map",
    "href": "datacamp/us_census/us_census.html#making-a-choropleth-map",
    "title": "Census data in r with tidycensus",
    "section": "Making a choropleth map",
    "text": "Making a choropleth map\nChoropleth maps, which visualize statistical variation through the shading of areas, are among the most popular ways to map demographic data. Census or ACS data acquired with tidycensus can be mapped in this way in ggplot2 with geom_sf using the estimate column as a fill aesthetic. In this exercise, you’ll make a choropleth map with ggplot2 of median owner-occupied home values by Census tract for Marin County, California.\n\n# Create a choropleth map with ggplot\nggplot(marin_value, aes(fill = estimate)) + \n  geom_sf()"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#modifying-map-colors",
    "href": "datacamp/us_census/us_census.html#modifying-map-colors",
    "title": "Census data in r with tidycensus",
    "section": "Modifying map colors",
    "text": "Modifying map colors\nggplot2 version 3.0 integrated the viridis color palettes, which are perceptually uniform and legible to colorblind individuals and in black and white. For these reasons, the viridis palettes have become very popular for data visualization, including for choropleth mapping. In this exercise, you’ll learn how to use the viridis palettes for choropleth mapping in ggplot2.\n\n# Set continuous viridis palettes for your map\nggplot(marin_value, aes(fill = estimate, color = estimate)) + \n  geom_sf() + \n  scale_fill_viridis_c() +  \n  scale_color_viridis_c()"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#customizing-the-map-output",
    "href": "datacamp/us_census/us_census.html#customizing-the-map-output",
    "title": "Census data in r with tidycensus",
    "section": "Customizing the map output",
    "text": "Customizing the map output\nNow that you’ve chosen an appropriate color palette for your choropleth map of median home values by Census tract in Marin County, you’ll want to customize the output. In this exercise, you’ll clean up some map elements and add some descriptive information to provide context to your map.\n\n# Set the color guide to FALSE and add a subtitle and caption to your map\nggplot(marin_value, aes(fill = estimate, color = estimate)) + \n  geom_sf() + \n  scale_fill_viridis_c(labels = scales::dollar) +  \n  scale_color_viridis_c(guide = FALSE) + \n  theme_minimal() + \n  coord_sf(crs = 26911, datum = NA) + \n  labs(title = \"Median owner-occupied housing value by Census tract\", \n       subtitle = \"Marin County, California\", \n       caption = \"Data source: 2012-2016 ACS.\\nData acquired with the R tidycensus package.\", \n       fill = \"ACS estimate\")"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#graduated-symbol-maps",
    "href": "datacamp/us_census/us_census.html#graduated-symbol-maps",
    "title": "Census data in r with tidycensus",
    "section": "Graduated symbol maps",
    "text": "Graduated symbol maps\nThere are many other effective ways to map statistical data besides choropleth maps. One such example is the graduated symbol map, which represents statistical variation by differently-sized symbols. In this exercise, you’ll learn how to use the st_centroid() tool in the sf package to create points at the center of each state to be used as inputs to a graduated symbol map in ggplot2.\n\nlibrary(sf)\n\n# Generate point centers\ncenters <- st_centroid(state_value)\n\n# Set size parameter and the size range\nggplot() + \n  geom_sf(data = state_value, fill = \"white\") + \n  geom_sf(data = centers, aes(size = estimate), shape = 21, \n          fill = \"lightblue\", alpha = 0.7, show.legend = \"point\") + \n  scale_size_continuous(range = c(1, 20))"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#faceted-maps-with-ggplot2",
    "href": "datacamp/us_census/us_census.html#faceted-maps-with-ggplot2",
    "title": "Census data in r with tidycensus",
    "section": "Faceted maps with ggplot2",
    "text": "Faceted maps with ggplot2\nOne of the most powerful features of ggplot2 is its support for faceted plotting, in which multiple plots are generated for unique values of a column in the data. Faceted maps can be produced with geom_sf() in this way as well if tidycensus data are in tidy format. In this exercise, you’ll produce faceted maps that show the racial and ethnic geography of Washington, DC from the 2010 decennial Census.\n\n# Check the first few rows of the loaded dataset dc_race\nhead(dc_race)\n\n# Remove the gridlines and generate faceted maps\nggplot(dc_race, aes(fill = percent, color = percent)) + \n  geom_sf() + \n  coord_sf(datum = NA) + \n  facet_wrap(~variable)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#interactive-visualization-with-mapview",
    "href": "datacamp/us_census/us_census.html#interactive-visualization-with-mapview",
    "title": "Census data in r with tidycensus",
    "section": "Interactive visualization with mapview",
    "text": "Interactive visualization with mapview\nThe mapview R package allows R users to interactively map spatial datasets in one line of R code. This makes it an essential tool for exploratory spatial data analysis in R. In this exercise, you’ll learn how to quickly map tidycensus data interactively using mapview and your Orange County, California median housing values dataset.\n\n# Add a legend to your map\nm <- mapview(orange_value, \n         zcol = \"estimate\", \n         legend = TRUE)\nm@map"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#generating-random-dots-with-sf",
    "href": "datacamp/us_census/us_census.html#generating-random-dots-with-sf",
    "title": "Census data in r with tidycensus",
    "section": "Generating random dots with sf",
    "text": "Generating random dots with sf\nDot-density maps are created by randomly placing dots within areas where each dot is proportional to a certain number of observations. In this exercise, you’ll learn how to create dots in this way with the sf package using the st_sample() function. You will generate dots that are proportional to about 100 people in the decennial Census, and then you will group the dots to speed up plotting with ggplot2.\n\n# Generate dots, create a group column, and group by group column\ndc_dots <- map(c(\"White\", \"Black\", \"Hispanic\", \"Asian\"), function(group) {\n  dc_race %>%\n    filter(variable == group) %>%\n    st_sample(., size = .$value / 100) %>%\n    st_sf() %>%\n    group_by(group = group) \n}) %>%\n  reduce(rbind) %>%\n  group_by(group) %>%\n  summarize()"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#obtaining-data-for-cartography-with-tigris",
    "href": "datacamp/us_census/us_census.html#obtaining-data-for-cartography-with-tigris",
    "title": "Census data in r with tidycensus",
    "section": "Obtaining data for cartography with tigris",
    "text": "Obtaining data for cartography with tigris\nBefore making your dot-density map of Washington, DC with ggplot2, it will be useful to acquire some ancillary cartographic data with the tigris package that will help map viewers understand what you’ve visualized. These datasets will include major roads in DC; area water features; and the boundary of the District of Columbia, which you’ll use as a background in your map.\n\n# Filter the DC roads object for major roads only\ndc_roads <- roads(\"DC\", \"District of Columbia\") %>%\n  filter(RTTYP %in% c(\"I\", \"S\", \"U\"))\n\n# Get an area water dataset for DC\ndc_water <- area_water(\"DC\", \"District of Columbia\")\n\n# Get the boundary of DC\ndc_boundary <- counties(\"DC\", cb = TRUE)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#making-a-dot-density-map-with-ggplot2",
    "href": "datacamp/us_census/us_census.html#making-a-dot-density-map-with-ggplot2",
    "title": "Census data in r with tidycensus",
    "section": "Making a dot-density map with ggplot2",
    "text": "Making a dot-density map with ggplot2\nIn your final exercise of this course, you are going to put together the datasets you’ve acquired and generated into a dot-density map with ggplot2. You’ll plot your generated dots and ancillary datasets with geom_sf(), and add some informative map elements to create a cartographic product.\n\n# Plot your datasets and give your map an informative caption\nggplot() + \n  geom_sf(data = dc_boundary, color = NA, fill = \"white\") + \n  geom_sf(data = dc_dots, aes(color = group, fill = group), size = 0.1) + \n  geom_sf(data = dc_roads, color = \"lightblue\", fill = \"lightblue\") + \n  geom_sf(data = dc_water, color = \"grey\") + \n  coord_sf(crs = 26918, datum = NA) + \n  scale_color_brewer(palette = \"Set1\", guide = FALSE) +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(title = \"The racial geography of Washington, DC\", \n       subtitle = \"2010 decennial U.S. Census\", \n       fill = \"\", \n       caption = \"1 dot = approximately 100 people.\\nData acquired with the R tidycensus and tigris packages.\")"
  },
  {
    "objectID": "datacamp/scalable_data_R/scalable_data_r.html",
    "href": "datacamp/scalable_data_R/scalable_data_r.html",
    "title": "Scalable Data Processing in R",
    "section": "",
    "text": "If you are processing all elements of two data sets, and one data set is bigger, then the bigger data set will take longer to process. However, it’s important to realize that how much longer it takes is not always directly proportional to how much bigger it is. That is, if you have two data sets and one is two times the size of the other, it is not guaranteed that the larger one will take twice as long to process. It could take 1.5 times longer or even four times longer. It depends on which operations are used to process the data set.\nIn this exercise, you’ll use the microbenchmark package, which was covered in the Writing Efficient R Code course.\nNote: Numbers are specified using scientific notation\n\n# Load the microbenchmark package\nlibrary(microbenchmark)\n\n# Compare the timings for sorting different sizes of vector\nmb <- microbenchmark(\n  # Sort a random normal vector length 1e5\n  \"1e5\" = sort(rnorm(1e5)),\n  # Sort a random normal vector length 2.5e5\n  \"2.5e5\" = sort(rnorm(2.5e5)),\n  # Sort a random normal vector length 5e5\n  \"5e5\" = sort(rnorm(5e5)),\n  \"7.5e5\" = sort(rnorm(7.5e5)),\n  \"1e6\" = sort(rnorm(1e6)),\n  times = 10\n)\n\n# Plot the resulting benchmark object\nplot(mb)"
  },
  {
    "objectID": "datacamp/scalable_data_R/scalable_data_r.html#reading-a-big.matrix-object",
    "href": "datacamp/scalable_data_R/scalable_data_r.html#reading-a-big.matrix-object",
    "title": "Scalable Data Processing in R",
    "section": "Reading a big.matrix object",
    "text": "Reading a big.matrix object\nIn this exercise, you’ll create your first file-backed big.matrix object using the read.big.matrix() function. The function is meant to look similar to read.table() but, in addition, it needs to know what type of numeric values you want to read (“char”, “short”, “integer”, “double”), it needs the name of the file that will hold the matrix’s data (the backing file), and it needs the name of the file to hold information about the matrix (a descriptor file). The result will be a file on the disk holding the value read in along with a descriptor file which holds extra information (like the number of columns and rows) about the resulting big.matrix object.\n\n# Load the bigmemory package\nlibrary(bigmemory)\n\n# Create the big.matrix object: x\nx <- read.big.matrix(\"mortgage-sample.csv\", header = TRUE, \n                     type = \"integer\", \n                     backingfile = \"mortgage-sample.bin\", \n                     descriptorfile = \"mortgage-sample.desc\")\n    \n# Find the dimensions of x\ndim(x)\n\n[1] 70000    16"
  },
  {
    "objectID": "datacamp/scalable_data_R/scalable_data_r.html#attaching-a-big.matrix-object",
    "href": "datacamp/scalable_data_R/scalable_data_r.html#attaching-a-big.matrix-object",
    "title": "Scalable Data Processing in R",
    "section": "Attaching a big.matrix object",
    "text": "Attaching a big.matrix object\nNow that the big.matrix object is on the disk, we can use the information stored in the descriptor file to instantly make it available during an R session. This means that you don’t have to reimport the data set, which takes more time for larger files. You can simply point the bigmemory package at the existing structures on the disk and begin accessing data without the wait.\n\n# Attach mortgage-sample.desc\nmort <- attach.big.matrix(\"mortgage-sample.desc\")\n\n# Find the dimensions of mort\ndim(mort)\n\n[1] 70000    16\n\n# Look at the first 6 rows of mort\nhead(mort)\n\n     enterprise record_number msa perc_minority tract_income_ratio\n[1,]          1           566   1             1                  3\n[2,]          1           116   1             3                  2\n[3,]          1           239   1             2                  2\n[4,]          1            62   1             2                  3\n[5,]          1           106   1             2                  3\n[6,]          1           759   1             3                  3\n     borrower_income_ratio loan_purpose federal_guarantee borrower_race\n[1,]                     1            2                 4             3\n[2,]                     1            2                 4             5\n[3,]                     3            8                 4             5\n[4,]                     3            2                 4             5\n[5,]                     3            2                 4             9\n[6,]                     2            2                 4             9\n     co_borrower_race borrower_gender co_borrower_gender num_units\n[1,]                9               2                  4         1\n[2,]                9               1                  4         1\n[3,]                5               1                  2         1\n[4,]                9               2                  4         1\n[5,]                9               3                  4         1\n[6,]                9               1                  2         2\n     affordability year type\n[1,]             3 2010    1\n[2,]             3 2008    1\n[3,]             4 2014    0\n[4,]             4 2009    1\n[5,]             4 2013    1\n[6,]             4 2010    1"
  },
  {
    "objectID": "datacamp/comm_tidyverse/rmarkdown_ex.html",
    "href": "datacamp/comm_tidyverse/rmarkdown_ex.html",
    "title": "The reduction in weekly working hours in Europe",
    "section": "",
    "text": "The International Labour Organization (ILO) has many data sets on working conditions. For example, one can look at how weekly working hours have been decreasing in many countries of the world, while monetary compensation has risen. In this report, the reduction in weekly working hours in European countries is analysed, and a comparison between 1996 and 2006 is made. All analysed countries have seen a decrease in weekly working hours since 1996 – some more than others."
  },
  {
    "objectID": "datacamp/comm_tidyverse/rmarkdown_ex.html#preparations",
    "href": "datacamp/comm_tidyverse/rmarkdown_ex.html#preparations",
    "title": "The reduction in weekly working hours in Europe",
    "section": "Preparations",
    "text": "Preparations\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(forcats)"
  },
  {
    "objectID": "datacamp/comm_tidyverse/rmarkdown_ex.html#analysis",
    "href": "datacamp/comm_tidyverse/rmarkdown_ex.html#analysis",
    "title": "The reduction in weekly working hours in Europe",
    "section": "Analysis",
    "text": "Analysis\n\nData\nThe herein used data can be found in the statistics database of the ILO. For the purpose of this course, it has been slightly preprocessed.\n\nload(url(\"http://s3.amazonaws.com/assets.datacamp.com/production/course_5807/datasets/ilo_data.RData\"))\n\nThe loaded data contains 380 rows.\n\n# Some summary statistics\nilo_data %>%\n  group_by(year) %>%\n  summarize(mean_hourly_compensation = mean(hourly_compensation),\n            mean_working_hours = mean(working_hours))\n\n# A tibble: 27 × 3\n   year  mean_hourly_compensation mean_working_hours\n   <fct>                    <dbl>              <dbl>\n 1 1980                      9.27               34.0\n 2 1981                      8.69               33.6\n 3 1982                      8.36               33.5\n 4 1983                      7.81               33.9\n 5 1984                      7.54               33.7\n 6 1985                      7.79               33.7\n 7 1986                      9.70               34.0\n 8 1987                     12.1                33.6\n 9 1988                     13.2                33.7\n10 1989                     13.1                33.5\n# … with 17 more rows\n\n  # pipe the above data frame into the knitr::kable function\n\nAs can be seen from the above table, the average weekly working hours of European countries have been decreasing since 1980.\n\n\nPreprocessing\nThe data is now filtered so it only contains the years 1996 and 2006 – a good time range for comparison.\n\nilo_data <- ilo_data %>%\n  filter(year == \"1996\" | year == \"2006\")\n  \n# Reorder country factor levels\nilo_data <- ilo_data %>%\n  # Arrange data frame first, so last is always 2006\n  arrange(year) %>%\n  # Use the fct_reorder function inside mutate to reorder countries by working hours in 2006\n  mutate(country = fct_reorder(country,\n                               working_hours,\n                               last))\n\n\n\nResults\nIn the following, a plot that shows the reduction of weekly working hours from 1996 to 2006 in each country is produced.\nFirst, a custom theme is defined.\n\n\n\nThen, the plot is produced.\n\n# Compute temporary data set for optimal label placement\nmedian_working_hours <- ilo_data %>%\n  group_by(country) %>%\n  summarize(median_working_hours_per_country = median(working_hours)) %>%\n  ungroup()\n\n# Have a look at the structure of this data set\nstr(median_working_hours)\n\ntibble [17 × 2] (S3: tbl_df/tbl/data.frame)\n $ country                         : Factor w/ 30 levels \"Netherlands\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ median_working_hours_per_country: num [1:17] 27 27.8 28.4 31 30.9 ...\n\n# Plot\nggplot(ilo_data) +\n  geom_path(aes(x = working_hours, y = country),\n            arrow = arrow(length = unit(1.5, \"mm\"), type = \"closed\")) +\n  # Add labels for values (both 1996 and 2006)\n  geom_text(\n        aes(x = working_hours,\n            y = country,\n            label = round(working_hours, 1),\n            hjust = ifelse(year == \"2006\", 1.4, -0.4)\n          ),\n        # Change the appearance of the text\n        size = 3,\n        family = \"Bookman\",\n        color = \"gray25\"\n   ) +\n  # Add labels for country\n  geom_text(data = median_working_hours,\n            aes(y = country,\n                x = median_working_hours_per_country,\n                label = country),\n            vjust = 2,\n            family = \"Bookman\",\n            color = \"gray25\") +\n  # Add titles\n  labs(\n    title = \"People work less in 2006 compared to 1996\",\n    subtitle = \"Working hours in European countries, development since 1996\",\n    caption = \"Data source: ILO, 2017\"\n  ) +\n  # Apply your theme \n  theme_ilo() +\n  # Remove axes and grids\n  theme(\n    axis.ticks = element_blank(),\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    panel.grid = element_blank(),\n    # Also, let's reduce the font size of the subtitle\n    plot.subtitle = element_text(size = 9)\n  ) +\n  # Reset coordinate system\n  coord_cartesian(xlim = c(25, 41))\n\n\n\n\n\n\n\n\n\nAn interesting correlation\nThe results of another analysis are shown here, even though they cannot be reproduced with the data at hand.\n\n\n\nThe relationship between weekly working hours and hourly compensation.\n\n\nAs you can see, there’s also an interesting relationship. The more people work, the less compensation they seem to receive, which seems kind of unfair. This is quite possibly related to other proxy variables like overall economic stability and performance of a country."
  },
  {
    "objectID": "datacamp/comm_tidyverse/tidyverse.html",
    "href": "datacamp/comm_tidyverse/tidyverse.html",
    "title": "Communicating with Data in the Tidyverse",
    "section": "",
    "text": "In the video, you have learned that the inner_join() function of dplyr needs to be given a “key” on which two data frames are joined. Actually, multiple keys that need to match can be specified. In this first exercise, you are going to join two data sets by two keys. The data frames ilo_hourly_compensation and ilo_working_hours are already loaded for you and are available in your workspace.\nThis course touches on a lot of concepts you may have forgotten, so if you ever need a quick refresher, download the tidyverse Cheat Sheet and keep it handy!\n\nlibrary(tidyverse)\nlibrary(data.table)\nload(\"ilo_working_hours.RData\")\nload(\"ilo_hourly_compensation.RData\")\n# Join both data frames\nilo_data <- ilo_hourly_compensation %>%\n  inner_join(ilo_working_hours, by = c(\"country\", \"year\"))\n\n# Count the resulting rows\nilo_data  %>% \n    count()\n\n# A tibble: 1 × 1\n      n\n  <int>\n1   612\n\n# Examine ilo_data\nilo_data %>% head()\n\n# A tibble: 6 × 4\n  country   year   hourly_compensation working_hours\n  <chr>     <chr>                <dbl>         <dbl>\n1 Australia 1980.0                8.44          34.6\n2 Canada    1980.0                8.87          34.8\n3 Denmark   1980.0               10.8           31.9\n4 Finland   1980.0                8.61          35.6\n5 France    1980.0                8.90          35.4\n6 Italy     1980.0                8.09          35.7"
  },
  {
    "objectID": "datacamp/comm_tidyverse/tidyverse.html#change-variable-types",
    "href": "datacamp/comm_tidyverse/tidyverse.html#change-variable-types",
    "title": "Communicating with Data in the Tidyverse",
    "section": "Change variable types",
    "text": "Change variable types\nFor displaying data in ggplot2 graphics, it is often helpful to convert all the variables to the right data type. Usually, categorical variables like country in this example should be converted to factors before plotting them. You can do so using as.factor(). In your data set, two columns are still of type “character” – use mutate() to turn them into factors.\n\n# Turn year and country into a factor\nilo_data_corrected <- ilo_data %>%\n  mutate(year = as.factor(as.integer(as.numeric(year))),\n        country = as.factor(country))\n\n# See the results\nilo_data_corrected %>% head()\n\n# A tibble: 6 × 4\n  country   year  hourly_compensation working_hours\n  <fct>     <fct>               <dbl>         <dbl>\n1 Australia 1980                 8.44          34.6\n2 Canada    1980                 8.87          34.8\n3 Denmark   1980                10.8           31.9\n4 Finland   1980                 8.61          35.6\n5 France    1980                 8.90          35.4\n6 Italy     1980                 8.09          35.7"
  },
  {
    "objectID": "datacamp/comm_tidyverse/tidyverse.html#filter-the-data-for-plotting",
    "href": "datacamp/comm_tidyverse/tidyverse.html#filter-the-data-for-plotting",
    "title": "Communicating with Data in the Tidyverse",
    "section": "Filter the data for plotting",
    "text": "Filter the data for plotting\nUse the filter() function of dplyr to remove non-European countries. A vector called european_countries has been created that contains all European countries which should remain in the data set. Here, the %in% operator introduced in the video might come in handy. Note that only a subset of European countries for which enough data exist is retained. For instance, Poland is missing.\n\nilo_data <- ilo_data %>%\n  mutate(year = as.integer(as.numeric(year)))\n\neuropean_countries <- c(\"Finland\", \"France\", \"Italy\", \"Norway\", \"Spain\", \"Sweden\", \n                        \"Switzerland\", \"United Kingdom\", \"Belgium\", \"Ireland\", \n                        \"Luxembourg\", \"Portugal\", \"Netherlands\", \"Germany\", \n                        \"Hungary\", \"Austria\", \"Czech Rep.\")\n\n\n# Only retain European countries\nilo_data <- ilo_data %>%\n  filter(country %in% european_countries)"
  },
  {
    "objectID": "datacamp/comm_tidyverse/tidyverse.html#some-summary-statistics",
    "href": "datacamp/comm_tidyverse/tidyverse.html#some-summary-statistics",
    "title": "Communicating with Data in the Tidyverse",
    "section": "Some summary statistics",
    "text": "Some summary statistics\nUse dplyrs group_by() and summarize() to compute summary statistics for both years.\n\n# Examine the structure of ilo_data\nstr(ilo_data)\n\ntibble [380 × 4] (S3: tbl_df/tbl/data.frame)\n $ country            : chr [1:380] \"Finland\" \"France\" \"Italy\" \"Norway\" ...\n $ year               : int [1:380] 1980 1980 1980 1980 1980 1980 1980 1980 1981 1981 ...\n $ hourly_compensation: num [1:380] 8.61 8.9 8.09 11.8 5.86 ...\n $ working_hours      : num [1:380] 35.6 35.4 35.7 30.4 36.8 ...\n\n# Group and summarize the data\nilo_data %>%\n  group_by(year) %>%\n  summarise(mean_hourly_compensation = mean(hourly_compensation),\n            mean_working_hours = mean(working_hours)) %>% \n    head()\n\n# A tibble: 6 × 3\n   year mean_hourly_compensation mean_working_hours\n  <int>                    <dbl>              <dbl>\n1  1980                     9.27               34.0\n2  1981                     8.69               33.6\n3  1982                     8.36               33.5\n4  1983                     7.81               33.9\n5  1984                     7.54               33.7\n6  1985                     7.79               33.7"
  },
  {
    "objectID": "datacamp/comm_tidyverse/tidyverse.html#a-basic-scatter-plot",
    "href": "datacamp/comm_tidyverse/tidyverse.html#a-basic-scatter-plot",
    "title": "Communicating with Data in the Tidyverse",
    "section": "A basic scatter plot",
    "text": "A basic scatter plot\nIn this exercise, you will create a very basic scatter plot with ggplot2. This is mostly a repetition of stuff you’ve learnt in the prerequisites for this course, so it should be easy for you.\n\n# Filter for 2006\nplot_data <- ilo_data %>%\n  filter(year == 2006)\n  \n# Create the scatter plot\nggplot(plot_data) +\n  geom_point(aes(x = working_hours, y = hourly_compensation))"
  },
  {
    "objectID": "datacamp/comm_tidyverse/tidyverse.html#add-labels-to-the-plot",
    "href": "datacamp/comm_tidyverse/tidyverse.html#add-labels-to-the-plot",
    "title": "Communicating with Data in the Tidyverse",
    "section": "Add labels to the plot",
    "text": "Add labels to the plot\nAs mentioned in the video, you’re going to enhance the plot from the previous exercise by adding a title, a subtitle, and a caption to the plot as well as giving the axes meaningful names. You’re going to use the labs() function to do this – try using ?labs in the console to see the available options.\n\n# Create the plot\nilo_plot <- ggplot(plot_data) +\n  geom_point(aes(x = working_hours, y = hourly_compensation)) +\n  # Add labels\n  labs(\n    x = \"Working hours per week\",\n    y = \"Hourly compensation\",\n    title = \"The more people work, the less compensation they seem to receive\",\n    subtitle = \"Working hours and hourly compensation in European countries, 2006\",\n    caption = \"Data source: ILO, 2017\"\n  )\n\nilo_plot"
  },
  {
    "objectID": "datacamp/comm_tidyverse/tidyverse.html#apply-a-default-theme",
    "href": "datacamp/comm_tidyverse/tidyverse.html#apply-a-default-theme",
    "title": "Communicating with Data in the Tidyverse",
    "section": "Apply a default theme",
    "text": "Apply a default theme\nAs you’ve learnt in the videos, ggplot2 comes with a set of predefined themes. Try out some of them!\n\n# Add a different theme\nilo_plot +\n  theme_minimal()"
  },
  {
    "objectID": "datacamp/comm_tidyverse/tidyverse.html#change-the-appearance-of-titles",
    "href": "datacamp/comm_tidyverse/tidyverse.html#change-the-appearance-of-titles",
    "title": "Communicating with Data in the Tidyverse",
    "section": "Change the appearance of titles",
    "text": "Change the appearance of titles\nBesides applying defined theme presets, you can tweak your plot even more by altering different style attributes of it. Hint: You can reuse and overwrite the ilo_plot variable generated in the previous exercise – the current plot is already shown in the window on the right.\n\nilo_plot <- ilo_plot +\n  theme_minimal() +\n  # Customize the \"minimal\" theme with another custom \"theme\" call\n  theme(\n    text = element_text(family = \"Bookman\"),\n    title = element_text(color = \"gray25\"),\n    plot.caption = element_text(color = \"gray30\"),\n    plot.subtitle = element_text(size = 12)\n  )\n\n# Render the plot object\nilo_plot"
  },
  {
    "objectID": "datacamp/comm_tidyverse/tidyverse.html#alter-background-color-and-add-margins",
    "href": "datacamp/comm_tidyverse/tidyverse.html#alter-background-color-and-add-margins",
    "title": "Communicating with Data in the Tidyverse",
    "section": "Alter background color and add margins",
    "text": "Alter background color and add margins\nFurther customize the look of your plot with more arguments to the theme function call. You can continue working on your ilo_plot object created in the last exercise.\n\nChange the overall background color of your plot to “gray95”.\nThis time, another function than element_text is needed – one for rectangular plot elements. Rewatch the video to know which.\nAdd margins to the plot: 5mm top and bottom, 10mm to the left and the right.\nThe margins need to be specified in the following order: top, right, bottom, and left.\n\n\nilo_plot +\n  # \"theme\" calls can be stacked upon each other, so this is already the third call of \"theme\"\n  theme(\n    plot.background = element_rect(fill = \"gray95\"),\n    plot.margin = unit(c(5, 10, 5, 10), units = \"mm\")\n  )"
  },
  {
    "objectID": "datacamp/comm_tidyverse/tidyverse.html#prepare-the-data-set-for-the-faceted-plot",
    "href": "datacamp/comm_tidyverse/tidyverse.html#prepare-the-data-set-for-the-faceted-plot",
    "title": "Communicating with Data in the Tidyverse",
    "section": "Prepare the data set for the faceted plot",
    "text": "Prepare the data set for the faceted plot\nYou’re now going to prepare your data set for producing the faceted scatter plot in the next exercise, as mentioned in the video. For this, the data set needs to contain only the years 1996 and 2006, because your plot will only have two facets. ilo_data has been pre-loaded for you.\n\n# Filter ilo_data to retain the years 1996 and 2006\nilo_datap <- ilo_data %>%\n  filter(year == 1996 | year == 2006)\n# Again, you save the plot object into a variable so you can save typing later on\nilo_plot <- ggplot(ilo_datap, aes(x = working_hours, y = hourly_compensation)) +\n  geom_point() +\n   labs(\n    x = \"Working hours per week\",\n    y = \"Hourly compensation\",\n    title = \"The more people work, the less compensation they seem to receive\",\n    subtitle = \"Working hours and hourly compensation in European countries, 2006\",\n    caption = \"Data source: ILO, 2017\"\n  ) +\n  # Add facets here\n  facet_grid(.~year)\n \nilo_plot"
  },
  {
    "objectID": "datacamp/comm_tidyverse/tidyverse.html#define-your-own-theme-function",
    "href": "datacamp/comm_tidyverse/tidyverse.html#define-your-own-theme-function",
    "title": "Communicating with Data in the Tidyverse",
    "section": "Define your own theme function",
    "text": "Define your own theme function\nIn the video you saw how a lot of typing can be saved by replacing code chunks with function calls. You saw how a function is usually defined, now you will apply this knowledge in order to make your previous two theme() calls reusable.\n\n# Define your own theme function below\ntheme_ilo <- function() {\n   theme_minimal() +\n  theme(\n    text = element_text(family = \"Bookman\", color = \"gray25\"),\n    plot.subtitle = element_text(size = 12),\n    plot.caption = element_text(color = \"gray30\"),\n    plot.background = element_rect(fill = \"gray95\"),\n    plot.margin = unit(c(5, 10, 5, 10), units = \"mm\")\n  )\n}"
  },
  {
    "objectID": "datacamp/comm_tidyverse/tidyverse.html#apply-the-new-theme-function-to-the-plot",
    "href": "datacamp/comm_tidyverse/tidyverse.html#apply-the-new-theme-function-to-the-plot",
    "title": "Communicating with Data in the Tidyverse",
    "section": "Apply the new theme function to the plot",
    "text": "Apply the new theme function to the plot\nOnce you have created your own theme_ilo() function, it is time to apply it to a plot object. In the video you saw that theme() calls can be chained. You’re going to make use of this and add another theme() call to adjust some peculiarities of the faceted plot.\n\n# Apply your theme function (dont't forget to call it with parentheses!)\nilo_plot <- ilo_plot +\n  theme_ilo()\n\n# Examine ilo_plot\nilo_plot\n\n\n\nilo_plot +\n  # Add another theme call\n  theme(\n    # Change the background fill and color\n    strip.background = element_rect(fill = \"gray60\", color = 'gray95'),\n    # Change the color of the text\n    strip.text = element_text(color = \"white\")\n  )"
  },
  {
    "objectID": "datacamp/comm_tidyverse/tidyverse.html#a-basic-dot-plot",
    "href": "datacamp/comm_tidyverse/tidyverse.html#a-basic-dot-plot",
    "title": "Communicating with Data in the Tidyverse",
    "section": "A basic dot plot",
    "text": "A basic dot plot\nAs shown in the video, use only geom_path() to create the basic structure of the dot plot.\n\n# Create the dot plot\nggplot(ilo_datap, aes(working_hours, country))+\ngeom_path()"
  },
  {
    "objectID": "datacamp/comm_tidyverse/tidyverse.html#add-arrows-to-the-lines-in-the-plot",
    "href": "datacamp/comm_tidyverse/tidyverse.html#add-arrows-to-the-lines-in-the-plot",
    "title": "Communicating with Data in the Tidyverse",
    "section": "Add arrows to the lines in the plot",
    "text": "Add arrows to the lines in the plot\nInstead of labeling years, use the arrow argument of the geom_path() call to show the direction of change. The arrows will point from 1996 to 2006, because that’s how the data set is ordered. The arrow() function takes two arguments: The first is length, which can be specified with a unit() call, which you might remember from previous exercises. The second is type which defines how the arrow head will look.\n\nggplot(ilo_datap) +\n  geom_path(aes(x = working_hours, y = country),\n  # Add an arrow to each path\n  arrow = arrow(length = unit(1.5, \"mm\"), type = \"closed\"))"
  },
  {
    "objectID": "datacamp/comm_tidyverse/tidyverse.html#add-some-labels-to-each-country",
    "href": "datacamp/comm_tidyverse/tidyverse.html#add-some-labels-to-each-country",
    "title": "Communicating with Data in the Tidyverse",
    "section": "Add some labels to each country",
    "text": "Add some labels to each country\nA nice thing that can be added to plots are annotations or labels, so readers see the value of each data point displayed in the plot panel. This often makes axes obsolete, an advantage you’re going to use in the last exercise of this chapter. These labels are usually added with geom_text() or geom_label(). The latter adds a background to each label, which is not needed here.\n\nggplot(ilo_datap) +\n  geom_path(aes(x = working_hours, y = country),\n            arrow = arrow(length = unit(1.5, \"mm\"), type = \"closed\")) +\n  # Add a geom_text() geometry\n  geom_text(\n          aes(x = working_hours,\n              y = country,\n              label = round(working_hours, 1))\n        )"
  },
  {
    "objectID": "datacamp/comm_tidyverse/tidyverse.html#reordering-elements-in-the-plot",
    "href": "datacamp/comm_tidyverse/tidyverse.html#reordering-elements-in-the-plot",
    "title": "Communicating with Data in the Tidyverse",
    "section": "Reordering elements in the plot",
    "text": "Reordering elements in the plot\nAs shown in the video, use mutate() and fct_reorder() to change the factor level ordering of a variable.\n\nilo_datap <- ilo_datap %>%\n    mutate(country = as.factor(country))\n\n\nlibrary(forcats)\n\n# Reorder country factor levels\nilo_datap <- ilo_datap %>%\n  # Arrange data frame\n  arrange(year) %>%\n  # Reorder countries by working hours in 2006\n  mutate(country = fct_reorder(country,\n                               working_hours,\n                               last))\n\n# Plot again\nggplot(ilo_datap) +\n  geom_path(aes(x = working_hours, y = country),\n            arrow = arrow(length = unit(1.5, \"mm\"), type = \"closed\")) +\n    geom_text(\n          aes(x = working_hours,\n              y = country,\n              label = round(working_hours, 1))\n          )"
  },
  {
    "objectID": "datacamp/comm_tidyverse/tidyverse.html#correct-ugly-label-positions",
    "href": "datacamp/comm_tidyverse/tidyverse.html#correct-ugly-label-positions",
    "title": "Communicating with Data in the Tidyverse",
    "section": "Correct ugly label positions",
    "text": "Correct ugly label positions\nThe labels still kind of overlap with the lines in the dot plot. Use a conditional hjust aesthetic in order to better place them, and change their appearance.\n\n# Save plot into an object for reuse\nilo_dot_plot <- ggplot(ilo_datap) +\n  geom_path(aes(x = working_hours, y = country),\n            arrow = arrow(length = unit(1.5, \"mm\"), type = \"closed\")) +\n    # Specify the hjust aesthetic with a conditional value\n    geom_text(\n          aes(x = working_hours,\n              y = country,\n              label = round(working_hours, 1),\n              hjust = ifelse(year == \"2006\", 1.4, -0.4)\n            ),\n          # Change the appearance of the text\n          size = 3,\n          family = \"Bookman\",\n          color = \"gray25\"\n          )\n\nilo_dot_plot"
  },
  {
    "objectID": "datacamp/comm_tidyverse/tidyverse.html#change-the-viewport-so-labels-dont-overlap-with-plot-border",
    "href": "datacamp/comm_tidyverse/tidyverse.html#change-the-viewport-so-labels-dont-overlap-with-plot-border",
    "title": "Communicating with Data in the Tidyverse",
    "section": "Change the viewport so labels don’t overlap with plot border",
    "text": "Change the viewport so labels don’t overlap with plot border\nUse a function introduced in the previous video to change the viewport of the plotting area. Also apply your custom theme.\n\n# Reuse ilo_dot_plot\nilo_dot_plot <- ilo_dot_plot +\n  # Add labels to the plot\n  labs(\n    x = \"Working hours per week\",\n    y = \"Country\",\n    title = \"People work less in 2006 compared to 1996\",\n    subtitle = \"Working hours in European countries, development since 1996\",\n    caption = \"Data source: ILO, 2017\"\n  ) +\n  # Apply your theme\n  theme_ilo() +\n  # Change the viewport\n  coord_cartesian(xlim = c(25, 41))\n  \n# View the plot\nilo_dot_plot"
  },
  {
    "objectID": "datacamp/comm_tidyverse/tidyverse.html#optimizing-the-plot-for-mobile-devices",
    "href": "datacamp/comm_tidyverse/tidyverse.html#optimizing-the-plot-for-mobile-devices",
    "title": "Communicating with Data in the Tidyverse",
    "section": "Optimizing the plot for mobile devices",
    "text": "Optimizing the plot for mobile devices\nThe x-axis title is already quite superfluous because you’ve added labels for both years. You’ll now add country labels to the plot, so all of the axes can be removed.\nIn this exercise, you’re going to encounter something that is probably new to you: New data sets can be given to single geometries like geom_text(), so these geometries don’t use the data set given to the initial ggplot() call. In this exercise, you are going to need this because you only want to add one label to each arrow. If you were to use the original data set ilo_data, two labels would be added because there are two observations for each country in the data set, one for 1996 and one for 2006.\n\n# Compute temporary data set for optimal label placement\nmedian_working_hours <- ilo_datap %>%\n  group_by(country) %>%\n  summarize(median_working_hours_per_country = median(working_hours)) %>%\n  ungroup()\n\n# Have a look at the structure of this data set\nstr(median_working_hours)\n\ntibble [17 × 2] (S3: tbl_df/tbl/data.frame)\n $ country                         : Factor w/ 17 levels \"Netherlands\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ median_working_hours_per_country: num [1:17] 27 27.8 28.4 31 30.9 ...\n\nilo_dot_plot +\n  # Add label for country\n  geom_text(data = median_working_hours,\n            aes(y = country,\n                x = median_working_hours_per_country,\n                label = country),\n            vjust = 2,\n            family = \"Bookman\",\n            color = \"gray25\") +\n  # Remove axes and grids\n  theme(\n    axis.ticks = element_blank(),\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    panel.grid = element_blank(),\n    # Also, let's reduce the font size of the subtitle\n    plot.subtitle = element_text(size = 9)\n  )"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html",
    "href": "posts/loan_prediction/loan_prediction.html",
    "title": "Loan Prediction Zindi",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(caret)\nlibrary(ggthemes)\nlibrary(lubridate)\nlibrary(DT)"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#reading-data",
    "href": "posts/loan_prediction/loan_prediction.html#reading-data",
    "title": "Loan Prediction Zindi",
    "section": "Reading data",
    "text": "Reading data\n\ntrainperf <- fread(\"trainperf.csv\")\ntraindemographics <-fread(\"traindemographics.csv\")\ntestperf <-fread(\"testperf.csv\")\ntestdemographics <-fread(\"testdemographics.csv\")\nSampleSubmission <- fread(\"SampleSubmission.csv\")"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#combine-test-set-and-train-set",
    "href": "posts/loan_prediction/loan_prediction.html#combine-test-set-and-train-set",
    "title": "Loan Prediction Zindi",
    "section": "Combine test set and train set",
    "text": "Combine test set and train set\n\nmakes it easier for cleaning purposes\n\n\ntrain_data <- merge(traindemographics, \n                    trainperf, all.y = T,\n                    by = \"customerid\")\n\ntest_data <- merge(testdemographics, \n                   testperf, all.y = T, \n                   by = \"customerid\")\n\ntrain_data[, approveddate := as.character(approveddate)]\ntrain_data[, creationdate := as.character(creationdate)]\nloan_data <- rbind(train_data[, set := \"train\"],\n                   test_data[, set := \"test\"], fill = T)\n\nloan_data %>% head() %>%\n  datatable(options = list(scrollX= TRUE))"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#bad-loans-distribution",
    "href": "posts/loan_prediction/loan_prediction.html#bad-loans-distribution",
    "title": "Loan Prediction Zindi",
    "section": "Bad loans distribution",
    "text": "Bad loans distribution\n\nloan_data[!is.na(good_bad_flag), .N, by = .(good_bad_flag)] %>%\n    .[, perc := round(N/sum(N) * 100, 2)] %>%\n    \n     ggplot(aes(good_bad_flag, perc, fill =good_bad_flag)) +\n     geom_bar(stat = \"identity\") +\n     geom_text(aes(good_bad_flag, perc, label = paste(perc, \"%\"),\n                   vjust = .05, hjust = .5),\n               size = 4)+\n     theme_hc()+\n    labs(title = \"Percentage of bad loans\")+\n     scale_fill_colorblind(name = \"\")+\n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#clean-string-variables",
    "href": "posts/loan_prediction/loan_prediction.html#clean-string-variables",
    "title": "Loan Prediction Zindi",
    "section": "Clean string variables",
    "text": "Clean string variables\n\nconvert empty chars to NA\n\n\nchars <- c(\"bank_account_type\", \"bank_name_clients\", \n           \"bank_branch_clients\", \"employment_status_clients\",\n           \"level_of_education_clients\")\n\nloan_data[, (chars) := lapply(.SD, function(x) ifelse(x == \"\" | x == \" \", NA, x)), .SDcols = chars]"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#missing-values-distribution",
    "href": "posts/loan_prediction/loan_prediction.html#missing-values-distribution",
    "title": "Loan Prediction Zindi",
    "section": "Missing values distribution",
    "text": "Missing values distribution\n\nnaVals <- colSums(is.na(loan_data))/nrow(loan_data) * 100 \n\nwithNa <- naVals[naVals>0]\nnms_na <- names(withNa)\nmissing_perc <- data.table(variables = nms_na, perc = withNa)"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#missing-values-distribution-plot",
    "href": "posts/loan_prediction/loan_prediction.html#missing-values-distribution-plot",
    "title": "Loan Prediction Zindi",
    "section": "Missing values distribution plot",
    "text": "Missing values distribution plot\n\nggplot(missing_perc, aes( reorder(variables, perc), perc))+\n    geom_bar(stat = \"identity\") +\n    theme_fivethirtyeight()+\n    coord_flip()"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#knn-imputation",
    "href": "posts/loan_prediction/loan_prediction.html#knn-imputation",
    "title": "Loan Prediction Zindi",
    "section": "KNN imputation",
    "text": "KNN imputation\n\nloan_data[, loannumber := as.numeric(loannumber)]\nmissing_var_del <- missing_perc[perc>50, variables]\n## KNN imputation\nlibrary(VIM)\nloan_data[, (dates):= NULL]\nloan_data[, referredby:= NULL]\nloan_data <- kNN(loan_data,useImputedDist = FALSE, k =10)\n\nsetDT(loan_data)\nnms_all <- names(loan_data)\nnms_imp <- nms_all[grepl(\"_imp$\", nms_all)]\n\n\nloan_data[, (nms_imp) := lapply(.SD, \n                            function(x) ifelse(x == FALSE, 0, 1)),\n      .SDcols = nms_imp]\n\ncol_sum_imp <- loan_data[, colSums(.SD), .SDcols = nms_imp]\ncol_sum_imp <- names(col_sum_imp[col_sum_imp == 0])\n#var_importants <- fread(\"var_importanta.csv\")\nloan_data[, (col_sum_imp) := NULL]\n\nloan_data %>% head() %>%\n  datatable(options = list(scrollX= TRUE))"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#more-cleaning",
    "href": "posts/loan_prediction/loan_prediction.html#more-cleaning",
    "title": "Loan Prediction Zindi",
    "section": "More cleaning",
    "text": "More cleaning\n\neg standardizing\n\n\nloan_data[, good_bad_flag := factor(good_bad_flag, levels = c(\"Bad\", \"Good\"))]\n\nnms_del1 <- c(\"set_imp\", \" good_bad_flag_imp\", \n              \"approve_year\",\"aprove_month\", \n              \"year\",\"systemloanid\" )\n\nloan_data[, (nms_del1) := NULL]\n\nclass_nms <- sapply(loan_data, class)\nnums <- class_nms[class_nms == \"numeric\"] %>% names()\nnums <- nums[!grepl(\"_imp|good_bad_flag\", nums)]\n\nzero_one <- function(x){\n    \n    myvar <- (x - min(x))/(max(x) - min(x))\n    \n    myvar\n}\n\n\nloan_data[, (nums) := lapply(.SD, zero_one), .SDcols = nums]\n\n\ntrain_data <- loan_data[set == \"train\"]\ntrain_data[, set:= NULL]\ntest_data <- loan_data[set == \"test\"]\ntest_data[, set:= NULL]"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#imbalanced-datasets",
    "href": "posts/loan_prediction/loan_prediction.html#imbalanced-datasets",
    "title": "Loan Prediction Zindi",
    "section": "Imbalanced Datasets",
    "text": "Imbalanced Datasets\n\nmake the proportion of good loans to be the same as that of bad loans\n\n\ntrain_bad <- train_data[good_bad_flag == \"Bad\"]\ntrain_good <- train_data[good_bad_flag == \"Good\"]\nn_row = nrow(train_good)\nn_row_dead = nrow(train_bad)\n\nset.seed(200)\nnot_sample <- sample(1:n_row, n_row_dead)\ntrain_good <- train_good[not_sample]\ntrain_sampled <- rbind(train_bad, train_good)"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#tuning-parameters-and-validation-sets",
    "href": "posts/loan_prediction/loan_prediction.html#tuning-parameters-and-validation-sets",
    "title": "Loan Prediction Zindi",
    "section": "Tuning parameters and validation sets",
    "text": "Tuning parameters and validation sets\n\n## Model Cross validation\n\nset.seed(100)\ncv_fold <- createFolds(train_sampled$good_bad_flag, k = 10)\n\ntrain_ctrl <- trainControl(method = \"cv\",\n                        number = 10,\n                        summaryFunction = twoClassSummary,\n                        classProbs = TRUE,\n                        allowParallel=T,\n                        index = cv_fold,\n                        verboseIter = FALSE,\n                        savePredictions = TRUE,\n                        search = \"grid\")\n\n\nxgb_grid <- expand.grid(nrounds = c(50,100),\n                        eta = 0.4,\n                        max_depth = c(2,3),\n                        gamma = c(0, .01),\n                        colsample_bytree = c(0.6, .8, 1),\n                        min_child_weight = 1,\n                        subsample =  c(.5, .8, 1))\n\n \nranger_grid <- expand.grid(splitrule = c(\"extratrees\", \"gini\"),\n                        mtry = c(10, 20, (ncol(train_data) - 2) ),\n                        min.node.size = c(1, 5))\n\nsvm_grid <- expand.grid(C = c( 1, 3, 5, 20),\n                        sigma = seq(0.001, 0.524 , length.out = 7))"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#model-fitting",
    "href": "posts/loan_prediction/loan_prediction.html#model-fitting",
    "title": "Loan Prediction Zindi",
    "section": "Model Fitting",
    "text": "Model Fitting\n\nlibrary(caret)\nlibrary(caretEnsemble)\nlibrary(tictoc)\n#tuneGrid= xgb_grid\ntic()\n\nmodel_list <- caretList(\n   good_bad_flag~.,\n    data=train_sampled[, .SD, .SDcols = !\"customerid\"],\n    metric = \"ROC\",\n    trControl=train_ctrl,\n    tuneList = list(caretModelSpec(method=\"xgbTree\",tuneGrid= xgb_grid ),\n                    caretModelSpec(method = \"svmRadial\", tuneGrid = svm_grid),\n                    caretModelSpec(method=\"ranger\", tuneGrid= ranger_grid)\n\n                   )\n)\n\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:16] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:17] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:18] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:19] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:20] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:21] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:24] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:27:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\ntoc()\n\n72.401 sec elapsed"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#model-output",
    "href": "posts/loan_prediction/loan_prediction.html#model-output",
    "title": "Loan Prediction Zindi",
    "section": "Model Output",
    "text": "Model Output\n\nmodel_list\n\n$xgbTree\neXtreme Gradient Boosting \n\n1906 samples\n  25 predictor\n   2 classes: 'Bad', 'Good' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 190, 191, 191, 190, 190, 191, ... \nResampling results across tuning parameters:\n\n  max_depth  gamma  colsample_bytree  subsample  nrounds  ROC        Sens     \n  2          0.00   0.6               0.5         50      0.6662383  0.6304141\n  2          0.00   0.6               0.5        100      0.6572987  0.6300688\n  2          0.00   0.6               0.8         50      0.7369880  0.6798483\n  2          0.00   0.6               0.8        100      0.7232959  0.6649271\n  2          0.00   0.6               1.0         50      0.7683784  0.6881188\n  2          0.00   0.6               1.0        100      0.7442732  0.6896402\n  2          0.00   0.8               0.5         50      0.6696103  0.6234342\n  2          0.00   0.8               0.5        100      0.6621089  0.6305437\n  2          0.00   0.8               0.8         50      0.7418353  0.6778659\n  2          0.00   0.8               0.8        100      0.7204278  0.6741425\n  2          0.00   0.8               1.0         50      0.7624512  0.6794903\n  2          0.00   0.8               1.0        100      0.7395393  0.6803123\n  2          0.00   1.0               0.5         50      0.6783603  0.6367149\n  2          0.00   1.0               0.5        100      0.6653246  0.6332128\n  2          0.00   1.0               0.8         50      0.7345845  0.6762321\n  2          0.00   1.0               0.8        100      0.7138288  0.6732095\n  2          0.00   1.0               1.0         50      0.7657585  0.6864901\n  2          0.00   1.0               1.0        100      0.7375936  0.6777533\n  2          0.01   0.6               0.5         50      0.6758601  0.6269224\n  2          0.01   0.6               0.5        100      0.6744400  0.6359037\n  2          0.01   0.6               0.8         50      0.7369448  0.6757706\n  2          0.01   0.6               0.8        100      0.7187844  0.6610769\n  2          0.01   0.6               1.0         50      0.7652788  0.6852036\n  2          0.01   0.6               1.0        100      0.7443608  0.6888261\n  2          0.01   0.8               0.5         50      0.6764669  0.6355476\n  2          0.01   0.8               0.5        100      0.6695862  0.6347397\n  2          0.01   0.8               0.8         50      0.7345265  0.6774004\n  2          0.01   0.8               0.8        100      0.7150567  0.6627131\n  2          0.01   0.8               1.0         50      0.7678358  0.6813589\n  2          0.01   0.8               1.0        100      0.7455061  0.6853307\n  2          0.01   1.0               0.5         50      0.6616132  0.6155033\n  2          0.01   1.0               0.5        100      0.6580491  0.6166669\n  2          0.01   1.0               0.8         50      0.7332131  0.6791509\n  2          0.01   1.0               0.8        100      0.7133721  0.6715786\n  2          0.01   1.0               1.0         50      0.7657585  0.6864901\n  2          0.01   1.0               1.0        100      0.7376530  0.6781030\n  3          0.00   0.6               0.5         50      0.6588931  0.6242449\n  3          0.00   0.6               0.5        100      0.6565096  0.6350874\n  3          0.00   0.6               0.8         50      0.7132400  0.6610800\n  3          0.00   0.6               0.8        100      0.6939222  0.6494219\n  3          0.00   0.6               1.0         50      0.7429646  0.6832275\n  3          0.00   0.6               1.0        100      0.7214971  0.6719249\n  3          0.00   0.8               0.5         50      0.6562285  0.6184012\n  3          0.00   0.8               0.5        100      0.6493876  0.6143253\n  3          0.00   0.8               0.8         50      0.7138199  0.6686590\n  3          0.00   0.8               0.8        100      0.6935337  0.6557247\n  3          0.00   0.8               1.0         50      0.7394236  0.6772880\n  3          0.00   0.8               1.0        100      0.7153968  0.6652808\n  3          0.00   1.0               0.5         50      0.6539294  0.6217836\n  3          0.00   1.0               0.5        100      0.6522004  0.6335637\n  3          0.00   1.0               0.8         50      0.7101406  0.6572371\n  3          0.00   1.0               0.8        100      0.6929877  0.6468590\n  3          0.00   1.0               1.0         50      0.7401911  0.6733219\n  3          0.00   1.0               1.0        100      0.7166700  0.6637643\n  3          0.01   0.6               0.5         50      0.6671624  0.6318144\n  3          0.01   0.6               0.5        100      0.6592827  0.6307664\n  3          0.01   0.6               0.8         50      0.7141084  0.6729710\n  3          0.01   0.6               0.8        100      0.6952477  0.6530348\n  3          0.01   0.6               1.0         50      0.7445266  0.6815998\n  3          0.01   0.6               1.0        100      0.7198063  0.6673785\n  3          0.01   0.8               0.5         50      0.6618728  0.6298400\n  3          0.01   0.8               0.5        100      0.6533159  0.6279765\n  3          0.01   0.8               0.8         50      0.7222956  0.6681943\n  3          0.01   0.8               0.8        100      0.7016452  0.6495441\n  3          0.01   0.8               1.0         50      0.7406605  0.6810144\n  3          0.01   0.8               1.0        100      0.7178792  0.6697106\n  3          0.01   1.0               0.5         50      0.6665283  0.6352079\n  3          0.01   1.0               0.5        100      0.6565039  0.6264591\n  3          0.01   1.0               0.8         50      0.7105078  0.6666780\n  3          0.01   1.0               0.8        100      0.6937690  0.6532691\n  3          0.01   1.0               1.0         50      0.7401911  0.6733219\n  3          0.01   1.0               1.0        100      0.7164916  0.6620161\n  Spec     \n  0.6037165\n  0.5848224\n  0.6650267\n  0.6589682\n  0.7064131\n  0.6722540\n  0.6161931\n  0.5996274\n  0.6802977\n  0.6466054\n  0.7080471\n  0.6771547\n  0.6161836\n  0.5942692\n  0.6635097\n  0.6365897\n  0.7097970\n  0.6724921\n  0.6208489\n  0.6161854\n  0.6694575\n  0.6539558\n  0.7082801\n  0.6714398\n  0.6230542\n  0.6116407\n  0.6661924\n  0.6508100\n  0.7184261\n  0.6762236\n  0.6204955\n  0.6105888\n  0.6532582\n  0.6322779\n  0.7097970\n  0.6721421\n  0.6000864\n  0.5875071\n  0.6405471\n  0.6216647\n  0.6729572\n  0.6421796\n  0.6017250\n  0.5918260\n  0.6447518\n  0.6203862\n  0.6716801\n  0.6469637\n  0.5911162\n  0.5854055\n  0.6476704\n  0.6275003\n  0.6668946\n  0.6432318\n  0.6010356\n  0.5963682\n  0.6432289\n  0.6280713\n  0.6756417\n  0.6502307\n  0.5978783\n  0.5921692\n  0.6540742\n  0.6403193\n  0.6681798\n  0.6506951\n  0.6017315\n  0.5953160\n  0.6375188\n  0.6245785\n  0.6668946\n  0.6434663\n\nTuning parameter 'eta' was held constant at a value of 0.4\nTuning\n parameter 'min_child_weight' was held constant at a value of 1\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were nrounds = 50, max_depth = 2, eta\n = 0.4, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample\n = 1.\n\n$svmRadial\nSupport Vector Machines with Radial Basis Function Kernel \n\n1906 samples\n  25 predictor\n   2 classes: 'Bad', 'Good' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 190, 191, 191, 190, 190, 191, ... \nResampling results across tuning parameters:\n\n  C   sigma       ROC        Sens       Spec     \n   1  0.00100000  0.4453725  0.6618623  0.3382454\n   1  0.08816667  0.7152954  0.6745925  0.6279675\n   1  0.17533333  0.7434152  0.6814689  0.6669078\n   1  0.26250000  0.7485946  0.6939500  0.6558324\n   1  0.34966667  0.7478360  0.6703960  0.6876617\n   1  0.43683333  0.7448373  0.6903251  0.6565333\n   1  0.52400000  0.7411549  0.6622398  0.6854481\n   3  0.00100000  0.5169879  0.3957886  0.6104006\n   3  0.08816667  0.7709807  0.6933649  0.7289349\n   3  0.17533333  0.7703822  0.7100361  0.7018791\n   3  0.26250000  0.7627949  0.6998931  0.6925529\n   3  0.34966667  0.7542314  0.6849706  0.6945330\n   3  0.43683333  0.7472168  0.6848579  0.6835785\n   3  0.52400000  0.7416933  0.6692354  0.6909249\n   5  0.00100000  0.5147158  0.4808126  0.5518522\n   5  0.08816667  0.7759460  0.7072356  0.7147025\n   5  0.17533333  0.7671066  0.7010583  0.7066601\n   5  0.26250000  0.7567144  0.6959267  0.6903293\n   5  0.34966667  0.7496970  0.6873027  0.6881211\n   5  0.43683333  0.7443882  0.6944083  0.6730835\n   5  0.52400000  0.7398689  0.6895172  0.6673739\n  20  0.00100000  0.6099611  0.5602352  0.5816956\n  20  0.08816667  0.7625153  0.7050182  0.6930138\n  20  0.17533333  0.7544452  0.6991918  0.6857897\n  20  0.26250000  0.7492683  0.7051376  0.6648002\n  20  0.34966667  0.7451051  0.7011745  0.6648036\n  20  0.43683333  0.7417310  0.6963985  0.6629417\n  20  0.52400000  0.7378154  0.6819478  0.6743656\n\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were sigma = 0.08816667 and C = 5.\n\n$ranger\nRandom Forest \n\n1906 samples\n  25 predictor\n   2 classes: 'Bad', 'Good' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 190, 191, 191, 190, 190, 191, ... \nResampling results across tuning parameters:\n\n  splitrule   mtry  min.node.size  ROC        Sens       Spec     \n  extratrees  10    1              0.8205728  0.7009392  0.7853503\n  extratrees  10    5              0.8205268  0.6956963  0.7957282\n  extratrees  20    1              0.8238454  0.6958107  0.8036563\n  extratrees  20    5              0.8240704  0.6939440  0.8077343\n  extratrees  25    1              0.8253890  0.6955776  0.8089001\n  extratrees  25    5              0.8254390  0.6912639  0.8139148\n  gini        10    1              0.7936937  0.7079485  0.7232171\n  gini        10    5              0.7936630  0.7045684  0.7247345\n  gini        20    1              0.7816264  0.6994381  0.7175003\n  gini        20    5              0.7812459  0.6901102  0.7256622\n  gini        25    1              0.7779426  0.6919764  0.7172689\n  gini        25    5              0.7790383  0.6877773  0.7206492\n\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were mtry = 25, splitrule = extratrees\n and min.node.size = 5.\n\nattr(,\"class\")\n[1] \"caretList\""
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#roc-ci",
    "href": "posts/loan_prediction/loan_prediction.html#roc-ci",
    "title": "Loan Prediction Zindi",
    "section": "ROC CI",
    "text": "ROC CI\n\nresamples_models <- resamples(model_list)\n\ndotplot(resamples_models, metric = \"ROC\")"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#model-statistics",
    "href": "posts/loan_prediction/loan_prediction.html#model-statistics",
    "title": "Loan Prediction Zindi",
    "section": "Model Statistics",
    "text": "Model Statistics\n\nnms_models <- names(model_list)\nresamples_stat_list <- list()\nfor (j in 1:length(nms_models)) {\n  model1 = model_list[[j]]\n  resample_stata <- thresholder(model1, \n                              threshold = seq(.0, 1, by = 0.01), \n                              final = TRUE, \n                              statistics = \"all\") %>% setDT()\n  p= ggplot(resample_stata , aes(x = prob_threshold, y = F1, col = \"F1\")) + \n  geom_point() + \n  geom_point(aes(y = Sensitivity, col = \"Sensitivity\"))+\n  scale_x_continuous(breaks = seq(0, 1, by =.1))+\n    ggtitle(nms_models[j])\n  print(p)\n  resample_stata[, model:= nms_models[j]]\n  resamples_stat_list[[j]] = resample_stata\n}"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#roc-curve",
    "href": "posts/loan_prediction/loan_prediction.html#roc-curve",
    "title": "Loan Prediction Zindi",
    "section": "ROC CURVE",
    "text": "ROC CURVE\n\nresamples_combined <- rbindlist(resamples_stat_list, fill = TRUE)\nlibrary(plotly)\nggplotly(ggplot(resamples_combined  , aes(x = 1-Specificity, y = Recall, color = model)) + \n  geom_line(size = 1) + \n  #geom_point(aes(y = Sensitivity, col = \"Sensitivity\"))+\n  scale_x_continuous(breaks = seq(0, 1, by =.1)) +\n  ggtitle(paste0(\"ROC for models\"))+\n  scale_color_viridis_d())"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#precision-recall-curve",
    "href": "posts/loan_prediction/loan_prediction.html#precision-recall-curve",
    "title": "Loan Prediction Zindi",
    "section": "Precision Recall Curve",
    "text": "Precision Recall Curve\n\nggplotly(ggplot(resamples_combined ,\n                aes( x = Recall, y = Precision, color = model)) + \n  geom_line(size = 1) + \n  #geom_point(aes(y = Sensitivity, col = \"Sensitivity\"))+\n  scale_x_continuous(breaks = seq(0, 1, by =.1))+\n  scale_color_viridis_d()+\n  ggtitle(paste0(\"Precision recall curve\")))"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#iml-models",
    "href": "posts/loan_prediction/loan_prediction.html#iml-models",
    "title": "Loan Prediction Zindi",
    "section": "IML models",
    "text": "IML models\n\nlibrary(iml)\nX_pred <-train_sampled[, .SD, .SDcols = !c(\"customerid\", \"good_bad_flag\")] %>%\n  as.data.frame()\n\nnms_models <- names(model_list)\n\niml_models <- list()\n\nfor (i in 1:length(nms_models)) {\n  \n  chain_rf_a <- model_list[[i]]\n  pred <- function(chain_rf_a, train_sampled)  {\n    results <- predict(chain_rf_a, newdata = train_sampled, type = \"prob\")\n    return(results[[1L]])\n  }\n  \n  # it does not know how to deal with char values\n\n\n# get predicted values\n  iml_models[[i]] <- Predictor$new(model = chain_rf_a, \n                      data =X_pred,\n                      predict.function = pred,\n                      y = train_sampled$good_bad_flag)\n\n\n}"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#feature-importance-plots",
    "href": "posts/loan_prediction/loan_prediction.html#feature-importance-plots",
    "title": "Loan Prediction Zindi",
    "section": "Feature Importance plots",
    "text": "Feature Importance plots\n\nplots <- list()\nfor (i in 1:length(nms_models)) {\n  model_this = iml_models[[i]]\n  impa <- FeatureImp$new(model_this, loss = \"ce\")\n  var_importanta <-impa$results %>% data.table()\n\n  #write.csv(var_importanta, file = \"var_importanta.csv\", row.names = F)\n  setorder(var_importanta, -importance)\n  var10a <- var_importanta[1:20]\n  if(i == 2) write.csv(var10a, file = \"svm_var.csv\", row.names = F)\n  plots[[i]] <- ggplot(var10a, aes(reorder(feature,importance), importance))+\n  geom_point()+\n  ggtitle(nms_models[i])+\n   geom_linerange(aes(ymin=importance.05, ymax= importance.95), width=.3,\n                  position=position_dodge(width = .7)) +\n  coord_flip()\n  \n  \n}\n\nplots\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#shap-values-calculation",
    "href": "posts/loan_prediction/loan_prediction.html#shap-values-calculation",
    "title": "Loan Prediction Zindi",
    "section": "Shap Values calculation",
    "text": "Shap Values calculation\n\nnms <- names(model_list)\nids <- which(nms == \"ranger\")\nshap_list <- vector(\"list\", nrow(X_pred))\nmodel_list_shap <- list()\nmodel_this <- iml_models[[ids]]\n\ntic()\n\n#shap_list[[1]] <- shap_import\n\nfor (i in 1:nrow(X_pred)) {\n  shap <- Shapley$new(model_this,  x.interest = X_pred[i, ], sample.size = 30)\n  shap_import <-shap$results %>% data.table()\n  shap_import <- shap_import[class == \"Bad\"]\n  shap_list[[i]] <- shap_import[,\n                                customerid := train_sampled[i, customerid]]\n\n  }\ntoc()\n\n911.816 sec elapsed\n\nshap_values <- rbindlist(shap_list, fill = T)\n\nwrite.csv(shap_values, file = \"shap_values.csv\", row.names = F)"
  },
  {
    "objectID": "posts/loan_prediction/loan_prediction.html#shap-values-plot",
    "href": "posts/loan_prediction/loan_prediction.html#shap-values-plot",
    "title": "Loan Prediction Zindi",
    "section": "Shap Values plot",
    "text": "Shap Values plot\n\nlibrary(ggforce)\nshap_values <-  fread(\"shap_values.csv\")\n\nshap_values[, phi2 := abs(phi)]\nshap_imp <- shap_values[, .(Med = median(phi2),\n                            Mean = mean(phi2)), by = feature] %>%\n    setorder(-Mean)\nshap_imp <- shap_imp[1:20, ]\n\nshap_values <- shap_values[feature %in%shap_imp$feature]\n\nshap_values[, feature := factor(feature, levels = rev(shap_imp$feature) )]\n\nggplot(shap_values, aes(feature, phi,  color = phi.var))+\n  geom_sina()+\n  geom_hline(yintercept = 0) +\n  scale_color_gradient(low=\"#2187E3\", high=\"#F32858\", \n                       breaks=c(0,1), labels=c(\"Low\",\"High\"))+ \n  theme_bw() + \n    theme(axis.line.y = element_blank(), \n          axis.ticks.y = element_blank(), # remove axis line\n          legend.position=\"bottom\") +\n  coord_flip()"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Okay I’m excited to finally sit down and work on my blog!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/association_analysis/association_analysis.html",
    "href": "posts/association_analysis/association_analysis.html",
    "title": "Association analysis",
    "section": "",
    "text": "Maybe you have heard of a grocery store that carried out analysis and found out that men who buy diapers between 5pm to 7pm were more likely to buy beer. The grocery store then moved the beer isle closer to the diaper isle and beer sales increased by 35%. This is called association analysis which was motivated by retail store data.\nIn this blog I will explore the basics of association analysis. The goal is to find out:\n\nItems frequently bought together in association analysis this is called support. Let say you have ten transactions and in those ten 3 transactions have maize floor, rice and bread the the support for maize floor, rice and bread is 3/10 = 0.3. This is just marginal probability. In other terms the percentage of transactions these items were bought together.\nIn this example the support is written as Support({bread, maize floor} –> {rice} ). In general this is written as Support of item one and item 2 is Support({item1} –> {item 2}). Item 1 and item 2 may contain one or more items.\nWe also want to find out if someone bought a set of items what other set of item(s) were they likely to buy. In association analysis this is called confidence. In our above example let say that you find the proportion of transactions that contained maize floor and bread are 0.4. Then the confidence is the proportion of those transactions with maize floor, bread and rice/proportion of transactions that contained maize floor and bread. Then the confidence is 0.3/0.4 which is 0.75. In other word 75% of those who bought maize floor and bread also bought rice.\n\nConfidence in this example is denoted as Confidence({bread, maize floor} –> {rice} ) and in general this is Confidence({item 1} –> {item 2} ).\n\nThe lift refers to how the chances of rice being purchased increased given that maize floor and bread are purchased. So the lift of rice is confidence of rice/support(rice). Support of rice is the number of transactions that contain rice.\n\nLift({Item 1} -> {Item 2 }) = (Confidence(Item1 -> Item2)) / (Support(Item2))\n\n\nTo make sense of all these I’m going to use a bakery to find association rules between items bought manually and then towards the end I will use r package arules which uses apriori algorithm to find association between items bought. The data set is available on kaggle as BreadBasket_DMS. We start by first having a glimpse of this data set.\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(data.table)\nlibrary(DT)\n\ndat <- setDT(read.csv( \"BreadBasket_DMS.csv\" ))\n\ndat <- dat[Item != \"NONE\"]\nhead(dat[sample(1:nrow(dat), 10)]) %>% datatable()\n\n\n\n\n\n\n\n\nFirst step is to transform the data set into wide format. Column headers will be items sold in the bakery and the rows will be populated with 1 and 0 indicating whether that item was bought for that transaction.\n\ndat2 <- dcast(Date+Time+Transaction~Item, data = dat, fun.aggregate = length)\n#dat2[, NONE := NULL]\n\nsample_cols <- sample(4:ncol(dat2), 5)\n\nitem_names <- names(dat2)[4:97]\n\ndat2[, (item_names) := lapply(.SD, function(x) ifelse(x==0, 0, 1)), .SDcols = item_names]\n\nhead(dat2[, c(1:3, sample_cols), with = F]) %>% datatable()\n\n\n\n\n\n\n\n\n\nOn average a transaction has 2 items. The median is also 2, this shows that atleast 50% of the transactions contained 2 or more items and atleast 25% of the transactions have 1 item.\n\nnumber_items <- rowSums(dat2[, 4:97, with =F])\n\ndat2[, number_items := number_items]\n\nhist(number_items, col = \"black\", main = \"Number of items bought\")\n\n\n\nsumStats <- dat2 %>%\n    summarise(Average = round(mean(number_items), 2), Stdev = round(sd(number_items), 2),\n              Median= median(number_items),\n              Minimum = min(number_items), Maximum = max(number_items),\n              First_qurtile = quantile(number_items,0.25,  na.rm = T),\n              Third_qurtile=quantile(number_items,0.75,  na.rm = T))\n\ndatatable(sumStats)\n\n\n\n\n\n\n\n\n\nTable below shows top ten most bought items and about 47.84% of the transactions contained coffee. Coffee was the most popular item in this bakery followed by bread.\n\nn_transacts_item_in <- colSums(dat2[, item_names, with = F])\n\ndata.frame(item = names(n_transacts_item_in),\n           number =n_transacts_item_in) %>%\n    mutate(Percentage = round(number/nrow(dat2)*100, 2)) %>%\n    arrange(desc(number)) %>% head(10) %>% datatable()\n\n\n\n\n\n\nSince we have transformed the data in the wide format and every transaction is in it’s row we can visualize how the baskets look like. This is done by extracting the column names for the transactions where the value is 1. For each transaction, 1 represent that item being in that transaction.\n\nitems_bought <- apply(dat2[, 4:97, with =F], 1, paste, collapse = \"\", sep = \"\")\n\nlist_items <- vector(mode = \"list\", length = length(items_bought))\nfor (i in 1:length(items_bought)) {\n    index <- unlist(gregexpr(\"1\", items_bought[i]))\n    items_transaction_i <- item_names[index]\n    items_transaction_i <- paste(items_transaction_i, sep = \" \", collapse = \" , \")\n    list_items[[i]] <- items_transaction_i\n}\n\nhead(unlist(list_items))\n\n[1] \"Bread\"                         \"Scandinavian\"                 \n[3] \"Cookies , Hot chocolate , Jam\" \"Muffin\"                       \n[5] \"Bread , Coffee , Pastry\"       \"Medialuna , Muffin , Pastry\"  \n\n\nData frame below shows how the baskets look like. Only 10 randomly selected rows are displayed. Items_bought column shows the baskets.\n\ndat2[, items_bought := unlist(list_items) ]\n\nhead(dat2[sample(1:nrow(dat2), 10),\n          .(Transaction, number_items,items_bought)]) %>%\n  datatable()\n\n\n\n\n\n\nA small example which I will work out manually to see what is the support for ({coffee, bread} –> {jam}). Generally I want to see how many transactions contained these 3 items. 0.12% of the transactions contained {bread, coffee, jam}\n\nmy_item_set <- Hmisc::Cs(Coffee , Jam , Bread)\n\nidx_sample <- grep( \"Transactio|^Coffee$|^Jam$|^Bread$\", names(dat2))\n\n\n\nitem_set_dat <- dat2[, idx_sample, with = F] \n\n#some transaction bought more thanone of \nitem_set_dat[, (my_item_set) := lapply(.SD, function(x) ifelse(x==0, 0, 1)), .SDcols = my_item_set]\n\nitem_set_dat[, total_items := rowSums(item_set_dat[, 2:4, with = F]) ]\n\nsupport_coffee_bread_jam <- table(item_set_dat$total_items)[\"3\"]/9531 \n\nsupport_coffee_bread_jam\n\n          3 \n0.001154129 \n\n\nTo calculate confidence({bread, coffee} –> {jam}) we should also calculate the support of ({bread, coffee}) which is the prorpotion of bread and coffee appearing together in the transactions which is about 8.9% of the transactions.\n\nitem_set_dat[, coffee_bread := rowSums(item_set_dat[, c(\"Bread\", \"Coffee\"), with = F]) ]\n\nhead(item_set_dat, 2)\n\n   Transaction Bread Coffee Jam total_items coffee_bread\n1:           1     1      0   0           1            1\n2:           2     0      0   0           0            0\n\nsupport_coffee_bread <- table(item_set_dat$coffee_bread)[\"2\"]/9531 \n\nsupport_coffee_bread\n\n         2 \n0.08939251 \n\n\n1.3% of the people who bought bread and coffee also bought jam. This is the confidence of({bread, coffee} –> {jam}) For statisticians this can be translated as conditional probability. In conditional probability notations P(Jam/bread, coffee) which is probability you will buy jam given that you have already bought bread and coffee. In association analysis we have {bread, coffee} >>{jam} bread and coffee implies jam. So the confidence measures the strength/probability of this implication.\n\nconfidence <- support_coffee_bread_jam/support_coffee_bread \n\nconfidence * 100\n\n      3 \n1.29108 \n\n\nUsing package arules we find the 10 rules with the highest confidence in descending order. Confidence({Toast} –>{Coffee}) had the highest confidence of 0.70440252. About 70.44% of the transactions that contained toast also contained coffee.\n\nlibrary(arules)\n\ntransactions <- as(split(dat$Item, dat$Transaction), \"transactions\")\n\nassoc_rules <- apriori(transactions,\n                 parameter = list(supp = 0.02, conf = 0.04, target = \"rules\"))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n       0.04    0.1    1 none FALSE            TRUE       5    0.02      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 189 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[94 item(s), 9465 transaction(s)] done [0.00s].\nsorting and recoding items ... [19 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 done [0.00s].\nwriting ... [38 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\nassoc_rules <- sort(assoc_rules, by='confidence', decreasing = TRUE)\n\n\ninspect(assoc_rules[1:30]) #%>% broom::tidy() %>% kable\n\n     lhs                rhs         support    confidence coverage   lift     \n[1]  {Toast}         => {Coffee}    0.02366614 0.70440252 0.03359746 1.4724315\n[2]  {Medialuna}     => {Coffee}    0.03518225 0.56923077 0.06180666 1.1898784\n[3]  {Pastry}        => {Coffee}    0.04754358 0.55214724 0.08610671 1.1541682\n[4]  {Juice}         => {Coffee}    0.02060222 0.53424658 0.03856313 1.1167500\n[5]  {Sandwich}      => {Coffee}    0.03824617 0.53235294 0.07184363 1.1127916\n[6]  {Cake}          => {Coffee}    0.05472795 0.52695829 0.10385631 1.1015151\n[7]  {Cookies}       => {Coffee}    0.02820919 0.51844660 0.05441099 1.0837229\n[8]  {Hot chocolate} => {Coffee}    0.02958267 0.50724638 0.05832013 1.0603107\n[9]  {}              => {Coffee}    0.47839408 0.47839408 1.00000000 1.0000000\n[10] {Tea}           => {Coffee}    0.04986793 0.34962963 0.14263074 0.7308402\n[11] {Pastry}        => {Bread}     0.02916006 0.33865031 0.08610671 1.0349774\n[12] {}              => {Bread}     0.32720549 0.32720549 1.00000000 1.0000000\n[13] {Bread}         => {Coffee}    0.09001585 0.27510494 0.32720549 0.5750592\n[14] {Cake}          => {Tea}       0.02377179 0.22889115 0.10385631 1.6047813\n[15] {Cake}          => {Bread}     0.02334918 0.22482197 0.10385631 0.6870972\n[16] {Tea}           => {Bread}     0.02810354 0.19703704 0.14263074 0.6021813\n[17] {Coffee}        => {Bread}     0.09001585 0.18816254 0.47839408 0.5750592\n[18] {Tea}           => {Cake}      0.02377179 0.16666667 0.14263074 1.6047813\n[19] {}              => {Tea}       0.14263074 0.14263074 1.00000000 1.0000000\n[20] {Coffee}        => {Cake}      0.05472795 0.11439929 0.47839408 1.1015151\n[21] {Coffee}        => {Tea}       0.04986793 0.10424028 0.47839408 0.7308402\n[22] {}              => {Cake}      0.10385631 0.10385631 1.00000000 1.0000000\n[23] {Coffee}        => {Pastry}    0.04754358 0.09938163 0.47839408 1.1541682\n[24] {Bread}         => {Pastry}    0.02916006 0.08911850 0.32720549 1.0349774\n[25] {}              => {Pastry}    0.08610671 0.08610671 1.00000000 1.0000000\n[26] {Bread}         => {Tea}       0.02810354 0.08588957 0.32720549 0.6021813\n[27] {Coffee}        => {Sandwich}  0.03824617 0.07994700 0.47839408 1.1127916\n[28] {Coffee}        => {Medialuna} 0.03518225 0.07354240 0.47839408 1.1898784\n[29] {}              => {Sandwich}  0.07184363 0.07184363 1.00000000 1.0000000\n[30] {Bread}         => {Cake}      0.02334918 0.07135938 0.32720549 0.6870972\n     count\n[1]   224 \n[2]   333 \n[3]   450 \n[4]   195 \n[5]   362 \n[6]   518 \n[7]   267 \n[8]   280 \n[9]  4528 \n[10]  472 \n[11]  276 \n[12] 3097 \n[13]  852 \n[14]  225 \n[15]  221 \n[16]  266 \n[17]  852 \n[18]  225 \n[19] 1350 \n[20]  518 \n[21]  472 \n[22]  983 \n[23]  450 \n[24]  276 \n[25]  815 \n[26]  266 \n[27]  362 \n[28]  333 \n[29]  680 \n[30]  221 \n\n\nI hope with this small example you can now understand how association analysis works."
  },
  {
    "objectID": "posts/kenya_population/household_assets_2019census.html",
    "href": "posts/kenya_population/household_assets_2019census.html",
    "title": "Kenya Household Assets",
    "section": "",
    "text": "Kenya selected households assets 2019 census\nI figured that it will be important for me to do this to show why it is impossible for online learning to be adopted in Kenya.\nI used 2019 census data set which can be found here and the counties shape file can be found here\n\n#Packages used\n\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sf)\nlibrary(DT)\nlibrary(tmap)\nlibrary(ggthemes)\nlibrary(ggiraph)\n\n\n\nRead data set\n\nhousehold_assets <- fread(\"percentage-distribution-of-conventional-households-by-ownership-of-selected-household-assets-201.csv\")\n\n#\nkenya_shapefile <- st_read(\"County\") %>% setDT()\n\nReading layer `County' from data source \n  `/home/mburu/personal_projects/github_io_blog/posts/kenya_population/County' \n  using driver `ESRI Shapefile'\nSimple feature collection with 47 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 33.91182 ymin: -4.702271 xmax: 41.90626 ymax: 5.430648\nGeodetic CRS:  WGS 84\n\nkenya_shapefile[, county := tolower(COUNTY)]\n\n# rename column\nsetnames(household_assets, \n         c(\"County / Sub-County\", \"Conventional Households\"  ), \n         c(\"county\", \"households\"))\n\n\n\nSome minor cleaning\n\n# remove commas \nhousehold_assets[, households := as.numeric(gsub(\",|AR K    \", \"\", households))]\n\nhousehold_assets[,  county := tolower(county)]\n\nsub_county <- household_assets %>% \n    group_by(county) %>%\n    filter(households == max(households)) %>% setDT()\n\n\nsub_county_melt <- melt(sub_county, \n                        id.vars = c(\"county\", \"households\"))\n\n\n\n% of households with various assets 2019 census\n\nThis is overall data set for the whole country computer devices is ownership about 8.8% this just means that about 91% of the students can’t access online learning. This is is just a naive estimation the number could be higher.\n\n\nkenya_dat <- sub_county_melt[county == \"kenya\"]\n\np <- ggplot(kenya_dat, aes(variable, value, tooltip = paste(variable, \" : \", value))) +\n    geom_bar_interactive(stat = \"identity\", width = 0.5, fill =\"slategray2\"  ) +\n    geom_text_interactive(aes(variable, value, label = paste0(value, \"%\")),\n              position = position_dodge(width = 0.5),\n              vjust = 0.001, size = 3)+\n    labs(x = \"Household assets\", y = \"%\",\n         title = \"% of households with various assets 2019 census\",\n         caption = \"twitter\\n@mmburu_w\")+\n    theme_fivethirtyeight()+\n  theme(\n    axis.text = element_text(size = 7, angle = 45, vjust = 1, hjust =1),\n    plot.title = element_text_interactive(size =11)\n  )\np1 <- girafe(ggobj = p, width_svg = 7, height_svg = 4.5, \n  options = list(\n    opts_sizing(rescale = T) )\n  )\n\np1\n\n\n\n\n\n\n\n\nMerge shapefile with asset data sets\n\nsub_county_melt[county == \"elgeyo/marakwet\", county := \"keiyo-marakwet\"]\nsub_county_melt[county == \"tharaka-nithi\", county := \"tharaka\"]\nsub_county_melt[county ==  \"taita/taveta\", county := \"taita taveta\"]\nsub_county_melt[county ==  \"nairobi city\", county := \"nairobi\"]\n\ncounty_shapes <- merge(kenya_shapefile, sub_county_melt, by = \"county\") \n\nsetnames(county_shapes, \"value\", \"Percentage\")\n\n\n\nComputer devices data\n\ncomputer <- county_shapes[variable  %in% c(\"Desk Top\\nComputer/\\nLaptop/ Tablet\")]\n\n# this converts to sf object\ncomputer <- st_set_geometry(computer, \"geometry\")\n\n\n\nPercentage of households with computer devices per county\n\nThat is if a household owns a tablet, laptop or a desktop\n\n\n#ttm()\ntm_shape(computer)+\n    tm_borders(col = \"grey\")+\n    tm_fill(col = \"Percentage\")+\n    tm_layout(title = \"% of households with laptop/tablet/desk top \\n 2019 census\",\n              title.size = 1, title.position = c(0.3, 0.95))\n\n\n\n#ttm()\n\n\n\n% of households that can access internet\n\nThis looks like is internet access through mobile phones\n\n\ninternet <- county_shapes[variable == \"Internet\"]\n\ninternet <- st_set_geometry(internet, \"geometry\")\n\n#ttm()\ntm_shape(internet)+\n    tm_borders(col = \"grey\")+\n    tm_fill(col = \"Percentage\")+\n    tm_layout(title = \"% of households with internet acess\",\n              title.size = 1, title.position = c(0.3, 0.95))"
  },
  {
    "objectID": "posts/kenya_population/kenya_debt_and_dollar_price.html",
    "href": "posts/kenya_population/kenya_debt_and_dollar_price.html",
    "title": "USD-KES Hisotrical Data",
    "section": "",
    "text": "library(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(DT)\nlibrary(ggthemes)\nlibrary(patchwork)\nusd_kes_hist <- fread(\"USD_KES Historical Data.csv\")\n\n\nusd_kes_hist[, Date :=  as.Date(Date, format = \"%b %d,%Y\")]\n\n\nx <- as.Date(\"2002-12-01\")\nx_end <- as.Date(\"2003-01-01\")\n\ny <-90\ny_end <- 77.500\n\nx_uhuru <- as.Date(\"2013-03-09\")\nx_end_uhuru <- as.Date(\"2013-04-09\")\n\ny_uhuru <-105\ny_end_uhuru <- 85.600\n\ndollar <- ggplot(usd_kes_hist,aes(Date, Price))+\n    geom_line()+\n    scale_x_date(date_breaks = \"3 year\", date_labels = \"%b-%y\")+\n    labs(y = \"1 USD to KES\", title = \"Historical Prices USD to KES\")+\n  \n    annotate(\n    geom = \"curve\", x = x, y = y, xend = x_end, yend = y_end, \n    curvature = .3, arrow = arrow(length = unit(3, \"mm\"))\n  ) +\n  annotate(geom = \"text\", x = x, y = y,\n           label = \"Pres Kibaki \\n takes Office\", hjust = \"left\",\n           angle = 30)+\n    annotate(\n    geom = \"curve\", x = x_uhuru, y = y_uhuru, \n    xend = x_end_uhuru, yend = y_end_uhuru, \n    curvature = .1, arrow = arrow(length = unit(3, \"mm\"))\n  ) +\n  annotate(geom = \"text\", x = x_uhuru, y = y_uhuru+2,\n           label = \"Pres Uhuru \\n takes Office\", hjust = \"left\",\n           angle = 30)\ndollar\n\n\n\n\n\nkenya_debt <- read_csv( \"kenya_debt/Public Debt (Ksh Million).csv\") %>% setDT()\n\nkenya_debt[, Date := as.Date(paste(Year, Month, \"01\", sep = \"-\"), format = \"%Y-%B-%d\")]\nkenya_debt[, perc_external := round(`External Debt`/Total* 100, 1)]\nkenya_debt[, Total := Total/1000000]\n\n\nx <- as.Date(\"2002-12-01\")\nx_end <- as.Date(\"2003-01-01\")\n\ny <-2.000000\ny_end <- .6152281\n\nx_uhuru <- as.Date(\"2013-03-09\")\nx_end_uhuru <- as.Date(\"2013-04-09\")\n\ny_uhuru <-4.000000\ny_end_uhuru <- 1.8824059\n\n\ndebt <- ggplot(kenya_debt, aes(Date, Total)) +\n    geom_line()+\n  \n    scale_x_date(date_breaks = \"3 year\", date_labels = \"%b-%y\")+\n  \n    labs(title = \"Kenya Debt from 1999 to 2020 June\",\n         y = \"Total Debt in Trillions(KES)\")+\n  \n    annotate(\n    geom = \"curve\", x = x, y = y, xend = x_end, yend = y_end, \n    curvature = .3, arrow = arrow(length = unit(2, \"mm\"))\n  ) +\n  annotate(geom = \"text\", x = x, y = y,\n           label = \"Pres Kibaki \\n takes Office\", hjust = \"left\",\n           angle = 30)+\n  \n    annotate(\n    geom = \"curve\", x = x_uhuru, y = y_uhuru, \n    xend = x_end_uhuru, yend = y_end_uhuru, \n    curvature = -.1, arrow = arrow(length = unit(3, \"mm\"))\n  ) +\n  \n  annotate(geom = \"text\", x = x_uhuru, y = y_uhuru+.5,\n           label = \"Pres Uhuru \\n takes Office\", hjust = \"right\",\n           angle = 30)\n \n\ndebt\n\n\n\n\n\nmy_breaks <- seq(15, 70, 5)\nexternal <- ggplot(kenya_debt, aes(Date, perc_external)) +\n    geom_line()+\n  \n    scale_x_date(date_breaks = \"2 year\", date_labels = \"%Y\")+\n  \n    labs(title = \"Kenya External Debt % from 1999 to 2020 June\",\n         y = \"External Debt (%)\")+\n    theme_hc()+\n    scale_y_continuous(breaks = my_breaks )\n\nexternal\n\n\n\n\n\ndebt_data <- fread(\"poverty_data/IDS-DRSCountries_WLD_Data.csv\")\n\n\nnms_old <-debt_data[1,]  %>% as.character()\nnms_old\n\n [1] \"Country Name\"          \"Country Code\"          \"Counterpart-Area Name\"\n [4] \"Counterpart-Area Code\" \"Series Name\"           \"Series Code\"          \n [7] \"1970\"                  \"1971\"                  \"1972\"                 \n[10] \"1973\"                  \"1974\"                  \"1975\"                 \n[13] \"1976\"                  \"1977\"                  \"1978\"                 \n[16] \"1979\"                  \"1980\"                  \"1981\"                 \n[19] \"1982\"                  \"1983\"                  \"1984\"                 \n[22] \"1985\"                  \"1986\"                  \"1987\"                 \n[25] \"1988\"                  \"1989\"                  \"1990\"                 \n[28] \"1991\"                  \"1992\"                  \"1993\"                 \n[31] \"1994\"                  \"1995\"                  \"1996\"                 \n[34] \"1997\"                  \"1998\"                  \"1999\"                 \n[37] \"2000\"                  \"2001\"                  \"2002\"                 \n[40] \"2003\"                  \"2004\"                  \"2005\"                 \n[43] \"2006\"                  \"2007\"                  \"2008\"                 \n[46] \"2009\"                  \"2010\"                  \"2011\"                 \n[49] \"2012\"                  \"2013\"                  \"2014\"                 \n[52] \"2015\"                  \"2016\"                  \"2017\"                 \n[55] \"2018\"                  \"2019\"                  \"2020\"                 \n[58] \"2021\"                  \"2022\"                  \"2023\"                 \n[61] \"2024\"                  \"2025\"                  \"2026\"                 \n[64] \"2027\"                 \n\ndebt_data <-debt_data[-1,]\nnames(debt_data) <- nms_old\nnms_new <- nms_old %>% tolower() \nnms_new <- gsub(\"\\\\s|-\", \"_\", nms_new)\nnms_new\n\n [1] \"country_name\"          \"country_code\"          \"counterpart_area_name\"\n [4] \"counterpart_area_code\" \"series_name\"           \"series_code\"          \n [7] \"1970\"                  \"1971\"                  \"1972\"                 \n[10] \"1973\"                  \"1974\"                  \"1975\"                 \n[13] \"1976\"                  \"1977\"                  \"1978\"                 \n[16] \"1979\"                  \"1980\"                  \"1981\"                 \n[19] \"1982\"                  \"1983\"                  \"1984\"                 \n[22] \"1985\"                  \"1986\"                  \"1987\"                 \n[25] \"1988\"                  \"1989\"                  \"1990\"                 \n[28] \"1991\"                  \"1992\"                  \"1993\"                 \n[31] \"1994\"                  \"1995\"                  \"1996\"                 \n[34] \"1997\"                  \"1998\"                  \"1999\"                 \n[37] \"2000\"                  \"2001\"                  \"2002\"                 \n[40] \"2003\"                  \"2004\"                  \"2005\"                 \n[43] \"2006\"                  \"2007\"                  \"2008\"                 \n[46] \"2009\"                  \"2010\"                  \"2011\"                 \n[49] \"2012\"                  \"2013\"                  \"2014\"                 \n[52] \"2015\"                  \"2016\"                  \"2017\"                 \n[55] \"2018\"                  \"2019\"                  \"2020\"                 \n[58] \"2021\"                  \"2022\"                  \"2023\"                 \n[61] \"2024\"                  \"2025\"                  \"2026\"                 \n[64] \"2027\"                 \n\nsetnames(debt_data, nms_old, nms_new)\nid_vars <- c(\"country_name\", \"country_code\", \"counterpart_area_name\", \n             \"counterpart_area_code\", \"series_name\", \"series_code\")\n\ndebt_data <- melt(debt_data,\n                                id.vars = id_vars, \n                           variable.factor = F,\n                           value.factor = F,\n                           variable.name = \"year\")\n\ndebt_data[, year := str_trim(year)]\ndebt_data[, year := as.numeric(year)]\ndebt_data <-  debt_data[!is.na(value)]\n\n\nindicator_name <- c(\"Currency composition of PPG debt, U.S. dollars (%)\",\n                    \"Interest payments on external debt (% of exports of goods, services and primary income)\",\n                    \"Interest payments on external debt (% of GNI)\",\n                     \"Short-term debt (% of total external debt)\",\n                     \"Multilateral debt (% of total external debt)\" )\n\n#debt_data[, unique(country_name)]\n#debt_data[, unique(series_name)]\n#\"Uganda\", \"Tanzania\"\nea_country <- c(\"Kenya\", \"Lower middle income\" )\n\n\ndebt_data <- debt_data[country_name %in% ea_country & series_name %in% indicator_name]\ndebt_data_split <- split(debt_data, f = debt_data$series_name)\n\nn <- length(debt_data_split)\nmy_plots <-htmltools::tagList()\nmy_plots <- list()\nfor (i in 1:n) {\n    df = debt_data_split[[i]]\n    my_title = df[, unique(series_name)]\n    mn = df[, min(year)]\n    mx = df[, max(year)]\n    breaks = seq(mn, mx,by = 3)\n    p = ggplot(df, aes(year, value, group = country_name, color = country_name) ) +\n        geom_line()+\n        theme_hc()+\n        labs(title = my_title, x = \"year\", y = \"%\")+\n      scale_color_viridis_d(name=\"\")+\n      scale_x_continuous(breaks = breaks)\n    \n   # my_plots[[i]] = ggplotly(p)\n    my_plots[[i]] = p\n    \n}\n\nmy_plots\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\nworld_debt_data  <- fread(\"poverty_data/API_DT.TDS.DECT.EX.ZS_DS2_en_csv_v2_1865914.csv\", \n                          skip = 4, header = T)\n\nnms_old <- names(world_debt_data)\n\nnms_new <- nms_old %>% tolower() \nnms_new <- gsub(\"\\\\s|-\", \"_\", nms_new)\nnms_new\n\n [1] \"country_name\"   \"country_code\"   \"indicator_name\" \"indicator_code\"\n [5] \"1960\"           \"1961\"           \"1962\"           \"1963\"          \n [9] \"1964\"           \"1965\"           \"1966\"           \"1967\"          \n[13] \"1968\"           \"1969\"           \"1970\"           \"1971\"          \n[17] \"1972\"           \"1973\"           \"1974\"           \"1975\"          \n[21] \"1976\"           \"1977\"           \"1978\"           \"1979\"          \n[25] \"1980\"           \"1981\"           \"1982\"           \"1983\"          \n[29] \"1984\"           \"1985\"           \"1986\"           \"1987\"          \n[33] \"1988\"           \"1989\"           \"1990\"           \"1991\"          \n[37] \"1992\"           \"1993\"           \"1994\"           \"1995\"          \n[41] \"1996\"           \"1997\"           \"1998\"           \"1999\"          \n[45] \"2000\"           \"2001\"           \"2002\"           \"2003\"          \n[49] \"2004\"           \"2005\"           \"2006\"           \"2007\"          \n[53] \"2008\"           \"2009\"           \"2010\"           \"2011\"          \n[57] \"2012\"           \"2013\"           \"2014\"           \"2015\"          \n[61] \"2016\"           \"2017\"           \"2018\"           \"2019\"          \n[65] \"2020\"           \"v66\"           \n\nsetnames(world_debt_data, nms_old, nms_new)\n\nid_vars_debt <- nms_new[1:4]\nworld_debt_data <- melt(world_debt_data,\n                           id.vars = id_vars_debt,\n                           variable.factor = F,\n                           value.factor = F,\n                           variable.name = \"year\")\nworld_debt_data[, year := as.numeric(year)]\nworld_debt_data[, value := as.numeric(value)]\nworld_debt_data <- world_debt_data[!is.na(year)]\nworld_debt_data <- world_debt_data[!is.na(value)]\nhead(world_debt_data[country_name == \"Kenya\"], 10) %>%\n  datatable(options = list(scrollX= T))\n\n\n\n\n\n\n\nea_country <- c(\"Kenya\", \"Lower middle income\" )\n\n\nworld_debt_data <- world_debt_data[country_name %in% ea_country]\nworld_debt_data_split <- split(world_debt_data, f = world_debt_data$indicator_name)\n\nn <- length(world_debt_data_split)\nmy_plots <-htmltools::tagList()\nmy_plots <- list()\nfor (i in 1:n) {\n    df = world_debt_data_split[[i]]\n    my_title = df[, unique(indicator_name)]\n    mn = df[, min(year)]\n    mx = df[, max(year)]\n    breaks = seq(mn, mx,by = 3)\n    p = ggplot(df, aes(year, value, group = country_name, color = country_name) ) +\n        geom_line()+\n        theme_hc()+\n        labs(title = my_title, x = \"year\", y = \"%\")+\n      scale_color_viridis_d(name=\"\")+\n      scale_x_continuous(breaks = breaks)\n    \n   # my_plots[[i]] = ggplotly(p)\n    my_plots[[i]] = p\n    \n}\n\nmy_plots\n\n[[1]]"
  },
  {
    "objectID": "posts/kenya_population/ea_poverty.html",
    "href": "posts/kenya_population/ea_poverty.html",
    "title": "East Africa Poverty Indicators",
    "section": "",
    "text": "kenya poverty data Uganda poverty data Tanzania poverty data Tanzania poverty data\n\n\n\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sf)\nlibrary(DT)\nlibrary(tmap)\nlibrary(ggthemes)"
  },
  {
    "objectID": "posts/kenya_population/ea_poverty.html#read-data",
    "href": "posts/kenya_population/ea_poverty.html#read-data",
    "title": "East Africa Poverty Indicators",
    "section": "Read data",
    "text": "Read data\n\nRead data and row bind\n\n\npoverty_data <- fread(\"poverty_data/9c15861e-aeec-486a-8e4c-8bd7c9a40275_Data.csv\", na.strings = c(\"NA\", \"..\", \" \"))\n\n\npoverty_data[sample(nrow(poverty_data), 10)] %>% datatable(options = list(scrollX = TRUE))"
  },
  {
    "objectID": "posts/kenya_population/ea_poverty.html#minor-cleaning",
    "href": "posts/kenya_population/ea_poverty.html#minor-cleaning",
    "title": "East Africa Poverty Indicators",
    "section": "Minor Cleaning",
    "text": "Minor Cleaning\n\nConvert to long\nConvert column names into lower\nReplace space with underscore\n\n\nnms_old <- names(poverty_data)[1:4]\npoverty_data <- melt(poverty_data,\n                                id.vars = nms_old, \n                           variable.factor = F,\n                           value.factor = F,\n                           variable.name = \"year\")\npoverty_data[, value:= as.numeric(value)]\n\npoverty_data[, year:= gsub(\"\\\\[.*\", \"\", year)]\npoverty_data[, year := str_trim(year)]\npoverty_data[, year := as.numeric(year)]\npoverty_data <-  poverty_data[!is.na(value)]\n\n\nnms_new <- nms_old %>% tolower() \nnms_new <- gsub(\"\\\\s\", \"_\", nms_new)\nnms_new\n\n[1] \"country_name\" \"country_code\" \"series_name\"  \"series_code\" \n\nsetnames(poverty_data, nms_old, nms_new)\npoverty_data[, value:= as.numeric(value)]\npoverty_data <-  poverty_data[!is.na(value)]\n\npoverty_data[sample(nrow(poverty_data), 10)] %>% datatable(options = list(scrollX = TRUE))"
  },
  {
    "objectID": "posts/kenya_population/ea_poverty.html#plots",
    "href": "posts/kenya_population/ea_poverty.html#plots",
    "title": "East Africa Poverty Indicators",
    "section": "Plots",
    "text": "Plots\n\npoverty_data[, year := as.numeric(year)]\nea_country <- c(\"Kenya\", \"Uganda\", \"Ghana\", \"Tanzania\")\n\npoverty_data[, unique(series_name)]\n\n [1] \"Population, total\"                                                                                                                                          \n [2] \"Gini index (World Bank estimate)\"                                                                                                                           \n [3] \"Income share held by fourth 20%\"                                                                                                                            \n [4] \"Income share held by highest 10%\"                                                                                                                           \n [5] \"Income share held by highest 20%\"                                                                                                                           \n [6] \"Income share held by lowest 10%\"                                                                                                                            \n [7] \"Income share held by lowest 20%\"                                                                                                                            \n [8] \"Income share held by second 20%\"                                                                                                                            \n [9] \"Income share held by third 20%\"                                                                                                                             \n[10] \"Number of poor at $1.90 a day (2011 PPP) (millions)\"                                                                                                        \n[11] \"Number of poor at $3.20 a day (2011 PPP) (millions)\"                                                                                                        \n[12] \"Number of poor at $5.50 a day (2011 PPP) (millions)\"                                                                                                        \n[13] \"Poverty gap at $1.90 a day (2011 PPP) (%)\"                                                                                                                  \n[14] \"Poverty gap at $3.20 a day (2011 PPP) (% of population)\"                                                                                                    \n[15] \"Poverty gap at $5.50 a day (2011 PPP) (% of population)\"                                                                                                    \n[16] \"Poverty headcount ratio at $1.90 a day (2011 PPP) (% of population)\"                                                                                        \n[17] \"Poverty headcount ratio at $3.20 a day (2011 PPP) (% of population)\"                                                                                        \n[18] \"Poverty headcount ratio at $5.50 a day (2011 PPP) (% of population)\"                                                                                        \n[19] \"Annualized growth in per capita real survey mean consumption or income, bottom 40% (%)\"                                                                     \n[20] \"Annualized growth in per capita real survey mean consumption or income, top 10% (%)\"                                                                        \n[21] \"Annualized growth in per capita real survey mean consumption or income, top 60% (%)\"                                                                        \n[22] \"Annualized growth in per capita real survey mean consumption or income, total population (%)\"                                                               \n[23] \"Annualized growth in per capita real survey median income or consumption expenditure (%)\"                                                                   \n[24] \"Median daily per capita income or consumption expenditure (2011 PPP)\"                                                                                       \n[25] \"Multidimensional poverty, Drinking water (% of population deprived)\"                                                                                        \n[26] \"Multidimensional poverty, Educational attainment (% of population deprived)\"                                                                                \n[27] \"Multidimensional poverty, Educational enrollment (% of population deprived)\"                                                                                \n[28] \"Multidimensional poverty, Electricity (% of population deprived)\"                                                                                           \n[29] \"Multidimensional poverty, Headcount ratio (% of population)\"                                                                                                \n[30] \"Multidimensional poverty, Monetary poverty (% of population deprived)\"                                                                                      \n[31] \"Multidimensional poverty, Sanitation (% of population deprived)\"                                                                                            \n[32] \"Poverty gap at national poverty lines (%)\"                                                                                                                  \n[33] \"Poverty gap at national poverty lines (%), including noncomparable values\"                                                                                  \n[34] \"Poverty headcount ratio at $1.90 a day, age 0-14  (2011 PPP) (% of population age 0-14)\"                                                                    \n[35] \"Poverty headcount ratio at $1.90 a day, age 15-64 (2011 PPP) (% of population age 15-64)\"                                                                   \n[36] \"Poverty headcount ratio at $1.90 a day, age 65+ (2011 PPP) (% of population age 65+)\"                                                                       \n[37] \"Poverty headcount ratio at $1.90 a day, Female (2011 PPP) (% of female population)\"                                                                         \n[38] \"Poverty headcount ratio at $1.90 a day, Male  (2011 PPP) (% of male population)\"                                                                            \n[39] \"Poverty headcount ratio at $1.90 a day, rural (2011 PPP) (% of rural population)\"                                                                           \n[40] \"Poverty headcount ratio at $1.90 a day, urban (2011 PPP) (% of urban population)\"                                                                           \n[41] \"Poverty headcount ratio at $1.90 a day, with primary education (2011 PPP) (% of population age 16+ with primary education)\"                                 \n[42] \"Poverty headcount ratio at $1.90 a day, with secondary education (2011 PPP) (% of population age 16+ with secondary education)\"                             \n[43] \"Poverty headcount ratio at $1.90 a day, without education (2011 PPP) (% of population age 16+ without education)\"                                           \n[44] \"Poverty headcount ratio at $1.90 a day,  with Tertiary/post-secondary education (2011 PPP) (% of population age 16+ with Tertiary/post-secondary education)\"\n[45] \"Poverty headcount ratio at national poverty lines (% of population)\"                                                                                        \n[46] \"Poverty headcount ratio at national poverty lines (% of population), including noncomparable values\"                                                        \n[47] \"Rural poverty gap at national poverty lines (%)\"                                                                                                            \n[48] \"Rural poverty gap at national poverty lines (%), including noncomparable values\"                                                                            \n[49] \"Rural poverty headcount ratio at national poverty lines (% of rural population)\"                                                                            \n[50] \"Rural poverty headcount ratio at national poverty lines (% of rural population), including noncomparable values\"                                            \n[51] \"Survey mean consumption or income per capita, bottom 40% (2011 PPP $ per day)\"                                                                              \n[52] \"Survey mean consumption or income per capita, top 10% (2011 PPP $ per day)\"                                                                                 \n[53] \"Survey mean consumption or income per capita, top 60% (2011 PPP $ per day)\"                                                                                 \n[54] \"Survey mean consumption or income per capita, total population (2011 PPP $ per day)\"                                                                        \n[55] \"Urban poverty gap at national poverty lines (%)\"                                                                                                            \n[56] \"Urban poverty gap at national poverty lines (%), including noncomparable values\"                                                                            \n[57] \"Urban poverty headcount ratio at national poverty lines (% of urban population)\"                                                                            \n[58] \"Urban poverty headcount ratio at national poverty lines (% of urban population), including noncomparable values\"                                            \n[59] \"\"                                                                                                                                                           \n\nindicator <- c(\"Poverty gap at $1.90 a day (2011 PPP) (%)\", \n               \"Income share held by highest 10%\",\n               \"Income share held by lowest 10%\", \n               \"Income share held by highest 20%\",\n               \"Income share held by lowest 20%\",\n               \"Multidimensional poverty, Drinking water (% of population deprived)\",\n               \"Multidimensional poverty, Educational attainment (% of population deprived)\",\n               \"Poverty gap at $3.20 a day (2011 PPP) (% of population)\")\n\npoverty_data <- poverty_data[country_name %in% ea_country & series_name %in% indicator]\npoverty_data_split <- split(poverty_data, f = poverty_data$series_name)\n\nn <- length(poverty_data_split)\nmy_plots <-htmltools::tagList()\nfor (i in 1:n) {\n    df = poverty_data_split[[i]]\n    my_title = df[, unique(series_name)]\n    mn = df[, min(year)]\n    mx = df[, max(year)]\n    breaks = seq(mn, mx,by = 3)\n    p = ggplot(df, aes(year, value, group = country_name, color = country_name) ) +\n        geom_line()+\n        theme_hc()+\n        labs(title = my_title, x = \"year\", y = \"%\")+\n      scale_color_viridis_d(name=\"\")+\n      scale_x_continuous(breaks = breaks)\n    \n    my_plots[[i]] = ggplotly(p)\n    \n}\n\nmy_plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neconomy_and_growth <- fread(\"API_3_DS2_en_csv_v2_1743760.csv\") \n\nnms_old <- economy_and_growth[1,]  %>% as.character()\nnms_old\n\n [1] \"Country Name\"   \"Country Code\"   \"Indicator Name\" \"Indicator Code\"\n [5] \"1960\"           \"1961\"           \"1962\"           \"1963\"          \n [9] \"1964\"           \"1965\"           \"1966\"           \"1967\"          \n[13] \"1968\"           \"1969\"           \"1970\"           \"1971\"          \n[17] \"1972\"           \"1973\"           \"1974\"           \"1975\"          \n[21] \"1976\"           \"1977\"           \"1978\"           \"1979\"          \n[25] \"1980\"           \"1981\"           \"1982\"           \"1983\"          \n[29] \"1984\"           \"1985\"           \"1986\"           \"1987\"          \n[33] \"1988\"           \"1989\"           \"1990\"           \"1991\"          \n[37] \"1992\"           \"1993\"           \"1994\"           \"1995\"          \n[41] \"1996\"           \"1997\"           \"1998\"           \"1999\"          \n[45] \"2000\"           \"2001\"           \"2002\"           \"2003\"          \n[49] \"2004\"           \"2005\"           \"2006\"           \"2007\"          \n[53] \"2008\"           \"2009\"           \"2010\"           \"2011\"          \n[57] \"2012\"           \"2013\"           \"2014\"           \"2015\"          \n[61] \"2016\"           \"2017\"           \"2018\"           \"2019\"          \n[65] \"2020\"           \"NA\"            \n\neconomy_and_growth <- economy_and_growth[-1,]\nnames(economy_and_growth) <- nms_old\nnms_new <- nms_old %>% tolower() \nnms_new <- gsub(\"\\\\s\", \"_\", nms_new)\nnms_new\n\n [1] \"country_name\"   \"country_code\"   \"indicator_name\" \"indicator_code\"\n [5] \"1960\"           \"1961\"           \"1962\"           \"1963\"          \n [9] \"1964\"           \"1965\"           \"1966\"           \"1967\"          \n[13] \"1968\"           \"1969\"           \"1970\"           \"1971\"          \n[17] \"1972\"           \"1973\"           \"1974\"           \"1975\"          \n[21] \"1976\"           \"1977\"           \"1978\"           \"1979\"          \n[25] \"1980\"           \"1981\"           \"1982\"           \"1983\"          \n[29] \"1984\"           \"1985\"           \"1986\"           \"1987\"          \n[33] \"1988\"           \"1989\"           \"1990\"           \"1991\"          \n[37] \"1992\"           \"1993\"           \"1994\"           \"1995\"          \n[41] \"1996\"           \"1997\"           \"1998\"           \"1999\"          \n[45] \"2000\"           \"2001\"           \"2002\"           \"2003\"          \n[49] \"2004\"           \"2005\"           \"2006\"           \"2007\"          \n[53] \"2008\"           \"2009\"           \"2010\"           \"2011\"          \n[57] \"2012\"           \"2013\"           \"2014\"           \"2015\"          \n[61] \"2016\"           \"2017\"           \"2018\"           \"2019\"          \n[65] \"2020\"           \"na\"            \n\nsetnames(economy_and_growth, nms_old, nms_new)\n\nid_vars <- c(\"country_name\", \"country_code\",\n             \"indicator_name\", \"indicator_code\")\n\neconomy_and_growth <- melt(economy_and_growth,\n                                id.vars = id_vars, \n                           variable.factor = F,\n                           value.factor = F,\n                           variable.name = \"year\")\neconomy_and_growth[, value:= as.numeric(value)]\n\neconomy_and_growth[, year := as.numeric(year)]\neconomy_and_growth <-  economy_and_growth[!is.na(value)|!is.na(year)]\neconomy_and_growth[sample(nrow(economy_and_growth), 10)] %>% datatable(options = list(scrollX = TRUE))\n\n\n\n\n\n\n\ngni_gdp_savings_vec <- c(\"Gross savings (% of GNI)\", \n                         \"Gross savings (% of GDP)\",\n                         \"Total debt service (% of exports of goods, services and primary income)\",\n                         \"Total debt service (% of GNI)\",\n                         \"Trade (% of GDP)\",\n                         \"Current account balance (% of GDP)\",\n                         \"Exports of goods and services (annual % growth)\",\n                         \"Manufacturing, value added (annual % growth)\",\n                         \"Price level ratio of PPP conversion factor (GDP) to market exchange rate\")\nea_country <- c(\"Kenya\", \"Uganda\",  \"Tanzania\")\neconomy_and_growth[, unique(country_name)]\n\n  [1] \"Aruba\"                                               \n  [2] \"Afghanistan\"                                         \n  [3] \"Angola\"                                              \n  [4] \"Albania\"                                             \n  [5] \"Andorra\"                                             \n  [6] \"Arab World\"                                          \n  [7] \"United Arab Emirates\"                                \n  [8] \"Argentina\"                                           \n  [9] \"Armenia\"                                             \n [10] \"American Samoa\"                                      \n [11] \"Antigua and Barbuda\"                                 \n [12] \"Australia\"                                           \n [13] \"Austria\"                                             \n [14] \"Azerbaijan\"                                          \n [15] \"Burundi\"                                             \n [16] \"Belgium\"                                             \n [17] \"Benin\"                                               \n [18] \"Burkina Faso\"                                        \n [19] \"Bangladesh\"                                          \n [20] \"Bulgaria\"                                            \n [21] \"Bahrain\"                                             \n [22] \"Bahamas, The\"                                        \n [23] \"Bosnia and Herzegovina\"                              \n [24] \"Belarus\"                                             \n [25] \"Belize\"                                              \n [26] \"Bermuda\"                                             \n [27] \"Bolivia\"                                             \n [28] \"Brazil\"                                              \n [29] \"Barbados\"                                            \n [30] \"Brunei Darussalam\"                                   \n [31] \"Bhutan\"                                              \n [32] \"Botswana\"                                            \n [33] \"Central African Republic\"                            \n [34] \"Canada\"                                              \n [35] \"Central Europe and the Baltics\"                      \n [36] \"Switzerland\"                                         \n [37] \"Channel Islands\"                                     \n [38] \"Chile\"                                               \n [39] \"China\"                                               \n [40] \"Cote d'Ivoire\"                                       \n [41] \"Cameroon\"                                            \n [42] \"Congo, Dem. Rep.\"                                    \n [43] \"Congo, Rep.\"                                         \n [44] \"Colombia\"                                            \n [45] \"Comoros\"                                             \n [46] \"Cabo Verde\"                                          \n [47] \"Costa Rica\"                                          \n [48] \"Caribbean small states\"                              \n [49] \"Cuba\"                                                \n [50] \"Curacao\"                                             \n [51] \"Cayman Islands\"                                      \n [52] \"Cyprus\"                                              \n [53] \"Czech Republic\"                                      \n [54] \"Germany\"                                             \n [55] \"Djibouti\"                                            \n [56] \"Dominica\"                                            \n [57] \"Denmark\"                                             \n [58] \"Dominican Republic\"                                  \n [59] \"Algeria\"                                             \n [60] \"East Asia & Pacific (excluding high income)\"         \n [61] \"Early-demographic dividend\"                          \n [62] \"East Asia & Pacific\"                                 \n [63] \"Europe & Central Asia (excluding high income)\"       \n [64] \"Europe & Central Asia\"                               \n [65] \"Ecuador\"                                             \n [66] \"Egypt, Arab Rep.\"                                    \n [67] \"Euro area\"                                           \n [68] \"Eritrea\"                                             \n [69] \"Spain\"                                               \n [70] \"Estonia\"                                             \n [71] \"Ethiopia\"                                            \n [72] \"European Union\"                                      \n [73] \"Fragile and conflict affected situations\"            \n [74] \"Finland\"                                             \n [75] \"Fiji\"                                                \n [76] \"France\"                                              \n [77] \"Faroe Islands\"                                       \n [78] \"Micronesia, Fed. Sts.\"                               \n [79] \"Gabon\"                                               \n [80] \"United Kingdom\"                                      \n [81] \"Georgia\"                                             \n [82] \"Ghana\"                                               \n [83] \"Gibraltar\"                                           \n [84] \"Guinea\"                                              \n [85] \"Gambia, The\"                                         \n [86] \"Guinea-Bissau\"                                       \n [87] \"Equatorial Guinea\"                                   \n [88] \"Greece\"                                              \n [89] \"Grenada\"                                             \n [90] \"Greenland\"                                           \n [91] \"Guatemala\"                                           \n [92] \"Guam\"                                                \n [93] \"Guyana\"                                              \n [94] \"High income\"                                         \n [95] \"Hong Kong SAR, China\"                                \n [96] \"Honduras\"                                            \n [97] \"Heavily indebted poor countries (HIPC)\"              \n [98] \"Croatia\"                                             \n [99] \"Haiti\"                                               \n[100] \"Hungary\"                                             \n[101] \"IBRD only\"                                           \n[102] \"IDA & IBRD total\"                                    \n[103] \"IDA total\"                                           \n[104] \"IDA blend\"                                           \n[105] \"Indonesia\"                                           \n[106] \"IDA only\"                                            \n[107] \"Isle of Man\"                                         \n[108] \"India\"                                               \n[109] \"Not classified\"                                      \n[110] \"Ireland\"                                             \n[111] \"Iran, Islamic Rep.\"                                  \n[112] \"Iraq\"                                                \n[113] \"Iceland\"                                             \n[114] \"Israel\"                                              \n[115] \"Italy\"                                               \n[116] \"Jamaica\"                                             \n[117] \"Jordan\"                                              \n[118] \"Japan\"                                               \n[119] \"Kazakhstan\"                                          \n[120] \"Kenya\"                                               \n[121] \"Kyrgyz Republic\"                                     \n[122] \"Cambodia\"                                            \n[123] \"Kiribati\"                                            \n[124] \"St. Kitts and Nevis\"                                 \n[125] \"Korea, Rep.\"                                         \n[126] \"Kuwait\"                                              \n[127] \"Latin America & Caribbean (excluding high income)\"   \n[128] \"Lao PDR\"                                             \n[129] \"Lebanon\"                                             \n[130] \"Liberia\"                                             \n[131] \"Libya\"                                               \n[132] \"St. Lucia\"                                           \n[133] \"Latin America & Caribbean\"                           \n[134] \"Least developed countries: UN classification\"        \n[135] \"Low income\"                                          \n[136] \"Liechtenstein\"                                       \n[137] \"Sri Lanka\"                                           \n[138] \"Lower middle income\"                                 \n[139] \"Low & middle income\"                                 \n[140] \"Lesotho\"                                             \n[141] \"Late-demographic dividend\"                           \n[142] \"Lithuania\"                                           \n[143] \"Luxembourg\"                                          \n[144] \"Latvia\"                                              \n[145] \"Macao SAR, China\"                                    \n[146] \"St. Martin (French part)\"                            \n[147] \"Morocco\"                                             \n[148] \"Monaco\"                                              \n[149] \"Moldova\"                                             \n[150] \"Madagascar\"                                          \n[151] \"Maldives\"                                            \n[152] \"Middle East & North Africa\"                          \n[153] \"Mexico\"                                              \n[154] \"Marshall Islands\"                                    \n[155] \"Middle income\"                                       \n[156] \"North Macedonia\"                                     \n[157] \"Mali\"                                                \n[158] \"Malta\"                                               \n[159] \"Myanmar\"                                             \n[160] \"Middle East & North Africa (excluding high income)\"  \n[161] \"Montenegro\"                                          \n[162] \"Mongolia\"                                            \n[163] \"Northern Mariana Islands\"                            \n[164] \"Mozambique\"                                          \n[165] \"Mauritania\"                                          \n[166] \"Mauritius\"                                           \n[167] \"Malawi\"                                              \n[168] \"Malaysia\"                                            \n[169] \"North America\"                                       \n[170] \"Namibia\"                                             \n[171] \"New Caledonia\"                                       \n[172] \"Niger\"                                               \n[173] \"Nigeria\"                                             \n[174] \"Nicaragua\"                                           \n[175] \"Netherlands\"                                         \n[176] \"Norway\"                                              \n[177] \"Nepal\"                                               \n[178] \"Nauru\"                                               \n[179] \"New Zealand\"                                         \n[180] \"OECD members\"                                        \n[181] \"Oman\"                                                \n[182] \"Other small states\"                                  \n[183] \"Pakistan\"                                            \n[184] \"Panama\"                                              \n[185] \"Peru\"                                                \n[186] \"Philippines\"                                         \n[187] \"Palau\"                                               \n[188] \"Papua New Guinea\"                                    \n[189] \"Poland\"                                              \n[190] \"Pre-demographic dividend\"                            \n[191] \"Puerto Rico\"                                         \n[192] \"Korea, Dem. People’s Rep.\"                           \n[193] \"Portugal\"                                            \n[194] \"Paraguay\"                                            \n[195] \"West Bank and Gaza\"                                  \n[196] \"Pacific island small states\"                         \n[197] \"Post-demographic dividend\"                           \n[198] \"French Polynesia\"                                    \n[199] \"Qatar\"                                               \n[200] \"Romania\"                                             \n[201] \"Russian Federation\"                                  \n[202] \"Rwanda\"                                              \n[203] \"South Asia\"                                          \n[204] \"Saudi Arabia\"                                        \n[205] \"Sudan\"                                               \n[206] \"Senegal\"                                             \n[207] \"Singapore\"                                           \n[208] \"Solomon Islands\"                                     \n[209] \"Sierra Leone\"                                        \n[210] \"El Salvador\"                                         \n[211] \"San Marino\"                                          \n[212] \"Somalia\"                                             \n[213] \"Serbia\"                                              \n[214] \"Sub-Saharan Africa (excluding high income)\"          \n[215] \"South Sudan\"                                         \n[216] \"Sub-Saharan Africa\"                                  \n[217] \"Small states\"                                        \n[218] \"Sao Tome and Principe\"                               \n[219] \"Suriname\"                                            \n[220] \"Slovak Republic\"                                     \n[221] \"Slovenia\"                                            \n[222] \"Sweden\"                                              \n[223] \"Eswatini\"                                            \n[224] \"Sint Maarten (Dutch part)\"                           \n[225] \"Seychelles\"                                          \n[226] \"Syrian Arab Republic\"                                \n[227] \"Turks and Caicos Islands\"                            \n[228] \"Chad\"                                                \n[229] \"East Asia & Pacific (IDA & IBRD countries)\"          \n[230] \"Europe & Central Asia (IDA & IBRD countries)\"        \n[231] \"Togo\"                                                \n[232] \"Thailand\"                                            \n[233] \"Tajikistan\"                                          \n[234] \"Turkmenistan\"                                        \n[235] \"Latin America & the Caribbean (IDA & IBRD countries)\"\n[236] \"Timor-Leste\"                                         \n[237] \"Middle East & North Africa (IDA & IBRD countries)\"   \n[238] \"Tonga\"                                               \n[239] \"South Asia (IDA & IBRD)\"                             \n[240] \"Sub-Saharan Africa (IDA & IBRD countries)\"           \n[241] \"Trinidad and Tobago\"                                 \n[242] \"Tunisia\"                                             \n[243] \"Turkey\"                                              \n[244] \"Tuvalu\"                                              \n[245] \"Tanzania\"                                            \n[246] \"Uganda\"                                              \n[247] \"Ukraine\"                                             \n[248] \"Upper middle income\"                                 \n[249] \"Uruguay\"                                             \n[250] \"United States\"                                       \n[251] \"Uzbekistan\"                                          \n[252] \"St. Vincent and the Grenadines\"                      \n[253] \"Venezuela, RB\"                                       \n[254] \"British Virgin Islands\"                              \n[255] \"Virgin Islands (U.S.)\"                               \n[256] \"Vietnam\"                                             \n[257] \"Vanuatu\"                                             \n[258] \"World\"                                               \n[259] \"Samoa\"                                               \n[260] \"Kosovo\"                                              \n[261] \"Yemen, Rep.\"                                         \n[262] \"South Africa\"                                        \n[263] \"Zambia\"                                              \n[264] \"Zimbabwe\"                                            \n\ngni_gdp_savings_df <- economy_and_growth[indicator_name %in% gni_gdp_savings_vec & country_name %in% ea_country ]\n\n\ngni_gdp_savings_df_split <- split(gni_gdp_savings_df, f = gni_gdp_savings_df$indicator_name)\nn <- length(gni_gdp_savings_df_split)\n#my_plots_econ <-htmltools::tagList()\nmy_plots_econ <-list()\nfor (i in 1:n) {\n    df = gni_gdp_savings_df_split[[i]]\n    my_title = df[, unique(indicator_name)]\n    mn = df[, min(year)]\n    mx = df[, max(year)]\n    breaks = seq(mn, mx,by = 5)\n    p = ggplot(df, aes(year, value, group = country_name, color = country_name) ) +\n        geom_line(size = .3)+\n        theme_hc()+\n        labs(title = my_title, x = \"year\", y = \"%\")+\n      scale_color_colorblind(name=\"\")+\n      scale_x_continuous(breaks = breaks)\n    \n    my_plots_econ[[i]] = ggplotly(p)\n    \n}\n\nmy_plots_econ\n\n[[1]]\n\n[[2]]\n\n[[3]]\n\n[[4]]\n\n[[5]]\n\n[[6]]\n\n[[7]]\n\n[[8]]\n\n[[9]]"
  },
  {
    "objectID": "posts/kenya_population/inflation_kenya.html",
    "href": "posts/kenya_population/inflation_kenya.html",
    "title": "Inflation Kenya",
    "section": "",
    "text": "library(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sf)\nlibrary(DT)\nlibrary(tmap)\nlibrary(ggthemes)\nlibrary(here)"
  },
  {
    "objectID": "posts/kenya_population/kenya_maps.html",
    "href": "posts/kenya_population/kenya_maps.html",
    "title": "Kenya Census Data",
    "section": "",
    "text": "library(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sf)\nlibrary(DT)\nlibrary(tmap)\nlibrary(rKenyaCensus)\n\n\n# kenya_5yr_births <- readGoogleSheet(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vSf-yNXNk68cIOH0Rb6alaDk9SxKEMm3h6kb2p8jxT8oMdfj4OqUDbs2Ln9OOGGCI9V7SNiZJCDWm4H/pubhtml\")\n# \n# kenya_5yr_births <- cleanGoogleTable(kenya_5yr_births , table = 1, skip = 1 ) %>%\n#   setDT()\ndata(V4_T2.40)\nkenya_5yr_births = V4_T2.40\nsetDT(kenya_5yr_births)\n\n\nold_nms_births <- names(kenya_5yr_births)\nnew_nms_births <- gsub(\"\\\\s\", \"_\", old_nms_births) %>%\n    tolower()\n\nsetnames(kenya_5yr_births, old_nms_births, new_nms_births)\n\n\nnumerics_nms <- c(\"total\", \"notified\", \"not_notified\",\n                  \"don’t_know\", \"not_stated\", \"percent_notified\")\n\nkenya_5yr_births[, (numerics_nms) := lapply(.SD, function(x) gsub(\",\", \"\", x)), .SDcols = numerics_nms]\nkenya_5yr_births[, (numerics_nms) := lapply(.SD, as.numeric), .SDcols = numerics_nms]\n\n\nkenya_counties <- st_read(\"County\")\n\nReading layer `County' from data source \n  `/home/mburu/personal_projects/github_io_blog/posts/kenya_population/County' \n  using driver `ESRI Shapefile'\nSimple feature collection with 47 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 33.91182 ymin: -4.702271 xmax: 41.90626 ymax: 5.430648\nGeodetic CRS:  WGS 84\n\nggplot(kenya_counties)+\n  geom_sf() +\n  theme_void()\n\n\n\n\n\nsetnames(kenya_counties, \"COUNTY\", \"county\")\n\n\nkenya_5yr_births[, county := tolower(gsub(\"\\\\s\", \"-\", county))]\nkenya_5yr_births[county == \"elgeyo-marakwet\", county := \"keiyo-marakwet\" ]\nkenya_5yr_births[county == \"nairobi-city\" , county := \"nairobi\"]\nkenya_5yr_births[county == \"tharaka-nithi\"  , county := \"tharaka\" ]\nkenya_5yr_births[county == \"homabay\"  , county := \"homa-bay\" ]\nsetDT(kenya_counties)\nkenya_counties[, county := tolower(gsub(\"\\\\s\", \"-\", county))]\n\nkenya_births_counties <- merge(kenya_counties, kenya_5yr_births, by = \"county\" )\nsetnames(kenya_births_counties, \"percent_notified\", \"per_cent_in_health_facility\")\nkenya_births_counties[, county := paste(county, \" \", per_cent_in_health_facility,\"%\", sep = \"\" )]\n\nkenya_births_counties <- st_set_geometry(kenya_births_counties, \"geometry\")\n\n\nmap1 <- tm_shape(kenya_births_counties)+\n  tm_borders(col = \"gold\")+\n  tm_polygons(col = \"per_cent_in_health_facility\")+\n  tm_layout(title = \"% Health facility births\",\n            title.position = c(0.3, \"top\"))\n\n\ntmap_leaflet(map1)\n\n\n\n\n\n\n\nkenyan_pop <- fread(\"distribution-of-population-by-age-and-sex-kenya-2019-census-volume-iii.csv\")\n\n\nnumerics_nms <- c(\"Male\", \"Female\", \"Intersex\")\nkenyan_pop[, (numerics_nms) := lapply(.SD, function(x) gsub(\",\", \"\", x)), .SDcols = numerics_nms]\nkenyan_pop[, (numerics_nms) := lapply(.SD, as.numeric), .SDcols = numerics_nms]\nkenyan_pop[, (numerics_nms) := lapply(.SD, function(x) ifelse(is.na(x), 0, x)), .SDcols = numerics_nms]\nkenyan_pop[, Age := gsub(\"Sep\", \"09\", Age)]\nkenyan_pop[, Age := gsub(\"Oct\", \"10\", Age)]\nkenyan_pop[,Total:= Reduce(`+`, .SD),.SDcols= numerics_nms]\n\n\nage_cat <- kenyan_pop[!grepl(\"-\", Age)]\n\nage_cat[is.na(age_cat)] <- NA\n#age_cat[,Total:= Reduce(`+`, .SD),.SDcols= numerics_nms]\n\n\nage_cat_m <- melt(age_cat, id.vars = c(\"Age\", \"Total\"), variable.name = \"Sex\")\nage_cat_m[, Perc := round(value/Total*100, 2)]\n\n\n\n\n\nlibrary(ggthemes)\nage_cat_m <- age_cat_m[Age != \"Total\"]\nage_cat_m[, Age := as.numeric(Age)]\npop_plot <- ggplot(age_cat_m, aes(Age, Perc, group = Sex))+\n  geom_line(aes(color = Sex)) +\n  labs(y = \"Percentage of Population\")+\n  scale_x_continuous(breaks = seq(0, 100, by = 10))+\n  scale_color_viridis_d()+\n  theme_hc()\n\nggplotly(pop_plot)\n\n\n\n\n\n\nage_cat[is.na(Age), Age := 100]\n\nage_cat[, Age := as.numeric(Age)]\nage_cat[, age_factor :=  cut(Age,\n                             breaks = c(0, 5, 15, 20, 30, 40, 50, 65, 100 ), \n                             include.lowest = T,\n                             labels = c(\"0-5\", \"6-15\", \"16-20\", \n                                        \"20-30\", \"31-40\", \"41,50]\", \n                                        \"51-65\", \"> 65\"))]\n\n\n\nage_cat_sum <- age_cat[, .(sum_total = sum(Total)), by = age_factor] %>%\n  .[, Perc := round(sum_total/sum(sum_total)*100,2)]"
  },
  {
    "objectID": "posts/kenya_population/kenya_inflation.html",
    "href": "posts/kenya_population/kenya_inflation.html",
    "title": "Kenya Inflation",
    "section": "",
    "text": "inflation_rates <- fread(here(\"kenya_debt/Inflation Rates.csv\"))\nsource(\".Rprofile\")\ninflation_rates <- nms_clean(inflation_rates)\ninflation_rates[, date := as.Date(paste(year, month, \"01\", sep = \"-\"), \"%Y-%b-%d\")]\nsetnames(inflation_rates, \n         \"12_month_inflation\", \n         \"twelve_month_inflation\")\n\n\nggplot(inflation_rates, aes(date, twelve_month_inflation))+\n  geom_line()\n\n\n\n\n\nexports <- fread(here(\"kenya_debt/Principal Exports Volume, Value and Unit Prices (Ksh Million).csv\"))\nexports_m <- melt(exports,\n                  id.vars = c(\"Year\", \"Month\"),\n                  variable.factor = F)\n\nexports_m[, type := fcase(str_detect(variable, \"^Volume\"), \"Volume\",\n                          str_detect(variable, \"^Value\"), \"Income\",\n                           str_detect(variable, \"^Average\"), \"price_per_tonne\",\n                          default = \"na\")]\nexports_m[, variable := tolower(variable)]\nexports_m[, crop := fcase(str_detect(variable, \"coffee\"), \"Coffee\",\n                          str_detect(variable, \"tea\"), \"Tea\",\n                           str_detect(variable, \"horticulture\"), \"Horticulture\",\n                          default = \"na\")]\nexports_mw <- dcast(Year+ Month + crop ~ type, \n                    value.var = \"value\", \n                    data = exports_m,\n                    fun.aggregate = NULL)\n\n\nexports_mw[, date := as.Date(paste(Year, Month, \"01\", sep = \"-\"), \"%Y-%b-%d\")]\nggplot(exports_mw, aes(date, Income, color = crop))+\n  geom_line()\n\n\n\nggplot(exports_mw, aes(date, price_per_tonne, color = crop))+\n  geom_line()\n\n\n\nggplot(exports_mw, aes(date, Volume, color = crop))+\n  geom_line()"
  },
  {
    "objectID": "posts/water_pumps_tz/water_pumps.html",
    "href": "posts/water_pumps_tz/water_pumps.html",
    "title": "Tanzanian Water Pumps",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(knitr)\nlibrary(broom)\nlibrary(caret)\nlibrary(kableExtra)\nlibrary(ggthemes)\nlibrary(DT)"
  },
  {
    "objectID": "posts/water_pumps_tz/water_pumps.html#section",
    "href": "posts/water_pumps_tz/water_pumps.html#section",
    "title": "Tanzanian Water Pumps",
    "section": "",
    "text": "train_y <- fread(\"0bf8bc6e-30d0-4c50-956a-603fc693d966.csv\")\ntrain_x <-  fread(\"4910797b-ee55-40a7-8668-10efd5c1b960.csv\")\n\ntest <- fread(\"702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv\")"
  },
  {
    "objectID": "posts/water_pumps_tz/water_pumps.html#section-1",
    "href": "posts/water_pumps_tz/water_pumps.html#section-1",
    "title": "Tanzanian Water Pumps",
    "section": "",
    "text": "train_data <- merge(train_y, train_x, by = \"id\")\ntrain_data[, set := \"train\"]\n\ntest[, set := \"test\"]\n\npump_data <- rbindlist(list(train_data, test), fill = T)\n\n\ntrain_data[, .N, by = status_group] %>%\n    .[, Perc :=  round(N/sum(N) * 100, 2)] %>%\n    datatable()\n\n\n\n\n\n\n\npump_data[amount_tsh != 0, .(Mean = mean(amount_tsh),\n               Median = median(amount_tsh),\n               Min = min(amount_tsh),\n               Max = max(amount_tsh),\n               First_qurtile = quantile(amount_tsh, .25),\n               Third_qurtile = quantile(amount_tsh, .75)),\n           by = status_group] %>%\n  datatable()\n\n\n\n\n\n\n\ncol_class <- sapply(train_data, class)\nchar_cols <- names(col_class[col_class == \"character\"])\n\nchar_cols <- char_cols[char_cols != \"date_recorded\"]\nchar_cols\n\n [1] \"status_group\"          \"funder\"                \"installer\"            \n [4] \"wpt_name\"              \"basin\"                 \"subvillage\"           \n [7] \"region\"                \"lga\"                   \"ward\"                 \n[10] \"recorded_by\"           \"scheme_management\"     \"scheme_name\"          \n[13] \"extraction_type\"       \"extraction_type_group\" \"extraction_type_class\"\n[16] \"management\"            \"management_group\"      \"payment\"              \n[19] \"payment_type\"          \"water_quality\"         \"quality_group\"        \n[22] \"quantity\"              \"quantity_group\"        \"source\"               \n[25] \"source_type\"           \"source_class\"          \"waterpoint_type\"      \n[28] \"waterpoint_type_group\" \"set\""
  },
  {
    "objectID": "posts/water_pumps_tz/water_pumps.html#a-lazy-way-of-collapsing-columns-please-do-not-do-it",
    "href": "posts/water_pumps_tz/water_pumps.html#a-lazy-way-of-collapsing-columns-please-do-not-do-it",
    "title": "Tanzanian Water Pumps",
    "section": "A lazy way of collapsing columns, Please DO NOT do it,",
    "text": "A lazy way of collapsing columns, Please DO NOT do it,\n\nIt’s best to go through all columns one by one to see how well lumping together will be beneficial\n\n\nfactor_cols <- pump_data[, ..char_cols]\nfactor_cols[, (char_cols) := lapply(.SD, str_to_lower), .SDcols = char_cols]\nfactor_cols[, (char_cols) := lapply(.SD,  fct_lump_n, n = 20), .SDcols = char_cols]\n#factor_cols[, (char_cols) := lapply(.SD,  fct_lump_n, n = 5), .SDcols = char_cols]\nfactor_cols[factor_cols == \"\"] = NA\nfactor_cols[factor_cols == \"0\"] = NA\nfactor_cols[, (char_cols) :=lapply(.SD, fct_explicit_na, na_level = \"missing\"), .SDcols = char_cols]\n\n\nchars_dat <- melt(factor_cols, id.vars = \"status_group\")\n\nchars_dat[, .(freq = .N), by = .(variable, value)] %>%\n  .[, perc := round(freq/sum(freq) * 100), by = .(variable)] %>%\n    datatable()\n\n\n\n\n\n\n\nfactor_cols[set == \"train\", .N, by = .(funder,status_group)] %>%\n    .[, perc := round(N/sum(N) * 100, 2), by = .(funder)] %>%\n    \n     ggplot(aes(funder, perc, fill = status_group)) +\n     geom_bar(stat = \"identity\") +\n     # geom_text(aes(funder, perc, label = paste0(perc, \"%\"),\n     #               vjust = .05, hjust = .5),\n     #           size = 3, position = position_stack(vjust = 0.5))+\n     theme_hc()+\n    labs(title = \"Percentage loans_\")+\n     scale_fill_economist(name = \"\")+\n    theme(legend.position = \"bottom\",\n          axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n\n\n\n\nfactor_cols[set == \"train\", .N, by = .(water_quality,status_group)] %>%\n    .[, perc := round(N/sum(N) * 100, 2), by = .(water_quality)] %>%\n    \n     ggplot(aes(water_quality, perc, fill = status_group)) +\n     geom_bar(stat = \"identity\") +\n     geom_text(aes(water_quality, perc, label = paste(perc, \"%\"),\n                   vjust = .05, hjust = .5),\n               size = 3, position = position_stack(vjust = 0.5))+\n     theme_hc()+\n    labs(title = \"\")+\n     scale_fill_economist(name = \"\")+\n    theme(legend.position = \"bottom\")\n\n\n\n\n\nfactor_cols[set == \"train\", .N, by = .(installer,status_group)] %>%\n    .[, perc := round(N/sum(N) * 100, 2), by = .(installer)] %>%\n    \n     ggplot(aes(installer, perc, fill = status_group)) +\n     geom_bar(stat = \"identity\") +\n     theme_hc()+\n    labs(title = \"\")+\n     scale_fill_economist(name = \"\")+\n    theme(legend.position = \"bottom\",\n          axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n\n\n\n\npump_data[construction_year == 0, construction_year := NA]\nsummary(pump_data$construction_year)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   1960    1988    2000    1997    2008    2013   25969 \n\npump_data[, construction_year2  := cut(construction_year, breaks = c(1959, 1988, 2000, 2008, 2013))]\npump_data[, construction_year2 := fct_explicit_na(construction_year2, na_level = \"unknown\")]\nsummary(pump_data$construction_year2)\n\n(1959,1988] (1988,2000] (2000,2008] (2008,2013]     unknown \n      12650       12585       13389        9657       25969 \n\n\n\n# train_set <- pump_data[set == \"train\",\n#                        .(status_group, go_funded, water_quality,\n#                          quantity,  construction_year2,\n#                          management_group, go_installer,\n#                          waterpoint_type_group1, longitude, latitude, basin,\n#                          management_group1)]\n\n#is there biase in recording\ndel_cols <- c( \"recorded_by\")\ndata_clean <- cbind(pump_data[, .(construction_year2, latitude, longitude)], factor_cols)\n\ntrain_set <- data_clean[set == \"train\"]\ntrain_set[, set := NULL]\nchar_cols2 <- char_cols[!char_cols %in% c(\"status_group\" )]\ntrain_set_dmmy <- dummies::dummy.data.frame(train_set, names = char_cols2) %>% setDT()\ntrain_set_dmmy[, status_group := factor(status_group,\n                                   levels = c(\"functional\", \"functional needs repair\", \"non functional\"),\n                                   labels  = c(\"functional\", \"functional_needs_repair\", \"non_functional\"))]\n\n\nset.seed(100)\n#train_set <- train_set[status_group %in% c(\"functional\", \"functional_needs_repair\") ]\ntrain_ind <- sample(1:nrow(train_set_dmmy), round(0.7 * nrow(train_set)))\ntrain_set_dmmy[, status_group := factor(status_group)]\n\ntrain_set1 <- train_set_dmmy[train_ind,]\nset.seed(100)\ncv_fold <- createFolds(train_set1$status_group, k = 3)\n\n\nlibrary(caretEnsemble)\n\ntrain_ctrl <- trainControl(method = \"cv\",\n                        number = 3,\n                        summaryFunction = multiClassSummary,\n                        classProbs = TRUE,\n                        allowParallel=T,\n                        index = cv_fold,\n                        verboseIter = TRUE,\n                        returnResamp = \"all\", \n                        savePredictions = \"final\", \n                        search = \"grid\")\n\n\nxgb_grid <-  expand.grid(nrounds = c(50,100),\n                        eta = 0.06,\n                        max_depth =c(10, 50),\n                        gamma = 6,\n                        colsample_bytree = 0.8,\n                        min_child_weight =0.8,\n                        subsample =  .8)\n\n\n\nranger_grid <- expand.grid(splitrule = \"extratrees\",\n                        mtry = c(20, 50, 100),\n                        min.node.size = 1)\n\n\nglmnet_grid <- expand.grid(alpha = c(0, 1),\n                           lambda = seq(0.0001, 1, length = 3))\n\n\nset.seed(100)\n\nlibrary(tictoc)\n\ntic()\n\nmodel_list <- caretList(\n   status_group~.,\n    data=train_set1,\n    trControl=train_ctrl,\n    tuneList = list(caretModelSpec(method=\"xgbTree\", tuneGrid= xgb_grid),\n                    caretModelSpec(method=\"ranger\", tuneGrid= ranger_grid),\n                    caretModelSpec(method=\"glmnet\", tuneGrid= glmnet_grid)\n                    \n                    )\n)\n\n+ Fold1: eta=0.06, max_depth=10, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n[09:50:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:50:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n- Fold1: eta=0.06, max_depth=10, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n+ Fold1: eta=0.06, max_depth=50, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n[09:51:22] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:51:23] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n- Fold1: eta=0.06, max_depth=50, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n+ Fold2: eta=0.06, max_depth=10, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n[09:51:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:51:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n- Fold2: eta=0.06, max_depth=10, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n+ Fold2: eta=0.06, max_depth=50, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n[09:52:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:52:25] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n- Fold2: eta=0.06, max_depth=50, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n+ Fold3: eta=0.06, max_depth=10, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n[09:52:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:52:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n- Fold3: eta=0.06, max_depth=10, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n+ Fold3: eta=0.06, max_depth=50, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n[09:53:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[09:53:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n- Fold3: eta=0.06, max_depth=50, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \nAggregating results\nSelecting tuning parameters\nFitting nrounds = 100, max_depth = 50, eta = 0.06, gamma = 6, colsample_bytree = 0.8, min_child_weight = 0.8, subsample = 0.8 on full training set\n+ Fold1: splitrule=extratrees, mtry= 20, min.node.size=1 \n- Fold1: splitrule=extratrees, mtry= 20, min.node.size=1 \n+ Fold1: splitrule=extratrees, mtry= 50, min.node.size=1 \n- Fold1: splitrule=extratrees, mtry= 50, min.node.size=1 \n+ Fold1: splitrule=extratrees, mtry=100, min.node.size=1 \nGrowing trees.. Progress: 81%. Estimated remaining time: 7 seconds.\n- Fold1: splitrule=extratrees, mtry=100, min.node.size=1 \n+ Fold2: splitrule=extratrees, mtry= 20, min.node.size=1 \n- Fold2: splitrule=extratrees, mtry= 20, min.node.size=1 \n+ Fold2: splitrule=extratrees, mtry= 50, min.node.size=1 \n- Fold2: splitrule=extratrees, mtry= 50, min.node.size=1 \n+ Fold2: splitrule=extratrees, mtry=100, min.node.size=1 \nGrowing trees.. Progress: 81%. Estimated remaining time: 7 seconds.\n- Fold2: splitrule=extratrees, mtry=100, min.node.size=1 \n+ Fold3: splitrule=extratrees, mtry= 20, min.node.size=1 \n- Fold3: splitrule=extratrees, mtry= 20, min.node.size=1 \n+ Fold3: splitrule=extratrees, mtry= 50, min.node.size=1 \n- Fold3: splitrule=extratrees, mtry= 50, min.node.size=1 \n+ Fold3: splitrule=extratrees, mtry=100, min.node.size=1 \nGrowing trees.. Progress: 83%. Estimated remaining time: 6 seconds.\n- Fold3: splitrule=extratrees, mtry=100, min.node.size=1 \nAggregating results\nSelecting tuning parameters\nFitting mtry = 20, splitrule = extratrees, min.node.size = 1 on full training set\n+ Fold1: alpha=0, lambda=1 \n- Fold1: alpha=0, lambda=1 \n+ Fold1: alpha=1, lambda=1 \n- Fold1: alpha=1, lambda=1 \n+ Fold2: alpha=0, lambda=1 \n- Fold2: alpha=0, lambda=1 \n+ Fold2: alpha=1, lambda=1 \n- Fold2: alpha=1, lambda=1 \n+ Fold3: alpha=0, lambda=1 \n- Fold3: alpha=0, lambda=1 \n+ Fold3: alpha=1, lambda=1 \n- Fold3: alpha=1, lambda=1 \nAggregating results\nSelecting tuning parameters\nFitting alpha = 0, lambda = 1e-04 on full training set\n\ntoc()\n\n765.67 sec elapsed\n\n\n\nmodel_list\n\n$xgbTree\neXtreme Gradient Boosting \n\n41580 samples\n  317 predictor\n    3 classes: 'functional', 'functional_needs_repair', 'non_functional' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 13860, 13860, 13860 \nResampling results across tuning parameters:\n\n  max_depth  nrounds  logLoss    AUC        prAUC      Accuracy   Kappa    \n  10          50      0.6187243  0.8459881  0.6802115  0.7547860  0.5157636\n  10         100      0.5926387  0.8524641  0.6887911  0.7603415  0.5298999\n  50          50      0.6069728  0.8524588  0.6887281  0.7629149  0.5350648\n  50         100      0.5779731  0.8594204  0.6984094  0.7674723  0.5473892\n  Mean_F1    Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value\n  0.5711633  0.5540417         0.8301047         0.7595898          \n  0.5830710  0.5640156         0.8356069         0.7594603          \n  0.5860981  0.5666008         0.8372974         0.7603948          \n  0.5977618  0.5766923         0.8423961         0.7548314          \n  Mean_Neg_Pred_Value  Mean_Precision  Mean_Recall  Mean_Detection_Rate\n  0.8722563            0.7595898       0.5540417    0.2515953          \n  0.8716314            0.7594603       0.5640156    0.2534472          \n  0.8736187            0.7603948       0.5666008    0.2543050          \n  0.8726430            0.7548314       0.5766923    0.2558241          \n  Mean_Balanced_Accuracy\n  0.6920732             \n  0.6998113             \n  0.7019491             \n  0.7095442             \n\nTuning parameter 'eta' was held constant at a value of 0.06\nTuning\n\nTuning parameter 'min_child_weight' was held constant at a value of 0.8\n\nTuning parameter 'subsample' was held constant at a value of 0.8\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were nrounds = 100, max_depth = 50, eta\n = 0.06, gamma = 6, colsample_bytree = 0.8, min_child_weight = 0.8\n and subsample = 0.8.\n\n$ranger\nRandom Forest \n\n41580 samples\n  317 predictor\n    3 classes: 'functional', 'functional_needs_repair', 'non_functional' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 13860, 13860, 13860 \nResampling results across tuning parameters:\n\n  mtry  logLoss    AUC        prAUC      Accuracy   Kappa      Mean_F1  \n   20   0.5492160  0.8714603  0.7145098  0.7794733  0.5791659  0.6402572\n   50   0.6023253  0.8673458  0.6758671  0.7755652  0.5770019  0.6462120\n  100   0.7126941  0.8630358  0.6359068  0.7730159  0.5748031  0.6475980\n  Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value  Mean_Neg_Pred_Value\n  0.6144159         0.8551479         0.7242051            0.8751245          \n  0.6242823         0.8562681         0.6974150            0.8701457          \n  0.6282465         0.8564829         0.6870313            0.8676719          \n  Mean_Precision  Mean_Recall  Mean_Detection_Rate  Mean_Balanced_Accuracy\n  0.7242051       0.6144159    0.2598244            0.7347819             \n  0.6974150       0.6242823    0.2585217            0.7402752             \n  0.6870313       0.6282465    0.2576720            0.7423647             \n\nTuning parameter 'splitrule' was held constant at a value of extratrees\n\nTuning parameter 'min.node.size' was held constant at a value of 1\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were mtry = 20, splitrule = extratrees\n and min.node.size = 1.\n\n$glmnet\nglmnet \n\n41580 samples\n  317 predictor\n    3 classes: 'functional', 'functional_needs_repair', 'non_functional' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 13860, 13860, 13860 \nResampling results across tuning parameters:\n\n  alpha  lambda   logLoss    AUC        prAUC      Accuracy   Kappa    \n  0      0.00010  0.6218298  0.8319479  0.6497870  0.7439514  0.5036845\n  0      0.50005  0.6746615  0.8169808  0.6330926  0.7311568  0.4643535\n  0      1.00000  0.7068795  0.8096260  0.6250271  0.7228114  0.4433703\n  1      0.00010  0.6251197  0.8319851  0.6495486  0.7437350  0.5066840\n  1      0.50005  0.8893577  0.5000000  0.0000000  0.5424242  0.0000000\n  1      1.00000  0.8893577  0.5000000  0.0000000  0.5424242  0.0000000\n  Mean_F1    Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value\n  0.5646638  0.5517857         0.8294169         0.6773631          \n        NaN  0.5068225         0.8139668               NaN          \n        NaN  0.4978057         0.8061242               NaN          \n  0.5783964  0.5619689         0.8310915         0.6650360          \n        NaN  0.3333333         0.6666667               NaN          \n        NaN  0.3333333         0.6666667               NaN          \n  Mean_Neg_Pred_Value  Mean_Precision  Mean_Recall  Mean_Detection_Rate\n  0.8538487            0.6773631       0.5517857    0.2479838          \n  0.8556091                  NaN       0.5068225    0.2437189          \n  0.8557185                  NaN       0.4978057    0.2409371          \n  0.8526489            0.6650360       0.5619689    0.2479117          \n        NaN                  NaN       0.3333333    0.1808081          \n        NaN                  NaN       0.3333333    0.1808081          \n  Mean_Balanced_Accuracy\n  0.6906013             \n  0.6603946             \n  0.6519649             \n  0.6965302             \n  0.5000000             \n  0.5000000             \n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were alpha = 0 and lambda = 1e-04.\n\nattr(,\"class\")\n[1] \"caretList\"\n\n\n\ntest_own <- train_set_dmmy[-train_ind]\n\n\naccuracy <- c()\nfor (i in 1:length(model_list)) {\n    \n    rf <- model_list[[i]]\n    \n    pred <- predict(rf,  newdata = test_own)\n    \n    \n   #auc[i] <-  caret::(pred, test_own$status_group)\n   accuracy[i] <-  round(Metrics::accuracy(pred, test_own$status_group) * 100, 2)\n   \n   \n    \n}\n\n\npred_df <- data.frame(models = names(model_list), accuracy)\n\nDT::datatable(pred_df)\n\n\n\n\n\n\n\nresamples_models <- resamples(model_list)\n\ndotplot(resamples_models, metric = \"AUC\")\n\n\n\n\n\n# Alternatively, you can put in dense matrix, i.e. basic R-matrix\n# library(lightgbm)\n# \n# train_x <- as.matrix(train_set1[, !status_group])\n# train_y <- train_set1$status_group\n# \n# \n# params = list('task'= 'train',\n#     'boosting_type'= 'gbdt',\n#     'objective'= 'multiclass',\n#     'num_class'=3,\n#     'metric'= 'multi_logloss',\n#     'learning_rate'= 0.002296,\n#     'max_depth'= 7,\n#     'num_leaves'= 17,\n#     'feature_fraction'= 0.4,\n#     'bagging_fraction'= 0.6,\n#     'bagging_freq'= 17)\n# \n# train_lgb = lgb.Dataset(data = train_x , label = train_y, params = params)\n# \n# print(\"Training lightgbm with Matrix\")\n# \n# bst <- lightgbm(\n#     data = train_lgb\n#     , num_leaves = 4L\n#     , learning_rate = 1.0\n#     , nrounds = 2L\n#     , objective = \"multiclass\"\n# )"
  },
  {
    "objectID": "posts/kaggle/climate_change/scripts/kenyan_temp.html",
    "href": "posts/kaggle/climate_change/scripts/kenyan_temp.html",
    "title": "The trend of Earth surface temperatures in Kenyan towns",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(here)\nlibrary(knitr)\nlibrary(foreach)\nlibrary(ggthemes)\n\n\n#global_land_temp <- fread(here(\"data/GlobalLandTemperaturesByCity.csv\"))\nload(here(\"data/GlobalLandTemperaturesByCity.rda\"))\nglobal_land_temp <- global_land_temp[!is.na(AverageTemperature)]\nglobal_land_temp[, lat := str_extract(Latitude, \"N|S\")]\n\nglobal_land_temp[, lon := str_extract(Longitude, \"E|W\")]\n\nlat_cols <- c(\"Latitude\", \"Longitude\")\nglobal_land_temp[ ,(lat_cols) := lapply(.SD, gsub, pattern = \"N|W|E|S\", replacement = \"\"), \n                                           .SDcols = lat_cols]\nglobal_land_temp[, Latitude := as.numeric(Latitude)]\nglobal_land_temp[, Latitude := ifelse(lat == \"S\", - Latitude, Latitude)]\nglobal_land_temp[, Longitude := as.numeric(Longitude)]\nglobal_land_temp[, Longitude := ifelse(lon == \"W\", - Longitude, Longitude)]\n\n\ncities_dates <- global_land_temp[, .(min_date = min(dt), \n                              max_date = max(dt), freq = .N), by  = City][\n                                min_date < \"1904-01-01\" & \n                                  max_date > \"2013-08-01\" & freq > 1400\n                              ]\n\nkenya <- global_land_temp[Country == \"Kenya\"]\n\n\nkisumu <- global_land_temp[City == \"Kisumu\"]\n\n\ntowns = unique(kenya$City)\n#towns <- sample(towns, 5)\n#kenya <- global_land_temp[City %in% towns ]\nkenyaTowns = split(kenya, kenya$City)\n\ncities <- foreach(i = 1:length(kenyaTowns)) %do% {\n  this = kenyaTowns[[i]]\n  this = this[!is.na(AverageTemperature) & dt >= \"1900-01-01\"]\n  this[, AverageTemperature := ts(AverageTemperature, start = c(1900, 1), end = c(2013, 8), frequency = 12)]\n  this[, AverageTemperature := stats::filter(AverageTemperature, rep(1, 120)/120, method = \"con\", sides = 2)]\n}\n\n\nsummaryStats <- list()\n\nfor(i in 1:length(kenyaTowns)){\n  this <- setDT(kenyaTowns[[i]])\n  this <- this %>% group_by(City, dt) %>%\n    summarise(max = max(AverageTemperature, na.rm = T)) %>% arrange(desc(max))\n  summaryStats[[i]] <- this[1:2, ]\n\n}\nrbindlist(summaryStats) %>% kable\n\n\n\n\nCity\ndt\nmax\n\n\n\n\nEldoret\n2005-02-01\n24.536\n\n\nEldoret\n1997-03-01\n24.052\n\n\nKisumu\n2005-02-01\n24.636\n\n\nKisumu\n1997-03-01\n24.301\n\n\nMombasa\n2003-03-01\n28.974\n\n\nMombasa\n1987-03-01\n28.903\n\n\nNairobi\n1987-03-01\n19.446\n\n\nNairobi\n2005-02-01\n19.431\n\n\nNakuru\n1987-03-01\n19.446\n\n\nNakuru\n2005-02-01\n19.431\n\n\nRuiru\n1987-03-01\n25.064\n\n\nRuiru\n2005-03-01\n24.554\n\n\n\n\n\n\nlibrary(plotly)\ntitl = paste(towns, \"simple Moving average surface temperature from 1900 to 2015\")\ny = expression(\"Average temperature (  \"  * degree~C * \" )\")\n#my_plots <-htmltools::tagList()\nmy_plots <- list()\nfor (i in 1:length(cities)) {\n  this = cities[[i]]\n  this[, dt2 := dt]\n  this[, Date := as.character(dt2)]\n  temp_max <- max(this$AverageTemperature, na.rm = T)\n  temp_min <- min(this$AverageTemperature, na.rm = T)\n  mybreaks = seq(temp_min, temp_max, by = .2) %>% round(2)\n  p = ggplot(this, aes(x = dt2, y = AverageTemperature, label = Date))+\n    geom_line(colour = \"blue\")+ theme_hc()+\n  labs(x = \"Year\", y = y, title = titl[i])+\n    scale_y_continuous(breaks = mybreaks)+\n  #ylim(min(this$AverageTemperature)-2, max(this$AverageTemperature)+2)+\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\")\n  my_plots[[i]] = p # ggplotly(p) %>% as_widget()\n  #p\n}\nmy_plots\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n[[6]]"
  },
  {
    "objectID": "posts/kaggle/malaysia_tourist/malaysia_tourist.html",
    "href": "posts/kaggle/malaysia_tourist/malaysia_tourist.html",
    "title": "Malaysian Tourist Sites",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(leaflet)\n\ntourist_site <-fread(\"dataset tempat perlancongan Malaysia.csv\")\n\ntourist_site[, site_label := paste0(Negeri, \", \" ,`Nama Tempat`)]"
  },
  {
    "objectID": "posts/kaggle/malaysia_tourist/malaysia_tourist.html#leaflet-map",
    "href": "posts/kaggle/malaysia_tourist/malaysia_tourist.html#leaflet-map",
    "title": "Malaysian Tourist Sites",
    "section": "Leaflet Map",
    "text": "Leaflet Map\n\nleaflet(tourist_site) %>%\n    addTiles() %>%\n    addMarkers(~Longitude, ~Latitude, label = ~site_label )"
  },
  {
    "objectID": "posts/kaggle/heart_failure/heart_failure.html",
    "href": "posts/kaggle/heart_failure/heart_failure.html",
    "title": "Heart Failure Prediction",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(xgboost)\nlibrary(knitr)\nlibrary(broom)\nlibrary(caret)\nlibrary(e1071)\nlibrary(kableExtra)\nlibrary(ggthemes)\nlibrary(DT)\nlibrary(here)\n\n\nheart_failure<- fread(\"data/heart_failure_clinical_records_dataset.csv\")\nnms <- names(heart_failure)  \nnms_new <- nms %>% tolower()\nsetnames(heart_failure, nms, nms_new)\n\ndatatable(heart_failure, \n          options = list(scrollX = T) )\n\n\n\n\n\n\n\nheart_failure[, death_char := factor(death_event,\n                                     levels = c(0, 1), \n                                     labels = c(\"Alive\", \"Death\"))]\n\n\ndeaths_tab <- heart_failure[, .(freq = .N),\n       by = death_char] %>% \n    .[, perc := round(100 * freq/sum(freq), 2)] \n\ndeaths_tab[, bar_text := paste0(\"N = \",freq, \", \", perc, \"%\")]\n#datatable(deaths_tab)\n\n\nggplot(deaths_tab, aes(death_char, perc))+\n    geom_bar(stat = \"identity\", width = 0.5)+\n    geom_text(aes(death_char, perc, label = bar_text),\n              position = position_dodge(width = 0.5),\n              vjust = 0.05)\n\n\n\n\n\nfind_factors <- function(x){\n    y = unique(x)\n    len_x = length(y)\n    val = len_x < 4\n    return(val)\n}\nnms_dt <- sapply(heart_failure, find_factors)\nnms_factors_all <- nms_dt[nms_dt == T] %>% names()\nnms_factors <- nms_factors_all[!nms_factors_all %in% c(\"death_event\")]\n\ndt_factors <- heart_failure[, ..nms_factors]\ndatatable(dt_factors[1:10], \n          options = list(scrollX = T) )\n\n\n\n\n\n\n\ndt_factors_m <- melt(dt_factors, \n                     id.vars = \"death_char\",\n                     variable.factor = F)\n\ndatatable(dt_factors_m[1:10], \n          options = list(scrollX = T) )\n\n\n\n\n\n\n\nsummary_factors <- dt_factors_m[, .(freq = .N), \n             by = c( \"variable\", \"death_char\", \"value\")] %>%\n    .[, perc := round(100 * freq/sum(freq), 2),\n      by = c( \"variable\",\"value\")] \n\n\nsummary_factors[, value := as.factor(value)]\nggplot(summary_factors, aes(value, perc, fill = death_char))+\n    geom_bar(stat = \"identity\")+\n    geom_text(aes(value, perc, label = perc),\n              position = position_stack(vjust = .5))+\n    facet_wrap(~variable)\n\n\n\n\n\nnms_numeric <- nms_dt[nms_dt == FALSE] %>% names()\n\nheart_failure[, (nms_numeric) := lapply(.SD, \n                                        function(x)scales::rescale(x) ),\n              .SDcols = nms_numeric]\n\n\nnms_numeric2 <- c(\"death_char\", nms_numeric)\ndt_num <- heart_failure[, ..nms_numeric2]\ndt_num_m <- melt(dt_num, id.vars = \"death_char\")\n\n\nggplot(dt_num_m, aes(death_char, value))+\n  geom_violin()+\n  facet_wrap(~variable)\n\n\n\n\n\nnms <- c(\"death_event\", \"time\")\ncv_fold <- createFolds(heart_failure$death_char, k = 10)\n\nheart_failure[, (nms) := NULL]\nxgb_ctrl <- trainControl(method = \"cv\",\n                        number = 10,\n                        summaryFunction = twoClassSummary,\n                        classProbs = TRUE,\n                        allowParallel=T,\n                        index = cv_fold,\n                        verboseIter = FALSE,\n                        savePredictions = TRUE,\n                        search = \"grid\")\n\nxgb_grid <- expand.grid(nrounds = c(10, 50, 100),\n                        eta = seq(0.06, .2, length.out = 3),\n                        max_depth = c(50, 80),\n                        gamma = c(0,.01, 0.1),\n                        colsample_bytree = c(0.6, 0.7,0.8),\n                        min_child_weight = 1,\n                        subsample =  .7\n                        \n    )\n\nxgb_model <-train(death_char~.,\n                 data=heart_failure,\n                 method=\"xgbTree\",\n                 trControl= xgb_ctrl,\n                 tuneGrid=xgb_grid,\n                 verbose=T,\n                 metric=\"ROC\",\n                 nthread =4\n                     \n    )\n\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:48] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:49] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:52] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:53] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:55] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:56] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:57] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:03:58] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\nxgb_model\n\neXtreme Gradient Boosting \n\n299 samples\n 11 predictor\n  2 classes: 'Alive', 'Death' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 29, 29, 30, 30, 31, 31, ... \nResampling results across tuning parameters:\n\n  eta   max_depth  gamma  colsample_bytree  nrounds  ROC        Sens     \n  0.06  50         0.00   0.6                10      0.6920310  0.9425569\n  0.06  50         0.00   0.6                50      0.7048482  0.9086141\n  0.06  50         0.00   0.6               100      0.7013511  0.8894644\n  0.06  50         0.00   0.7                10      0.6918159  0.9118687\n  0.06  50         0.00   0.7                50      0.7028664  0.9042275\n  0.06  50         0.00   0.7               100      0.7042209  0.8834534\n  0.06  50         0.00   0.8                10      0.6755050  0.9031466\n  0.06  50         0.00   0.8                50      0.7023756  0.9069837\n  0.06  50         0.00   0.8               100      0.6996528  0.8823395\n  0.06  50         0.01   0.6                10      0.6923404  0.9168108\n  0.06  50         0.01   0.6                50      0.7057584  0.9091605\n  0.06  50         0.01   0.6               100      0.7053699  0.8954963\n  0.06  50         0.01   0.7                10      0.7043386  0.9108059\n  0.06  50         0.01   0.7                50      0.7065002  0.9042455\n  0.06  50         0.01   0.7               100      0.7036037  0.8938420\n  0.06  50         0.01   0.8                10      0.7184022  0.9157479\n  0.06  50         0.01   0.8                50      0.7073854  0.9042395\n  0.06  50         0.01   0.8               100      0.7036152  0.8894584\n  0.06  50         0.10   0.6                10      0.6792724  0.9310935\n  0.06  50         0.10   0.6                50      0.7040034  0.9080766\n  0.06  50         0.10   0.6               100      0.6986339  0.8938540\n  0.06  50         0.10   0.7                10      0.6899647  0.9124002\n  0.06  50         0.10   0.7                50      0.6992610  0.9003963\n  0.06  50         0.10   0.7               100      0.6977326  0.8807332\n  0.06  50         0.10   0.8                10      0.6941779  0.9118657\n  0.06  50         0.10   0.8                50      0.6982817  0.9086171\n  0.06  50         0.10   0.8               100      0.7002679  0.8862037\n  0.06  80         0.00   0.6                10      0.6868663  0.9316369\n  0.06  80         0.00   0.6                50      0.7122139  0.9173903\n  0.06  80         0.00   0.6               100      0.7027464  0.8812827\n  0.06  80         0.00   0.7                10      0.6971972  0.9173993\n  0.06  80         0.00   0.7                50      0.7056163  0.9080976\n  0.06  80         0.00   0.7               100      0.7034062  0.8911037\n  0.06  80         0.00   0.8                10      0.6903542  0.9277728\n  0.06  80         0.00   0.8                50      0.7088431  0.9102504\n  0.06  80         0.00   0.8               100      0.7012258  0.8916532\n  0.06  80         0.01   0.6                10      0.7010626  0.9315889\n  0.06  80         0.01   0.6                50      0.7073839  0.9135441\n  0.06  80         0.01   0.6               100      0.7068252  0.8927340\n  0.06  80         0.01   0.7                10      0.7043807  0.9337957\n  0.06  80         0.01   0.7                50      0.7075954  0.9086261\n  0.06  80         0.01   0.7               100      0.7035512  0.8856512\n  0.06  80         0.01   0.8                10      0.7015114  0.9233982\n  0.06  80         0.01   0.8                50      0.7050451  0.9129917\n  0.06  80         0.01   0.8               100      0.6999855  0.8894824\n  0.06  80         0.10   0.6                10      0.6968202  0.9031166\n  0.06  80         0.10   0.6                50      0.7151121  0.9107848\n  0.06  80         0.10   0.6               100      0.7027532  0.8933075\n  0.06  80         0.10   0.7                10      0.6829111  0.8982315\n  0.06  80         0.10   0.7                50      0.7056200  0.9064433\n  0.06  80         0.10   0.7               100      0.6986761  0.8850958\n  0.06  80         0.10   0.8                10      0.7001073  0.9162673\n  0.06  80         0.10   0.8                50      0.7037713  0.9031556\n  0.06  80         0.10   0.8               100      0.6999984  0.8845403\n  0.13  50         0.00   0.6                10      0.7054617  0.9146520\n  0.13  50         0.00   0.6                50      0.6956956  0.8829130\n  0.13  50         0.00   0.6               100      0.6902444  0.8588242\n  0.13  50         0.00   0.7                10      0.6995214  0.9020207\n  0.13  50         0.00   0.7                50      0.7098836  0.8872906\n  0.13  50         0.00   0.7               100      0.6998780  0.8654026\n  0.13  50         0.00   0.8                10      0.6767171  0.9096769\n  0.13  50         0.00   0.8                50      0.6915199  0.8730589\n  0.13  50         0.00   0.8               100      0.6931832  0.8506215\n  0.13  50         0.01   0.6                10      0.6738310  0.9124932\n  0.13  50         0.01   0.6                50      0.6934687  0.8856482\n  0.13  50         0.01   0.6               100      0.6809143  0.8577313\n  0.13  50         0.01   0.7                10      0.6933363  0.8971357\n  0.13  50         0.01   0.7                50      0.6936917  0.8785414\n  0.13  50         0.01   0.7               100      0.6856211  0.8604756\n  0.13  50         0.01   0.8                10      0.7130818  0.9113313\n  0.13  50         0.01   0.8                50      0.7018228  0.8818081\n  0.13  50         0.01   0.8               100      0.6973589  0.8599171\n  0.13  50         0.10   0.6                10      0.6857819  0.9069507\n  0.13  50         0.10   0.6                50      0.6999861  0.8872816\n  0.13  50         0.10   0.6               100      0.6894078  0.8473248\n  0.13  50         0.10   0.7                10      0.7095341  0.9200925\n  0.13  50         0.10   0.7                50      0.7049715  0.8845583\n  0.13  50         0.10   0.7               100      0.6945529  0.8615685\n  0.13  50         0.10   0.8                10      0.7044326  0.8873146\n  0.13  50         0.10   0.8                50      0.7051311  0.8839849\n  0.13  50         0.10   0.8               100      0.6913503  0.8560920\n  0.13  80         0.00   0.6                10      0.6932453  0.8976971\n  0.13  80         0.00   0.6                50      0.6995467  0.8823545\n  0.13  80         0.00   0.6               100      0.6915954  0.8533417\n  0.13  80         0.00   0.7                10      0.6915321  0.9113193\n  0.13  80         0.00   0.7                50      0.6919086  0.8763376\n  0.13  80         0.00   0.7               100      0.6831381  0.8418753\n  0.13  80         0.00   0.8                10      0.6855081  0.8911307\n  0.13  80         0.00   0.8                50      0.6924063  0.8719570\n  0.13  80         0.00   0.8               100      0.6849398  0.8522759\n  0.13  80         0.01   0.6                10      0.7169270  0.9074881\n  0.13  80         0.01   0.6                50      0.6994667  0.8801507\n  0.13  80         0.01   0.6               100      0.6917072  0.8588332\n  0.13  80         0.01   0.7                10      0.7011242  0.9283252\n  0.13  80         0.01   0.7                50      0.6978035  0.8763256\n  0.13  80         0.01   0.7               100      0.6906899  0.8544316\n  0.13  80         0.01   0.8                10      0.7174569  0.9048069\n  0.13  80         0.01   0.8                50      0.7000947  0.8812616\n  0.13  80         0.01   0.8               100      0.6959577  0.8610100\n  0.13  80         0.10   0.6                10      0.6955050  0.9261424\n  0.13  80         0.10   0.6                50      0.6961442  0.8905783\n  0.13  80         0.10   0.6               100      0.6891676  0.8643127\n  0.13  80         0.10   0.7                10      0.6868013  0.8948778\n  0.13  80         0.10   0.7                50      0.7020057  0.8823575\n  0.13  80         0.10   0.7               100      0.6898299  0.8626554\n  0.13  80         0.10   0.8                10      0.6820069  0.8921936\n  0.13  80         0.10   0.8                50      0.6929813  0.8725035\n  0.13  80         0.10   0.8               100      0.6886524  0.8544557\n  0.20  50         0.00   0.6                10      0.6794014  0.8796523\n  0.20  50         0.00   0.6                50      0.6865066  0.8697952\n  0.20  50         0.00   0.6               100      0.6860612  0.8517384\n  0.20  50         0.00   0.7                10      0.6863091  0.9146400\n  0.20  50         0.00   0.7                50      0.6969367  0.8670330\n  0.20  50         0.00   0.7               100      0.6866341  0.8451630\n  0.20  50         0.00   0.8                10      0.7078974  0.9146370\n  0.20  50         0.00   0.8                50      0.6821927  0.8561040\n  0.20  50         0.00   0.8               100      0.6818595  0.8374857\n  0.20  50         0.01   0.6                10      0.6774521  0.8888879\n  0.20  50         0.01   0.6                50      0.6884005  0.8686693\n  0.20  50         0.01   0.6               100      0.6872244  0.8495436\n  0.20  50         0.01   0.7                10      0.6911060  0.9102594\n  0.20  50         0.01   0.7                50      0.6975546  0.8642917\n  0.20  50         0.01   0.7               100      0.6869037  0.8467904\n  0.20  50         0.01   0.8                10      0.6855491  0.8823545\n  0.20  50         0.01   0.8                50      0.6816911  0.8637573\n  0.20  50         0.01   0.8               100      0.6767133  0.8495316\n  0.20  50         0.10   0.6                10      0.6808037  0.9042034\n  0.20  50         0.10   0.6                50      0.6977902  0.8681018\n  0.20  50         0.10   0.6               100      0.6902593  0.8385786\n  0.20  50         0.10   0.7                10      0.7036799  0.9113703\n  0.20  50         0.10   0.7                50      0.6949366  0.8681499\n  0.20  50         0.10   0.7               100      0.6877988  0.8593647\n  0.20  50         0.10   0.8                10      0.6907288  0.8829340\n  0.20  50         0.10   0.8                50      0.6852445  0.8610190\n  0.20  50         0.10   0.8               100      0.6792689  0.8391221\n  0.20  80         0.00   0.6                10      0.6933188  0.9053534\n  0.20  80         0.00   0.6                50      0.6902076  0.8708641\n  0.20  80         0.00   0.6               100      0.6828314  0.8544527\n  0.20  80         0.00   0.7                10      0.6979775  0.9113463\n  0.20  80         0.00   0.7                50      0.6943688  0.8621299\n  0.20  80         0.00   0.7               100      0.6872186  0.8511650\n  0.20  80         0.00   0.8                10      0.7054326  0.9058728\n  0.20  80         0.00   0.8                50      0.6881542  0.8533598\n  0.20  80         0.00   0.8               100      0.6813132  0.8353029\n  0.20  80         0.01   0.6                10      0.6828876  0.8910827\n  0.20  80         0.01   0.6                50      0.6902391  0.8561070\n  0.20  80         0.01   0.6               100      0.6810993  0.8429532\n  0.20  80         0.01   0.7                10      0.6961030  0.8949589\n  0.20  80         0.01   0.7                50      0.6913212  0.8615595\n  0.20  80         0.01   0.7               100      0.6864380  0.8424188\n  0.20  80         0.01   0.8                10      0.6802384  0.9053474\n  0.20  80         0.01   0.8                50      0.6835023  0.8593827\n  0.20  80         0.01   0.8               100      0.6739576  0.8386057\n  0.20  80         0.10   0.6                10      0.7009007  0.8899748\n  0.20  80         0.10   0.6                50      0.6872766  0.8533718\n  0.20  80         0.10   0.6               100      0.6840067  0.8254489\n  0.20  80         0.10   0.7                10      0.6935701  0.8916652\n  0.20  80         0.10   0.7                50      0.6962228  0.8675824\n  0.20  80         0.10   0.7               100      0.6840144  0.8402180\n  0.20  80         0.10   0.8                10      0.6871588  0.9080856\n  0.20  80         0.10   0.8                50      0.6910315  0.8538942\n  0.20  80         0.10   0.8               100      0.6859180  0.8369333\n  Spec     \n  0.2082197\n  0.3092622\n  0.3265036\n  0.2826250\n  0.3149559\n  0.3542903\n  0.3033146\n  0.3033547\n  0.3334937\n  0.2802727\n  0.3172280\n  0.3345763\n  0.2848303\n  0.3230420\n  0.3346431\n  0.3203689\n  0.3253007\n  0.3415932\n  0.2177092\n  0.3057204\n  0.3241914\n  0.2768511\n  0.3275862\n  0.3519513\n  0.3009890\n  0.3010158\n  0.3219059\n  0.2243117\n  0.2802058\n  0.3276397\n  0.2788025\n  0.2997327\n  0.3381048\n  0.2732424\n  0.3194600\n  0.3346565\n  0.2026196\n  0.3160118\n  0.3369420\n  0.2511093\n  0.3078722\n  0.3403101\n  0.2861133\n  0.3183641\n  0.3438385\n  0.3251938\n  0.3126303\n  0.3241780\n  0.2813553\n  0.3148623\n  0.3450414\n  0.2951350\n  0.3010960\n  0.3300454\n  0.3206629\n  0.3507351\n  0.3681903\n  0.3265036\n  0.3345095\n  0.3694199\n  0.2616279\n  0.3473403\n  0.3810612\n  0.2360331\n  0.3427559\n  0.3705694\n  0.2999064\n  0.3346298\n  0.3589548\n  0.3333200\n  0.3600107\n  0.3856188\n  0.2894814\n  0.3508019\n  0.3868083\n  0.2929297\n  0.3542769\n  0.3682171\n  0.3392141\n  0.3518979\n  0.3786554\n  0.2996792\n  0.3450013\n  0.3694333\n  0.2742716\n  0.3473670\n  0.3751537\n  0.3115477\n  0.3578188\n  0.3844427\n  0.3070035\n  0.3242315\n  0.3659449\n  0.2743651\n  0.3427960\n  0.3705560\n  0.3331729\n  0.3473403\n  0.3681502\n  0.2443331\n  0.3310746\n  0.3565891\n  0.3171211\n  0.3496258\n  0.3740444\n  0.3286554\n  0.3474071\n  0.3694066\n  0.3044373\n  0.3542903\n  0.3787089\n  0.2858327\n  0.3658915\n  0.3867950\n  0.2790029\n  0.3519914\n  0.3786955\n  0.2884523\n  0.3428228\n  0.3706496\n  0.2894012\n  0.3705694\n  0.3729217\n  0.3297380\n  0.3589816\n  0.3728415\n  0.2802727\n  0.3555333\n  0.3879979\n  0.3056135\n  0.3497862\n  0.3659449\n  0.3239241\n  0.3647688\n  0.3809543\n  0.3044507\n  0.3635926\n  0.3787089\n  0.2986635\n  0.3728281\n  0.3809810\n  0.3240310\n  0.3705560\n  0.3833601\n  0.3162657\n  0.3543037\n  0.3764101\n  0.2907244\n  0.3647554\n  0.3752072\n  0.2778535\n  0.3670944\n  0.3752606\n  0.3416600\n  0.3682972\n  0.4018578\n  0.3286554\n  0.3600775\n  0.3902700\n  0.2915932\n  0.3728949\n  0.3787356\n\nTuning parameter 'min_child_weight' was held constant at a value of 1\n\nTuning parameter 'subsample' was held constant at a value of 0.7\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were nrounds = 10, max_depth = 50, eta\n = 0.06, gamma = 0.01, colsample_bytree = 0.8, min_child_weight = 1\n and subsample = 0.7."
  },
  {
    "objectID": "posts/kaggle/newyork_airbnb/newyork_airbnb.html",
    "href": "posts/kaggle/newyork_airbnb/newyork_airbnb.html",
    "title": "New york Airbnb",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(DT)\nlibrary(ggthemes)\nlibrary(leaflet)\nlibrary(ggmap)\nlibrary(plotly)\nlibrary(lubridate)"
  },
  {
    "objectID": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#first-few-variables",
    "href": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#first-few-variables",
    "title": "New york Airbnb",
    "section": "First few variables",
    "text": "First few variables\n\nairbnb_nyc <- fread(\"AB_NYC_2019.csv\")\n\nhead(airbnb_nyc) %>% \n    datatable(options = list(scrollX = TRUE))"
  },
  {
    "objectID": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#neighbourhood-disitribution",
    "href": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#neighbourhood-disitribution",
    "title": "New york Airbnb",
    "section": "Neighbourhood Disitribution",
    "text": "Neighbourhood Disitribution\n\np1 <- airbnb_nyc[, .(freq = .N), by =  neighbourhood_group] %>%\n    .[, perc := round(freq/sum(freq) *100, 2)] %>%\n    ggplot(aes(neighbourhood_group, perc))+\n    geom_bar(stat = \"identity\", width = 0.5, fill =\"slategray2\"  ) +\n    geom_text(aes(neighbourhood_group, perc, label = paste0(perc, \"%\")),\n              position = position_dodge(width = 0.5),\n              vjust = 0.07)+\n    theme_fivethirtyeight() \np1"
  },
  {
    "objectID": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#popular-room-types-in-neighbourhoods-disitribution",
    "href": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#popular-room-types-in-neighbourhoods-disitribution",
    "title": "New york Airbnb",
    "section": "Popular room types in neighbourhoods Disitribution",
    "text": "Popular room types in neighbourhoods Disitribution\n\nairbnb_nyc[, .(freq =.N), by = .(neighbourhood_group, room_type)] %>%\n  .[, perc := round(freq/sum(freq) *100, 2), by = neighbourhood_group] %>%\n  ggplot(aes(neighbourhood_group, perc, fill = room_type))+\n  geom_bar(stat = \"identity\", width = 0.5 ) +\n    geom_text(aes(neighbourhood_group, perc, label = paste0(perc, \"%\")),\n              position = position_stack(vjust = 0.5),\n              vjust = 0.07)+\n  scale_fill_viridis_d(name = \"\")+\n    theme_fivethirtyeight()"
  },
  {
    "objectID": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#summary-stats-for-price-based-on-location",
    "href": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#summary-stats-for-price-based-on-location",
    "title": "New york Airbnb",
    "section": "Summary Stats for price based on location",
    "text": "Summary Stats for price based on location\n\nairbnb_nyc[, price := as.double(price)]\n\nsummary_function <- function(by_col){\n    \n    summary_stats <- airbnb_nyc[!is.na(price)&price !=0, \n                            .(Mean = round(mean(price), 2), \n                              Median = median(price),\n                              First_quartile = quantile(price, .25),\n                              Third_quartile = quantile(price, .75),\n                              Min = min(price),\n                              Max = max(price)),\n                            by = by_col]\n    return(summary_stats)\n}\n\n\ndatatable(summary_function(by_col = \"neighbourhood_group\"))"
  },
  {
    "objectID": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#summary-stats-price-based-on-room-type",
    "href": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#summary-stats-price-based-on-room-type",
    "title": "New york Airbnb",
    "section": "Summary Stats price based on room type",
    "text": "Summary Stats price based on room type\n\ndatatable(summary_function(by_col = \"room_type\"))"
  },
  {
    "objectID": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#median-price-roomtype-in-different-neighbourhoods",
    "href": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#median-price-roomtype-in-different-neighbourhoods",
    "title": "New york Airbnb",
    "section": "Median price roomtype in different neighbourhoods",
    "text": "Median price roomtype in different neighbourhoods\n\nairbnb_nyc[, .(Median = median(price)), \n           by = .(neighbourhood_group, room_type)] %>%\n  dcast(neighbourhood_group ~room_type, value.var = \"Median\") %>%\n  datatable()"
  },
  {
    "objectID": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#get-map-using-ggmap",
    "href": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#get-map-using-ggmap",
    "title": "New york Airbnb",
    "section": "Get map using ggmap",
    "text": "Get map using ggmap\n\nnewyork_map <- get_map(c(left = min(airbnb_nyc$longitude) - .0001,\n                         bottom = min(airbnb_nyc$latitude) - .0001,\n                         right = max(airbnb_nyc$longitude) + .0001,\n                         top = max(airbnb_nyc$latitude) + .0001),\n                       maptype = \"watercolor\", source = \"stamen\")"
  },
  {
    "objectID": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#mapping-function",
    "href": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#mapping-function",
    "title": "New york Airbnb",
    "section": "Mapping function",
    "text": "Mapping function\n\nmap_plot <- function(df, color_col, continues_color_col = TRUE){\n  \n  if(continues_color_col) {\n    \n    scale_fill <- scale_color_viridis_c()\n    \n    } else{\n      \n      scale_fill <- scale_color_viridis_d()\n      \n    }\n    \n    \n  ggmap(newyork_map) +\n    geom_point(data =df, \n               aes_string(\"longitude\", \"latitude\",\n                          color = color_col), size = 1)+\n    theme(legend.position = \"bottom\")+\n                           \n    scale_fill\n  \n  \n}"
  },
  {
    "objectID": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#newyork-price-map",
    "href": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#newyork-price-map",
    "title": "New york Airbnb",
    "section": "Newyork price map",
    "text": "Newyork price map\n\nper95 <- airbnb_nyc[, quantile(price, 0.95)]\nmap_plot(df = airbnb_nyc[price <=per95  ], \n         color_col = \"price\")"
  },
  {
    "objectID": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#categorise-price-variable",
    "href": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#categorise-price-variable",
    "title": "New york Airbnb",
    "section": "Categorise price variable",
    "text": "Categorise price variable\n\nbreaks <-  quantile(airbnb_nyc$price, seq(0, 1, by = .1))\n\nairbnb_nyc[, price_factor := cut(price, breaks = breaks,\n                                 include.lowest = TRUE)]\n\nmap_plot(df = airbnb_nyc, \n         color_col = \"price_factor\",\n         continues_color_col = FALSE)"
  },
  {
    "objectID": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#reviews-per-month",
    "href": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#reviews-per-month",
    "title": "New york Airbnb",
    "section": "Reviews per month",
    "text": "Reviews per month\n\nWe can use this as a proxy of host receiving a lot of guests\nwe could use this for instance to check if some neighborhoods are more popular\n\n\nper95_rev <- airbnb_nyc[!is.na(last_review) & !is.na(reviews_per_month),\n                        quantile(reviews_per_month, 0.95)]\n\nmap_plot(df = airbnb_nyc[reviews_per_month < per95_rev ], \n         color_col = \"reviews_per_month\")"
  },
  {
    "objectID": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#dates",
    "href": "posts/kaggle/newyork_airbnb/newyork_airbnb.html#dates",
    "title": "New york Airbnb",
    "section": "Dates",
    "text": "Dates\n\nairbnb_nyc[, last_review := ymd(last_review)]\nairbnb_nyc[, summary(last_review)]\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"2011-03-28\" \"2018-07-08\" \"2019-05-19\" \"2018-10-04\" \"2019-06-23\" \"2019-07-08\" \n        NA's \n     \"10052\" \n\n\n\nWork in progress !"
  },
  {
    "objectID": "posts/kaggle/mnist_digits/mnist.html",
    "href": "posts/kaggle/mnist_digits/mnist.html",
    "title": "MNIST Digits",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(keras)\nlibrary(caret)\nlibrary(DT)\nlibrary(caretEnsemble)\nlibrary(tictoc)\n\ntrain_data <- fread(\"data/train.csv\")\n\ntest_data <- fread(\"data/test.csv\")\n\n\n\n\n\nggplot(train_data, aes(x = factor(label))) +\n    geom_bar()\n\n\n\n\n\n#  image coordinates\nxy_axis <- data.frame(x = expand.grid(1:28, 28:1)[,1],\n                      y = expand.grid(1:28, 28:1)[,2])\n\n\n# get 12 images\nset.seed(100)\nsample_10 <- train_data[sample(1:.N, 12), -1] %>% as.matrix()\n\ndatatable(sample_10, \n          options = list(scrollX = TRUE))\n\nsample_10 <- t(sample_10)\n\nplot_data <- cbind(xy_axis, sample_10 )\n\nsetDT(plot_data, keep.rownames = \"pixel\")\n\n# Observe the first records\nhead(plot_data) %>% datatable(options = list(scrollX = TRUE))\n\n\n\n\n\nplot_data_m <- melt(plot_data, id.vars = c(\"pixel\", \"x\", \"y\"))\n\n# Plot the image using ggplot()\nggplot(plot_data_m, aes(x, y, fill = value)) +\n    geom_raster()+\n     facet_wrap(~variable)+\n    scale_fill_gradient(low = \"white\",\n                        high = \"black\", guide = FALSE)+\n    theme(axis.line = element_blank(),\n                  axis.text = element_blank(),\n                  axis.ticks = element_blank(),\n                  axis.title = element_blank(),\n                  panel.background = element_blank(),\n                  panel.border = element_blank(),\n                  panel.grid.major = element_blank(),\n                  panel.grid.minor = element_blank(),\n                  plot.background = element_blank())\n\n\n\n\n\nDecided to have a self test set\n\n\nnmst <- names(train_data)\nnmst <- nmst[nmst != \"label\"]\nminmax <- function(x) {\n  top =  (x - min(x))\n  bottom = (max(x) - min(x))\n  if(bottom == 0){ \n    return(0)\n  }else{\n      return(top/bottom)\n    }\n}\n#train_data[, (nmst) := lapply(.SD,  function(x) x/255), .SDcols = nmst]\ntrain_data[, (nmst) := lapply(.SD,  minmax), .SDcols = nmst]\nset.seed(100)\nN1 = nrow(train_data)\nsample_one <- sample(N1, 5000)\ntrain_data <- train_data[sample_one]\nN = nrow(train_data)\nsample_train <- sample(N, size = round(0.75 *N ))\ntest_own <- train_data[-sample_train]\ntrain_data2 <- train_data[sample_train, ]\ntrain_y <-to_categorical(train_data2$label, 10)\n\ntrain_x <- train_data2[, -1]\n#convert to matrix\ntrain_x <- train_x %>%\n    as.matrix()\n\n#train_x <- train_x/255\n\n\n\n\n\nmodel <- keras_model_sequential() \nmodel %>% \n    layer_dense(units = 784, activation = 'relu', input_shape = 784) %>%\n    layer_dropout(rate = 0.3) %>%\n    layer_dense(units = 784, activation = 'relu') %>%\n    layer_dropout(rate = 0.3) %>%\n    layer_dense(units = 392, activation = 'relu') %>%\n    layer_dropout(rate = 0.2) %>%\n    layer_dense(units = 200, activation = 'tanh') %>%\n    #layer_dropout(rate = 0.) %>%\n    layer_dense(units = 10, activation = 'softmax')\n\n\n\n\n\nmodel %>% compile(\n    loss = 'categorical_crossentropy',\n    optimizer = 'adam',\n    metrics = c('accuracy'))\n\n\n\n\n\nhist <- model %>% fit(train_x, train_y, \n                      epochs = 9, batch_size = 1000,\n                      validation_split = .2)\n\n\nplot(hist,type = \"b\")\n\n\n\n\n\ntest_own_x <- test_own[, -1] %>% as.matrix()\n\ntest_own_pred <- model %>% predict_classes(test_own_x) %>% factor()\n\nconfusionMatrix(data = test_own_pred, reference = factor(test_own$label))\n\n\n\n\ntest_x <- as.matrix(test_data)/255\n\ntest_pred <- model %>% predict_classes(test_x)\n#head(test_pred)\n\ndf_pred1 <- data.frame(ImageId = 1:length(test_pred),\n                       Label = test_pred)\n\nwrite.csv(df_pred1, file = \"sample_submission.csv\", row.names = F)"
  },
  {
    "objectID": "posts/kaggle/mnist_digits/mnist.html#tsne",
    "href": "posts/kaggle/mnist_digits/mnist.html#tsne",
    "title": "MNIST Digits",
    "section": "TSNE",
    "text": "TSNE\n\nlibrary(Rtsne)\n\ntsne_output <- Rtsne(train_x, check_duplicates = FALSE)\n\n# Generate a data frame to plot the result\ntsne_train <- data.table(tsne_x = tsne_output$Y[,1],\n                        tsne_y = tsne_output$Y[,2],\n                        label =  train_data2$label)\n\n# Plot the embedding usign ggplot and the label\nggplot(tsne_train,\n       aes(x = tsne_x, y = tsne_y, color = factor(label))) + \n  ggtitle(\"t-SNE of MNIST data set\") + \n  geom_text(aes(label = label)) +\n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/kaggle/mnist_digits/mnist.html#plot-tsne-group-means",
    "href": "posts/kaggle/mnist_digits/mnist.html#plot-tsne-group-means",
    "title": "MNIST Digits",
    "section": "Plot tsne group means",
    "text": "Plot tsne group means\n\ntsne_mean <- tsne_train[, \n                        .(mean_x = mean(tsne_x), mean_y = mean(tsne_y)),\n                        by = label]\n\n\nggplot(tsne_mean,\n       aes(x = mean_x, y = mean_y, color = factor(label))) + \n  ggtitle(\"t-SNE of MNIST data set group means\") + \n  geom_text(aes(label = label)) +\n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/kaggle/mnist_digits/mnist.html#kmeans-to-see-if-tsne-and-kmeans-agree",
    "href": "posts/kaggle/mnist_digits/mnist.html#kmeans-to-see-if-tsne-and-kmeans-agree",
    "title": "MNIST Digits",
    "section": "Kmeans to see if tsne and kmeans agree",
    "text": "Kmeans to see if tsne and kmeans agree\n\nset.seed(123)\nk_means_mnist <- kmeans(train_x, 10)\n\ntsne_train[, cluster := k_means_mnist$cluster]\nggplot(tsne_train,\n       aes(x = tsne_x, y = tsne_y, color = factor(cluster))) + \n  geom_point()+\n  ggtitle(\"t-SNE of MNIST data set\") + \n    theme(legend.position = \"none\")\n\ntsne_train[, cluster := NULL]\n\n\nlibrary(kernlab)\ncol_sum <- colSums(train_data2[, .SD, .SDcols = !\"label\"])\n\nzero_var_cols <- col_sum[col_sum == 0] %>% names()\ntrain_data2 <- train_data2[, .SD, .SDcols = !zero_var_cols]\ndf_nms <-  data.frame(vars = names(train_data2))\nwrite.csv(df_nms, file = \"df_nms.csv\", row.names = F)\nmnist_matrix <- train_data2[, .SD, .SDcols = !\"label\"] %>% na.omit %>% as.matrix()\n\n\nspec_models <- list()\ntot_withinss <- c()\nfor(i in 1:10){\n    \n    spec_fit <- specc(mnist_matrix, centers=i+1)\n    tot_withinss[i] <-withinss(spec_fit) %>% median()\n    spec_models[[i]] <- spec_fit\n}\n\nplot( tot_withinss)\n\n\nspec_fit_final <- spec_models[[4]]"
  },
  {
    "objectID": "posts/kaggle/microsoft_malware_prediction/malware_prediction.html",
    "href": "posts/kaggle/microsoft_malware_prediction/malware_prediction.html",
    "title": "Malware prediction",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(ggthemes)\nlibrary(caret)"
  },
  {
    "objectID": "posts/kaggle/microsoft_malware_prediction/malware_prediction.html#section-1",
    "href": "posts/kaggle/microsoft_malware_prediction/malware_prediction.html#section-1",
    "title": "Malware prediction",
    "section": "",
    "text": "set.seed(100)\nsample_sub <- sample(nrow(micro_train), 1000000)\nmicro_train_sub <- micro_train[sample_sub,]\n\nnrow_train <- nrow(micro_train_sub)\n\nsample_train <- sample(nrow_train, as.integer(nrow_train * 0.7))\n\nmalware_train <- micro_train_sub[sample_train]\n\nmalware_test <- micro_train_sub[-sample_train]\n\n\nna_tally <- round(colSums(is.na(micro_train_sub))/nrow(micro_train_sub) * 100, 2)\n\nna_tally <- na_tally[na_tally != 0]\n\nnms_na <- names(na_tally)\n\nna_dt <- data.table(var = nms_na, perc = na_tally)\n\nggplot(na_dt, aes(reorder(var, perc), perc)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip()+\n    theme(axis.text.x = element_text(size = 10))\n\n\nna_over25 <- na_dt[perc>25, var]\n\nmicro_train_sub[, (na_over25) := NULL]\n\nna_under25dt <- na_dt[perc <= 25]\nval_del <- na_dt[between(perc,6, 25), var]\n\nggplot(na_under25dt, aes(reorder(var, perc), perc)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip()+\n    theme(axis.text.y = element_text(size = 10))"
  },
  {
    "objectID": "posts/kaggle/who_stats/who_stats.html",
    "href": "posts/kaggle/who_stats/who_stats.html",
    "title": "World Health 2020 STATS",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(DT)\nlibrary(ggthemes)\nlibrary(leaflet)\nlibrary(ggmap)\nlibrary(plotly)\nlibrary(lubridate)\nadolescentBirthRate <- fread(\"data/adolescentBirthRate.csv\")\n\n\nold_nms <- names(adolescentBirthRate)\nold_nms\n\n[1] \"Location\"      \"Period\"        \"Indicator\"     \"First Tooltip\"\n\nold_nms <- old_nms %>% tolower()\nold_nms <- gsub(\"\\\\s\", \"_\", old_nms)\nold_nms\n\n[1] \"location\"      \"period\"        \"indicator\"     \"first_tooltip\"\n\nnames(adolescentBirthRate) <- old_nms\n\nhead(adolescentBirthRate, 10) %>% datatable()\n\n\n\n\n\n\n\nea_country <- c(\"Kenya\", \"Uganda\",  \"Tanzania\")\n\nea_data <- adolescentBirthRate[location %in% ea_country ]\n\np = ggplot(ea_data, aes(period, first_tooltip, group = location, color = location) ) +\n    geom_line(sizee = 1)+\n    theme_hc()+\n    labs(title = \"\", x = \"Year\", y = \"%\")+\n    scale_color_viridis_d(name=\"\")\np"
  },
  {
    "objectID": "posts/kaggle/heart_disease/heart_disease.html",
    "href": "posts/kaggle/heart_disease/heart_disease.html",
    "title": "Heart Disease Explainable ML",
    "section": "",
    "text": "correlation network plot ml In health research we are not only interested with prediction level but explanability of the fitted algorithm what people in health research call risk factors. We can use heart disease data set to figure out how we can utilize some of the mc.\nI have stolen your idea. For someone working in health research your notebook is a gold mine. In health especially cohort studies, clinical trials people are largely still using generalized linear models because at least from them you can get odds/risk/rate ratios, p values, AIC and so on. I have implemented the same with R(see here). I have also searched for other materials online see here. Thank you.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(caret)\nlibrary(DT)\nlibrary(iml)\nlibrary(patchwork)\nlibrary(gridExtra)\nheart <- fread(\"heart.csv\")\n\nheart %>% head %>%\n    datatable(options = list(scrollX = TRUE))"
  },
  {
    "objectID": "posts/kaggle/heart_disease/heart_disease.html#some-cleaning",
    "href": "posts/kaggle/heart_disease/heart_disease.html#some-cleaning",
    "title": "Heart Disease Explainable ML",
    "section": "Some Cleaning",
    "text": "Some Cleaning\nYou can can check the variable levels from here\n\nheart[, sex := factor(sex, \n                      levels = 0:1, \n                      labels = c(\"female\", \"male\"))]\n\n\nheart[, cp := factor(cp,\n                     levels = 0:3,\n                     labels = c(\"angina\", \"atypical angina\",\n                                \"non-anginal pain\", \"asymptomatic\") )]\n\nheart[, fbs := factor(fbs, levels = 0:1,\n                      labels = c(\"fasting blood sugar <= 120 mg/dl)\",\n                                 \"fasting blood sugar > 120 mg/dl)\"))]\n\n\nlnls_restg <- c(\"normal\", \n                 \"having ST-T wave abnormality \\n (T wave inversions and/or \\n ST elevation or depression\n                of >0.05 mV)\", \n                 \"showing probable or definite \\n left ventricular hypertrophy \\n by Estes' criteria\")\n \nheart[, restecg := factor(restecg,\n                          levels = 0:2,\n                          labels = lnls_restg)]\n\n\n\nheart[, exang := factor(exang, \n                        levels = 0:1,\n                        labels = c(\"yes\", \"no\"))]\n\n\n \nheart[, slope := factor(slope,\n                         levels = 0:2,\n                         labels = c(\"upsloping\", \"flat\", \"downsloping\"))]\n \nheart[thal == 0, thal := 1 ]\nheart[, thal := factor(thal,\n                        levels =  1:3,\n                        labels = c(\"normal\", \"fixed defect\", \"reversable defect\"))]\n\nheart[, ca := factor(ca)]\n\nheart[, target := factor(target,\n                         levels = 0:1,\n                         labels = c(\"No_heart_disease\", \"Heart_disease\"))]"
  },
  {
    "objectID": "posts/kaggle/heart_disease/heart_disease.html#visualize-categorical-variables",
    "href": "posts/kaggle/heart_disease/heart_disease.html#visualize-categorical-variables",
    "title": "Heart Disease Explainable ML",
    "section": "Visualize categorical variables",
    "text": "Visualize categorical variables\n\nnms <- names(heart)\ncateg_nms <- nms[sapply(heart, is.factor)]\n\n\ncateg_nms <- categ_nms[categ_nms != \"target\"]\nplots_categ <- list()\nfor (i in categ_nms) {\n    \n   plots_categ[[i]] = heart[, .(freq = .N), by = c(i,\"target\")] %>%\n        .[, perc := round(freq/sum(freq) * 100, 2), by = i] %>%\n        ggplot(aes_string(i, \"perc\", fill = \"target\")) +\n        geom_bar(stat = \"identity\", width = 0.5)+\n       geom_text(aes_string(i, \"perc\", label = \"perc\"),\n                 size = 3 , position =  position_stack(vjust = 0.5))+\n       theme(legend.position = \"bottom\")+\n       scale_fill_brewer(palette  = \"Dark2\")\n\n}\n\ngrid.arrange(grobs = plots_categ, ncol = 2)"
  },
  {
    "objectID": "posts/kaggle/heart_disease/heart_disease.html#visualize-numeric-variable",
    "href": "posts/kaggle/heart_disease/heart_disease.html#visualize-numeric-variable",
    "title": "Heart Disease Explainable ML",
    "section": "Visualize numeric variable",
    "text": "Visualize numeric variable\n\nnum_nms <- nms[sapply(heart, is.numeric)]\n\n# zero_one <- function(x){\n#   minx = min(x, na.rm = T)\n#   maxx = max(x, na.rm = T)\n#   \n#   z = (x - minx)/(maxx - minx)\n# }\n# \n# heart[, (num_nms) := lapply(.SD, zero_one), .SDcols = num_nms]\n\nnum_nms <- num_nms[num_nms != \"target\"]\n\nfor (i in num_nms) {\n    \n        p = ggplot(heart, aes_string(\"target\", i)) +\n            geom_boxplot()\n   print(p)\n}"
  },
  {
    "objectID": "posts/kaggle/heart_disease/heart_disease.html#fit-logistic-regression",
    "href": "posts/kaggle/heart_disease/heart_disease.html#fit-logistic-regression",
    "title": "Heart Disease Explainable ML",
    "section": "Fit logistic regression",
    "text": "Fit logistic regression\n\nglm_heart <- glm(target ~., data = heart,\n                 family = binomial())\n\nlibrary(broom)\n\ntidy(glm_heart) %>% datatable() %>%\n  formatRound(columns = c(\"estimate\", \"std.error\", \"statistic\", \"p.value\"), digits = 4)"
  },
  {
    "objectID": "posts/kaggle/heart_disease/heart_disease.html#chisq-test",
    "href": "posts/kaggle/heart_disease/heart_disease.html#chisq-test",
    "title": "Heart Disease Explainable ML",
    "section": "Chisq test",
    "text": "Chisq test\n\ndrop1(glm_heart, test = \"Chisq\") %>% tidy %>%\n    datatable()  %>%\n  formatRound(columns = c(\"df\", \"Deviance\", \"AIC\", \"LRT\", \"p.value\"), digits = 4)"
  },
  {
    "objectID": "posts/kaggle/heart_disease/heart_disease.html#ml-model",
    "href": "posts/kaggle/heart_disease/heart_disease.html#ml-model",
    "title": "Heart Disease Explainable ML",
    "section": "ML model",
    "text": "ML model\n\ntrain_ctrl <- trainControl(method = \"cv\",\n                        number = 5,\n                        summaryFunction = twoClassSummary,\n                        classProbs = TRUE,\n                        allowParallel=T,\n                        #index = cv_fold,\n                        verboseIter = FALSE,\n                        returnResamp = \"all\", \n                        savePredictions = \"final\", \n                        search = \"grid\")\n\n\n\n\n\nranger_grid <- expand.grid(splitrule = \"extratrees\",\n                        mtry = c(2, 5, 10),\n                        min.node.size = c(2, 5, 7))\n\n\nheart_randomforest <- train(target~ .,\n      data = heart,\n      trControl = train_ctrl,\n      tuneGrid = ranger_grid,\n      method = \"ranger\")\nheart_randomforest \n\nRandom Forest \n\n303 samples\n 13 predictor\n  2 classes: 'No_heart_disease', 'Heart_disease' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 242, 242, 242, 243, 243 \nResampling results across tuning parameters:\n\n  mtry  min.node.size  ROC        Sens       Spec     \n   2    2              0.9128908  0.7899471  0.8484848\n   2    5              0.9124659  0.7902116  0.8484848\n   2    7              0.9097563  0.7973545  0.8303030\n   5    2              0.9063813  0.7976190  0.8363636\n   5    5              0.9046737  0.8050265  0.8363636\n   5    7              0.9072551  0.8047619  0.8363636\n  10    2              0.8991903  0.7756614  0.8363636\n  10    5              0.9050585  0.7902116  0.8484848\n  10    7              0.9013468  0.7902116  0.8484848\n\nTuning parameter 'splitrule' was held constant at a value of extratrees\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were mtry = 2, splitrule = extratrees\n and min.node.size = 2."
  },
  {
    "objectID": "posts/kaggle/heart_disease/heart_disease.html#var-importance",
    "href": "posts/kaggle/heart_disease/heart_disease.html#var-importance",
    "title": "Heart Disease Explainable ML",
    "section": "Var Importance",
    "text": "Var Importance\n\npred <- function(heart_randomforest, heart)  {\n  results <- predict(heart_randomforest, newdata = heart, type = \"prob\")\n  return(results[[2L]])\n}"
  },
  {
    "objectID": "posts/kaggle/heart_disease/heart_disease.html#partial-dependence",
    "href": "posts/kaggle/heart_disease/heart_disease.html#partial-dependence",
    "title": "Heart Disease Explainable ML",
    "section": "Partial Dependence",
    "text": "Partial Dependence\n\nX_pred <- heart[, .SD, .SDcols = !\"target\"] %>%\n  as.data.frame()\nmodel <- Predictor$new(model = heart_randomforest, \n                      data =X_pred,\n                      predict.function = pred,\n                      y = heart$target)\neffect <- FeatureEffects$new(model)"
  },
  {
    "objectID": "posts/kaggle/heart_disease/heart_disease.html#section",
    "href": "posts/kaggle/heart_disease/heart_disease.html#section",
    "title": "Heart Disease Explainable ML",
    "section": "",
    "text": "effect$plot(features = c( \"trestbps\"))\n\n\n\n#effect"
  },
  {
    "objectID": "posts/kaggle/heart_disease/heart_disease.html#section-1",
    "href": "posts/kaggle/heart_disease/heart_disease.html#section-1",
    "title": "Heart Disease Explainable ML",
    "section": "",
    "text": "imp <- FeatureImp$new(model, loss =\"ce\" )\n#imp\n\nvar_important <-imp$results %>% data.table()\n\nsetorder(var_important, -importance)\nplot(imp)\n\n\n\n\n\ninteract <- Interaction$new(model, feature = \"thal\")\nplot(interact)\n\n\n\n\n\nlibrary(tictoc)\ntic()\n\nshap_list <- vector(\"list\", nrow(X_pred)) \n\nfor (i in 1:nrow(X_pred)) {\n  shap <- Shapley$new(model,  x.interest = X_pred[i, ], sample.size = 30)\n  shap_import <-shap$results %>% data.table()\n  shap_import <- shap_import[class == \"Heart_disease\"]\n  shap_list[[i]] <- shap_import[, record_id := i]\n\n  }\ntoc()\n\n60.202 sec elapsed\n\nshap_values <- rbindlist(shap_list, fill = T)\n\n\nlibrary(ggforce)\n\nshap_values[, feature := factor(feature, levels = rev(var_important$feature) )]\nminx <- shap_values[, min(phi.var)]\nmaxx <- shap_values[, max(phi.var)]\nggplot(shap_values, aes(feature, phi,  color = phi.var))+\n  #geom_point()+\n    ggbeeswarm::geom_quasirandom(groupOnX = FALSE, varwidth = TRUE, size = 0.9, alpha = 0.25) +\n  geom_hline(yintercept = 0) +\n  scale_color_gradient(low=\"#2187E3\", high=\"#F32858\", \n                       breaks=c(minx,maxx), labels=c(\"Low\",\"High\"), limits=c(minx,maxx))+ \n  theme_bw() + \n    theme(axis.line.y = element_blank(), \n          axis.ticks.y = element_blank(), # remove axis line\n          legend.position=\"bottom\") +\n  coord_flip()"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/clustering_fashion_mnist.html",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/clustering_fashion_mnist.html",
    "title": "Clustering Fashion mnist",
    "section": "",
    "text": "library(tidyverse)\nlibrary(knitr)\nlibrary(data.table)\n\nload(\"fashion_mnist.rda\")"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/clustering_fashion_mnist.html#process",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/clustering_fashion_mnist.html#process",
    "title": "Clustering Fashion mnist",
    "section": "Process",
    "text": "Process\n\n##\nxy_axis <- data.frame(x = expand.grid(1:28, 28:1)[,1],\n                      y = expand.grid(1:28, 28:1)[,2])\n\n\n# Get the data from the last image\nsample_10 <- fashion_mnist[sample(1:.N, 10), -1] %>% as.matrix()\nsample_10 <- t(sample_10)\n\nplot_data <- cbind(xy_axis, sample_10 )\n\nsetDT(plot_data, keep.rownames = \"pixel\")\n# Observe the first records\nhead(plot_data) %>% kable()"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/clustering_fashion_mnist.html#plot-data",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/clustering_fashion_mnist.html#plot-data",
    "title": "Clustering Fashion mnist",
    "section": "plot Data",
    "text": "plot Data\n\nplot_data_m <- melt(plot_data, id.vars = c(\"pixel\", \"x\", \"y\"))\n\n# Plot the image using ggplot()\nggplot(plot_data_m, aes(x, y, fill = value)) +\n    geom_raster()+\n     facet_wrap(~variable)+\n    scale_fill_gradient(low = \"white\",\n                        high = \"black\", guide = FALSE)+\n    theme(axis.line = element_blank(),\n                  axis.text = element_blank(),\n                  axis.ticks = element_blank(),\n                  axis.title = element_blank(),\n                  panel.background = element_blank(),\n                  panel.border = element_blank(),\n                  panel.grid.major = element_blank(),\n                  panel.grid.minor = element_blank(),\n                  plot.background = element_blank())"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/clustering_fashion_mnist.html#choosing-k",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/clustering_fashion_mnist.html#choosing-k",
    "title": "Clustering Fashion mnist",
    "section": "Choosing K",
    "text": "Choosing K\n\nset.seed(100)\n\nfashion_small <- fashion_mnist[sample(1:.N, 10000)]\n\nkmeans_list <- list()\ntot_withinss <- c()\nfor (k in 1:20) {\n  \n  kmeans_mnist <- kmeans(fashion_small[, -1], centers =  k)\n  tot_withinss[k] <- kmeans_mnist$tot.withinss\n}\n\n\nplot(tot_withinss)"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "",
    "text": "You will use the MNIST dataset in several exercises through the course. Let’s do some data exploration to gain a better understanding. Remember that the MNIST dataset contains a set of records that represent handwritten digits using 28x28 features, which are stored into a 784-dimensional vector.\nmnistInput  Each record of the MNIST dataset corresponds to a handwritten digit and each feature represents one pixel of the digit image. In this exercise, a sample of 200 records of the MNIST dataset named mnist_sample is loaded for you.\n\nload(\"mnist-sample-200.RData\")\n# Have a look at the MNIST dataset names\n#names(mnist_sample)\n\n# Show the first records\n#str(mnist_sample)\n\n# Labels of the first 6 digits\nhead(mnist_sample$label)\n\n[1] 5 0 7 0 9 3"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#digits-features",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#digits-features",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Digits features",
    "text": "Digits features\nLet’s continue exploring the dataset. Firstly, it would be helpful to know how many different digits are present by computing a histogram of the labels. Next, the basic statistics (min, mean, median, maximum) of the features for all digits can be calculated. Finally, you will compute the basic statistics for only those digits with label 0. The MNIST sample data is loaded for you as mnist_sample.\n\n# Plot the histogram of the digit labels\nhist(mnist_sample$label)\n\n\n\n# Compute the basic statistics of all records\n#summary(mnist_sample)\n\n# Compute the basic statistics of digits with label 0\n#summary(mnist_sample[, mnist_sample$label == 0])"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#euclidean-distance",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#euclidean-distance",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Euclidean distance",
    "text": "Euclidean distance\nEuclidean distance is the basis of many measures of similarity and is the most important distance metric. You can compute the Euclidean distance in R using the dist() function. In this exercise, you will compute the Euclidean distance between the first 10 records of the MNIST sample data.\nThe mnist_sample object is loaded for you.\n\n# Show the labels of the first 10 records\nmnist_sample$label[1:10]\n\n [1] 5 0 7 0 9 3 4 1 2 6\n\n# Compute the Euclidean distance of the first 10 records\ndistances <- dist(mnist_sample[1:10, -1])\n\n# Show the distances values\ndistances\n\n          1        2        3        4        5        6        7        8\n2  2185.551                                                               \n3  2656.407 2869.979                                                      \n4  2547.027 2341.249 2936.772                                             \n5  2406.509 2959.108 1976.406 2870.928                                    \n6  2343.982 2759.681 2452.568 2739.470 2125.723                           \n7  2464.388 2784.158 2573.667 2870.918 2174.322 2653.703                  \n8  2149.872 2668.903 1999.892 2585.980 2067.044 2273.248 2407.962         \n9  2959.129 3209.677 2935.262 3413.821 2870.746 3114.508 2980.663 2833.447\n10 2728.657 3009.771 2574.752 2832.521 2395.589 2655.864 2464.301 2550.126\n          9\n2          \n3          \n4          \n5          \n6          \n7          \n8          \n9          \n10 2695.166\n\n# Plot the numeric matrix of the distances in a heatmap\nheatmap(as.matrix(distances), \n        Rowv = NA, symm = TRUE, \n        labRow = mnist_sample$label[1:10], \n        labCol = mnist_sample$label[1:10])"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#minkowsky-distance",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#minkowsky-distance",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Minkowsky distance",
    "text": "Minkowsky distance\nThere are other well-known distance metrics besides the Euclidean distance, like the Minkowski distance. This metric can be considered a generalisation of both the Euclidean and Manhattan distance. In R, you can calculate the Minkowsky distance of order p by using dist(…, method = “minkowski”, p).\nThe MNIST sample data is loaded for you as mnist_sample\n\n# Minkowski distance or order 3\ndistances_3 <- dist(mnist_sample[1:10, -1], method = \"minkowski\", p = 3)\n\ndistances_3 \n\n           1         2         3         4         5         6         7\n2  1002.6468                                                            \n3  1169.6470 1228.8295                                                  \n4  1127.4919 1044.9182 1249.6133                                        \n5  1091.3114 1260.3549  941.1654 1231.7432                              \n6  1063.7026 1194.1212 1104.2581 1189.9558  996.2687                    \n7  1098.4279 1198.8891 1131.4498 1227.7888 1005.7588 1165.4475          \n8  1006.9070 1169.4720  950.6812 1143.3503  980.6450 1056.1814 1083.2255\n9  1270.0240 1337.2068 1257.4052 1401.2461 1248.0777 1319.2768 1271.7095\n10 1186.9620 1268.1539 1134.0371 1219.1388 1084.5416 1166.9129 1096.3586\n           8         9\n2                     \n3                     \n4                     \n5                     \n6                     \n7                     \n8                     \n9  1236.9178          \n10 1133.2929 1180.7970\n\nheatmap(as.matrix(distances_3 ), \n        Rowv = NA, symm = TRUE, \n        labRow = mnist_sample$label[1:10], \n        labCol = mnist_sample$label[1:10])\n\n\n\n# Minkowski distance of order 2\ndistances_2 <- dist(mnist_sample[1:10, -1], method = \"minkowski\", p = 2)\ndistances_2\n\n          1        2        3        4        5        6        7        8\n2  2185.551                                                               \n3  2656.407 2869.979                                                      \n4  2547.027 2341.249 2936.772                                             \n5  2406.509 2959.108 1976.406 2870.928                                    \n6  2343.982 2759.681 2452.568 2739.470 2125.723                           \n7  2464.388 2784.158 2573.667 2870.918 2174.322 2653.703                  \n8  2149.872 2668.903 1999.892 2585.980 2067.044 2273.248 2407.962         \n9  2959.129 3209.677 2935.262 3413.821 2870.746 3114.508 2980.663 2833.447\n10 2728.657 3009.771 2574.752 2832.521 2395.589 2655.864 2464.301 2550.126\n          9\n2          \n3          \n4          \n5          \n6          \n7          \n8          \n9          \n10 2695.166\n\nheatmap(as.matrix(distances_2), \n        Rowv = NA, symm = TRUE, \n        labRow = mnist_sample$label[1:10], \n        labCol = mnist_sample$label[1:10])\n\n\n\n\n\nVery Good! As you can see, when using Minkowski distance of order 2 the most similar digits are in positions 3 and 5 of the heatmap grid which corresponds to digits 7 and 9."
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#kl-divergence",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#kl-divergence",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "KL divergence",
    "text": "KL divergence\nThere are more distance metrics that can be used to compute how similar two feature vectors are. For instance, the philentropy package has the function distance(), which implements 46 different distance metrics. For more information, use ?distance in the console. In this exercise, you will compute the KL divergence and check if the results differ from the previous metrics. Since the KL divergence is a measure of the difference between probability distributions you need to rescale the input data by dividing each input feature by the total pixel intensities of that digit. The philentropy package and mnist_sample data have been loaded.\n\nlibrary(philentropy)\nlibrary(tidyverse)\n# Get the first 10 records\nmnist_10 <- mnist_sample[1:10, -1]\n\n# Add 1 to avoid NaN when rescaling\nmnist_10_prep <- mnist_10 + 1 \n\n# Compute the sums per row\nsums <- rowSums(mnist_10_prep)\n\n# Compute KL divergence\ndistances <- distance(mnist_10_prep/sums, method = \"kullback-leibler\")\nheatmap(as.matrix(distances), \n        Rowv = NA, symm = TRUE, \n        labRow = mnist_sample$label[1:10], \n        labCol = mnist_sample$label[1:10])"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#generating-pca-from-mnist-sample",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#generating-pca-from-mnist-sample",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Generating PCA from MNIST sample",
    "text": "Generating PCA from MNIST sample\nYou are going to compute a PCA with the previous mnist_sample dataset. The goal is to have a good representation of each digit in a lower dimensional space. PCA will give you a set of variables, named principal components, that are a linear combination of the input variables. These principal components are ordered in terms of the variance they capture from the original data. So, if you plot the first two principal components you can see the digits in a 2-dimensional space. A sample of 200 records of the MNIST dataset named mnist_sample is loaded for you.\n\n# Get the principal components from PCA\npca_output <- prcomp(mnist_sample[, -1])\n\n# Observe a summary of the output\n#summary(pca_output)\n\n# Store the first two coordinates and the label in a data frame\npca_plot <- data.frame(pca_x = pca_output$x[, \"PC1\"], pca_y = pca_output$x[, \"PC2\"], \n                       label = as.factor(mnist_sample$label))\n\n# Plot the first two principal components using the true labels as color and shape\nggplot(pca_plot, aes(x = pca_x, y = pca_y, color = label)) + \n    ggtitle(\"PCA of MNIST sample\") + \n    geom_text(aes(label = label)) + \n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#t-sne-output-from-mnist-sample",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#t-sne-output-from-mnist-sample",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "t-SNE output from MNIST sample",
    "text": "t-SNE output from MNIST sample\nYou have seen that PCA has some limitations in correctly classifying digits, mainly due to its linear nature. In this exercise, you are going to use the output from the t-SNE algorithm on the MNIST sample data, named tsne_output and visualize the obtained results. In the next chapter, you will focus on the t-SNE algorithm and learn more about how to use it! The MNIST sample dataset mnist_sample as well as the tsne_output are available in your workspace.\n\n# Explore the tsne_output structure\nlibrary(Rtsne)\nlibrary(tidyverse)\ntsne_output <- Rtsne(mnist_sample[, -1])\n#str(tsne_output)\n\n# Have a look at the first records from the t-SNE output\n#head(tsne_output)\n\n# Store the first two coordinates and the label in a data.frame\ntsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], \n                        label = as.factor(mnist_sample$label))\n\n# Plot the t-SNE embedding using the true labels as color and shape\nggplot(tsne_plot, aes(x =tsne_x, y = tsne_y, color = label)) + \n    ggtitle(\"T-Sne output\") + \n    geom_text(aes(label = label)) + \n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-t-sne",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-t-sne",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Computing t-SNE",
    "text": "Computing t-SNE\nAs we have seen, the t-SNE embedding can be computed in R using the Rtsne() function from the Rtsne package in CRAN. Performing a PCA is a common step before running the t-SNE algorithm, but we can skip this step by setting the parameter PCA to FALSE. The dimensionality of the embedding generated by t-SNE can be indicated with the dims parameter. In this exercise, we will generate a three-dimensional embedding from the mnist_sample dataset without doing the PCA step and then, we will plot the first two dimensions. The MNIST sample dataset mnist_sample, as well as the Rtsne and ggplot2 packages, are already loaded.\n\n# Compute t-SNE without doing the PCA step\ntsne_output <- Rtsne(mnist_sample[,-1], PCA = FALSE, dim = 3)\n\n# Show the obtained embedding coordinates\nhead(tsne_output$Y)\n\n           [,1]       [,2]       [,3]\n[1,]  4.4274487  7.7605276 -0.5197074\n[2,]  9.9893878  4.9104756  4.4038968\n[3,] -6.5046061  1.6543631 -6.7623865\n[4,] 11.6806192  8.4540456 -1.6885239\n[5,] -6.9255404 -0.1748614 -3.4563984\n[6,]  0.1309388  7.9495000 -6.9920607\n\n# Store the first two coordinates and plot them \ntsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], \n                        digit = as.factor(mnist_sample$label))\n\n# Plot the coordinates\nggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = digit)) + \n    ggtitle(\"t-SNE of MNIST sample\") + \n    geom_text(aes(label = digit)) + \n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#understanding-t-sne-output",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#understanding-t-sne-output",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Understanding t-SNE output",
    "text": "Understanding t-SNE output\nThe most important t-SNE output are those related to the K-L divergence of the points in the original high dimensions and in the new lower dimensional space. Remember that the goal of t-SNE is to minimize the K-L divergence between the original space and the new one. In the returned object, the itercosts structure indicates the total cost from the K-L divergence of all the objects in each 50th iteration and the cost structure indicates the K-L divergence of each record in the final iteration. The Rtsne package and the tsne_output object have been loaded for you.\n\n# Inspect the output object's structure\nstr(tsne_output)\n\nList of 14\n $ N                  : int 200\n $ Y                  : num [1:200, 1:3] 4.43 9.99 -6.5 11.68 -6.93 ...\n $ costs              : num [1:200] 0.002776 0.002078 0.000567 0.002233 0.002067 ...\n $ itercosts          : num [1:20] 52.9 52 53.2 54.8 52.1 ...\n $ origD              : int 50\n $ perplexity         : num 30\n $ theta              : num 0.5\n $ max_iter           : num 1000\n $ stop_lying_iter    : int 250\n $ mom_switch_iter    : int 250\n $ momentum           : num 0.5\n $ final_momentum     : num 0.8\n $ eta                : num 200\n $ exaggeration_factor: num 12\n - attr(*, \"class\")= chr [1:2] \"Rtsne\" \"list\"\n\n# Show total costs after each 50th iteration\ntsne_output$itercosts\n\n [1] 52.9276654 52.0308168 53.1766488 54.7704357 52.1195009  0.8625006\n [7]  0.5818295  0.5451470  0.5259562  0.5093388  0.5035012  0.4968422\n[13]  0.4934233  0.4912052  0.4898623  0.4908332  0.4892283  0.4886758\n[19]  0.4886087  0.4905542\n\n# Plot the evolution of the KL divergence at each 50th iteration\nplot(tsne_output$itercosts, type = \"l\")\n\n\n\n# Inspect the output object's structure\nstr(tsne_output)\n\nList of 14\n $ N                  : int 200\n $ Y                  : num [1:200, 1:3] 4.43 9.99 -6.5 11.68 -6.93 ...\n $ costs              : num [1:200] 0.002776 0.002078 0.000567 0.002233 0.002067 ...\n $ itercosts          : num [1:20] 52.9 52 53.2 54.8 52.1 ...\n $ origD              : int 50\n $ perplexity         : num 30\n $ theta              : num 0.5\n $ max_iter           : num 1000\n $ stop_lying_iter    : int 250\n $ mom_switch_iter    : int 250\n $ momentum           : num 0.5\n $ final_momentum     : num 0.8\n $ eta                : num 200\n $ exaggeration_factor: num 12\n - attr(*, \"class\")= chr [1:2] \"Rtsne\" \"list\"\n\n# Show the K-L divergence of each record after the final iteration\ntsne_output$costs\n\n  [1]  2.775998e-03  2.078056e-03  5.667433e-04  2.232501e-03  2.066504e-03\n  [6]  3.964030e-03  2.703930e-03  1.339691e-03  7.859118e-04  1.524972e-03\n [11]  1.886400e-03  3.702240e-03  2.506202e-03  1.517840e-03  4.725808e-03\n [16]  9.332649e-04  2.994200e-03  2.713781e-03  6.038009e-04  2.512080e-03\n [21]  1.749325e-03  1.807814e-03  3.296634e-03  2.281078e-03  2.418445e-03\n [26]  2.183296e-03  1.678566e-03  4.949457e-03  2.833002e-03  1.018518e-03\n [31]  5.036012e-03  2.437067e-03  4.265377e-03  1.513867e-03  2.124027e-03\n [36]  3.172187e-03  4.820149e-03  2.950721e-03  1.557523e-03  3.296434e-03\n [41]  4.261416e-03  3.347000e-03  2.548277e-03  1.445304e-03  2.717932e-03\n [46]  3.077103e-03  2.345960e-03  5.048646e-03  1.353809e-03  4.242547e-03\n [51]  2.658459e-03  3.992626e-03  1.465439e-03  5.024664e-03  2.501205e-03\n [56]  7.049714e-03  1.233719e-03  4.437742e-03  1.930034e-04  2.176070e-03\n [61]  2.846461e-03  1.297340e-03  8.581182e-04  8.945278e-04  2.237074e-03\n [66]  1.061661e-03  8.388055e-04  1.153219e-03  1.483181e-03  8.788746e-04\n [71]  2.811429e-03  6.801166e-03  5.280635e-04  2.916963e-03  1.343067e-03\n [76]  2.958862e-03  4.410710e-04  2.218482e-03  1.263025e-03  1.646616e-03\n [81]  2.644623e-03  2.317014e-03  4.825757e-03  1.491847e-03  6.199187e-03\n [86]  2.483666e-03  1.442001e-03  3.590866e-03  1.073715e-03  2.678226e-03\n [91]  4.055305e-03  7.048150e-04  3.504859e-03  5.330879e-05  6.233375e-03\n [96]  3.658606e-03  1.254033e-03  1.032559e-03  1.886144e-03  2.377601e-03\n[101]  1.398113e-03  2.735677e-03  1.495671e-03  1.175715e-03  1.775032e-03\n[106]  1.840000e-03  1.965743e-03  2.655318e-03  1.744841e-03  2.524692e-03\n[111]  1.766776e-03  3.781543e-03  1.758568e-03  2.618387e-03  2.896275e-03\n[116]  3.521323e-03  2.542332e-03  3.541741e-03  2.956777e-04  4.606826e-03\n[121]  2.338036e-03  4.462357e-03  5.491110e-04  1.594398e-03  4.079463e-03\n[126]  4.112513e-03  3.361246e-03  2.211740e-03  3.523704e-03  3.011101e-03\n[131]  8.552411e-03  1.804303e-03  2.977308e-03  1.557557e-03  1.031122e-03\n[136]  2.831289e-03  4.644114e-03  9.511080e-04  1.363849e-03  3.527327e-03\n[141]  1.506728e-03  2.987430e-03  2.924926e-03  1.078563e-03  1.067037e-03\n[146]  3.851335e-03  2.433180e-03  1.377925e-03  1.585860e-03  2.167820e-03\n[151] -6.910941e-05  1.337391e-03  2.449147e-03  2.454175e-03  5.212642e-03\n[156]  2.659607e-03  6.626958e-03  3.359798e-03  2.503062e-04  1.239913e-03\n[161]  1.684649e-03  4.350569e-04  3.986517e-03  1.703512e-03  1.264220e-03\n[166]  1.560058e-03  1.420816e-03  1.941856e-03  3.537382e-03  1.365938e-03\n[171]  9.452384e-04  1.334534e-03  1.836216e-03  2.440162e-03  2.809922e-03\n[176]  4.514816e-03  2.289119e-03  2.215245e-03  4.518395e-03  2.462566e-03\n[181]  1.580016e-03  1.994960e-03  2.258175e-03  1.126741e-03  1.895611e-03\n[186]  1.231663e-03  1.108926e-03  3.456170e-03  3.377794e-03  1.930576e-03\n[191]  3.578342e-03  2.068121e-03  2.769818e-03  2.580929e-03  2.579200e-03\n[196]  1.310345e-03  1.862954e-03  2.731467e-03  3.290436e-03  1.300175e-03\n\n# Plot the K-L divergence of each record after the final iteration\nplot(tsne_output$costs, type = \"l\")"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#reproducing-results",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#reproducing-results",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Reproducing results",
    "text": "Reproducing results\nt-SNE is a stochastic algorithm, meaning there is some randomness inherent to the process. To ensure reproducible results it is necessary to fix a seed before every new execution. This way, you can tune the algorithm hyper-parameters and isolate the effect of the randomness. In this exercise, the goal is to generate two embeddings and check that they are identical. The mnist_sample dataset is available in your workspace.\n\n# Generate a three-dimensional t-SNE embedding without PCA\ntsne_output <- Rtsne(mnist_sample[, -1], PCA = FALSE, dim = 3)\n\n# Generate a new t-SNE embedding with the same hyper-parameter values\ntsne_output_new <- Rtsne(mnist_sample[, -1], PCA = FALSE, dim = 3)\n\n# Check if the two outputs are identical\nidentical(tsne_output, tsne_output_new)\n\n[1] FALSE\n\n# Generate a three-dimensional t-SNE embedding without PCA\nset.seed(1234)\ntsne_output <- Rtsne(mnist_sample[, -1], PCA = FALSE, dims = 3)\n\n# Generate a new t-SNE embedding with the same hyper-parameter values\nset.seed(1234)\ntsne_output_new <- Rtsne(mnist_sample[, -1], PCA = FALSE, dims = 3)\n\n# Check if the two outputs are identical\nidentical(tsne_output, tsne_output_new)\n\n[1] TRUE"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#optimal-number-of-iterations",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#optimal-number-of-iterations",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Optimal number of iterations",
    "text": "Optimal number of iterations\nA common hyper-parameter to optimize in t-SNE is the optimal number of iterations. As you have seen before it is important to always use the same seed before you can compare different executions. To optimize the number of iterations, you can increase the max_iter parameter of Rtsne() and observe the returned itercosts to find the minimum K-L divergence. The mnist_sample dataset and the Rtsne package have been loaded for you.\n\n# Set seed to ensure reproducible results\nset.seed(1234)\n\n# Execute a t-SNE with 2000 iterations\ntsne_output <- Rtsne(mnist_sample[, -1], max_iter = 2000,PCA = TRUE, dims = 2)\n\n# Observe the output costs \ntsne_output$itercosts\n\n [1] 53.3290786 53.2244985 52.2552243 53.0855081 53.2869529  1.0962201\n [7]  0.8253614  0.7929360  0.7815495  0.7787592  0.7786192  0.7713815\n[13]  0.7651633  0.7471433  0.7454131  0.7458568  0.7475261  0.7453661\n[19]  0.7439994  0.7298887  0.6989778  0.6993673  0.7012341  0.6989279\n[25]  0.6974807  0.6975004  0.6955154  0.6984345  0.7003433  0.7004715\n[31]  0.7002442  0.7002176  0.7003945  0.6978038  0.6970268  0.6995675\n[37]  0.6978879  0.6997786  0.6996063  0.7009296\n\n# Get the 50th iteration with the minimum K-L cost\nwhich.min(tsne_output$itercosts)\n\n[1] 27"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#perplexity-of-mnist-sample",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#perplexity-of-mnist-sample",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Perplexity of MNIST sample",
    "text": "Perplexity of MNIST sample\nThe perplexity parameter indicates the balance between the local and global aspect of the input data. The parameter is an estimate of the number of close neighbors of each original point. Typical values of this parameter fall in the range of 5 to 50. We will generate three different t-SNE executions with the same number of iterations and perplexity values of 5, 20, and 50 and observe the differences in the K-L divergence costs. The optimal number of iterations we found in the last exercise (1200) will be used here. The mnist_sample dataset and the Rtsne package have been loaded for you.\n\n# Set seed to ensure reproducible results\npar(mfrow = c(3, 1))\nset.seed(1234)\n\nperp <- c(5, 20, 50)\nmodels <- list()\nfor (i in 1:length(perp)) {\n        \n        # Execute a t-SNE with perplexity 5\n        perplexity  = perp[i]\n        tsne_output <- Rtsne(mnist_sample[, -1], perplexity = perplexity, max_iter = 1300)\n        # Observe the returned K-L divergence costs at every 50th iteration\n        models[[i]] <- tsne_output\n        plot(tsne_output$itercosts,\n             main = paste(\"Perplexity\", perplexity),\n             type = \"l\", ylab = \"itercosts\")\n}\n\n\n\nnames(models) <- paste0(\"perplexity\",perp)"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#perplexity-of-bigger-mnist-dataset",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#perplexity-of-bigger-mnist-dataset",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Perplexity of bigger MNIST dataset",
    "text": "Perplexity of bigger MNIST dataset\nNow, let’s investigate the effect of the perplexity values with a bigger MNIST dataset of 10.000 records. It would take a lot of time to execute t-SNE for this many records on the DataCamp platform. This is why the pre-loaded output of two t-SNE embeddings with perplexity values of 5 and 50, named tsne_output_5 and tsne_output_50 are available in the workspace. We will look at the K-L costs and plot them using the digit label from the mnist_10k dataset, which is also available in the environment. The Rtsne and ggplot2 packages have been loaded.\n\nI used mnist smaller data set\n\n\n# Observe the K-L divergence costs with perplexity 5 and 50\ntsne_output_5 <- models$perplexity5\ntsne_output_50  <- models$perplexity50\n# Generate the data frame to visualize the embedding\ntsne_plot_5 <- data.frame(tsne_x = tsne_output_5$Y[, 1], tsne_y = tsne_output_5$Y[, 2], digit = as.factor(mnist_sample$label))\n\ntsne_plot_50 <- data.frame(tsne_x = tsne_output_50$Y[, 1], tsne_y = tsne_output_50$Y[, 2], digit = as.factor(mnist_sample$label))\n\n# Plot the obtained embeddings\nggplot(tsne_plot_5, aes(x = tsne_x, y = tsne_y, color = digit)) + \n    ggtitle(\"MNIST t-SNE with 1300 iter and Perplexity=5\") +\n    geom_text(aes(label = digit)) + \n    theme(legend.position=\"none\")\n\n\n\nggplot(tsne_plot_50, aes(x = tsne_x, y = tsne_y, color = digit)) + \n    ggtitle(\"MNIST t-SNE with 1300 iter and Perplexity=50\") + \n    geom_text(aes(label = digit)) + \n    theme(legend.position=\"none\")"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#plotting-spatial-distribution-of-true-classes",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#plotting-spatial-distribution-of-true-classes",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Plotting spatial distribution of true classes",
    "text": "Plotting spatial distribution of true classes\nAs seen in the video, you can use the obtained representation of t-SNE in a lower dimension space to classify new digits based on the Euclidean distance to known clusters of digits. For this task, let’s start with plotting the spatial distribution of the digit labels in the embedding space. You are going to use the output of a t-SNE execution of 10K MNIST records named tsne and the true labels can be found in a dataset named mnist_10k. In this exercise, you will use the first 5K records of tsne and mnist_10k datasets and the goal is to visualize the obtained t-SNE embedding. The ggplot2 package has been loaded for you.\n\nlibrary(data.table)\nmnist_10k <- readRDS(\"mnist_10k.rds\") %>% setDT()\ntsne <- Rtsne(mnist_10k[, -1], perplexity = 50, max_iter = 1500)\n# Prepare the data.frame\ntsne_plot <- data.frame(tsne_x = tsne$Y[1:5000, 1], \n                        tsne_y = tsne$Y[1:5000, 2], \n                        digit = as.factor(mnist_10k[1:5000, ]$label))\n\n# Plot the obtained embedding\nggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = digit)) + \n    ggtitle(\"MNIST embedding of the first 5K digits\") + \n    geom_text(aes(label = digit)) + \n    theme(legend.position=\"none\")"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-the-centroids-of-each-class",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-the-centroids-of-each-class",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Computing the centroids of each class",
    "text": "Computing the centroids of each class\nSince the previous visual representation of the digit in a low dimensional space makes sense, you want to compute the centroid of each class in this lower dimensional space. This centroid can be used as a prototype of the digit and you can classify new digits based on their Euclidean distance to these ones. The MNIST data mnist_10k and t-SNE output tsne are available in the workspace. The data.table package has been loaded for you.\n\n# Get the first 5K records and set the column names\ndt_prototypes <- as.data.table(tsne$Y[1:5000,])\nsetnames(dt_prototypes, c(\"X\",\"Y\"))\n\n# Paste the label column as factor\ndt_prototypes[, label := as.factor(mnist_10k[1:5000,]$label)]\n\n# Compute the centroids per label\ndt_prototypes[, mean_X := mean(X), by = label]\ndt_prototypes[, mean_Y := mean(Y), by = label]\n\n# Get the unique records per label\ndt_prototypes <- unique(dt_prototypes, by = \"label\")\ndt_prototypes\n\n             X          Y label     mean_X     mean_Y\n 1:  34.835576  16.996845     7  30.167171  13.979335\n 2:  16.417303   3.914098     4  23.015486  -4.668565\n 3:   8.523417  24.050519     1   1.162703  33.822984\n 4:   9.346855 -26.402177     6   3.616021 -27.402390\n 5: -11.365115  -6.049854     5  -6.425186  -8.957745\n 6:  -5.704105   7.608415     8  -8.415119   6.278887\n 7: -34.408062   2.209488     3 -25.853224  -1.354711\n 8: -29.504694  16.977549     2 -20.059132  21.122323\n 9: -19.469866 -30.616292     0 -17.952830 -32.331439\n10:  27.605614   4.103024     9  20.453515  -3.364576"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-similarities-of-digits-1-and-0",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-similarities-of-digits-1-and-0",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Computing similarities of digits 1 and 0",
    "text": "Computing similarities of digits 1 and 0\nOne way to measure the label similarity for each digit is by computing the Euclidean distance in the lower dimensional space obtained from the t-SNE algorithm. You need to use the previously calculated centroids stored in dt_prototypes and compute the Euclidean distance to the centroid of digit 1 for the last 5000 records from tsne and mnist_10k datasets that are labeled either as 1 or 0. Note that the last 5000 records of tsne were not used before. The MNIST data mnist_10k and t-SNE output tsne are available in the workspace. The data.table package has been loaded for you.\n\n# Store the last 5000 records in distances and set column names\ndistances <- as.data.table(tsne$Y[5001:10000,])\nsetnames(distances, c(\"X\", \"Y\"))\n# Paste the true label\ndistances[, label := mnist_10k[5001:10000,]$label]\ndistances[, mean_X := mean(X), by = label]\ndistances[, mean_Y := mean(Y), by = label]\n\n\n# Filter only those labels that are 1 or 0 \ndistances_filtered <- distances[label == 1 | label == 0]\n\n# Compute Euclidean distance to prototype of digit 1\ndistances_filtered[, dist_1 := sqrt( (X - dt_prototypes[label == 1,]$mean_X)^2 + \n                             (Y - dt_prototypes[label == 1,]$mean_Y)^2)]"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#plotting-similarities-of-digits-1-and-0",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#plotting-similarities-of-digits-1-and-0",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Plotting similarities of digits 1 and 0",
    "text": "Plotting similarities of digits 1 and 0\nIn distances, the distances of 1108 records to the centroid of digit 1 are stored in dist_1. Those records correspond to digits you already know are 1’s or 0’s. You can have a look at the basic statistics of the distances from records that you know are 0 and 1 (label column) to the centroid of class 1 using summary(). Also, if you plot a histogram of those distances and fill them with the label you can check if you are doing a good job identifying the two classes with this t-SNE classifier. The data.table and ggplot2 packages, as well as the distances object, have been loaded for you.\n\n# Compute the basic statistics of distances from records of class 1\nsummary(distances_filtered[label == 1]$dist_1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.9694  6.5282  8.8059  9.0494 11.6443 54.7206 \n\n# Compute the basic statistics of distances from records of class 0\nsummary(distances_filtered[label == 0]$dist_1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  49.54   64.67   68.42   68.86   73.02   78.97 \n\n# Plot the histogram of distances of each class\nggplot(distances_filtered, \n       aes(x = dist_1, fill = as.factor(label))) +\n    geom_histogram(binwidth = 5, alpha = .5, \n                   position = \"identity\", show.legend = FALSE) + \n    ggtitle(\"Distribution of Euclidean distance 1 vs 0\")"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#exploring-credit-card-fraud-dataset",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#exploring-credit-card-fraud-dataset",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Exploring credit card fraud dataset",
    "text": "Exploring credit card fraud dataset\nIn this exercise, you will do some data exploration on a sample of the credit card fraud detection dataset from Kaggle. For any problem, starting with some data exploration is a good practice and helps us better understand the characteristics of the data.\nThe credit card fraud dataset is already loaded in the environment as a data table with the name creditcard. As you saw in the video, it consists of 30 numerical variables. The Class column indicates if the transaction is fraudulent. The ggplot2 package has been loaded for you.\n\nload(\"creditcard.RData\") \n\nsetDT(creditcard)\n# Look at the data dimensions\ndim(creditcard)\n\n[1] 28923    31\n\n# Explore the column names\n#names(creditcard)\n\n# Explore the structure\n#str(creditcard)\n\n# Generate a summary\nsummary(creditcard)\n\n      Time              V1                  V2                  V3          \n Min.   :    26   Min.   :-56.40751   Min.   :-72.71573   Min.   :-31.1037  \n 1st Qu.: 54230   1st Qu.: -0.96058   1st Qu.: -0.58847   1st Qu.: -0.9353  \n Median : 84512   Median : -0.02400   Median :  0.08293   Median :  0.1659  \n Mean   : 94493   Mean   : -0.08501   Mean   :  0.05955   Mean   : -0.1021  \n 3rd Qu.:139052   3rd Qu.:  1.30262   3rd Qu.:  0.84003   3rd Qu.:  1.0119  \n Max.   :172788   Max.   :  2.41150   Max.   : 22.05773   Max.   :  3.8771  \n       V4                  V5                  V6           \n Min.   :-5.071241   Min.   :-31.35675   Min.   :-26.16051  \n 1st Qu.:-0.824978   1st Qu.: -0.70869   1st Qu.: -0.78792  \n Median : 0.007618   Median : -0.06071   Median : -0.28396  \n Mean   : 0.073391   Mean   : -0.04367   Mean   : -0.02722  \n 3rd Qu.: 0.789293   3rd Qu.:  0.61625   3rd Qu.:  0.37911  \n Max.   :16.491217   Max.   : 34.80167   Max.   : 20.37952  \n       V7                  V8                  V9           \n Min.   :-43.55724   Min.   :-50.42009   Min.   :-13.43407  \n 1st Qu.: -0.57404   1st Qu.: -0.21025   1st Qu.: -0.66974  \n Median :  0.02951   Median :  0.01960   Median : -0.06343  \n Mean   : -0.08873   Mean   : -0.00589   Mean   : -0.04295  \n 3rd Qu.:  0.57364   3rd Qu.:  0.33457   3rd Qu.:  0.58734  \n Max.   : 29.20587   Max.   : 20.00721   Max.   :  8.95567  \n      V10                 V11                V12                V13           \n Min.   :-24.58826   Min.   :-4.11026   Min.   :-18.6837   Min.   :-3.844974  \n 1st Qu.: -0.54827   1st Qu.:-0.75404   1st Qu.: -0.4365   1st Qu.:-0.661168  \n Median : -0.09843   Median :-0.01036   Median :  0.1223   Median :-0.009685  \n Mean   : -0.08468   Mean   : 0.06093   Mean   : -0.0943   Mean   :-0.002110  \n 3rd Qu.:  0.44762   3rd Qu.: 0.77394   3rd Qu.:  0.6172   3rd Qu.: 0.664794  \n Max.   : 15.33174   Max.   :12.01891   Max.   :  4.8465   Max.   : 4.569009  \n      V14                 V15                 V16                 V17          \n Min.   :-19.21432   Min.   :-4.498945   Min.   :-14.12985   Min.   :-25.1628  \n 1st Qu.: -0.44507   1st Qu.:-0.595272   1st Qu.: -0.48770   1st Qu.: -0.4951  \n Median :  0.04865   Median : 0.045992   Median :  0.05736   Median : -0.0742  \n Mean   : -0.09653   Mean   :-0.007251   Mean   : -0.06186   Mean   : -0.1046  \n 3rd Qu.:  0.48765   3rd Qu.: 0.646584   3rd Qu.:  0.52147   3rd Qu.:  0.3956  \n Max.   :  7.75460   Max.   : 5.784514   Max.   :  5.99826   Max.   :  7.2150  \n      V18                V19                 V20            \n Min.   :-9.49875   Min.   :-4.395283   Min.   :-20.097918  \n 1st Qu.:-0.51916   1st Qu.:-0.462158   1st Qu.: -0.211663  \n Median :-0.01595   Median : 0.010494   Median : -0.059160  \n Mean   :-0.04344   Mean   : 0.009424   Mean   :  0.006943  \n 3rd Qu.: 0.48634   3rd Qu.: 0.471172   3rd Qu.:  0.141272  \n Max.   : 3.88618   Max.   : 5.228342   Max.   : 24.133894  \n      V21                  V22                 V23           \n Min.   :-22.889347   Min.   :-8.887017   Min.   :-36.66600  \n 1st Qu.: -0.230393   1st Qu.:-0.550210   1st Qu.: -0.16093  \n Median : -0.028097   Median :-0.000187   Median : -0.00756  \n Mean   :  0.004995   Mean   :-0.006271   Mean   :  0.00418  \n 3rd Qu.:  0.190465   3rd Qu.: 0.516596   3rd Qu.:  0.15509  \n Max.   : 27.202839   Max.   : 8.361985   Max.   : 13.65946  \n      V24                 V25                 V26           \n Min.   :-2.822684   Min.   :-6.712624   Min.   :-1.658162  \n 1st Qu.:-0.354367   1st Qu.:-0.319410   1st Qu.:-0.328496  \n Median : 0.038722   Median : 0.011815   Median :-0.054131  \n Mean   : 0.000741   Mean   :-0.002847   Mean   :-0.002546  \n 3rd Qu.: 0.440797   3rd Qu.: 0.351797   3rd Qu.: 0.237782  \n Max.   : 3.962197   Max.   : 5.376595   Max.   : 3.119295  \n      V27                 V28               Amount            Class          \n Min.   :-8.358317   Min.   :-8.46461   Min.   :    0.00   Length:28923      \n 1st Qu.:-0.071275   1st Qu.:-0.05424   1st Qu.:    5.49   Class :character  \n Median : 0.002727   Median : 0.01148   Median :   22.19   Mode  :character  \n Mean   :-0.000501   Mean   : 0.00087   Mean   :   87.90                     \n 3rd Qu.: 0.095974   3rd Qu.: 0.08238   3rd Qu.:   78.73                     \n Max.   : 7.994762   Max.   :33.84781   Max.   :11898.09                     \n\n# Plot a histogram of the transaction time\nggplot(creditcard, aes(x = Time)) + \n    geom_histogram()"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#generating-training-and-test-sets",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#generating-training-and-test-sets",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Generating training and test sets",
    "text": "Generating training and test sets\nBefore we can apply the t-SNE algorithm to perform a dimensionality reduction, we need to split the original data into a training and test set. Next, we will perform an under-sampling of the majority class and generate a balanced training set. Generating a balanced dataset is a good practice when we are using tree-based models. In this exercise you already have the creditcard dataset loaded in the environment. The ggplot2 and data.table packages are already loaded.\n\n# Extract positive and negative instances of fraud\ncreditcard_pos <- creditcard[Class == 1]\ncreditcard_neg <- creditcard[Class == 0]\n\n# Fix the seed\nset.seed(1234)\n\n# Create a new negative balanced dataset by undersampling\ncreditcard_neg_bal <- creditcard_neg[sample(1:nrow(creditcard_neg), nrow(creditcard_pos))]\n\n# Generate a balanced train set\ncreditcard_train <- rbind(creditcard_pos, creditcard_neg_bal)"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-original-features",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-original-features",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Training a random forest with original features",
    "text": "Training a random forest with original features\nIn this exercise, we are going to train a random forest model using the original features from the credit card dataset. The goal is to detect new fraud instances in the future and we are doing that by learning the patterns of fraud instances in the balanced training set. Remember that a random forest can be trained with the following piece of code: randomForest(x = features, y = label, ntree = 100) The only pre-processing that has been done to the original features was to scale the Time and Amount variables. You have the balanced training dataset available in the environment as creditcard_train. The randomForest package has been loaded.\n\n# Fix the seed\nset.seed(1234)\nlibrary(randomForest)\n# Separate x and y sets\ntrain_x <- creditcard_train[,-31]\ntrain_y <- creditcard_train$Class %>% as.factor()\n\n# Train a random forests\nrf_model <- randomForest(x = train_x, y = train_y, ntree = 100)\n\n# Plot the error evolution and variable importance\nplot(rf_model, main = \"Error evolution vs number of trees\")\n# Fix the seed\nset.seed(1234)\n\n# Separate x and y sets\ntrain_x <- creditcard_train[,-31]\ntrain_y <- creditcard_train$Class %>%as.factor()\n\n# Train a random forests\nrf_model <- randomForest(x = train_x, y = train_y, ntree = 100)\n\n# Plot the error evolution and variable importance\nplot(rf_model, main = \"Error evolution vs number of trees\")\nlegend(\"topright\", colnames(rf_model$err.rate),col=1:3,cex=0.8,fill=1:3)\n\n\n\nvarImpPlot(rf_model, main = \"Variable importance\")"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-and-visualising-the-t-sne-embedding",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-and-visualising-the-t-sne-embedding",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Computing and visualising the t-SNE embedding",
    "text": "Computing and visualising the t-SNE embedding\nIn this exercise, we are going to generate a t-SNE embedding using only the balanced training set creditcard_train. The idea is to train a random forest using the two coordinates of the generated embedding instead of the original 30 dimensions. Due to computational restrictions, we are going to compute the embedding of the training data only, but note that in order to generate predictions from the test set we should compute the embedding of the test set together with the train set. Then, we will visualize the obtained embedding highlighting the two classes in order to clarify if we can differentiate between fraud and non-fraud transactions. The creditcard_train data, as well as the Rtsne and ggplot2 packages, have been loaded.\n\n# Set the seed\n#set.seed(1234)\n\n# Generate the t-SNE embedding \ncreditcard_train[, Time := scale(Time)]\nnms <- names(creditcard_train)\npred_nms <- nms[nms != \"Class\"]\nrange01 <- function(x){(x-min(x))/(max(x)-min(x))}\ncreditcard_train[, (pred_nms) := lapply(.SD ,range01), .SDcols = pred_nms]\n\ntsne_output <- Rtsne(as.matrix(creditcard_train[, -31]), check_duplicates = FALSE, PCA = FALSE)\n\n# Generate a data frame to plot the result\ntsne_plot <- data.frame(tsne_x = tsne_output$Y[,1],\n                        tsne_y = tsne_output$Y[,2],\n                        Class = creditcard_train$Class)\n\n# Plot the embedding usign ggplot and the label\nggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = factor(Class))) + \n  ggtitle(\"t-SNE of credit card fraud train set\") + \n  geom_text(aes(label = Class)) + theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-embedding-features",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-embedding-features",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Training a random forest with embedding features",
    "text": "Training a random forest with embedding features\nIn this exercise, we are going to train a random forest model using the embedding features from the previous t-SNE embedding. So, in this case, we are going to use a two-dimensional dataset that has been generated from the original input features. In the rest of the chapter, we are going to verify if we have a worse, similar, or better performance for this model in comparison to the random forest trained with the original features. In the environment two objects named train_tsne_x and train_tsne_y that contain the features and the Class variable are available. The randomForest package has been loaded as well.\n\n# Fix the seed\nset.seed(1234)\ntrain_tsne_x <- tsne_output$Y\n# Train a random forest\nrf_model_tsne <- randomForest(x = train_tsne_x, y = train_y, ntree = 100)\n\n# Plot the error evolution\n\nplot(rf_model_tsne)\n\n\n\n# Plot the variable importance\nvarImpPlot(rf_model_tsne)"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#predicting-data-using-original-features",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#predicting-data-using-original-features",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Predicting data using original features",
    "text": "Predicting data using original features\nIn this exercise, we are using the random forest trained with the original features and generate predictions using the test set. These predictions will be plotted to see the distribution and will be evaluated using the ROCR package by considering the area under the curve.\nThe random forest model, named rf_model, and the test set, named creditcard_test, are available in the environment. The randomForest and ROCR packages have been loaded for you\n\n# Predict on the test set using the random forest \ncreditcard_test <- creditcard\npred_rf <- predict(rf_model, creditcard_test, type = \"prob\")\n\n# Plot a probability distibution of the target class\nhist(pred_rf[,2])\n\n\n\nlibrary(ROCR)\n# Compute the area under the curve\npred <-  prediction(pred_rf[,2], creditcard_test$Class)\nperf <- performance(pred, measure = \"auc\") \nperf@y.values\n\n[[1]]\n[1] 0.9995958"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#predicting-data-using-embedding-random-forest",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#predicting-data-using-embedding-random-forest",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Predicting data using embedding random forest",
    "text": "Predicting data using embedding random forest\nNow, we are going to do the same analysis, but instead of using the random forest trained with the original features, we will make predictions using the random forest trained with the t-SNE embedding coordinates. The random forest model is pre-loaded in an object named rf_model_tsne and the t-SNE embedding features from the original test set are stored in the object test_x. Finally, the test set labels are stored in creditcard_test. The randomForest and ROCR packages have been loaded for you.\n\ncreditcard_test[, (pred_nms) := lapply(.SD ,range01), .SDcols = pred_nms]\n\ntsne_output <- Rtsne(as.matrix(creditcard_test[, -31]), check_duplicates = FALSE, PCA = FALSE)\n\ntest_x <- tsne_output$Y\n# Predict on the test set using the random forest generated with t-SNE features\npred_rf <- predict(rf_model_tsne, test_x, type = \"prob\")\n\n# Plot a probability distibution of the target class\nhist(pred_rf[, 2])\n\n\n\n# Compute the area under the curve\npred <- prediction(pred_rf[, 2] , creditcard_test$Class)\nperf <- performance(pred, measure = \"auc\") \nperf@y.values\n\n[[1]]\n[1] 0.3418133"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#exploring-neural-network-layer-output",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#exploring-neural-network-layer-output",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Exploring neural network layer output",
    "text": "Exploring neural network layer output\nIn this exercise, we will have a look at the data that is being generated in a specific layer of a neural network. In particular, this data corresponds to the third layer, composed of 128 neurons, of a neural network trained with the balanced credit card fraud dataset generated before. The goal of the exercise is to perform an exploratory data analysis.\n\n# Observe the dimensions\n#dim(layer_128_train)\n\n# Show the first six records of the last ten columns\n#head(layer_128_train[, 119:128])\n\n# Generate a summary of all columns\n#summary(layer_128_train)"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#using-t-sne-to-visualise-a-neural-network-layer",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#using-t-sne-to-visualise-a-neural-network-layer",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Using t-SNE to visualise a neural network layer",
    "text": "Using t-SNE to visualise a neural network layer\nNow, we would like to visualize the patterns obtained from the neural network model, in particular from the last layer of the neural network. As we mentioned before this last layer has 128 neurons and we have pre-loaded the weights of these neurons in an object named layer_128_train. The goal is to compute a t-SNE embedding using the output of the neurons from this last layer and visualize the embedding colored according to the class. The Rtsne and ggplot2 packages as well as the layer_128_train and creditcard_train have been loaded for you\n\n# Set the seed\nset.seed(1234)\n\n# Generate the t-SNE\n#tsne_output <- Rtsne(as.matrix(layer_128_train), check_duplicates = FALSE, max_iter = 400, perplexity = 50)\n\n# Prepare data.frame\n#tsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], \n#                        Class = creditcard_train$Class)\n\n# Plot the data \n# ggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = Class)) + \n#   geom_point() + \n#   ggtitle(\"Credit card embedding of Last Neural Network Layer\")"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#using-t-sne-to-visualise-a-neural-network-layer-1",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#using-t-sne-to-visualise-a-neural-network-layer-1",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Using t-SNE to visualise a neural network layer",
    "text": "Using t-SNE to visualise a neural network layer\nNow, we would like to visualize the patterns obtained from the neural network model, in particular from the last layer of the neural network. As we mentioned before this last layer has 128 neurons and we have pre-loaded the weights of these neurons in an object named layer_128_train. The goal is to compute a t-SNE embedding using the output of the neurons from this last layer and visualize the embedding colored according to the class. The Rtsne and ggplot2 packages as well as the layer_128_train and creditcard_train have been loaded for you\n\n# Set the seed\n# set.seed(1234)\n# \n# # Generate the t-SNE\n# tsne_output <- Rtsne(as.matrix(layer_128_train), check_duplicates = FALSE, max_iter = 400, perplexity = 50)\n# \n# # Prepare data.frame\n# tsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], \n#                         Class = creditcard_train$Class)\n# \n# # Plot the data \n# ggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = Class)) + \n#   geom_point() + \n#   ggtitle(\"Credit card embedding of Last Neural Network Layer\")"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#exploring-fashion-mnist",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#exploring-fashion-mnist",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Exploring fashion MNIST",
    "text": "Exploring fashion MNIST\nThe Fashion MNIST dataset contains grayscale images of 10 clothing categories. The first thing to do when you are analyzing a new dataset is to perform an exploratory data analysis in order to understand the data. A sample of the fashion MNIST dataset fashion_mnist, with only 500 records, is pre-loaded for you.\n\nlibrary(data.table)\n#load(\"fashion_mnist_500.RData\")\nload(\"fashion_mnist.rda\")\nset.seed(100)\n\nind <- sample(1:nrow(fashion_mnist), 1000)\n\nfashion_mnist <- fashion_mnist[ind, ]\n# Show the dimensions\ndim(fashion_mnist)\n\n[1] 1000  785\n\n# Create a summary of the last six columns \nsummary(fashion_mnist[, 780:785])\n\n    pixel779         pixel780         pixel781        pixel782      \n Min.   :  0.00   Min.   :  0.00   Min.   :  0.0   Min.   :  0.000  \n 1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:  0.0   1st Qu.:  0.000  \n Median :  0.00   Median :  0.00   Median :  0.0   Median :  0.000  \n Mean   : 22.16   Mean   : 18.64   Mean   : 10.6   Mean   :  3.781  \n 3rd Qu.:  1.00   3rd Qu.:  0.00   3rd Qu.:  0.0   3rd Qu.:  0.000  \n Max.   :236.00   Max.   :255.00   Max.   :231.0   Max.   :188.000  \n    pixel783          pixel784     \n Min.   :  0.000   Min.   : 0.000  \n 1st Qu.:  0.000   1st Qu.: 0.000  \n Median :  0.000   Median : 0.000  \n Mean   :  0.934   Mean   : 0.043  \n 3rd Qu.:  0.000   3rd Qu.: 0.000  \n Max.   :147.000   Max.   :39.000  \n\n# Table with the class distribution\ntable(fashion_mnist$label)\n\n\n  0   1   2   3   4   5   6   7   8   9 \n100  94 115  90  97  98  97 105 102 102"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#visualizing-fashion-mnist",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#visualizing-fashion-mnist",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Visualizing fashion MNIST",
    "text": "Visualizing fashion MNIST\nIn this exercise, we are going to visualize an example image of the fashion MNIST dataset. Basically, we are going to plot the 28x28 pixels values. To do this we use:\nA custom ggplot theme named plot_theme. A data structure named xy_axis where the pixels values are stored. A character vector named class_names with the names of each class. The fashion_mnist dataset with 500 examples is available in the workspace. The `ggplot2 package is loaded. Note that you can access the definition of the custom theme by typing plot_theme in the console.\n\nlibrary(tidyverse)\nplot_theme <- list(\n    raster = geom_raster(hjust = 0, vjust = 0),\n    gradient_fill = scale_fill_gradient(low = \"white\",\n                                        high = \"black\", guide = FALSE),\n    theme = theme(axis.line = element_blank(),\n                  axis.text = element_blank(),\n                  axis.ticks = element_blank(),\n                  axis.title = element_blank(),\n                  panel.background = element_blank(),\n                  panel.border = element_blank(),\n                  panel.grid.major = element_blank(),\n                  panel.grid.minor = element_blank(),\n                  plot.background = element_blank()))\n\n\nclass_names <-  c(\"T-shirt/top\", \"Trouser\", \"Pullover\", \n                  \"Dress\", \"Coat\", \"Sandal\", \"Shirt\",\n                  \"Sneaker\", \"Bag\", \"Ankle\", \"boot\")\n\n\nxy_axis <- data.frame(x = expand.grid(1:28, 28:1)[,1],\n                      y = expand.grid(1:28, 28:1)[,2])\n\n# Get the data from the last image\nplot_data <- cbind(xy_axis, fill = as.data.frame(t(fashion_mnist[500, -1]))[,1])\n\n# Observe the first records\nhead(plot_data)\n\n  x  y fill\n1 1 28    0\n2 2 28    0\n3 3 28    0\n4 4 28    0\n5 5 28    0\n6 6 28    0\n\n# Plot the image using ggplot()\nggplot(plot_data, aes(x, y, fill = fill)) + \n  ggtitle(class_names[as.integer(fashion_mnist[500, 1])]) + \n  plot_theme"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#reducing-data-with-glrm",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#reducing-data-with-glrm",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Reducing data with GLRM",
    "text": "Reducing data with GLRM\nWe are going to reduce the dimensionality of the fashion MNIST sample data using the GLRM implementation of h2o. In order to do this, in the next steps we are going to: Start a connection to a h2o cluster by invoking the method h2o.init(). Store the fashion_mnist data into the h2o cluster with as.h2o(). Launch a GLRM model with K=2 (rank-2 model) using the h2o.glrm() function. As we have discussed in the video session, it is important to check the convergence of the objective function. Note that here we are also fixing the seed to ensure the same results. The h2o package and fashion_mnist data are pre-loaded in the environment.\n\nlibrary(h2o)\n# Start a connection with the h2o cluster\nh2o.init()\n\n Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         1 hours 18 minutes \n    H2O cluster timezone:       Africa/Nairobi \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.38.0.1 \n    H2O cluster version age:    4 months and 24 days !!! \n    H2O cluster name:           H2O_started_from_R_mburu_cvk380 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   3.15 GB \n    H2O cluster total cores:    8 \n    H2O cluster allowed cores:  8 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.2.2 Patched (2022-11-10 r83330) \n\n# Store the data into h2o cluster\nfashion_mnist.hex <- as.h2o(fashion_mnist, \"fashion_mnist.hex\")\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Launch a GLRM model over fashion_mnist data\nmodel_glrm <- h2o.glrm(training_frame = fashion_mnist.hex,\n                       cols = 2:ncol(fashion_mnist), \n                       k = 2,\n                       seed = 123,\n                       max_iterations = 2100)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |======================================================================| 100%\n\n# Plotting the convergence\nplot(model_glrm)"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#improving-model-convergence",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#improving-model-convergence",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Improving model convergence",
    "text": "Improving model convergence\nIn the previous exercise, we didn’t get good convergence values for the GLRM model. Improving convergence values can sometimes be achieved by applying a transformation to the input data. In this exercise, we are going to normalize the input data before we start building the GLRM model. This can be achieved by setting the transform parameter of h2o.glrm() equal to “NORMALIZE”. The h2o package and fashion_mnist dataset are pre-loaded.\n\n# Start a connection with the h2o cluster\n#h2o.init()\n\n# Store the data into h2o cluster\n#fashion_mnist.hex <- as.h2o(fashion_mnist, \"fashion_mnist.hex\")\n\n# Launch a GLRM model with normalized fashion_mnist data  \nmodel_glrm <- h2o.glrm(training_frame = fashion_mnist.hex, \n                       transform = \"NORMALIZE\",\n                       cols = 2:ncol(fashion_mnist), \n                       k = 2, \n                       seed = 123,\n                       max_iterations = 2100)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |======================================================================| 100%\n\n# Plotting the convergence\nplot(model_glrm)"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#visualizing-the-output-of-glrm",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#visualizing-the-output-of-glrm",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Visualizing the output of GLRM",
    "text": "Visualizing the output of GLRM\nA GLRM model generates the X and Y matrixes. In this exercise, we are going to visualize the obtained low-dimensional representation of the input records in the new K-dimensional space. The output of the X matrix from the previous GLRM model has been loaded with the name X_matrix. This matrix has been obtained by calling:\n\nX_matrix <- as.data.table(h2o.getFrame(model_glrm@model$representation_name))\n# Dimension of X_matrix\ndim(X_matrix)\n\n[1] 1000    2\n\n# First records of X_matrix\nhead(X_matrix)\n\n        Arch1       Arch2\n1: -1.4562898  0.28620951\n2:  0.7132191  1.18922464\n3:  0.5600450 -1.29628758\n4:  0.9997013 -0.51894405\n5:  1.3377989 -0.05616662\n6:  1.0687898  0.07447071\n\n# Plot the records in the new two dimensional space\nggplot(as.data.table(X_matrix), aes(x= Arch1, y = Arch2, color =  fashion_mnist$label)) + \n    ggtitle(\"Fashion Mnist GLRM Archetypes\") + \n    geom_text(aes(label =  fashion_mnist$label)) + \n    theme(legend.position=\"none\")"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#visualizing-the-prototypes",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#visualizing-the-prototypes",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Visualizing the prototypes",
    "text": "Visualizing the prototypes\nNow, we are going to compute the centroids of the coordinates for each of the two archetypes for each label. We did something similar before for the t-SNE embedding. The goal is to have a representation or prototype of each label in this new two-dimensional space.\nThe ggplot2 and data.table packages are pre-loaded, as well as the X_matrix object and the fashion_mnist dataset.\n\n# Store the label of each record and compute the centroids\nX_matrix[, label := as.numeric(fashion_mnist$label)]\nX_matrix[, mean_x := mean(Arch1), by = label]\nX_matrix[, mean_y := mean(Arch2), by = label]\n\n# Get one record per label and create a vector with class names\nX_mean <- unique(X_matrix, by = \"label\")\n\nlabel_names <- c(\"T-shirt/top\", \"Trouser\", \"Pullover\",\n                 \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \n                 \"Sneaker\", \"Bag\", \"Ankle boot\")\n\n# Plot the centroids\nX_mean[, label := factor(label, levels = 0:9, labels = label_names)]\nggplot(X_mean, aes(x = mean_x, y = mean_y, color = label_names)) + \n    ggtitle(\"Fashion Mnist GLRM class centroids\") + \n    geom_text(aes(label = label_names)) +\n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#imputing-missing-data",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#imputing-missing-data",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Imputing missing data",
    "text": "Imputing missing data\nIn this exercise, we will use GLRM to impute missing data. We are going to build a GLRM model from a dataset named fashion_mnist_miss, where 20% of values are missing. The goal is to fill these values by making a prediction using h2o.predict() with the GLRM model. In this exercise an h2o instance is already running, so it is not necessary to call h2o.init(). The h2o package and fashion_mnist_miss have been loaded\n\nfashion_mnist_miss <- h2o.insertMissingValues(fashion_mnist.hex, \n                                              fraction = 0.2, seed = 1234)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Store the input data in h2o\nfashion_mnist_miss.hex <- as.h2o(fashion_mnist_miss, \"fashion_mnist_miss.hex\")\n\n# Build a GLRM model\nmodel_glrm <- h2o.glrm(training_frame = fashion_mnist_miss.hex,\n                       k = 2,\n                       transform = \"NORMALIZE\",\n                       max_iterations = 100)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |======================================================================| 100%\n\n# Impute missing values\nfashion_pred <- predict(model_glrm, fashion_mnist_miss.hex)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Observe the statistics of the first 5 pixels\nsummary(fashion_pred[, 1:5])\n\n reconstr_label     reconstr_pixel2      reconstr_pixel3     \n Min.   :-0.47950   Min.   :-0.0015568   Min.   :-0.0048536  \n 1st Qu.:-0.26059   1st Qu.:-0.0008525   1st Qu.:-0.0022268  \n Median :-0.07530   Median :-0.0002846   Median :-0.0003030  \n Mean   :-0.06522   Mean   :-0.0002330   Mean   :-0.0001716  \n 3rd Qu.: 0.12724   3rd Qu.: 0.0003203   3rd Qu.: 0.0018569  \n Max.   : 0.39351   Max.   : 0.0018132   Max.   : 0.0058790  \n reconstr_pixel4      reconstr_pixel5     \n Min.   :-0.0060428   Min.   :-0.0035266  \n 1st Qu.:-0.0030836   1st Qu.:-0.0019180  \n Median :-0.0011572   Median : 0.0003117  \n Mean   :-0.0009483   Mean   : 0.0001418  \n 3rd Qu.: 0.0012697   3rd Qu.: 0.0019282  \n Max.   : 0.0066305   Max.   : 0.0044367"
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-original-data",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-original-data",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Training a random forest with original data",
    "text": "Training a random forest with original data\nIn this exercise, we are going to train a random forest using the original fashion MNIST dataset with 500 examples. This dataset is preloaded in the environment with the name fashion_mnist. We are going to train a random forest with 20 trees and we will look at the time it takes to compute the model and the out-of-bag error in the 20th tree. The randomForest package is loaded.\n\n# Get the starting timestamp\nlibrary(randomForest)\n\ntime_start <- proc.time()\n\n# Train the random forest\nfashion_mnist[, label := factor(label)]\nrf_model <- randomForest(label~., ntree = 20,\n                         data = fashion_mnist)\n\n# Get the end timestamp\ntime_end <- timetaken(time_start)\n\n# Show the error and the time\nrf_model$err.rate[20]\n\n[1] 0.26\n\ntime_end\n\n[1] \"0.661s elapsed (0.661s cpu)\""
  },
  {
    "objectID": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-compressed-data",
    "href": "posts/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-compressed-data",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Training a random forest with compressed data",
    "text": "Training a random forest with compressed data\nNow, we are going to train a random forest using a compressed representation of the previous 500 input records, using only 8 dimensions!\nIn this exercise, you a dataset named train_x that contains the compressed training data and another one named train_y that contains the labels are pre-loaded. We are going to calculate computation time and accuracy, similar to what was done in the previous exercise. Since the dimensionality of this dataset is much smaller, we can train a random forest using 500 trees in less time. The randomForest package is already loaded.\n\nmodel_glrm <- h2o.glrm(training_frame = fashion_mnist.hex, \n                       transform = \"NORMALIZE\",\n                       cols = 2:ncol(fashion_mnist), \n                       k = 8, \n                       seed = 123,\n                       max_iterations = 1000)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |======================================================================| 100%\n\ntrain_x <- as.data.table(h2o.getFrame(model_glrm@model$representation_name))\ntrain_y <- fashion_mnist$label %>% as.factor()\n\n\nlibrary(randomForest)\n# Get the starting timestamp\ntime_start <- proc.time()\n\n# Train the random forest\nrf_model <- randomForest(x = train_x, y = train_y, ntree = 500)\n\n# Get the end timestamp\ntime_end <- timetaken(time_start)\n\n\n# Show the error and the time\nrf_model$err.rate[500]\n\n[1] 0.252\n\ntime_end\n\n[1] \"0.313s elapsed (0.313s cpu)\""
  },
  {
    "objectID": "posts/busara_task/busara data analysis.html",
    "href": "posts/busara_task/busara data analysis.html",
    "title": "Busara Data Analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\nlibrary(data.table)\nlibrary(knitr)\nlibrary(ggthemes)\n#library(kableExtra)\n\n\nI was not sure if the presentation is about insights from the data or how I solved the problem.\nI decided to to combine both with R presentation.\n\n\nTask 1\n\nUnderstanding the demographics of company xyz.\nAtleast half are youth average age = 33.5\nAtleast half earn 5557\n\n\nxyz <- setDT(read_csv(\"XYZ.csv\"))\n\nxyz_sub <- xyz[, .(Gender, Age, Income)]\n\nxyz_subm <- melt(xyz_sub, id.vars = \"Gender\")\n\n\n\nSummary Statistics Age and Income\n\nxyz_subm %>% group_by(variable) %>%\n    summarise(Average = mean(value), Median = median(value),\n              Min = min(value), Max = max(value)) %>%\n    \n    kable() #%>% kable_styling() %>%\n\n\n\n\nvariable\nAverage\nMedian\nMin\nMax\n\n\n\n\nAge\n33.506\n33\n18\n50\n\n\nIncome\n5498.844\n5557\n1000\n9897\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\n% gender Gap\n\nMale/Female % al most equal\nThere are 5.2% more men than women\n\n\ngender <- xyz %>% group_by(Gender) %>%\n    summarise(freq = n()) %>%\n    mutate(Perc = round(freq/sum(freq) * 100, 2))\n\ngender %>% kable() #%>% kable_styling() %>%\n\n\n\n\nGender\nfreq\nPerc\n\n\n\n\nFemale\n237\n47.4\n\n\nMale\n263\n52.6\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\n% gender Gap\n\ngender[, -2] %>% spread(Gender, Perc) %>% \n    mutate(Percentage_Gender_Gap = Male - Female) %>% \n    kable() #%>% kable_styling() %>%\n\n\n\n\nFemale\nMale\nPercentage_Gender_Gap\n\n\n\n\n47.4\n52.6\n5.2\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n#Single Ladies Nyeri\n\n12(2.4% of the company employees) single ladies from Nyeri county\n\n\nsingle_nyeri <- xyz[Gender == \"Female\" & Marital_Status == \"Single\" & County == \"Nyeri\",]\nnrow(xyz)\n\n[1] 500\n\ncat(\"The Number of single ladies in Nyeri is \", nrow(single_nyeri))\n\nThe Number of single ladies in Nyeri is  12\n\n\n\n\nSummary Statistics Single Ladies Nyeri\n\nAverage age 36 and medium income is about $50\n\n\nsingle_nyeri %>%\n    summarise(Average_Age = mean(Age), Median_Income = median(Income)) %>%\n    kable() #%>%  kable_styling() %>%\n\n\n\n\nAverage_Age\nMedian_Income\n\n\n\n\n36\n5557\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nNumber of Juniors\n\n28 juniors\n\n\njuniors_26 <-xyz[!grepl(\"Operartions|Data\",Department)  & xyz$Age < 26 & grepl(\"Junior\", Role),]\n\ncat(\"The Number of juniors \", nrow(juniors_26))\n\nThe Number of juniors  28\n\njuniors_26 %>% group_by(Department) %>%\n    summarise(freq = n()) %>%\n    mutate(Perc = round(freq/sum(freq) * 100, 2)) %>%\n    kable() #%>% kable_styling() %>%\n\n\n\n\nDepartment\nfreq\nPerc\n\n\n\n\nAssociate\n5\n17.86\n\n\nFinance\n11\n39.29\n\n\nOperations\n8\n28.57\n\n\nResearch Analyst\n4\n14.29\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"40%\")\n\n\n\nDifference in mean income between male and female\n\nThe Operations has the biggest difference in mean income\nFemale/Males average earnings in different departments\n\n\nincome_gender <- xyz %>% group_by(Gender, Department) %>%\n    summarise(Average = mean(Income))\n\n\nincome_gender_dcast <- dcast(Department ~ Gender, data = income_gender) \n\nincome_gender_dcast %>% mutate( Difference = Male - Female) %>%\n    kable()#%>% kable_styling() %>%\n\n\n\n\nDepartment\nFemale\nMale\nDifference\n\n\n\n\nAssociate\n5345.047\n5071.941\n-273.1053\n\n\nData\n5613.420\n5270.396\n-343.0238\n\n\nFinance\n5574.714\n5936.750\n362.0357\n\n\nOperations\n5043.286\n6284.854\n1241.5685\n\n\nResearch Analyst\n5264.522\n5533.327\n268.8055\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"45%\")\n\n\n\nFunction to plot categorical variables\n\nbar_plot <- function(data, title,...) {\n    #load ggplot2\n    #function takes a data frame\n    #and other arguments that ggplot\n    #function from ggplot2 takes\n    # the other arguments are aesthetic mappings\n    require(ggplot2)\n    ggplot(data) + geom_bar(aes(...))+\n        ggtitle(title)+\n        ggthemes::theme_hc()+\n        ggthemes::scale_fill_hc()+\n        theme(legend.position = \"none\")\n        \n}\n\n\n\nFunction to plot categorical variables test 1\n\nbar_plot(xyz, Department, title = \"Department Distribution\", fill = Department)\n\n\n\n\n\n\nFunction to plot categorical variables test 2\n\nbar_plot(xyz, Gender, title = \"Gender Distibution\", fill = Gender)\n\n\n\n\n\n\nTask 2\nRead Files\n\nRead files using the patterns\n\n\nmy_files <- dir(path = \"Education\",pattern = \"^Chi|^Sch|^Persi|Secon|^Progr|Pri\")\n\nmy_files <- paste0(\"Education/\", my_files)\n\nlibrary(readxl)\n\nlist_files <- list()\n\nfor (i in 1:length(my_files)) {\n    \n    \n    x = read_excel(my_files[i]) \n    id = grep(\"Country Name\", x$`Data Source`)\n    nms <- x[id,]\n    names(x) <- nms %>% as.character()\n    list_files[[i]] <- x[-c(1:id),] \n    cat(\"...\")\n    \n}\n\n......................................................\n\n\n\n\nCombine Files\n\nSince files are stored in a list combine them\n\n\ndf_world <- rbindlist(list_files) %>% setDT()\n\ndf_world_melt <- melt(df_world, id.vars = names(df_world)[1:4])\n\nnms2 <- Hmisc::Cs(Country_Name, Country_Code,   \n          Indicator_Name,   Indicator_Code, Year,   Indicator_value)\n\nnames(df_world_melt) <- nms2\n\ndf_world_melt[,  Year := as.numeric(as.character(df_world_melt$Year))]\n\n\n\nHead output data frame\n\nhead(df_world_melt) %>% kable() #%>% \n\n\n\n\n\n\n\n\n\n\n\n\nCountry_Name\nCountry_Code\nIndicator_Name\nIndicator_Code\nYear\nIndicator_value\n\n\n\n\nAruba\nABW\nChildren out of school, primary, female\nSE.PRM.UNER.FE\n1960\nNA\n\n\nAfghanistan\nAFG\nChildren out of school, primary, female\nSE.PRM.UNER.FE\n1960\nNA\n\n\nAngola\nAGO\nChildren out of school, primary, female\nSE.PRM.UNER.FE\n1960\nNA\n\n\nAlbania\nALB\nChildren out of school, primary, female\nSE.PRM.UNER.FE\n1960\nNA\n\n\nAndorra\nAND\nChildren out of school, primary, female\nSE.PRM.UNER.FE\n1960\nNA\n\n\nArab World\nARB\nChildren out of school, primary, female\nSE.PRM.UNER.FE\n1960\nNA\n\n\n\n\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nHead output kenya data\n\nkenya_2011 <- df_world_melt[Country_Name == \"Kenya\" & Year >= 2011]\n\nhead(kenya_2011) #%>% kable() %>%\n\n   Country_Name Country_Code\n1:        Kenya          KEN\n2:        Kenya          KEN\n3:        Kenya          KEN\n4:        Kenya          KEN\n5:        Kenya          KEN\n6:        Kenya          KEN\n                                               Indicator_Name    Indicator_Code\n1:                    Children out of school, primary, female    SE.PRM.UNER.FE\n2: Persistence to last grade of primary, female (% of cohort) SE.PRM.PRSL.FE.ZS\n3:   Persistence to last grade of primary, male (% of cohort) SE.PRM.PRSL.MA.ZS\n4:  Primary completion rate, female (% of relevant age group) SE.PRM.CMPT.FE.ZS\n5:    Primary completion rate, male (% of relevant age group) SE.PRM.CMPT.MA.ZS\n6:                Progression to secondary school, female (%) SE.SEC.PROG.FE.ZS\n   Year Indicator_value\n1: 2011            <NA>\n2: 2011            <NA>\n3: 2011            <NA>\n4: 2011            <NA>\n5: 2011            <NA>\n6: 2011            <NA>\n\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"70%\")\n\n\n\nHead output kenya data and saving files\n\nwrite.csv(head(kenya_2011, 15), file = \"kenya data.csv\", row.names = F)\n\nkenya_2011_na <- kenya_2011[!is.na(kenya_2011$Indicator_value),]\nwrite.csv(head(kenya_2011_na, 15), file = \"kenya data without na.csv\", row.names = F)\n\nhead(kenya_2011_na) %>% kable() #%>%\n\n\n\n\n\n\n\n\n\n\n\n\nCountry_Name\nCountry_Code\nIndicator_Name\nIndicator_Code\nYear\nIndicator_value\n\n\n\n\nKenya\nKEN\nChildren out of school, primary, female\nSE.PRM.UNER.FE\n2012\n537736\n\n\nKenya\nKEN\nSchool enrollment, primary (gross), gender parity index (GPI)\nSE.ENR.PRIM.FM.ZS\n2012\n1.0080599784851101\n\n\nKenya\nKEN\nSchool enrollment, primary, female (% gross)\nSE.PRM.ENRR.FE\n2012\n112.41464233398401\n\n\nKenya\nKEN\nSchool enrollment, primary, male (% gross)\nSE.PRM.ENRR.MA\n2012\n111.51609802246099\n\n\nKenya\nKEN\nPrimary completion rate, female (% of relevant age group)\nSE.PRM.CMPT.FE.ZS\n2014\n100.183967590332\n\n\nKenya\nKEN\nPrimary completion rate, male (% of relevant age group)\nSE.PRM.CMPT.MA.ZS\n2014\n98.815101623535199\n\n\n\n\n   # kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"70%\")\n\n#Task 3\n\nfigari_sheet1 <- read_excel(\"Figari Bank.xlsx\" ) %>% setDT()\n\nfigari_sheet2 <- read_excel(\"Figari Bank.xlsx\", sheet = 2 ) %>% setDT()\n\nfigari_sheet2[,  Dates := as.Date(Dates, origin = \"1900-01-01\")]\n\nfigari_sheet2[,  year := year(Dates)]\n\n\nfigari_sheet2[,  month := month(Dates)]\nfigari_sheet2[, month := ifelse(nchar(month) == 1 ,paste(0, month), month)]\n\nfigari_sheet2[,  week_day := as.POSIXlt(Dates)$wday+1]\nfigari_sheet2[, week_day := ifelse(nchar(week_day) == 1 ,paste(0, week_day), week_day)]\nfigari_sheet2[,  week_no := week(Dates)]\n\nfigari_sheet2[, week_no := ifelse(nchar(week_no) == 1 ,paste(0, week_no), week_no)]\n\nfigari_sheet2[,  day_month := format(Dates, \"%d\")]\n\nfigari_sheet2_m <- melt(figari_sheet2[, c(3:9), with = F], id.vars = c(\"Amount\", \"Saving Mode\"))\n\n\n\nTask 3 Plots\n\nTime series will enable us too see if there is seasonal/cyclic effects/trend\nweek number after every two weeks, maybe end month\nSmoothing/decoposing often needed to see trend\n\n\nfigari_dat <- figari_sheet2_m %>% group_by(`Saving Mode`,variable, value) %>%\n    summarise(Average = mean(Amount)) \ntitles <- levels(as.factor(figari_dat$`Saving Mode`))\ntitles <- paste(\"Average Savings for\", titles)\nfigari_dat_split <- split(figari_dat, figari_dat$`Saving Mode`)\nplots_figari <- list()\nfor ( i in 1:length(figari_dat_split)) {\n    this = figari_dat_split[[i]]\n    #write.csv(this, file = \"this.csv\", row.names = F)\n   plots_figari[[i]] <- ggplot(this, aes(value, Average)) +\n       facet_wrap(~variable, scales = \"free_x\", ncol = 1)+\n       geom_line(data = this, aes(value, Average, group = 1)) +\n       ggthemes::theme_hc()+\n       labs(x = \"\", y = \"Average amount saved (KES)\", title = titles[i])# +\n       #theme(axis.text.x = element_text(angle = 30, vjust = 1, hjust = 1))\n    \n}\n\n\n\nAverage Amount saved at the Bank\n\nYear no visible trend/ few years\nfirst two months higher savings\nlowest between month 3 to month 8\nSave more from day 2 - day 4\nWeek 1- 3 more savings drops to week 10\nmore save less in around day 10 of the month\ndecreasing trend trend from day 4 -10\n\n\nplots_figari[[2]]\n\n\n\n\n\n\nAverage Amount saved at the Agent\n\nSave more from March to June\nIncreasing trend from week 1 to 23 then decreasing\nSave less towards end of a month\n\n\nplots_figari[[1]]\n\n\n\n\n\n\nAverage Amount saved Mobile money\n\non average\nsave less from month 3 to 6\nsave less from week 9 to 22\n\n\nplots_figari[[3]]\n\n\n\n\n\n\nEnd Month Savings Favourite tool\n\nI’m thinking about the number of times someone saves. Average maybe skewed.\nWomen prefer to save using agent\nIn regions no Nyeri\n\n\nnames(figari_sheet2)[1] = names(figari_sheet1)[1]\n\nfigari_comb <- merge(figari_sheet2, figari_sheet1, by = \"CustomerID\")\n\nend_month <- figari_comb %>% \n    group_by(day_month, `Saving Mode`) %>%\n    summarise(Freq = n()) %>%\n    mutate(perc = round(100 * Freq/sum(Freq), 2)) %>% ungroup()\n#The number of times one deposits\nggplot(end_month, aes(day_month, Freq )) +\n    geom_line(aes(color =`Saving Mode`, group =`Saving Mode` ), size = 1)+\n    theme_hc()+\n    scale_color_hc(name = \"\") +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\nHistogram Deposits\n\nWhat you would expect.\n\n\ndeposits <- figari_sheet2[, .(freq = .N), by = CustomerID] \n\n#approximmately poison\nhist(deposits$freq, col = \"black\",\n     main = \"Deposits\", \n     xlab = \"Deposits\")\n\n\n\n\n\n\nSubset People who have made one deposit\n\nfigari_deposits <- merge(deposits, figari_sheet1, by = \"CustomerID\")\nfigari_deposits_one <- figari_deposits[freq == 1] \n\n\n\nDemographic characteristics of those who have only made one deposit\n\n\nGender\n\nfigari_deposits_one %>% group_by(Gender) %>%\n    summarise(freq= n()) %>%\n    mutate(Perc =round(freq/sum(freq) *100, 2) ) %>%\n    kable()# %>%\n\n\n\n\nGender\nfreq\nPerc\n\n\n\n\nFemale\n141\n49.3\n\n\nMale\n145\n50.7\n\n\n\n\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nRegion\n\nfigari_deposits_one %>% group_by(Region) %>%\n    summarise(freq= n()) %>% \n    mutate(Perc =round(freq/sum(freq) *100, 2) ) %>%\n    kable() #%>%\n\n\n\n\nRegion\nfreq\nPerc\n\n\n\n\nBondo\n25\n8.74\n\n\nGatitu\n53\n18.53\n\n\nKawangware\n21\n7.34\n\n\nKayole\n13\n4.55\n\n\nKibera\n31\n10.84\n\n\nKilimani\n41\n14.34\n\n\nKirinyaga\n18\n6.29\n\n\nRongai\n10\n3.50\n\n\nRuai\n45\n15.73\n\n\nTaita\n29\n10.14\n\n\n\n\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"100%\")\n\n\n\nAge\n\nfigari_deposits_one %>% \n    summarise(Mean= round(mean(Age), 2), Median  = median(Age)) %>%\n    kable() #%>%\n\n\n\n\nMean\nMedian\n\n\n\n\n54.71\n56\n\n\n\n\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nTask 4\n\n\nProject Motivation\nData has the potential to transform business and drive the creation of business value. It can be used for a range of tasks such visualization relationships between variables to predicting if an event will occur. The later is one of the heavily reaserched areas in recent times. The reason for this is that data has grown exponentially and so does the computing power. Banks and financial institutions used data analytics for a range of value such as fraud detetction customer segment, recruiting, credit scoring and so on.\nIn this study I will use Bogoza data set to build a credit model where an applicat will be avaluated on whether they will default or not.\nHigh accuracy for this model will be required because predicting false positives will eventually cause a business to make a loss and false negatives means that the financial instituion looses business.\n\n\nData Cleaning\nFirst step is data cleaning. This ensures that columns are consistent. For instance the target variable had values such as Y y yes where all of them represent yes.\n\n#some algorithms like xgboost take numeric data\n#you can convert binary vars to 1,0\n# and form dummie variables using library dummies\n#for variables with more than 2 categories\nborogoza <- setDT(read_csv( \"Bagorogoza Loan.csv\"))\n\nborogoza[, Target := ifelse(grepl(\"y|Y\", Target), 1, 0)]\n\nborogoza[, Gender := ifelse(grepl(\"^m$|^male$\", tolower(Gender)), 0, 1)]\n\nborogoza[, Married := ifelse(grepl(\"Yes\",Married), 1, 0)]\n\nborogoza[, Education := ifelse(grepl(\"not\", tolower(Education)), 0, 1)]\n\nborogoza[, Self_Employed := ifelse(grepl(\"Yes\",Self_Employed), 1, 0)]\n\nborogoza[, Property_Area := ifelse(grepl(\"rural\",tolower(Property_Area)), \"Rural\", Property_Area)]\n\nborogoza[, Property_Area := ifelse(grepl(\"semi\",tolower(Property_Area)), \"Semi-urban\", Property_Area)]\n\nborogoza[, Property_Area := ifelse(grepl(\"^urban$\",tolower(Property_Area)), \"Urban\", Property_Area)]\n\n\n\nVariable Selection\n\nWhere we run descripte statistics\n\n\n\nVisualize Categorical variables\nVisualization and summary statistics is an impostant step before fitting any model as this will give you a glimpse of how the variables are associated with target variable. In this case I will use stacked barplot as from them you can see if the prorpotions of defaulters and non defaulters is equal in defferent categories of a variable. From the graphs we can see that the prorpotion of defaulters and non defaulters is defferent for the different credit history categories. This is aslo seen in the prorpety area. From the categorical variables we can therefore conclude that one of the best predictors is credit history.\n\nnumeric_vars <- Hmisc::Cs(ApplicantIncome,CoapplicantIncome, LoanAmount )\n\nnms_bo <- names(borogoza)[-1]\n\ncat_vars <- nms_bo[!nms_bo %in% numeric_vars]\n\n\nborogoza_catm <- melt(borogoza[, cat_vars, with = F], id.vars = \"Target\")\n\nborogoza_catm_perc <-borogoza_catm  %>%  group_by(variable, value, Target) %>%\n    summarise(freq= n()) %>% mutate(perc =round(freq/sum(freq) *100, 2) )\n\nlibrary(ggthemes)\nggplot(borogoza_catm_perc, aes(value, perc, fill = factor(Target) )) +\n    geom_bar(stat = \"identity\") +facet_wrap(~variable, scales = \"free_x\")+\n    scale_fill_economist(name = \"\")+\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\nVisualize numeric variables\nFor the numeric variables boxplot help us visualize which distribution is different from the other. Non overlapping boxplot for defaulters and non defaulters may indicate that the mean/median values in the two groups was significantly different. From this we can see that it’s unlikely that education and self employment affect loan repayment and for this we drop this two variables\n\nborogoza_numm <- melt(borogoza[, c(numeric_vars, \"Target\"), with = F], id.vars = \"Target\")\n\nggplot(borogoza_numm, aes(as.factor(Target), scale(value ))) +\n    geom_boxplot() +facet_wrap(~variable, scales = \"free_y\")\n\n\n\n\n\n\nOne-Hot Encoding for categorical variables with more than 2 levels\nIn this step variables with more than two categories are converted to dummies variables. The first column in each category is dropped as it’s linearly dependent with the second column.\n\nchars <- unlist(lapply(borogoza[, -1, with = F], is.character)) \n\nchars <- nms_bo[chars]\n\nlibrary(dummies)\nborogoza_dummy <- dummy.data.frame(borogoza, names = c(chars, \"Loan_Amount_Term\")) %>%\n    setDT()\n\n\nborogoza_dummy[, Loan_ID := NULL]\nborogoza_dummy[, Loan_Amount_Term36 := NULL]\nborogoza_dummy[, `Property_AreaSemi-urban` := NULL]\nborogoza_dummy[, `Dependents1` := NULL]\n\n\n\nScale variables\nIt’s important to scale your variables since it leads to faster convergence and since some algorithm use distances to find decision boundary this means that variables with big values will have a big influence.\n\nxvars <- names(borogoza_dummy)[!names(borogoza_dummy) %in% \"Target\"]\nborogoza_dummy[, (xvars) := lapply(.SD, function(x) scale(x)), .SDcols = xvars ]\n\n\n\nSplit test and train sets\nThis is important as it helps evaluate your model on data it has never seen. The model will be trained on one set(training set) and tested using test set.\n\nset.seed(200) # for reproducibility\ntrain_sample <- sample(1:nrow(borogoza_dummy), round(0.7*nrow(borogoza_dummy)))\n\ntrain <- borogoza_dummy[train_sample,]\ntest <- borogoza_dummy[-train_sample,]\n\n\n\nFit Logistic Regression\nLogistic regression was fit to predict the probability of someone defaulting. The advantages of logistic regression is interpret table, ie you can see the association between a predictor and response value, it also gives a probability. This is very important when you want to have your own cut off point eg you want to label someone as a defaulter if you the predicted probability is more than 0.7. This increases precision but lowers recall. Using step wise selection the model was used to select the variables that best predict loan default.\n\nfit_glm <- glm(Target ~ Married + CoapplicantIncome + Loan_Amount_Term60 + \n    Loan_Amount_Term180 + Loan_Amount_Term300 + \n     Loan_Amount_Term360 + Credit_History + Property_AreaRural + \n     Property_AreaUrban ,data = train, family = binomial)\n\nborogoza_dummy <- borogoza_dummy[, .(Target,Married , CoapplicantIncome , Loan_Amount_Term60 , \n     Loan_Amount_Term180 , Loan_Amount_Term300 , \n     Loan_Amount_Term360 , Credit_History , Property_AreaRural , \n     Property_AreaUrban)]\n\n\ntrain <- borogoza_dummy[train_sample,]\ntest <- borogoza_dummy[-train_sample,]\nsummary(fit_glm) %>% xtable::xtable() %>% kable()# %>%\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nz value\nPr(>|z|)\n\n\n\n\n(Intercept)\n1.1415592\n5.3056959\n0.2151573\n0.8296447\n\n\nMarried\n0.1436465\n0.1721719\n0.8343199\n0.4041008\n\n\nCoapplicantIncome\n-0.1281571\n0.1406808\n-0.9109779\n0.3623070\n\n\nLoan_Amount_Term60\n1.1450622\n73.3785540\n0.0156049\n0.9875496\n\n\nLoan_Amount_Term180\n0.5524587\n0.2784012\n1.9843977\n0.0472115\n\n\nLoan_Amount_Term300\n0.0344328\n0.1576171\n0.2184586\n0.8270718\n\n\nLoan_Amount_Term360\n0.5348545\n0.2558935\n2.0901448\n0.0366048\n\n\nCredit_History\n1.4385841\n0.2260642\n6.3636094\n0.0000000\n\n\nProperty_AreaRural\n-0.4079017\n0.2058167\n-1.9818693\n0.0474939\n\n\nProperty_AreaUrban\n-0.4665362\n0.2048849\n-2.2770650\n0.0227823\n\n\n\n\n    # kable_styling() %>%\n    # scroll_box(width = \"100%\", height = \"100%\")\n\n#MASS::stepAIC(fit_glm)\n\nThe estimate column shows the log odds. Positive values means that the variable makes it more likely for a person to repay their loan negative values means that the person is less likely to repay.\n\n\nConfusion Matrix Logistic regression\nThe confusion matrix evaluate correctly classified cases. A perfect fit will have all values in the main diagnose while the entries of lower/upper triangular should be zeros. In this case we have 14 cases of false positives and 7 cases of false negatives the accuracy of the model is 0.82 with and f1 score of 0.87. F1 score is a very important evaluation metric where there is unbalanced classes.\n\nlibrary(caret)\npred_glm <- predict(fit_glm,newdata = test)\n\npred_glm <- ifelse(pred_glm>0.7, 1 , 0)\n\ntable(test$Target, pred_glm) %>% kable()# %>%\n\n\n\n\n\n0\n1\n\n\n\n\n0\n20\n19\n\n\n1\n3\n73\n\n\n\n\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nAccuracy Logistic regression\n\nlibrary(broom)\nlibrary(pROC)\ntable(test$Target, pred_glm) %>% \n    confusionMatrix(positive = \"1\") %>% \n    tidy() %>% kable()# %>%\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nclass\nestimate\nconf.low\nconf.high\np.value\n\n\n\n\naccuracy\nNA\n0.8086957\n0.724814\n0.8760546\n0.4628434\n\n\nkappa\nNA\n0.5258621\nNA\nNA\nNA\n\n\nmcnemar\nNA\nNA\nNA\nNA\n0.0013838\n\n\nsensitivity\n1\n0.7934783\nNA\nNA\nNA\n\n\nspecificity\n1\n0.8695652\nNA\nNA\nNA\n\n\npos_pred_value\n1\n0.9605263\nNA\nNA\nNA\n\n\nneg_pred_value\n1\n0.5128205\nNA\nNA\nNA\n\n\nprecision\n1\n0.9605263\nNA\nNA\nNA\n\n\nrecall\n1\n0.7934783\nNA\nNA\nNA\n\n\nf1\n1\n0.8690476\nNA\nNA\nNA\n\n\nprevalence\n1\n0.8000000\nNA\nNA\nNA\n\n\ndetection_rate\n1\n0.6347826\nNA\nNA\nNA\n\n\ndetection_prevalence\n1\n0.6608696\nNA\nNA\nNA\n\n\nbalanced_accuracy\n1\n0.8315217\nNA\nNA\nNA\n\n\n\n\n    # kable_styling() %>%\n    # scroll_box(width = \"100%\", height = \"100%\")\n\n\n\nArea under curve\nThis is important as it will help you know if the suffers from high false negatives or false positives. A value greater than 0.8 is normally desired in this case we achieve 0.74.\n\nroc(as.numeric(test$Target), pred_glm, print.auc=T, print.auc.y=0.5, levels =0:1 ) \n\n\nCall:\nroc.default(response = as.numeric(test$Target), predictor = pred_glm,     levels = 0:1, print.auc = T, print.auc.y = 0.5)\n\nData: pred_glm in 39 controls (as.numeric(test$Target) 0) < 76 cases (as.numeric(test$Target) 1).\nArea under the curve: 0.7367\n\n\n\n\nCross Validation SVM\nNext we fit Support vector machine model. We start by finding the best parameters using cross validation. We use 10 fold this where train set is randomly split into 10 sets. In each cases one of the 1 set is used as a validation/test set while the other 9 are used to train the model.\n\nlibrary(e1071)\ntune.out = tune(svm, as.factor(Target)~., data = train, kernel =\"radial\", \n                type =\"C-classification\",\n                ranges =list (cost=c(0.01, 0.1, 1 ,5 ,  10),\n                              gamma = c(0.01,  0.1, 1 ,5 )))\n\nsummary(tune.out)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost gamma\n    5  0.01\n\n- best performance: 0.1595442 \n\n- Detailed performance results:\n    cost gamma     error dispersion\n1   0.01  0.01 0.2747863 0.06706580\n2   0.10  0.01 0.2747863 0.06706580\n3   1.00  0.01 0.1596866 0.06510749\n4   5.00  0.01 0.1595442 0.06956599\n5  10.00  0.01 0.1595442 0.06956599\n6   0.01  0.10 0.2747863 0.06706580\n7   0.10  0.10 0.1967236 0.07767111\n8   1.00  0.10 0.1670940 0.06780670\n9   5.00  0.10 0.1633903 0.06543082\n10 10.00  0.10 0.1633903 0.06543082\n11  0.01  1.00 0.2747863 0.06706580\n12  0.10  1.00 0.2747863 0.06706580\n13  1.00  1.00 0.2041311 0.06754684\n14  5.00  1.00 0.2004274 0.07183672\n15 10.00  1.00 0.2078348 0.07591537\n16  0.01  5.00 0.2747863 0.06706580\n17  0.10  5.00 0.2747863 0.06706580\n18  1.00  5.00 0.2078348 0.06963224\n19  5.00  5.00 0.2152422 0.07506115\n20 10.00  5.00 0.2189459 0.08024003\n\n\n\n\nConfusion Matrix SVM\n\nfit_svm <- svm(as.factor(Target)~., data = train, cost =5  , gamma = .01, \n               kernel = \"radial\", type =\"C-classification\")\n\n\npred_svm <-predict(fit_svm, newdata = test)\ntable(test$Target, pred_svm) %>% kable() #%>%\n\n\n\n\n\n0\n1\n\n\n\n\n0\n16\n23\n\n\n1\n2\n74\n\n\n\n\n   # kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nArea under curve\n\nroc(test$Target, as.numeric(pred_svm), print.auc=T, print.auc.y=0.5, levels =0:1 ) \n\n\nCall:\nroc.default(response = test$Target, predictor = as.numeric(pred_svm),     levels = 0:1, print.auc = T, print.auc.y = 0.5)\n\nData: as.numeric(pred_svm) in 39 controls (test$Target 0) < 76 cases (test$Target 1).\nArea under the curve: 0.692\n\n\n\n\nAccuracy SVM\n\ntable(test$Target, pred_svm) %>% \n    confusionMatrix(positive = \"1\") %>% \n    tidy() %>% kable() #%>%\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nclass\nestimate\nconf.low\nconf.high\np.value\n\n\n\n\naccuracy\nNA\n0.7826087\n0.6960357\n0.8541027\n0.9685321\n\n\nkappa\nNA\n0.4418560\nNA\nNA\nNA\n\n\nmcnemar\nNA\nNA\nNA\nNA\n0.0000633\n\n\nsensitivity\n1\n0.7628866\nNA\nNA\nNA\n\n\nspecificity\n1\n0.8888889\nNA\nNA\nNA\n\n\npos_pred_value\n1\n0.9736842\nNA\nNA\nNA\n\n\nneg_pred_value\n1\n0.4102564\nNA\nNA\nNA\n\n\nprecision\n1\n0.9736842\nNA\nNA\nNA\n\n\nrecall\n1\n0.7628866\nNA\nNA\nNA\n\n\nf1\n1\n0.8554913\nNA\nNA\nNA\n\n\nprevalence\n1\n0.8434783\nNA\nNA\nNA\n\n\ndetection_rate\n1\n0.6434783\nNA\nNA\nNA\n\n\ndetection_prevalence\n1\n0.6608696\nNA\nNA\nNA\n\n\nbalanced_accuracy\n1\n0.8258877\nNA\nNA\nNA\n\n\n\n\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"100%\")\n\n\n\nValidation Curves\nThe two models almost give equal results based on accuracy, f1 score and area under the curve. In this section we will evaluate the models using learning curves to see if they suffer from high variance or bias. In this case the model suffers from high bias. It’s evident that adding more data won’t solve accuracy problems. In this case additional features would help.\n\nsets <- seq(from = 50, to = nrow(train), by = 50)\nsets[length(sets)] <-nrow(train) \ntrain.err <- c()\ntest.err<- c()\nfor (i in 1:length(sets)) {\n    \n    traini = train[1:sets[i],]\n    fit_svm <- svm(as.factor(Target)~., data = traini, cost =5  , gamma = .01, \n               kernel = \"radial\", type =\"C-classification\")\n\n    \n    pred_train = predict(fit_svm, newdata = traini)\n    train.err[i] =1 -  mean(pred_train == traini$Target)\n    pred_test <- predict(fit_svm, newdata = test)\n    test.err[i] = 1 - mean(test$Target == pred_test)\n    \n    cat(i,\" \")\n    \n}\n\n1  2  3  4  5  \n\nmatplot(sets, cbind(test.err, train.err), pch = 19, col = c(\"red\", \"blue\"),\n        type = \"b\", ylab = \"Error\", xlab = \"Train sample size\", main = \"SVM Training and Validation errors\")\nlegend(\"topright\", legend = c(\"Test\", \"Train\"), pch = 19, col = c(\"red\", \"blue\"))\n\n\n\n\n\n\nDeployment\nOther model like Xgboost which uses boosting and bagging could first be used to see if the model performs better on this data. The problem could after this be intergrated with a loan evaluation software where it can help loan officers decide if the will award a loan."
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer data.html",
    "href": "posts/breast_cancer_prediction/cancer data.html",
    "title": "Cancer Data",
    "section": "",
    "text": "In this tutorial I’m going to predict whether a breast cancer tumor is benign or malignant. Using Wiscosin breast cancer data set available on Kaggle. The 30 predictors are divided into three parts first is Mean ( variables 3-13), Standard Error(13-23) and Worst(23-32) and each contain 10 parameter (radius, texture,area, perimeter, smoothness,compactness,concavity,concave points,symmetry and fractal dimension of the tumor). When predicting cancer breast tumor types there two types of cost;\n\nThe cost of telling someone who has malignant tumor that they have benign these are the false negatives in this case someone might not seek medical help which is can cause death.\nTelling someone that they have malignant type of tumor but they don’t which is usually false positives. In this case you subject someone to unnecessary stress\n\nSo it’s highly desirable that our model has good accuracy $ f_1 score$ and high recall.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(xgboost)\nlibrary(knitr)\nlibrary(broom)\nlibrary(caret)\nlibrary(e1071)\nlibrary(kableExtra)\nlibrary(ggthemes)\n\nlibrary(glmnet)\n\ncancer <- setDT(read_csv(\"data.csv\"))\n\ncancer[, X33 := NULL]\n\n\nhead(cancer)  %>%\n  kable() \n\n\n\n \n  \n    id \n    diagnosis \n    radius_mean \n    texture_mean \n    perimeter_mean \n    area_mean \n    smoothness_mean \n    compactness_mean \n    concavity_mean \n    concave points_mean \n    symmetry_mean \n    fractal_dimension_mean \n    radius_se \n    texture_se \n    perimeter_se \n    area_se \n    smoothness_se \n    compactness_se \n    concavity_se \n    concave points_se \n    symmetry_se \n    fractal_dimension_se \n    radius_worst \n    texture_worst \n    perimeter_worst \n    area_worst \n    smoothness_worst \n    compactness_worst \n    concavity_worst \n    concave points_worst \n    symmetry_worst \n    fractal_dimension_worst \n    ...33 \n  \n \n\n  \n    842302 \n    M \n    17.99 \n    10.38 \n    122.80 \n    1001.0 \n    0.11840 \n    0.27760 \n    0.3001 \n    0.14710 \n    0.2419 \n    0.07871 \n    1.0950 \n    0.9053 \n    8.589 \n    153.40 \n    0.006399 \n    0.04904 \n    0.05373 \n    0.01587 \n    0.03003 \n    0.006193 \n    25.38 \n    17.33 \n    184.60 \n    2019.0 \n    0.1622 \n    0.6656 \n    0.7119 \n    0.2654 \n    0.4601 \n    0.11890 \n    NA \n  \n  \n    842517 \n    M \n    20.57 \n    17.77 \n    132.90 \n    1326.0 \n    0.08474 \n    0.07864 \n    0.0869 \n    0.07017 \n    0.1812 \n    0.05667 \n    0.5435 \n    0.7339 \n    3.398 \n    74.08 \n    0.005225 \n    0.01308 \n    0.01860 \n    0.01340 \n    0.01389 \n    0.003532 \n    24.99 \n    23.41 \n    158.80 \n    1956.0 \n    0.1238 \n    0.1866 \n    0.2416 \n    0.1860 \n    0.2750 \n    0.08902 \n    NA \n  \n  \n    84300903 \n    M \n    19.69 \n    21.25 \n    130.00 \n    1203.0 \n    0.10960 \n    0.15990 \n    0.1974 \n    0.12790 \n    0.2069 \n    0.05999 \n    0.7456 \n    0.7869 \n    4.585 \n    94.03 \n    0.006150 \n    0.04006 \n    0.03832 \n    0.02058 \n    0.02250 \n    0.004571 \n    23.57 \n    25.53 \n    152.50 \n    1709.0 \n    0.1444 \n    0.4245 \n    0.4504 \n    0.2430 \n    0.3613 \n    0.08758 \n    NA \n  \n  \n    84348301 \n    M \n    11.42 \n    20.38 \n    77.58 \n    386.1 \n    0.14250 \n    0.28390 \n    0.2414 \n    0.10520 \n    0.2597 \n    0.09744 \n    0.4956 \n    1.1560 \n    3.445 \n    27.23 \n    0.009110 \n    0.07458 \n    0.05661 \n    0.01867 \n    0.05963 \n    0.009208 \n    14.91 \n    26.50 \n    98.87 \n    567.7 \n    0.2098 \n    0.8663 \n    0.6869 \n    0.2575 \n    0.6638 \n    0.17300 \n    NA \n  \n  \n    84358402 \n    M \n    20.29 \n    14.34 \n    135.10 \n    1297.0 \n    0.10030 \n    0.13280 \n    0.1980 \n    0.10430 \n    0.1809 \n    0.05883 \n    0.7572 \n    0.7813 \n    5.438 \n    94.44 \n    0.011490 \n    0.02461 \n    0.05688 \n    0.01885 \n    0.01756 \n    0.005115 \n    22.54 \n    16.67 \n    152.20 \n    1575.0 \n    0.1374 \n    0.2050 \n    0.4000 \n    0.1625 \n    0.2364 \n    0.07678 \n    NA \n  \n  \n    843786 \n    M \n    12.45 \n    15.70 \n    82.57 \n    477.1 \n    0.12780 \n    0.17000 \n    0.1578 \n    0.08089 \n    0.2087 \n    0.07613 \n    0.3345 \n    0.8902 \n    2.217 \n    27.19 \n    0.007510 \n    0.03345 \n    0.03672 \n    0.01137 \n    0.02165 \n    0.005082 \n    15.47 \n    23.75 \n    103.40 \n    741.6 \n    0.1791 \n    0.5249 \n    0.5355 \n    0.1741 \n    0.3985 \n    0.12440 \n    NA"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer data.html#percentage-of-women-with-malignant-tumor",
    "href": "posts/breast_cancer_prediction/cancer data.html#percentage-of-women-with-malignant-tumor",
    "title": "Cancer Data",
    "section": "Percentage of women with malignant tumor",
    "text": "Percentage of women with malignant tumor\nThe percentage of women with malignant tumor is 37.26%(212 out 569) while the rest 62.74%(357) had benign tumors.\n\ncancer[, .(freq = .N),\n       by = diagnosis] %>% \n    .[, perc := round(100 * freq/sum(freq), 2)] %>%\n  \nggplot(aes(x=diagnosis, y=perc, fill = diagnosis)) + \n    geom_bar(stat = \"identity\", width  = 0.5)+ theme_hc() +\n    geom_text(aes(x=diagnosis, y=perc, label = paste(perc, \"%\")),\n              position =  position_dodge(width = 0.5),\n              vjust = 0.05, hjust = 0.5, size = 5)+\n    scale_fill_hc(name = \"\")+\n    labs(x = \"Cancer Type\",\n         y = \"Percentage\",\n         title = \"Percentage of women with benign or malignant breast bancer\")+\n    theme(legend.position = \"none\",\n          axis.title = element_text(size =12))"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer data.html#boxplots",
    "href": "posts/breast_cancer_prediction/cancer data.html#boxplots",
    "title": "Cancer Data",
    "section": "Boxplots",
    "text": "Boxplots\nFrom the boxplots we can identify variables where we expect there is a significance difference between the two groups of cancer tumors. When using a boxplot if two distributions do not averlap or more than 75% of two boxplot do not overlap then we expect that there is a significance difference in the mean/median between the two groups. Some of the variables where the distribution of two cancer tumors are significantly different are radius_mean, texture_mean etc. The visible differences between malignant tumors and benign tumors can be seen in means of all cells and worst means where worst means is the average of all the worst cells. The distribution of malignant tumors have higher scores than the benign tumors in this cases.\n\ncancerm <- melt(cancer[, -1, with = F], id.vars = \"diagnosis\")\n\nggplot(cancerm, aes(x = diagnosis, y = value))+\n    geom_boxplot() + facet_wrap(~variable, scales = \"free_y\")"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer data.html#features-scaling",
    "href": "posts/breast_cancer_prediction/cancer data.html#features-scaling",
    "title": "Cancer Data",
    "section": "Features Scaling",
    "text": "Features Scaling\nWe find that some variables are highly correlated. We can use principle component analysis for dimension reduction. Since variables are correlated it’s evident that we can use a smaller set of features to build our models.\n\ncancer[, id := NULL]\npredictors <- names(cancer)[3:31]\ncancer[, (predictors) := lapply(.SD, function(x) scale(x)), .SDcols = predictors ]\ncancer[, diagnosis := as.factor(diagnosis)]"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer data.html#correlation-matrix",
    "href": "posts/breast_cancer_prediction/cancer data.html#correlation-matrix",
    "title": "Cancer Data",
    "section": "Correlation matrix",
    "text": "Correlation matrix\n\ncor(cancer[, -(1:2), with = F]) %>% kable(format = \"html\") \n\n\n\n \n  \n      \n    texture_mean \n    perimeter_mean \n    area_mean \n    smoothness_mean \n    compactness_mean \n    concavity_mean \n    concave points_mean \n    symmetry_mean \n    fractal_dimension_mean \n    radius_se \n    texture_se \n    perimeter_se \n    area_se \n    smoothness_se \n    compactness_se \n    concavity_se \n    concave points_se \n    symmetry_se \n    fractal_dimension_se \n    radius_worst \n    texture_worst \n    perimeter_worst \n    area_worst \n    smoothness_worst \n    compactness_worst \n    concavity_worst \n    concave points_worst \n    symmetry_worst \n    fractal_dimension_worst \n    ...33 \n  \n \n\n  \n    texture_mean \n    1.0000000 \n    0.3348467 \n    0.3249314 \n    -0.0168606 \n    0.2397721 \n    0.3055518 \n    0.2969848 \n    0.0733047 \n    -0.0753503 \n    0.2763845 \n    0.3860911 \n    0.2823893 \n    0.2612411 \n    0.0065162 \n    0.1949659 \n    0.1459032 \n    0.1687328 \n    0.0075214 \n    0.0553608 \n    0.3567016 \n    0.9120685 \n    0.3622521 \n    0.3466688 \n    0.0818589 \n    0.2811574 \n    0.3046919 \n    0.3002693 \n    0.1052501 \n    0.1210405 \n    NA \n  \n  \n    perimeter_mean \n    0.3348467 \n    1.0000000 \n    0.9866392 \n    0.1996206 \n    0.5555185 \n    0.7154316 \n    0.8506045 \n    0.1810399 \n    -0.2641085 \n    0.6935563 \n    -0.0857974 \n    0.6946630 \n    0.7458003 \n    -0.2031263 \n    0.2480418 \n    0.2255805 \n    0.4035749 \n    -0.0794948 \n    -0.0067665 \n    0.9694784 \n    0.3065262 \n    0.9703763 \n    0.9418037 \n    0.1454752 \n    0.4538151 \n    0.5621815 \n    0.7699618 \n    0.1895116 \n    0.0487847 \n    NA \n  \n  \n    area_mean \n    0.3249314 \n    0.9866392 \n    1.0000000 \n    0.1713832 \n    0.4971456 \n    0.6851828 \n    0.8227435 \n    0.1496752 \n    -0.2849760 \n    0.7335726 \n    -0.0654844 \n    0.7274347 \n    0.8004136 \n    -0.1669271 \n    0.2104089 \n    0.2056822 \n    0.3695625 \n    -0.0708589 \n    -0.0208312 \n    0.9626243 \n    0.2899147 \n    0.9589863 \n    0.9591717 \n    0.1196673 \n    0.3886654 \n    0.5110918 \n    0.7209618 \n    0.1436859 \n    0.0019592 \n    NA \n  \n  \n    smoothness_mean \n    -0.0168606 \n    0.1996206 \n    0.1713832 \n    1.0000000 \n    0.6592250 \n    0.5209008 \n    0.5522783 \n    0.5583905 \n    0.5869046 \n    0.3036836 \n    0.0711235 \n    0.2977854 \n    0.2461527 \n    0.3355193 \n    0.3156441 \n    0.2449380 \n    0.3745982 \n    0.2067719 \n    0.2839774 \n    0.2075276 \n    0.0406274 \n    0.2334198 \n    0.2027650 \n    0.8044566 \n    0.4704797 \n    0.4321145 \n    0.4990937 \n    0.3974496 \n    0.4997254 \n    NA \n  \n  \n    compactness_mean \n    0.2397721 \n    0.5555185 \n    0.4971456 \n    0.6592250 \n    1.0000000 \n    0.8828571 \n    0.8307113 \n    0.6020379 \n    0.5650475 \n    0.4979140 \n    0.0470419 \n    0.5492389 \n    0.4553265 \n    0.1355576 \n    0.7381045 \n    0.5695991 \n    0.6412006 \n    0.2318879 \n    0.5071996 \n    0.5340153 \n    0.2501012 \n    0.5890589 \n    0.5085351 \n    0.5641497 \n    0.8654824 \n    0.8158053 \n    0.8151742 \n    0.5107226 \n    0.6870027 \n    NA \n  \n  \n    concavity_mean \n    0.3055518 \n    0.7154316 \n    0.6851828 \n    0.5209008 \n    0.8828571 \n    1.0000000 \n    0.9212134 \n    0.4998999 \n    0.3361432 \n    0.6324820 \n    0.0770646 \n    0.6608196 \n    0.6173056 \n    0.0987689 \n    0.6695255 \n    0.6906301 \n    0.6824520 \n    0.1797733 \n    0.4490990 \n    0.6874240 \n    0.3018644 \n    0.7288675 \n    0.6753025 \n    0.4470230 \n    0.7543916 \n    0.8838333 \n    0.8611496 \n    0.4098191 \n    0.5142737 \n    NA \n  \n  \n    concave points_mean \n    0.2969848 \n    0.8506045 \n    0.8227435 \n    0.5522783 \n    0.8307113 \n    0.9212134 \n    1.0000000 \n    0.4615960 \n    0.1659497 \n    0.6988770 \n    0.0223649 \n    0.7113133 \n    0.6903631 \n    0.0278020 \n    0.4891079 \n    0.4378626 \n    0.6142249 \n    0.0972080 \n    0.2571272 \n    0.8298005 \n    0.2950146 \n    0.8554852 \n    0.8091983 \n    0.4506263 \n    0.6665609 \n    0.7516804 \n    0.9099942 \n    0.3761656 \n    0.3676905 \n    NA \n  \n  \n    symmetry_mean \n    0.0733047 \n    0.1810399 \n    0.1496752 \n    0.5583905 \n    0.6020379 \n    0.4998999 \n    0.4615960 \n    1.0000000 \n    0.4795281 \n    0.3034588 \n    0.1286989 \n    0.3138581 \n    0.2234723 \n    0.1875038 \n    0.4207298 \n    0.3416388 \n    0.3920322 \n    0.4507152 \n    0.3314725 \n    0.1841225 \n    0.0918556 \n    0.2176222 \n    0.1759235 \n    0.4255388 \n    0.4723426 \n    0.4327416 \n    0.4291826 \n    0.7001724 \n    0.4378056 \n    NA \n  \n  \n    fractal_dimension_mean \n    -0.0753503 \n    -0.2641085 \n    -0.2849760 \n    0.5869046 \n    0.5650475 \n    0.3361432 \n    0.1659497 \n    0.4795281 \n    1.0000000 \n    0.0000416 \n    0.1646192 \n    0.0396863 \n    -0.0906731 \n    0.4021255 \n    0.5595095 \n    0.4461487 \n    0.3405013 \n    0.3460195 \n    0.6880329 \n    -0.2556041 \n    -0.0505544 \n    -0.2069980 \n    -0.2332194 \n    0.5047833 \n    0.4583209 \n    0.3455588 \n    0.1741173 \n    0.3340644 \n    0.7671484 \n    NA \n  \n  \n    radius_se \n    0.2763845 \n    0.6935563 \n    0.7335726 \n    0.3036836 \n    0.4979140 \n    0.6324820 \n    0.6988770 \n    0.3034588 \n    0.0000416 \n    1.0000000 \n    0.2133232 \n    0.9727997 \n    0.9519587 \n    0.1645214 \n    0.3563471 \n    0.3325557 \n    0.5147737 \n    0.2407817 \n    0.2277365 \n    0.7161497 \n    0.1949942 \n    0.7208133 \n    0.7521854 \n    0.1421222 \n    0.2873241 \n    0.3809970 \n    0.5322775 \n    0.0945375 \n    0.0494913 \n    NA \n  \n  \n    texture_se \n    0.3860911 \n    -0.0857974 \n    -0.0654844 \n    0.0711235 \n    0.0470419 \n    0.0770646 \n    0.0223649 \n    0.1286989 \n    0.1646192 \n    0.2133232 \n    1.0000000 \n    0.2233106 \n    0.1119175 \n    0.3972618 \n    0.2327921 \n    0.1959300 \n    0.2323539 \n    0.4113713 \n    0.2800532 \n    -0.1109476 \n    0.4087503 \n    -0.1014722 \n    -0.0825642 \n    -0.0726345 \n    -0.0917546 \n    -0.0681867 \n    -0.1187947 \n    -0.1281991 \n    -0.0451762 \n    NA \n  \n  \n    perimeter_se \n    0.2823893 \n    0.6946630 \n    0.7274347 \n    0.2977854 \n    0.5492389 \n    0.6608196 \n    0.7113133 \n    0.3138581 \n    0.0396863 \n    0.9727997 \n    0.2233106 \n    1.0000000 \n    0.9377260 \n    0.1510926 \n    0.4165056 \n    0.3625534 \n    0.5575457 \n    0.2668348 \n    0.2440735 \n    0.6980490 \n    0.2006903 \n    0.7219556 \n    0.7311760 \n    0.1299378 \n    0.3420315 \n    0.4191757 \n    0.5559178 \n    0.1099197 \n    0.0852686 \n    NA \n  \n  \n    area_se \n    0.2612411 \n    0.7458003 \n    0.8004136 \n    0.2461527 \n    0.4553265 \n    0.6173056 \n    0.6903631 \n    0.2234723 \n    -0.0906731 \n    0.9519587 \n    0.1119175 \n    0.9377260 \n    1.0000000 \n    0.0752054 \n    0.2842785 \n    0.2703448 \n    0.4155800 \n    0.1348199 \n    0.1267974 \n    0.7576888 \n    0.1972630 \n    0.7615596 \n    0.8115073 \n    0.1242661 \n    0.2826826 \n    0.3846798 \n    0.5382818 \n    0.0741011 \n    0.0169344 \n    NA \n  \n  \n    smoothness_se \n    0.0065162 \n    -0.2031263 \n    -0.1669271 \n    0.3355193 \n    0.1355576 \n    0.0987689 \n    0.0278020 \n    0.1875038 \n    0.4021255 \n    0.1645214 \n    0.3972618 \n    0.1510926 \n    0.0752054 \n    1.0000000 \n    0.3372000 \n    0.2690422 \n    0.3296606 \n    0.4136480 \n    0.4274635 \n    -0.2309737 \n    -0.0748480 \n    -0.2175733 \n    -0.1822882 \n    0.3155812 \n    -0.0555247 \n    -0.0582726 \n    -0.1021296 \n    -0.1073384 \n    0.1015963 \n    NA \n  \n  \n    compactness_se \n    0.1949659 \n    0.2480418 \n    0.2104089 \n    0.3156441 \n    0.7381045 \n    0.6695255 \n    0.4891079 \n    0.4207298 \n    0.5595095 \n    0.3563471 \n    0.2327921 \n    0.4165056 \n    0.2842785 \n    0.3372000 \n    1.0000000 \n    0.8008449 \n    0.7434467 \n    0.3969261 \n    0.8035464 \n    0.2023127 \n    0.1448150 \n    0.2583557 \n    0.1976044 \n    0.2245372 \n    0.6779850 \n    0.6381903 \n    0.4814987 \n    0.2781120 \n    0.5904310 \n    NA \n  \n  \n    concavity_se \n    0.1459032 \n    0.2255805 \n    0.2056822 \n    0.2449380 \n    0.5695991 \n    0.6906301 \n    0.4378626 \n    0.3416388 \n    0.4461487 \n    0.3325557 \n    0.1959300 \n    0.3625534 \n    0.2703448 \n    0.2690422 \n    0.8008449 \n    1.0000000 \n    0.7714894 \n    0.3112920 \n    0.7274698 \n    0.1847825 \n    0.1018163 \n    0.2246401 \n    0.1867238 \n    0.1656877 \n    0.4837067 \n    0.6617779 \n    0.4388323 \n    0.1978935 \n    0.4385803 \n    NA \n  \n  \n    concave points_se \n    0.1687328 \n    0.4035749 \n    0.3695625 \n    0.3745982 \n    0.6412006 \n    0.6824520 \n    0.6142249 \n    0.3920322 \n    0.3405013 \n    0.5147737 \n    0.2323539 \n    0.5575457 \n    0.4155800 \n    0.3296606 \n    0.7434467 \n    0.7714894 \n    1.0000000 \n    0.3164954 \n    0.6118084 \n    0.3551410 \n    0.0896509 \n    0.3921456 \n    0.3400904 \n    0.2104063 \n    0.4508531 \n    0.5477819 \n    0.6001139 \n    0.1434168 \n    0.3092755 \n    NA \n  \n  \n    symmetry_se \n    0.0075214 \n    -0.0794948 \n    -0.0708589 \n    0.2067719 \n    0.2318879 \n    0.1797733 \n    0.0972080 \n    0.4507152 \n    0.3460195 \n    0.2407817 \n    0.4113713 \n    0.2668348 \n    0.1348199 \n    0.4136480 \n    0.3969261 \n    0.3112920 \n    0.3164954 \n    1.0000000 \n    0.3698176 \n    -0.1265357 \n    -0.0785659 \n    -0.1020896 \n    -0.1090824 \n    -0.0102341 \n    0.0619658 \n    0.0389277 \n    -0.0281902 \n    0.3896615 \n    0.0791539 \n    NA \n  \n  \n    fractal_dimension_se \n    0.0553608 \n    -0.0067665 \n    -0.0208312 \n    0.2839774 \n    0.5071996 \n    0.4490990 \n    0.2571272 \n    0.3314725 \n    0.6880329 \n    0.2277365 \n    0.2800532 \n    0.2440735 \n    0.1267974 \n    0.4274635 \n    0.8035464 \n    0.7274698 \n    0.6118084 \n    0.3698176 \n    1.0000000 \n    -0.0385109 \n    -0.0026818 \n    -0.0019697 \n    -0.0234881 \n    0.1698550 \n    0.3898961 \n    0.3797140 \n    0.2146318 \n    0.1110761 \n    0.5911918 \n    NA \n  \n  \n    radius_worst \n    0.3567016 \n    0.9694784 \n    0.9626243 \n    0.2075276 \n    0.5340153 \n    0.6874240 \n    0.8298005 \n    0.1841225 \n    -0.2556041 \n    0.7161497 \n    -0.1109476 \n    0.6980490 \n    0.7576888 \n    -0.2309737 \n    0.2023127 \n    0.1847825 \n    0.3551410 \n    -0.1265357 \n    -0.0385109 \n    1.0000000 \n    0.3626411 \n    0.9936859 \n    0.9840695 \n    0.2129780 \n    0.4742604 \n    0.5725939 \n    0.7865764 \n    0.2438378 \n    0.0918326 \n    NA \n  \n  \n    texture_worst \n    0.9120685 \n    0.3065262 \n    0.2899147 \n    0.0406274 \n    0.2501012 \n    0.3018644 \n    0.2950146 \n    0.0918556 \n    -0.0505544 \n    0.1949942 \n    0.4087503 \n    0.2006903 \n    0.1972630 \n    -0.0748480 \n    0.1448150 \n    0.1018163 \n    0.0896509 \n    -0.0785659 \n    -0.0026818 \n    0.3626411 \n    1.0000000 \n    0.3678800 \n    0.3478229 \n    0.2287448 \n    0.3631098 \n    0.3708700 \n    0.3632638 \n    0.2332126 \n    0.2203558 \n    NA \n  \n  \n    perimeter_worst \n    0.3622521 \n    0.9703763 \n    0.9589863 \n    0.2334198 \n    0.5890589 \n    0.7288675 \n    0.8554852 \n    0.2176222 \n    -0.2069980 \n    0.7208133 \n    -0.1014722 \n    0.7219556 \n    0.7615596 \n    -0.2175733 \n    0.2583557 \n    0.2246401 \n    0.3921456 \n    -0.1020896 \n    -0.0019697 \n    0.9936859 \n    0.3678800 \n    1.0000000 \n    0.9776273 \n    0.2332165 \n    0.5279936 \n    0.6170916 \n    0.8155807 \n    0.2698600 \n    0.1373786 \n    NA \n  \n  \n    area_worst \n    0.3466688 \n    0.9418037 \n    0.9591717 \n    0.2027650 \n    0.5085351 \n    0.6753025 \n    0.8091983 \n    0.1759235 \n    -0.2332194 \n    0.7521854 \n    -0.0825642 \n    0.7311760 \n    0.8115073 \n    -0.1822882 \n    0.1976044 \n    0.1867238 \n    0.3400904 \n    -0.1090824 \n    -0.0234881 \n    0.9840695 \n    0.3478229 \n    0.9776273 \n    1.0000000 \n    0.2064632 \n    0.4370153 \n    0.5422236 \n    0.7468777 \n    0.2092681 \n    0.0783430 \n    NA \n  \n  \n    smoothness_worst \n    0.0818589 \n    0.1454752 \n    0.1196673 \n    0.8044566 \n    0.5641497 \n    0.4470230 \n    0.4506263 \n    0.4255388 \n    0.5047833 \n    0.1421222 \n    -0.0726345 \n    0.1299378 \n    0.1242661 \n    0.3155812 \n    0.2245372 \n    0.1656877 \n    0.2104063 \n    -0.0102341 \n    0.1698550 \n    0.2129780 \n    0.2287448 \n    0.2332165 \n    0.2064632 \n    1.0000000 \n    0.5666866 \n    0.5165971 \n    0.5450990 \n    0.4951906 \n    0.6173523 \n    NA \n  \n  \n    compactness_worst \n    0.2811574 \n    0.4538151 \n    0.3886654 \n    0.4704797 \n    0.8654824 \n    0.7543916 \n    0.6665609 \n    0.4723426 \n    0.4583209 \n    0.2873241 \n    -0.0917546 \n    0.3420315 \n    0.2826826 \n    -0.0555247 \n    0.6779850 \n    0.4837067 \n    0.4508531 \n    0.0619658 \n    0.3898961 \n    0.4742604 \n    0.3631098 \n    0.5279936 \n    0.4370153 \n    0.5666866 \n    1.0000000 \n    0.8919686 \n    0.8005448 \n    0.6151295 \n    0.8103007 \n    NA \n  \n  \n    concavity_worst \n    0.3046919 \n    0.5621815 \n    0.5110918 \n    0.4321145 \n    0.8158053 \n    0.8838333 \n    0.7516804 \n    0.4327416 \n    0.3455588 \n    0.3809970 \n    -0.0681867 \n    0.4191757 \n    0.3846798 \n    -0.0582726 \n    0.6381903 \n    0.6617779 \n    0.5477819 \n    0.0389277 \n    0.3797140 \n    0.5725939 \n    0.3708700 \n    0.6170916 \n    0.5422236 \n    0.5165971 \n    0.8919686 \n    1.0000000 \n    0.8549979 \n    0.5332111 \n    0.6861550 \n    NA \n  \n  \n    concave points_worst \n    0.3002693 \n    0.7699618 \n    0.7209618 \n    0.4990937 \n    0.8151742 \n    0.8611496 \n    0.9099942 \n    0.4291826 \n    0.1741173 \n    0.5322775 \n    -0.1187947 \n    0.5559178 \n    0.5382818 \n    -0.1021296 \n    0.4814987 \n    0.4388323 \n    0.6001139 \n    -0.0281902 \n    0.2146318 \n    0.7865764 \n    0.3632638 \n    0.8155807 \n    0.7468777 \n    0.5450990 \n    0.8005448 \n    0.8549979 \n    1.0000000 \n    0.5037338 \n    0.5104293 \n    NA \n  \n  \n    symmetry_worst \n    0.1052501 \n    0.1895116 \n    0.1436859 \n    0.3974496 \n    0.5107226 \n    0.4098191 \n    0.3761656 \n    0.7001724 \n    0.3340644 \n    0.0945375 \n    -0.1281991 \n    0.1099197 \n    0.0741011 \n    -0.1073384 \n    0.2781120 \n    0.1978935 \n    0.1434168 \n    0.3896615 \n    0.1110761 \n    0.2438378 \n    0.2332126 \n    0.2698600 \n    0.2092681 \n    0.4951906 \n    0.6151295 \n    0.5332111 \n    0.5037338 \n    1.0000000 \n    0.5380530 \n    NA \n  \n  \n    fractal_dimension_worst \n    0.1210405 \n    0.0487847 \n    0.0019592 \n    0.4997254 \n    0.6870027 \n    0.5142737 \n    0.3676905 \n    0.4378056 \n    0.7671484 \n    0.0494913 \n    -0.0451762 \n    0.0852686 \n    0.0169344 \n    0.1015963 \n    0.5904310 \n    0.4385803 \n    0.3092755 \n    0.0791539 \n    0.5911918 \n    0.0918326 \n    0.2203558 \n    0.1373786 \n    0.0783430 \n    0.6173523 \n    0.8103007 \n    0.6861550 \n    0.5104293 \n    0.5380530 \n    1.0000000 \n    NA \n  \n  \n    ...33 \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    NA \n    1"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer data.html#principle-component-analysis",
    "href": "posts/breast_cancer_prediction/cancer data.html#principle-component-analysis",
    "title": "Cancer Data",
    "section": "Principle Component Analysis",
    "text": "Principle Component Analysis\nUsing the elbow rule we can use the first 5 principle components. Using 15 principle components we will have achieved al most 100% of the variance from the original data set.\n\npca <- prcomp(cancer[, predictors, with = F], scale. = F)"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer data.html#variance-explained",
    "href": "posts/breast_cancer_prediction/cancer data.html#variance-explained",
    "title": "Cancer Data",
    "section": "Variance Explained",
    "text": "Variance Explained\nSince PCA forms new characteristics the variance explained plot shows the amount of variation of the original features captured by each principle component. The new features are simply linear combinations of the old features.\n\nstdpca <- pca$sdev\n\nvarpca <- stdpca^2\n\nprop_var <- varpca/sum(varpca)\nprop_var * 100\n\n [1] 43.674096171 18.515078926  9.719517309  6.799035251  5.699152690\n [6]  4.156646615  2.281687293  1.640413610  1.365230638  1.193505447\n[11]  1.013597084  0.899798372  0.832034896  0.533922980  0.323984133\n[16]  0.265622350  0.198426127  0.178711402  0.154147124  0.107221373\n[21]  0.102310566  0.093538442  0.082887917  0.058770552  0.053457423\n[26]  0.026927589  0.022780572  0.005125742  0.002371405\n\nsum(prop_var[1:15])\n\n[1] 0.986477"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer data.html#scree-plot",
    "href": "posts/breast_cancer_prediction/cancer data.html#scree-plot",
    "title": "Cancer Data",
    "section": "Scree plot",
    "text": "Scree plot\nScree plot shows the variance explained by each principle component which reduces as the number of principle components increase.\n\nplot(prop_var, xlab = \"Principal Component\",\n     ylab = \"Proportion of Variance Explained\",\n     type = \"b\", xlim = c(0, 30))"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer data.html#cumulative-variance-explained",
    "href": "posts/breast_cancer_prediction/cancer data.html#cumulative-variance-explained",
    "title": "Cancer Data",
    "section": "Cumulative Variance Explained",
    "text": "Cumulative Variance Explained\nThe cumulative of variance plot helps to choose the number of features based on the amount of variation from original data set you want captured. In this case, I wanted to use number of principle components that capture almost 100% of the variation. After trying with different number of principle components I found out that the accuracy of the models did not increase after the 15th principle components.\n\ncum_var <- cumsum(prop_var)\nplot(cum_var, xlab = \"Principal Component\",\n     ylab = \"Cumulative Proportion of Variance Explained\",\n     type = \"b\", xlim = c(0, 30))"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer data.html#construct-new-data-set",
    "href": "posts/breast_cancer_prediction/cancer data.html#construct-new-data-set",
    "title": "Cancer Data",
    "section": "Construct new data set",
    "text": "Construct new data set\nWe use the first 15 principle components as our new predictors, then we randomly split data into training and test set in 7:3 ratio.\n\nset.seed(100)\ntrain_sample <- sample(1:nrow(cancer), round(0.7*nrow(cancer)))\n\npcadat <- data.table( label = cancer$diagnosis, pca$x[,1:15]) \npcadat[, label := factor(label, levels = c(\"M\", \"B\"))]\ntrain <- pcadat[train_sample,]\ntest <- pcadat[-train_sample,]"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer data.html#logistic-regression",
    "href": "posts/breast_cancer_prediction/cancer data.html#logistic-regression",
    "title": "Cancer Data",
    "section": "Logistic regression",
    "text": "Logistic regression\nThis is one of generalized linear models which deals with binary data. There is a generalization of this model which is called multinomial regression where you can fit multi class data. The equation for logistic regression model is:\n\\[log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1*X_1 + ... \\beta_n * X_n\\] and using mle the cost function can be derived as: \\[J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m} y^i log(h_\\theta(x^i)) + (1-y^i) log(1 - h_\\theta(x^i))\\] Given that \\[y = 0\\] \\[y = 1\\] . Finding \\[\\beta\\] s we minimizing the cost function.\n\nfit_glm <- glm(label ~., data = train, family = binomial)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer data.html#regularization-in-logistic-regression",
    "href": "posts/breast_cancer_prediction/cancer data.html#regularization-in-logistic-regression",
    "title": "Cancer Data",
    "section": "Regularization in logistic regression",
    "text": "Regularization in logistic regression\nThe warning “glm.fit: fitted probabilities numerically 0 or 1 occurred” shows that there is a perfect separation/over fitting. In this case you can load glmnet library and fit a regularized logistic regression. These can be achieved by adding a regularization term to the cost function.The L1 regularization(Lasso) adds a penalty equal to the sum of the absolute values of the coefficients.\n\\[J(\\theta) = -\\frac{1}{m}\\sum_{i=0}^{m} y^i log(h_\\theta(x^i)) + (1-y^i) log(1 - h_\\theta(x^i)) + \\frac {\\lambda}{2m}\\sum_{j=1}^{n} |\\theta^i|\\]\n\ntrainx <- train[,-1]\n\ny_train <- factor(train$label, levels = c(\"B\", \"M\"), labels = 0:1)\n#y <- as.numeric(as.character(y))\n\ny_test <- factor(test$label, levels = c(\"B\", \"M\"), labels = 0:1) %>% as.character() %>% as.numeric()\n#ytest <- as.numeric(as.character(ytest))\n\ntestx <- data.matrix(test[, -1]) \n\nTo find the optimal values \\(\\lambda\\) we use cross validation. We choose \\(\\lambda\\) which gives the highest cross validation accuracy.\n\ncv_fold <- createFolds(train$label, k = 10)\n\nmyControl <- trainControl(\n  method = \"cv\", \n  number = 10,\n  summaryFunction = twoClassSummary,\n  savePredictions = \"all\",\n  classProbs = TRUE,\n  verboseIter = FALSE,\n  index = cv_fold,\n  allowParallel = TRUE\n  \n)\n\ntuneGrid <-  expand.grid(\n    alpha = 0:1,\n    lambda = seq(0.001, 1, length.out = 10))\n    \nglmnet_model <- train(\n  label ~.,\n  data = train,\n  method = \"glmnet\",\n  metric = \"ROC\",\n  trControl = myControl,\n  tuneGrid = tuneGrid\n)\n\ns\n\nplot(glmnet_model) \n\n\n\n#lamda_min <- cv_glm$lambda.min\n\n\nresample_glmnet <- thresholder(glmnet_model, \n                              threshold = seq(.2, 1, by = 0.05), \n                              final = TRUE, \n                              statistics = \"all\")\n\nggplot(resample_glmnet , aes(x = prob_threshold, y = F1)) + \n  geom_point() + \n  geom_point(aes(y = Sensitivity), col = \"blue\")\n\n\n\n\n\nlibrary(caTools)\n\npred_glm <- predict(glmnet_model, test, type = \"prob\")\n\ncolAUC(pred_glm , test$label, plotROC = TRUE)\n\n\n\n\n                M         B\nM vs. B 0.9686957 0.9686957\n\npred_glm1 <- ifelse(pred_glm[, \"M\"] > 0.4, \"M\", \"B\")\n#pred_glm1 <- predict(glmnet_model, test, type = \"raw\")\n\n\npred_glm1 <- factor(pred_glm1, levels = levels(test$label))\n\n\nconfusionMatrix(pred_glm1, test$label,positive = \"M\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   M   B\n         M  48  10\n         B   7 105\n                                          \n               Accuracy : 0.9             \n                 95% CI : (0.8447, 0.9407)\n    No Information Rate : 0.6765          \n    P-Value [Acc > NIR] : 6.539e-12       \n                                          \n                  Kappa : 0.7747          \n                                          \n Mcnemar's Test P-Value : 0.6276          \n                                          \n            Sensitivity : 0.8727          \n            Specificity : 0.9130          \n         Pos Pred Value : 0.8276          \n         Neg Pred Value : 0.9375          \n             Prevalence : 0.3235          \n         Detection Rate : 0.2824          \n   Detection Prevalence : 0.3412          \n      Balanced Accuracy : 0.8929          \n                                          \n       'Positive' Class : M"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer data.html#svm",
    "href": "posts/breast_cancer_prediction/cancer data.html#svm",
    "title": "Cancer Data",
    "section": "SVM",
    "text": "SVM\nSupport Vector Machines is a type of supervised learning algorithm that is used for classification and regression. Most of the times however, it’s used for classification.\nTo understand how SVM works consider the following example of linearly separable data. It’s clear that we can separate the two classes using a straight line(decision boundary). Which is normally referred to a separating hyperplane.\n\n\n\n\n\nThe question is, since there exists many lines that can separate the red and the black classes which is the best one. This introduces us to the maximal margin classification, In short SVM finds the hyperplane/line that gives the biggest margin/gap between the two classes. In this case SVM will choose the solid line as the hyperplane while the margins are the dotted lines. The circled points that lie directly on the margin, or on the wrong side of the margin for their class, are known as support vectors. This shows that SVM uses this points to come up with a the decision boundary, the other points are not used. In this case since it’s a two dimensional space the equation of the separating line will be \\[latex\\beta_0 + \\beta_1X_1 + \\beta_2X_2\\]. Then when equations evaluates to more than 0 then 1 is predicted \\[latex\\beta_0 + \\beta_1X_1 + \\beta_2X_2 > 0, y = 1\\] and when it evaluates to less than zero then predicted class is -1 \\[latex\\beta_0 + \\beta_1X_1 + \\beta_2X_2 < 0, \\; y = -1\\] This becomes maximisation problem \\[latexwidth \\; of \\;the \\; margin = M \\] \\[\\sum_{j=1}^{n}\\beta_j = 1\\]\n\\[latexy_i(\\beta_0 + \\beta_1X_1 + \\beta_2X_2) >= M\\]\n\n\n\n\n\nThis is a best case scenario but in most cases the classes are noisy. Consider the plot below no matter which line you choose some points are bound to be on the wrong side of the desicion boundary. Thus maximal margin classification would not work.\n\n\n\n\n\nSVM then introduces what is called a soft margin. In naive explanation you can think of this as a margin that allows some points to be on the wrong side. By introducing an error term we allow for some slack. Thus in a two case the maximisation becomes \\[y_i(\\beta_0 + \\beta_1X_1 + \\beta_2X_2) >= M(1- \\epsilon)\\]\n\\[\\sum_{i=0}^{n} \\epsilon_i <= C\\] C is a tuning parameter which determines the width of the margin while \\[\\epsilon_i  \\;'s\\] are slack variables. that allow individual observations to fall on the wrong side of the margin. In some cases the decision boundary maybe non linear. In case your are dealing with logistic regression you will be forced to introduce polynomial terms which might result in a very large feature space. SVM then introduces what are called kernels"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer data.html#tuning-svm",
    "href": "posts/breast_cancer_prediction/cancer data.html#tuning-svm",
    "title": "Cancer Data",
    "section": "Tuning SVM",
    "text": "Tuning SVM\n\nsvm_tune <-  expand.grid(\n    C =c(1 ,5 ,  10, 100, 150),\n    sigma = seq(0, .01, length.out = 5))\n    \nsvm_model <- train(\n  label ~.,\n  data = train,\n   metric=\"ROC\",\n  method = \"svmRadial\",\n  trControl = myControl,\n  tuneGrid = svm_tune,\n  verbose = FALSE\n)\n\n\nresample_svm <- thresholder(svm_model, \n                              threshold = seq(.0, 1, by = 0.05), \n                              final = TRUE, \n                              statistics = \"all\")\n\nggplot(resample_svm , aes(x = prob_threshold, y = F1, col = \"F1\")) + \n  geom_point() + \n  geom_point(aes(y = Sensitivity,  col = \"Sensitivity\"))+\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1))\n\n\n\n#mean(pred_svm == ytest)\n\n\npred_svm <-predict(svm_model, newdata = test, type = \"prob\")\n\npred_svm <- ifelse(pred_svm[, \"M\"] > 0.40, \"M\", \"B\")\n\npred_svm <- factor(pred_svm, levels = levels(test$label))\n\nconfusionMatrix(test$label, pred_svm, positive = \"M\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   M   B\n         M  53   2\n         B   2 113\n                                          \n               Accuracy : 0.9765          \n                 95% CI : (0.9409, 0.9936)\n    No Information Rate : 0.6765          \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.9462          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.9636          \n            Specificity : 0.9826          \n         Pos Pred Value : 0.9636          \n         Neg Pred Value : 0.9826          \n             Prevalence : 0.3235          \n         Detection Rate : 0.3118          \n   Detection Prevalence : 0.3235          \n      Balanced Accuracy : 0.9731          \n                                          \n       'Positive' Class : M"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer data.html#xgboost",
    "href": "posts/breast_cancer_prediction/cancer data.html#xgboost",
    "title": "Cancer Data",
    "section": "Xgboost",
    "text": "Xgboost\nXGBoost is a type of an ensemble learner. Ensemble learning is where multiple machine learning algorithms are used at the same time for prediction. A good example will be Random Forests. In random Forest multiple decision trees are used together for prediction. There are two main types of ensemble learners, bagging and boosting. Random forest use the bagging approach. Trees are built from random subsets(rows and columns) of training set and then the final prediction is the weighted sum of all decision trees functions. Boosting methods are similar but in boosting samples are selected sequentially. For instance the first sample is selected and a decision tree is fitted, The model then picks the examples that were hard to learn and using this examples and a few others selected randomly from the training set the second model is fitted, Using the first model and the second model prediction is made, the model is evaluated and hard examples are picked and together with another randomly selected new examples from training set another model is trained. This is the process for boosting algorithms which continues for a specified number of n.\nIn gradient boosting the first model is fitted to the original training set. Let say your fitting a simple regression model for ease of explanation. Then your first model will be \\(latex y = f(x) + \\epsilon\\). When you find that the error is too large one of the things you might try to do is add more features, use another algorithm, tune your algorithm, look for more training data etc. But what if the error is not white noise and it has some relationship with output \\(y\\) . Then we can fit a second model. \\(latex \\epsilon = f_1(x) + \\epsilon_1\\). then this process can continue lets say until n times. Then the final model will be\n\\(latex \\epsilon_n = f_{n}(x) + \\epsilon_{n-1}\\).\nThen the final step is to add this models together with some weighting criteria \\(latex weights = \\alpha 's\\) which gives us the final function used for prediction.\n\\(y =latex \\alpha * f(x) + \\alpha_1 * f_1(x) + \\alpha_2 * f_2(x)...+ \\alpha_n * f_n + \\epsilon\\)\n\n# \"subsample\" is the fraction of the training samples (randomly selected) that will be used to train each tree.\n# \"colsample_by_tree\" is the fraction of features (randomly selected) that will be used to train each tree.\n# \"colsample_bylevel\" is the fraction of features (randomly selected) that will be used in each node to train each tree.\n#eta learning rate\n\n\n\nxgb_ctrl <- trainControl(method = \"cv\",\n                        number = 5,\n                        summaryFunction = twoClassSummary,\n                        classProbs = TRUE,\n                        allowParallel=T,\n                        index = cv_fold,\n                        verboseIter = FALSE,\n                        savePredictions = TRUE,\n                        search = \"grid\")\n\nxgb_grid <- expand.grid(nrounds = c(10, 50, 100),\n                        eta = seq(0.06, .2, length.out = 3),\n                        max_depth = c(50, 80),\n                        gamma = c(0,.01, 0.1),\n                        colsample_bytree = c(0.6, 0.7,0.8),\n                        min_child_weight = 1,\n                        subsample =  .7\n                        \n    )\n\n    \nxgb_model <-train(label~.,\n                 data=train,\n                 method=\"xgbTree\",\n                 trControl= xgb_ctrl,\n                 tuneGrid=xgb_grid,\n                 verbose=T,\n                 metric=\"ROC\",\n                 nthread =3\n                     \n    )\n\n[10:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:29] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:30] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:31] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:32] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:26:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\n\nIncreasing cut of increases the precision. A greater fraction of those who will be predicted that they have cancer will turn out that they have, but the algorithm is likely to have lower recall. If we want to avoid too many cases of people cancer being predicted that they do not have cancer. It will be very bad to tell someone that they do not have cancer but they have. If we lower the probability let say to 0.3 then we want to make sure that even if there is a 30% chance you have cancer then you should be flagged.\n\nresample_xgb <- thresholder(xgb_model, \n                              threshold = seq(.0, 1, by = 0.01), \n                              final = TRUE, \n                              statistics = \"all\")\n\nggplot(resample_xgb , aes(x = prob_threshold, y = F1, col = \"F1\")) + \n  geom_point() + \n  geom_point(aes(y = Sensitivity, col = \"Sensitivity\"))+\n  scale_x_continuous(breaks = seq(0, 1, by =.1))\n\n\n\n\n\npred_xgb <-predict(xgb_model, newdata = test, type = \"prob\")\npred_xgb1 <- ifelse(pred_xgb[, \"M\"] > 0.4, \"M\", \"B\")\npred_xgb1 <- factor(pred_xgb1, levels = levels(test$label))\n\nconfusionMatrix(pred_xgb1,test$label,  positive = \"M\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   M   B\n         M  52   3\n         B   3 112\n                                          \n               Accuracy : 0.9647          \n                 95% CI : (0.9248, 0.9869)\n    No Information Rate : 0.6765          \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.9194          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.9455          \n            Specificity : 0.9739          \n         Pos Pred Value : 0.9455          \n         Neg Pred Value : 0.9739          \n             Prevalence : 0.3235          \n         Detection Rate : 0.3059          \n   Detection Prevalence : 0.3235          \n      Balanced Accuracy : 0.9597          \n                                          \n       'Positive' Class : M"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer data.html#learning-curves",
    "href": "posts/breast_cancer_prediction/cancer data.html#learning-curves",
    "title": "Cancer Data",
    "section": "Learning Curves",
    "text": "Learning Curves\n\nsets <- seq(from = 50, to = nrow(train), by = 50)\nsets[length(sets)] <-nrow(train) \ntrain.err <- c()\ntest.err<- c()\ntune_grid <- expand.grid( nrounds = 50, max_depth = 50, eta = 0.06, gamma = 0.01, \n                         colsample_bytree = 0.6, min_child_weight = 1, subsample = 0.7)\nfor (i in 1:length(sets)) {\n    \n    traini = train[1:sets[i],]\n    fit_svm <- train(label ~., data = traini, metric=\"Accuracy\", method = \"svmRadial\",\n                 trControl = trainControl(method = \"none\", summaryFunction = twoClassSummary,\n                                          classProbs = TRUE),\n                 tuneGrid = expand_grid( sigma = 0.0075, C = 5),\n                 )\n    \n    # fit_svm <-train(label~.,\n    #              data=traini,\n    #              method=\"xgbTree\",\n    #              trControl= xgb_ctrl,\n    #              tuneGrid= tune_grid ,\n    #              verbose=T,\n    #              metric=\"ROC\",\n    #              nthread =3\n    #                  \n    # )\n    pred_train = predict(fit_svm, newdata = traini, type = \"prob\")\n    pred_train = ifelse(pred_train[[\"M\"]] > 0.4, \"M\", \"B\")\n    train.err[i] =1 -  mean(pred_train == traini$label)\n    pred_test = predict(fit_svm, newdata = test, type = 'prob')\n    pred_test = ifelse(pred_test[, \"M\"] > 0.4, \"M\", \"B\")\n    test.err[i] = 1 - mean(test$label == pred_test)\n    \n    cat(i,\" \")\n    \n}\n\n1  2  3  4  5  6  7  \n\ntrain.err\n\n[1] 0.00000000 0.01000000 0.03333333 0.02000000 0.02800000 0.01666667 0.02512563\n\nmatplot(sets, cbind(test.err, train.err), pch = 19, col = c(\"red\", \"blue\"),\n        type = \"b\", ylab = \"Error\", xlab = \"Train sample size\", main = \"SVM Learning Curves\")\nlegend(\"topright\", legend = c(\"Test\", \"Train\"), pch = 19, col = c(\"red\", \"blue\"))"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer data.html#error-analysis",
    "href": "posts/breast_cancer_prediction/cancer data.html#error-analysis",
    "title": "Cancer Data",
    "section": "Error Analysis",
    "text": "Error Analysis\nLook at the examples that the algorithm misclassified to see if there is a trend. Generally you are trying to find out the weak points of your algorithm. Checking why your algorithm is making those errors. For instance, from the boxplots below the malignant tumors that were misclassified had lower radius mean compared to mislassified benign tumors. This contrary to what we saw in the first boxplots graph.\n\ndf <- data.frame(cancer[-train_sample,], pred_svm) %>%\n    setDT()\n\n\ntest_mis_svm <- df[(diagnosis == \"M\" & pred_svm == 0) |( diagnosis == \"B\" & pred_svm == \"M\")]\n\n\n# test_mis_svm_m <- melt(test_mis_svm, \n#                 id.vars = c(\"diagnosis\", \"pred_svm\"))\n# \n# ggplot(test_mis_svm_m , aes(x = pred_svm, y = value))+\n#     geom_boxplot() + facet_wrap(~variable, scales = \"free_y\")"
  },
  {
    "objectID": "posts/breast_cancer_prediction/feature_selection.html",
    "href": "posts/breast_cancer_prediction/feature_selection.html",
    "title": "Feature Selection Comparison",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(knitr)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(ggthemes)\n\ncancer <- fread(\"data.csv\")\n\ncancer[, V33 := NULL]\ncancer[, diagnosis := factor(diagnosis)]\nnms <- names(cancer)\nnms <- gsub(\" \", \"_\", nms)\nnames(cancer) <- nms\nstr(cancer)\ncancer[, id := NULL]"
  },
  {
    "objectID": "posts/breast_cancer_prediction/feature_selection.html#test-train",
    "href": "posts/breast_cancer_prediction/feature_selection.html#test-train",
    "title": "Feature Selection Comparison",
    "section": "Test train",
    "text": "Test train\n\nset.seed(100)\ntrain_sample <- sample(1:nrow(cancer), round(0.7*nrow(cancer)))\ntrain_set <- cancer[train_sample,]\ntest_set <- cancer[-train_sample,]"
  },
  {
    "objectID": "posts/breast_cancer_prediction/feature_selection.html#fit-model",
    "href": "posts/breast_cancer_prediction/feature_selection.html#fit-model",
    "title": "Feature Selection Comparison",
    "section": "Fit model",
    "text": "Fit model\n\nlibrary(broom)\nglm_mod <- glm(diagnosis ~ .,\n               data = train_set, \n               family = binomial())\n\n\ntidy(glm_mod) %>% kable"
  },
  {
    "objectID": "posts/breast_cancer_prediction/feature_selection.html#forward",
    "href": "posts/breast_cancer_prediction/feature_selection.html#forward",
    "title": "Feature Selection Comparison",
    "section": "Forward",
    "text": "Forward\n\nforward_select <- step(glm_mod, direction = \"forward\")"
  },
  {
    "objectID": "posts/breast_cancer_prediction/feature_selection.html#backward",
    "href": "posts/breast_cancer_prediction/feature_selection.html#backward",
    "title": "Feature Selection Comparison",
    "section": "Backward",
    "text": "Backward\n\nback_select <- step(glm_mod, direction = \"backward\")"
  },
  {
    "objectID": "posts/breast_cancer_prediction/feature_selection.html#using-entropy-based-feature-selection-algorithms",
    "href": "posts/breast_cancer_prediction/feature_selection.html#using-entropy-based-feature-selection-algorithms",
    "title": "Feature Selection Comparison",
    "section": "Using Entropy-Based Feature Selection Algorithms",
    "text": "Using Entropy-Based Feature Selection Algorithms\n\nlibrary(FSelectorRcpp)\nx <- information_gain(diagnosis ~ ., train_set)\nx %>% arrange(desc(importance)) %>%\n  kable()"
  },
  {
    "objectID": "posts/breast_cancer_prediction/feature_selection.html#recursive-feature-elimination-rfe",
    "href": "posts/breast_cancer_prediction/feature_selection.html#recursive-feature-elimination-rfe",
    "title": "Feature Selection Comparison",
    "section": "Recursive Feature Elimination (RFE)",
    "text": "Recursive Feature Elimination (RFE)\n\nctrl <- rfeControl(functions = rfFuncs,\n                   method = \"repeatedcv\",\n                   repeats = 5,\n                   verbose = FALSE)\n\nlmProfile <- rfe(diagnosis ~ ., \n                 data = train_set,\n                 rfeControl = ctrl)\n\nlmProfile\n\nlmProfile$optVariables\n\n\nvar"
  },
  {
    "objectID": "posts/breast_cancer_prediction/feature_selection.html#model",
    "href": "posts/breast_cancer_prediction/feature_selection.html#model",
    "title": "Feature Selection Comparison",
    "section": "Model",
    "text": "Model\n\ncv_fold <- createFolds(train_set$diagnosis, k = 5)\n\ntrain_ctrl <- trainControl(method = \"cv\",\n                        number = 5,\n                        summaryFunction = twoClassSummary,\n                        classProbs = TRUE,\n                        allowParallel=T,\n                        index = cv_fold,\n                        verboseIter = FALSE,\n                        savePredictions = TRUE,\n                        search = \"grid\")\nglm_grid <- expand.grid(\n                       alpha = 0:1,\n                       lambda = seq(0.0001, 1, length = 10)\n                      )\n\n\nfull_model <- train(\n    diagnosis~.,\n    data = train_set,\n    method = \"glmnet\",\n    metric = \"ROC\",\n    trControl = train_ctrl,\n    tuneGrid = glm_grid\n)\n\nfull_model"
  },
  {
    "objectID": "posts/breast_cancer_prediction/feature_selection.html#forward-model",
    "href": "posts/breast_cancer_prediction/feature_selection.html#forward-model",
    "title": "Feature Selection Comparison",
    "section": "Forward model",
    "text": "Forward model\n\nforward_model <- train(\n    forward_select$formula,\n    data = train_set,\n    method = \"glmnet\",\n    metric = \"ROC\",\n    trControl = train_ctrl,\n    tuneGrid = glm_grid\n)\n\nforward_model"
  },
  {
    "objectID": "posts/breast_cancer_prediction/feature_selection.html#fit-model-with-variables-selected-from-backward-selection",
    "href": "posts/breast_cancer_prediction/feature_selection.html#fit-model-with-variables-selected-from-backward-selection",
    "title": "Feature Selection Comparison",
    "section": "Fit model with variables selected from backward selection",
    "text": "Fit model with variables selected from backward selection\n\nback_model <- train(\n    back_select$formula,\n    data = train_set,\n    method = \"glmnet\",\n    metric = \"ROC\",\n    trControl = train_ctrl,\n    tuneGrid = glm_grid\n)\n\nback_model"
  },
  {
    "objectID": "posts/breast_cancer_prediction/feature_selection.html#fit-model-with-variables-selected-from-backward-selection-1",
    "href": "posts/breast_cancer_prediction/feature_selection.html#fit-model-with-variables-selected-from-backward-selection-1",
    "title": "Feature Selection Comparison",
    "section": "Fit model with variables selected from backward selection",
    "text": "Fit model with variables selected from backward selection\n\nback_model <- train(\n    back_select$formula,\n    data = train_set,\n    method = \"glmnet\",\n    metric = \"ROC\",\n    trControl = train_ctrl,\n    tuneGrid = glm_grid\n)\n\nback_model"
  },
  {
    "objectID": "posts/breast_cancer_prediction/feature_selection.html#fit-model-with-variables-selected-from-entropy",
    "href": "posts/breast_cancer_prediction/feature_selection.html#fit-model-with-variables-selected-from-entropy",
    "title": "Feature Selection Comparison",
    "section": "Fit model with variables selected from entropy",
    "text": "Fit model with variables selected from entropy\n\nsetDT(x)\n#selector predictors with importance of more than 0.05\npredictors <- x[importance > 0.05, attributes]\n\nentropy_predctors <- train_set[, ..predictors]\nentropy_y <- train_set$diagnosis\nentropy_model <- train(\n    entropy_predctors,\n    entropy_y,\n    method = \"glm\",\n    metric = \"ROC\",\n    trControl = train_ctrl\n)\n\nentropy_model"
  },
  {
    "objectID": "posts/breast_cancer_prediction/feature_selection.html#fit-model-with-variables-selected-recursive-feature-elimination",
    "href": "posts/breast_cancer_prediction/feature_selection.html#fit-model-with-variables-selected-recursive-feature-elimination",
    "title": "Feature Selection Comparison",
    "section": "Fit model with variables selected Recursive Feature Elimination",
    "text": "Fit model with variables selected Recursive Feature Elimination\n\nrecu_pred <- lmProfile$optVariables\nrecursive_predctors <- train_set[, ..recu_pred]\nrecursive_y <- train_set$diagnosis\nrecu_model <- train(\n    recursive_predctors,\n    recursive_y,\n    method = \"glm\",\n    metric = \"ROC\",\n    trControl = train_ctrl\n)\n\nrecu_model"
  },
  {
    "objectID": "posts/breast_cancer_prediction/feature_selection.html#full-model-test-accuracy",
    "href": "posts/breast_cancer_prediction/feature_selection.html#full-model-test-accuracy",
    "title": "Feature Selection Comparison",
    "section": "Full model test accuracy",
    "text": "Full model test accuracy\n\nfor_glm <- predict(full_model, test_set, type = \"prob\")\n\n\nfor_glm1 <- ifelse(for_glm[, \"M\"] > 0.5, \"M\", \"B\")\nfor_glm1 <- factor(for_glm1, levels = levels(test_set$diagnosis))\n\n\n\nconfusionMatrix(for_glm1, test_set$diagnosis,positive = \"M\")"
  },
  {
    "objectID": "posts/breast_cancer_prediction/feature_selection.html#forward-test-accuracy",
    "href": "posts/breast_cancer_prediction/feature_selection.html#forward-test-accuracy",
    "title": "Feature Selection Comparison",
    "section": "Forward test accuracy",
    "text": "Forward test accuracy\n\nfor_glm <- predict(forward_model, test_set, type = \"prob\")\n\n\nfor_glm1 <- ifelse(for_glm[, \"M\"] > 0.5, \"M\", \"B\")\nfor_glm1 <- factor(for_glm1, levels = levels(test_set$diagnosis))\n\n\n\nconfusionMatrix(for_glm1, test_set$diagnosis,positive = \"M\")"
  },
  {
    "objectID": "posts/breast_cancer_prediction/feature_selection.html#backward-test-accuracy",
    "href": "posts/breast_cancer_prediction/feature_selection.html#backward-test-accuracy",
    "title": "Feature Selection Comparison",
    "section": "Backward test accuracy",
    "text": "Backward test accuracy\n\nfor_glm <- predict(back_model, test_set, type = \"prob\")\n\n\nfor_glm1 <- ifelse(for_glm[, \"M\"] > 0.5, \"M\", \"B\")\nfor_glm1 <- factor(for_glm1, levels = levels(test_set$diagnosis))\n\n\n\nconfusionMatrix(for_glm1, test_set$diagnosis,positive = \"M\")"
  },
  {
    "objectID": "posts/breast_cancer_prediction/feature_selection.html#entropy-method-test-accuracy",
    "href": "posts/breast_cancer_prediction/feature_selection.html#entropy-method-test-accuracy",
    "title": "Feature Selection Comparison",
    "section": "entropy method test accuracy",
    "text": "entropy method test accuracy\n\nfor_glm <- predict(entropy_model, test_set, type = \"prob\")\n\n\nfor_glm1 <- ifelse(for_glm[, \"M\"] > 0.5, \"M\", \"B\")\nfor_glm1 <- factor(for_glm1, levels = levels(test_set$diagnosis))\n\n\n\nconfusionMatrix(for_glm1, test_set$diagnosis,positive = \"M\")"
  },
  {
    "objectID": "posts/breast_cancer_prediction/feature_selection.html#recursive-feature-elimination-method-test-accuracy",
    "href": "posts/breast_cancer_prediction/feature_selection.html#recursive-feature-elimination-method-test-accuracy",
    "title": "Feature Selection Comparison",
    "section": "Recursive Feature Elimination method test accuracy",
    "text": "Recursive Feature Elimination method test accuracy\n\nfor_glm <- predict(recu_model, test_set, type = \"prob\")\n\n\nfor_glm1 <- ifelse(for_glm[, \"M\"] > 0.5, \"M\", \"B\")\nfor_glm1 <- factor(for_glm1, levels = levels(test_set$diagnosis))\n\n\n\nconfusionMatrix(for_glm1, test_set$diagnosis,positive = \"M\")"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html",
    "href": "posts/breast_cancer_prediction/cancer_data.html",
    "title": "Cancer Data",
    "section": "",
    "text": "In this tutorial I’m going to predict whether a breast cancer tumor is benign or malignant. Using Wiscosin breast cancer data set available on Kaggle. The 30 predictors are divided into three parts first is Mean ( variables 3-13), Standard Error(13-23) and Worst(23-32) and each contain 10 parameter (radius, texture,area, perimeter, smoothness,compactness,concavity,concave points,symmetry and fractal dimension of the tumor). When predicting cancer breast tumor types there two types of cost;\n\nThe cost of telling someone who has malignant tumor that they have benign these are the false negatives in this case someone might not seek medical help which is can cause death.\nTelling someone that they have malignant type of tumor but they don’t which is usually false positives. In this case you subject someone to unnecessary stress\n\nSo it’s highly desirable that our model has good accuracy $ f_1 score$ and high recall.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(xgboost)\nlibrary(knitr)\nlibrary(broom)\nlibrary(caret)\nlibrary(e1071)\nlibrary(kableExtra)\nlibrary(ggthemes)\nlibrary(DT)\nlibrary(glmnet)\n\noptions(scipen = 1, digits = 4)\ncancer <- fread(\"data.csv\")\n\ncancer[, V33 := NULL]\n\n\nhead(cancer)  %>%\n  datatable(options = list(scrollX = TRUE))"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#percentage-of-women-with-malignant-tumor",
    "href": "posts/breast_cancer_prediction/cancer_data.html#percentage-of-women-with-malignant-tumor",
    "title": "Cancer Data",
    "section": "Percentage of women with malignant tumor",
    "text": "Percentage of women with malignant tumor\nThe percentage of women with malignant tumor is 37.26%(212 out 569) while the rest 62.74%(357) had benign tumors.\n\ncancer[, .(freq = .N),\n       by = diagnosis] %>% \n    .[, perc := round(100 * freq/sum(freq), 2)] %>%\n  \nggplot(aes(x=diagnosis, y=perc, fill = diagnosis)) + \n    geom_bar(stat = \"identity\", width  = 0.5)+ theme_hc() +\n    geom_text(aes(x=diagnosis, y=perc, label = paste(perc, \"%\")),\n              position =  position_dodge(width = 0.5),\n              vjust = 0.05, hjust = 0.5, size = 5)+\n    scale_fill_hc(name = \"\")+\n    labs(x = \"Cancer Type\",\n         y = \"Percentage\",\n         title = \"Percentage of women with benign or malignant breast bancer\")+\n    theme(legend.position = \"none\",\n          axis.title = element_text(size =12))"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#boxplots",
    "href": "posts/breast_cancer_prediction/cancer_data.html#boxplots",
    "title": "Cancer Data",
    "section": "Boxplots",
    "text": "Boxplots\nFrom the boxplots we can identify variables where we expect there is a significance difference between the two groups of cancer tumors. When using a boxplot if two distributions do not averlap or more than 75% of two boxplot do not overlap then we expect that there is a significance difference in the mean/median between the two groups. Some of the variables where the distribution of two cancer tumors are significantly different are radius_mean, texture_mean etc. The visible differences between malignant tumors and benign tumors can be seen in means of all cells and worst means where worst means is the average of all the worst cells. The distribution of malignant tumors have higher scores than the benign tumors in this cases.\n\ncancerm <- melt(cancer[, -1, with = F], id.vars = \"diagnosis\")\n\nggplot(cancerm, aes(x = diagnosis, y = value))+\n    geom_boxplot() + facet_wrap(~variable, scales = \"free_y\")"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#features-scaling",
    "href": "posts/breast_cancer_prediction/cancer_data.html#features-scaling",
    "title": "Cancer Data",
    "section": "Features Scaling",
    "text": "Features Scaling\nWe find that some variables are highly correlated. We can use principle component analysis for dimension reduction. Since variables are correlated it’s evident that we can use a smaller set of features to build our models.\n\ncancer[, id := NULL]\npredictors <- names(cancer)[3:31]\ncancer[, (predictors) := lapply(.SD, function(x) scale(x)), .SDcols = predictors ]\ncancer[, diagnosis := as.factor(diagnosis)]"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#correlation-matrix",
    "href": "posts/breast_cancer_prediction/cancer_data.html#correlation-matrix",
    "title": "Cancer Data",
    "section": "Correlation matrix",
    "text": "Correlation matrix\n\ncor(cancer[, -(1:2), with = F]) %>%\n  datatable(options = list(scrollX = TRUE), style = \"bootstrap4\")"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#principle-component-analysis",
    "href": "posts/breast_cancer_prediction/cancer_data.html#principle-component-analysis",
    "title": "Cancer Data",
    "section": "Principle Component Analysis",
    "text": "Principle Component Analysis\nUsing the elbow rule we can use the first 5 principle components. Using 15 principle components we will have achieved al most 100% of the variance from the original data set.\n\npca <- prcomp(cancer[, predictors, with = F], scale. = F)"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#variance-explained",
    "href": "posts/breast_cancer_prediction/cancer_data.html#variance-explained",
    "title": "Cancer Data",
    "section": "Variance Explained",
    "text": "Variance Explained\nSince PCA forms new characteristics the variance explained plot shows the amount of variation of the original features captured by each principle component. The new features are simply linear combinations of the old features.\n\nstdpca <- pca$sdev\n\nvarpca <- stdpca^2\n\nprop_var <- varpca/sum(varpca)\nprop_var * 100\n\n [1] 43.706363 18.472237  9.716239  6.816736  5.676223  4.161723  2.292352\n [8]  1.643434  1.363238  1.191515  1.011032  0.897368  0.832105  0.539193\n[15]  0.323823  0.269517  0.198317  0.178851  0.153573  0.107095  0.102579\n[22]  0.093821  0.082603  0.058725  0.053331  0.027514  0.022985  0.005110\n[29]  0.002394\n\nsum(prop_var[1:15])\n\n[1] 0.9864"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#scree-plot",
    "href": "posts/breast_cancer_prediction/cancer_data.html#scree-plot",
    "title": "Cancer Data",
    "section": "Scree plot",
    "text": "Scree plot\nScree plot shows the variance explained by each principle component which reduces as the number of principle components increase.\n\nplot(prop_var, xlab = \"Principal Component\",\n     ylab = \"Proportion of Variance Explained\",\n     type = \"b\", xlim = c(0, 30))"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#cumulative-variance-explained",
    "href": "posts/breast_cancer_prediction/cancer_data.html#cumulative-variance-explained",
    "title": "Cancer Data",
    "section": "Cumulative Variance Explained",
    "text": "Cumulative Variance Explained\nThe cumulative of variance plot helps to choose the number of features based on the amount of variation from original data set you want captured. In this case, I wanted to use number of principle components that capture almost 100% of the variation. After trying with different number of principle components I found out that the accuracy of the models did not increase after the 15th principle components.\n\ncum_var <- cumsum(prop_var)\nplot(cum_var, xlab = \"Principal Component\",\n     ylab = \"Cumulative Proportion of Variance Explained\",\n     type = \"b\", xlim = c(0, 30))"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#construct-new-data-set",
    "href": "posts/breast_cancer_prediction/cancer_data.html#construct-new-data-set",
    "title": "Cancer Data",
    "section": "Construct new data set",
    "text": "Construct new data set\nWe use the first 15 principle components as our new predictors, then we randomly split data into training and test set in 7:3 ratio.\n\nset.seed(100)\ntrain_sample <- sample(1:nrow(cancer), round(0.7*nrow(cancer)))\n\npcadat <- data.table( label = cancer$diagnosis, pca$x[,1:15]) \npcadat[, label := factor(label, levels = c(\"M\", \"B\"))]\ntrain <- pcadat[train_sample,]\ntest <- pcadat[-train_sample,]"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#logistic-regression",
    "href": "posts/breast_cancer_prediction/cancer_data.html#logistic-regression",
    "title": "Cancer Data",
    "section": "Logistic regression",
    "text": "Logistic regression\nThis is one of generalized linear models which deals with binary data. There is a generalization of this model which is called multinomial regression where you can fit multi class data. The equation for logistic regression model is:\n\\[log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1*X_1 + ... \\beta_n * X_n\\] and using mle the cost function can be derived as: \\[J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m} y^i log(h_\\theta(x^i)) + (1-y^i) log(1 - h_\\theta(x^i))\\] Given that \\[y = 0\\] \\[y = 1\\] . Finding \\[\\beta\\] s we minimizing the cost function.\n\nfit_glm <- glm(label ~., data = train, family = binomial)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#regularization-in-logistic-regression",
    "href": "posts/breast_cancer_prediction/cancer_data.html#regularization-in-logistic-regression",
    "title": "Cancer Data",
    "section": "Regularization in logistic regression",
    "text": "Regularization in logistic regression\nThe warning “glm.fit: fitted probabilities numerically 0 or 1 occurred” shows that there is a perfect separation/over fitting. In this case you can load glmnet library and fit a regularized logistic regression. These can be achieved by adding a regularization term to the cost function.The L1 regularization(Lasso) adds a penalty equal to the sum of the absolute values of the coefficients.\n\\[J(\\theta) = -\\frac{1}{m}\\sum_{i=0}^{m} y^i log(h_\\theta(x^i)) + (1-y^i) log(1 - h_\\theta(x^i)) + \\frac {\\lambda}{2m}\\sum_{j=1}^{n} |\\theta^i|\\]\n\ntrainx <- train[,-1]\n\ny_train <- factor(train$label, levels = c(\"B\", \"M\"), labels = 0:1)\n#y <- as.numeric(as.character(y))\n\ny_test <- factor(test$label, levels = c(\"B\", \"M\"), labels = 0:1) %>% as.character() %>% as.numeric()\n#ytest <- as.numeric(as.character(ytest))\n\ntestx <- data.matrix(test[, -1]) \n\nTo find the optimal values \\(\\lambda\\) we use cross validation. We choose \\(\\lambda\\) which gives the highest cross validation accuracy.\n\ncv_fold <- createFolds(train$label, k = 10)\n\nmyControl <- trainControl(\n  method = \"cv\", \n  number = 10,\n  summaryFunction = twoClassSummary,\n  savePredictions = \"all\",\n  classProbs = TRUE,\n  verboseIter = FALSE,\n  index = cv_fold,\n  allowParallel = TRUE\n  \n)\n\ntuneGrid <-  expand.grid(\n    alpha = 0:1,\n    lambda = seq(0.001, 1, length.out = 10))\n    \nglmnet_model <- train(\n  label ~.,\n  data = train,\n  method = \"glmnet\",\n  metric = \"ROC\",\n  trControl = myControl,\n  tuneGrid = tuneGrid\n)\n\ns\n\nplot(glmnet_model) \n\n\n\n#lamda_min <- cv_glm$lambda.min\n\n\nresample_glmnet <- thresholder(glmnet_model, \n                              threshold = seq(.2, 1, by = 0.05), \n                              final = TRUE, \n                              statistics = \"all\")\n\nggplot(resample_glmnet , aes(x = prob_threshold, y = F1)) + \n  geom_point() + \n  geom_point(aes(y = Sensitivity), col = \"blue\")\n\n\n\n\n\nlibrary(caTools)\n\npred_glm <- predict(glmnet_model, test, type = \"prob\")\n\ncolAUC(pred_glm , test$label, plotROC = TRUE)\n\n\n\n\n             M      B\nM vs. B 0.9683 0.9683\n\npred_glm1 <- ifelse(pred_glm[, \"M\"] > 0.4, \"M\", \"B\")\n#pred_glm1 <- predict(glmnet_model, test, type = \"raw\")\n\n\npred_glm1 <- factor(pred_glm1, levels = levels(test$label))\n\n\nconfusionMatrix(pred_glm1, test$label,positive = \"M\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   M   B\n         M  53   7\n         B   7 104\n                                        \n               Accuracy : 0.918         \n                 95% CI : (0.866, 0.955)\n    No Information Rate : 0.649         \n    P-Value [Acc > NIR] : <2e-16        \n                                        \n                  Kappa : 0.82          \n                                        \n Mcnemar's Test P-Value : 1             \n                                        \n            Sensitivity : 0.883         \n            Specificity : 0.937         \n         Pos Pred Value : 0.883         \n         Neg Pred Value : 0.937         \n             Prevalence : 0.351         \n         Detection Rate : 0.310         \n   Detection Prevalence : 0.351         \n      Balanced Accuracy : 0.910         \n                                        \n       'Positive' Class : M"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#svm",
    "href": "posts/breast_cancer_prediction/cancer_data.html#svm",
    "title": "Cancer Data",
    "section": "SVM",
    "text": "SVM\nSupport Vector Machines is a type of supervised learning algorithm that is used for classification and regression. Most of the times however, it’s used for classification.\nTo understand how SVM works consider the following example of linearly separable data. It’s clear that we can separate the two classes using a straight line(decision boundary). Which is normally referred to a separating hyperplane.\n\n\n\n\n\nThe question is, since there exists many lines that can separate the red and the black classes which is the best one. This introduces us to the maximal margin classification, In short SVM finds the hyperplane/line that gives the biggest margin/gap between the two classes. In this case SVM will choose the solid line as the hyperplane while the margins are the dotted lines. The circled points that lie directly on the margin, or on the wrong side of the margin for their class, are known as support vectors. This shows that SVM uses this points to come up with a the decision boundary, the other points are not used. In this case since it’s a two dimensional space the equation of the separating line will be \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2\\]. Then when equations evaluates to more than 0 then 1 is predicted \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2 > 0, y = 1\\] and when it evaluates to less than zero then predicted class is -1 \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2 < 0, \\; y = -1\\] This becomes maximisation problem \\[width \\; of \\;the \\; margin = M \\] \\[\\sum_{j=1}^{n}\\beta_j = 1\\]\n\\[y_i(\\beta_0 + \\beta_1X_1 + \\beta_2X_2) >= M\\]\n\n\n\n\n\nThis is a best case scenario but in most cases the classes are noisy. Consider the plot below no matter which line you choose some points are bound to be on the wrong side of the desicion boundary. Thus maximal margin classification would not work.\n\n\n\n\n\nSVM then introduces what is called a soft margin. In naive explanation you can think of this as a margin that allows some points to be on the wrong side. By introducing an error term we allow for some slack. Thus in a two case the maximisation becomes \\[y_i(\\beta_0 + \\beta_1X_1 + \\beta_2X_2) >= M(1- \\epsilon)\\]\n\\[\\sum_{i=0}^{n} \\epsilon_i <= C\\] C is a tuning parameter which determines the width of the margin while \\[\\epsilon_i  \\;'s\\] are slack variables. that allow individual observations to fall on the wrong side of the margin. In some cases the decision boundary maybe non linear. In case your are dealing with logistic regression you will be forced to introduce polynomial terms which might result in a very large feature space. SVM then introduces what are called kernels"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#tuning-svm",
    "href": "posts/breast_cancer_prediction/cancer_data.html#tuning-svm",
    "title": "Cancer Data",
    "section": "Tuning SVM",
    "text": "Tuning SVM\n\nsvm_tune <-  expand.grid(\n    C =c(1 ,5 ,  10, 100, 150),\n    sigma = seq(0, .01, length.out = 5))\n    \nsvm_model <- train(\n  label ~.,\n  data = train,\n   metric=\"ROC\",\n  method = \"svmRadial\",\n  trControl = myControl,\n  tuneGrid = svm_tune,\n  verbose = FALSE\n)\n\n\nresample_svm <- thresholder(svm_model, \n                              threshold = seq(.0, 1, by = 0.05), \n                              final = TRUE, \n                              statistics = \"all\")\n\nggplot(resample_svm , aes(x = prob_threshold, y = F1, col = \"F1\")) + \n  geom_point() + \n  geom_point(aes(y = Sensitivity,  col = \"Sensitivity\"))+\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1))\n\n\n\n#mean(pred_svm == ytest)\n\n\npred_svm <-predict(svm_model, newdata = test, type = \"prob\")\n\npred_svm <- ifelse(pred_svm[, \"M\"] > 0.40, \"M\", \"B\")\n\npred_svm <- factor(pred_svm, levels = levels(test$label))\n\nconfusionMatrix(test$label, pred_svm, positive = \"M\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   M   B\n         M  58   2\n         B   2 109\n                                        \n               Accuracy : 0.977         \n                 95% CI : (0.941, 0.994)\n    No Information Rate : 0.649         \n    P-Value [Acc > NIR] : <2e-16        \n                                        \n                  Kappa : 0.949         \n                                        \n Mcnemar's Test P-Value : 1             \n                                        \n            Sensitivity : 0.967         \n            Specificity : 0.982         \n         Pos Pred Value : 0.967         \n         Neg Pred Value : 0.982         \n             Prevalence : 0.351         \n         Detection Rate : 0.339         \n   Detection Prevalence : 0.351         \n      Balanced Accuracy : 0.974         \n                                        \n       'Positive' Class : M"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#xgboost",
    "href": "posts/breast_cancer_prediction/cancer_data.html#xgboost",
    "title": "Cancer Data",
    "section": "Xgboost",
    "text": "Xgboost\nXGBoost is a type of an ensemble learner. Ensemble learning is where multiple machine learning algorithms are used at the same time for prediction. A good example will be Random Forests. In random Forest multiple decision trees are used together for prediction. There are two main types of ensemble learners, bagging and boosting. Random forest use the bagging approach. Trees are built from random subsets(rows and columns) of training set and then the final prediction is the weighted sum of all decision trees functions. Boosting methods are similar but in boosting samples are selected sequentially. For instance the first sample is selected and a decision tree is fitted, The model then picks the examples that were hard to learn and using this examples and a few others selected randomly from the training set the second model is fitted, Using the first model and the second model prediction is made, the model is evaluated and hard examples are picked and together with another randomly selected new examples from training set another model is trained. This is the process for boosting algorithms which continues for a specified number of n.\nIn gradient boosting the first model is fitted to the original training set. Let say your fitting a simple regression model for ease of explanation. Then your first model will be $ y = f(x) + $. When you find that the error is too large one of the things you might try to do is add more features, use another algorithm, tune your algorithm, look for more training data etc. But what if the error is not white noise and it has some relationship with output \\(y\\) . Then we can fit a second model. $ = f_1(x) + _1$. then this process can continue lets say until n times. Then the final model will be\n$ n = f*{n}(x) + _{n-1}$.\nThen the final step is to add this models together with some weighting criteria $ weights = ’s$ which gives us the final function used for prediction.\n\\(y = \\alpha * f(x) + \\alpha_1 * f_1(x) + \\alpha_2 * f_2(x)...+ \\alpha_n * f_n + \\epsilon\\)\n\n# \"subsample\" is the fraction of the training samples (randomly selected) that will be used to train each tree.\n# \"colsample_by_tree\" is the fraction of features (randomly selected) that will be used to train each tree.\n# \"colsample_bylevel\" is the fraction of features (randomly selected) that will be used in each node to train each tree.\n#eta learning rate\n\n\n\nxgb_ctrl <- trainControl(method = \"cv\",\n                        number = 5,\n                        summaryFunction = twoClassSummary,\n                        classProbs = TRUE,\n                        allowParallel=T,\n                        index = cv_fold,\n                        verboseIter = FALSE,\n                        savePredictions = TRUE,\n                        search = \"grid\")\n\nxgb_grid <- expand.grid(nrounds = c(10, 50, 100),\n                        eta = seq(0.06, .2, length.out = 3),\n                        max_depth = c(50, 80),\n                        gamma = c(0,.01, 0.1),\n                        colsample_bytree = c(0.6, 0.7,0.8),\n                        min_child_weight = 1,\n                        subsample =  .7\n                        \n    )\n\nxgb_model <-train(label~.,\n                 data=train,\n                 method=\"xgbTree\",\n                 trControl= xgb_ctrl,\n                 tuneGrid=xgb_grid,\n                 verbosity=0,\n                 metric=\"ROC\",\n                 nthread =4\n                     \n    )\n\nIncreasing cut of increases the precision. A greater fraction of those who will be predicted that they have cancer will turn out that they have, but the algorithm is likely to have lower recall. If we want to avoid too many cases of people cancer being predicted that they do not have cancer. It will be very bad to tell someone that they do not have cancer but they have. If we lower the probability let say to 0.3 then we want to make sure that even if there is a 30% chance you have cancer then you should be flagged.\n\nresample_xgb <- thresholder(xgb_model, \n                              threshold = seq(.0, 1, by = 0.01), \n                              final = TRUE, \n                              statistics = \"all\")\n\nggplot(resample_xgb , aes(x = prob_threshold, y = F1, col = \"F1\")) + \n  geom_point() + \n  geom_point(aes(y = Sensitivity, col = \"Sensitivity\"))+\n  scale_x_continuous(breaks = seq(0, 1, by =.1))\n\n\n\n\n\npred_xgb <-predict(xgb_model, newdata = test, type = \"prob\")\npred_xgb1 <- ifelse(pred_xgb[, \"M\"] > 0.4, \"M\", \"B\")\npred_xgb1 <- factor(pred_xgb1, levels = levels(test$label))\n\nconfusionMatrix(pred_xgb1,test$label,  positive = \"M\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   M   B\n         M  59   2\n         B   1 109\n                                       \n               Accuracy : 0.982        \n                 95% CI : (0.95, 0.996)\n    No Information Rate : 0.649        \n    P-Value [Acc > NIR] : <2e-16       \n                                       \n                  Kappa : 0.962        \n                                       \n Mcnemar's Test P-Value : 1            \n                                       \n            Sensitivity : 0.983        \n            Specificity : 0.982        \n         Pos Pred Value : 0.967        \n         Neg Pred Value : 0.991        \n             Prevalence : 0.351        \n         Detection Rate : 0.345        \n   Detection Prevalence : 0.357        \n      Balanced Accuracy : 0.983        \n                                       \n       'Positive' Class : M"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#learning-curves",
    "href": "posts/breast_cancer_prediction/cancer_data.html#learning-curves",
    "title": "Cancer Data",
    "section": "Learning Curves",
    "text": "Learning Curves\n\nsets <- seq(from = 50, to = nrow(train), by = 50)\nsets[length(sets)] <-nrow(train) \ntrain.err <- c()\ntest.err<- c()\ntune_grid <- expand.grid( nrounds = 50, max_depth = 50, eta = 0.06, gamma = 0.01, \n                         colsample_bytree = 0.6, min_child_weight = 1, subsample = 0.7)\nfor (i in 1:length(sets)) {\n    \n    traini = train[1:sets[i],]\n    fit_svm <- train(label ~., data = traini, metric=\"Accuracy\", method = \"svmRadial\",\n                 trControl = trainControl(method = \"none\", summaryFunction = twoClassSummary,\n                                          classProbs = TRUE),\n                 tuneGrid = expand_grid( sigma = 0.0075, C = 5),\n                 )\n    \n    # fit_svm <-train(label~.,\n    #              data=traini,\n    #              method=\"xgbTree\",\n    #              trControl= xgb_ctrl,\n    #              tuneGrid= tune_grid ,\n    #              verbose=T,\n    #              metric=\"ROC\",\n    #              nthread =3\n    #                  \n    # )\n    pred_train = predict(fit_svm, newdata = traini, type = \"prob\")\n    pred_train = ifelse(pred_train[[\"M\"]] > 0.4, \"M\", \"B\")\n    train.err[i] =1 -  mean(pred_train == traini$label)\n    pred_test = predict(fit_svm, newdata = test, type = 'prob')\n    pred_test = ifelse(pred_test[, \"M\"] > 0.4, \"M\", \"B\")\n    test.err[i] = 1 - mean(test$label == pred_test)\n    \n    cat(i,\" \")\n    \n}\n\n1  2  3  4  5  6  7  \n\ntrain.err\n\n[1] 0.00000 0.03000 0.03333 0.01500 0.02000 0.02000 0.02261\n\nmatplot(sets, cbind(test.err, train.err), pch = 19, col = c(\"red\", \"blue\"),\n        type = \"b\", ylab = \"Error\", xlab = \"Train sample size\", main = \"SVM Learning Curves\")\nlegend(\"topright\", legend = c(\"Test\", \"Train\"), pch = 19, col = c(\"red\", \"blue\"))"
  },
  {
    "objectID": "posts/breast_cancer_prediction/cancer_data.html#error-analysis",
    "href": "posts/breast_cancer_prediction/cancer_data.html#error-analysis",
    "title": "Cancer Data",
    "section": "Error Analysis",
    "text": "Error Analysis\nLook at the examples that the algorithm misclassified to see if there is a trend. Generally you are trying to find out the weak points of your algorithm. Checking why your algorithm is making those errors. For instance, from the boxplots below the malignant tumors that were misclassified had lower radius mean compared to mislassified benign tumors. This contrary to what we saw in the first boxplots graph.\n\ndf <- data.frame(cancer[-train_sample,], pred_svm) %>%\n    setDT()\n\n\ntest_mis_svm <- df[(diagnosis == \"M\" & pred_svm == 0) |( diagnosis == \"B\" & pred_svm == \"M\")]\n\n\n# test_mis_svm_m <- melt(test_mis_svm, \n#                 id.vars = c(\"diagnosis\", \"pred_svm\"))\n# \n# ggplot(test_mis_svm_m , aes(x = pred_svm, y = value))+\n#     geom_boxplot() + facet_wrap(~variable, scales = \"free_y\")"
  },
  {
    "objectID": "posts/breast_cancer_prediction/shap_vals.html",
    "href": "posts/breast_cancer_prediction/shap_vals.html",
    "title": "Shap Calculation R",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(xgboost)\nlibrary(knitr)\nlibrary(broom)\nlibrary(caret)\nlibrary(ggthemes)\nlibrary(DT)\nlibrary(glmnet)\n\ncancer <- fread(\"data.csv\")\n\ncancer[, V33 := NULL]"
  },
  {
    "objectID": "posts/breast_cancer_prediction/shap_vals.html#head",
    "href": "posts/breast_cancer_prediction/shap_vals.html#head",
    "title": "Shap Calculation R",
    "section": "Head",
    "text": "Head\nA mini example of calculating shap values in R. I used an open source data set from Kaggle. See more about Wiscosin breast cancer data set. I use IML package from R I prefer it since you can calculate from any model. At this time I have tested it using models from caret package but they have examples from mlr package and h20 packages.\n\nhead(cancer)  %>%\n  datatable(options = list(scrollX = TRUE))"
  },
  {
    "objectID": "posts/breast_cancer_prediction/shap_vals.html#scale-predictors",
    "href": "posts/breast_cancer_prediction/shap_vals.html#scale-predictors",
    "title": "Shap Calculation R",
    "section": "Scale predictors",
    "text": "Scale predictors\n\nnms <- names(cancer)\nnew_nms <- gsub(\"\\\\s\", \"_\", nms) %>% tolower() %>% str_trim()\nsetnames(cancer, nms, new_nms)\npat_id <- cancer$id\ncancer[, id := NULL]\npredictors <- names(cancer)[3:31]\nscale01 <- function(x){\n  y = (x - min(x))/(max(x) - min(x))\n  return(y)\n}\ncancer[, (predictors) := lapply(.SD, function(x) scale01(x)), .SDcols = predictors ]\ncancer[, diagnosis := factor(diagnosis, levels = c(\"M\", \"B\"))]\n\nhead(cancer)  %>%\n  datatable(options = list(scrollX = TRUE))"
  },
  {
    "objectID": "posts/breast_cancer_prediction/shap_vals.html#boxplots",
    "href": "posts/breast_cancer_prediction/shap_vals.html#boxplots",
    "title": "Shap Calculation R",
    "section": "Boxplots",
    "text": "Boxplots\n\nI like this especially when you want to see if there is difference between groups when the indipedent var is numeric and outcome categorical.\n\n\ncancerm <- melt(cancer, id.vars = \"diagnosis\")\n\nggplot(cancerm, aes(x = diagnosis, y = value))+\n    geom_boxplot() + facet_wrap(~variable, scales = \"free_y\")"
  },
  {
    "objectID": "posts/breast_cancer_prediction/shap_vals.html#split-test-train",
    "href": "posts/breast_cancer_prediction/shap_vals.html#split-test-train",
    "title": "Shap Calculation R",
    "section": "Split test, train",
    "text": "Split test, train\n\nn_row <- nrow(cancer)\ntrain_size <- (0.7 * n_row) %>% as.integer()\ntrain_ids <- sample(1:n_row, train_size)\ntrain_data <- cancer[train_ids, ]\ntest_data <- cancer\ntest_pat_id <- pat_id[-train_ids]"
  },
  {
    "objectID": "posts/breast_cancer_prediction/shap_vals.html#fit-xgboost-random-forest",
    "href": "posts/breast_cancer_prediction/shap_vals.html#fit-xgboost-random-forest",
    "title": "Shap Calculation R",
    "section": "Fit xgboost, random forest,",
    "text": "Fit xgboost, random forest,\n\ncv_fold <- createFolds(train_data$diagnosis, k = 10)\ntrain_ctrl <- trainControl(method = \"cv\",\n                        number = 10,\n                        summaryFunction = twoClassSummary,\n                        classProbs = TRUE,\n                        allowParallel=T,\n                        index = cv_fold,\n                        verboseIter = FALSE,\n                        savePredictions = TRUE,\n                        search = \"grid\")\n\nxgb_grid <- expand.grid(nrounds = c(10, 50, 100),\n                        eta = seq(0.06, .2, length.out = 3),\n                        max_depth = c( 5, 10),\n                        gamma = c(0,.01, 0.1),\n                        colsample_bytree = c(0.6, 0.7,0.8),\n                        min_child_weight = c(0.7, 0.95),\n                        subsample =  c(0.5, 0.7, 0.9))\n\ntrees_ranger <-  seq(from = 5, to = (ncol(cancer) - 1),\n                     length.out = 5 ) %>% as.integer()\n\nranger_grid <- expand.grid(splitrule = c(\"extratrees\", \"gini\"),\n                        mtry = trees_ranger,\n                        min.node.size = c(0.85, .95))\n\nsvm_grid <- expand.grid(C = c(0.5, 1, 10),\n                        sigma = seq(0.001, 0.1, length.out = 10))"
  },
  {
    "objectID": "posts/breast_cancer_prediction/shap_vals.html#fit-random-forest",
    "href": "posts/breast_cancer_prediction/shap_vals.html#fit-random-forest",
    "title": "Shap Calculation R",
    "section": "Fit Random forest",
    "text": "Fit Random forest\n\nlibrary(caretEnsemble)\nlibrary(foreach)\nlibrary(doParallel) # ignore  it's used for parallel\nlibrary(tictoc) # ignore it's used for timing\ncl <- makeCluster(3)\nregisterDoParallel(cl)\ntic()\nset.seed(100)\n\n\nrf  <- caret::train(\n  diagnosis~.,\n  data= train_data,\n  trControl=train_ctrl,\n  metric = \"ROC\",\n  method = \"ranger\",\n  tuneGrid= ranger_grid)\n\ntoc()\n\nregisterDoSEQ()\n\nrf"
  },
  {
    "objectID": "posts/breast_cancer_prediction/shap_vals.html#calculate-shap-values",
    "href": "posts/breast_cancer_prediction/shap_vals.html#calculate-shap-values",
    "title": "Shap Calculation R",
    "section": "Calculate Shap values",
    "text": "Calculate Shap values\n\nuse the shapper package it uses python shap package so you must have python installed + python shap library\nhttps://github.com/ModelOriented/shapper\ninstall from cran\n\n\nlibrary(shapper)\n# X_pred <- train_data[, .SD, .SDcols = !c(\"diagnosis\")] %>%\n#   as.data.frame() # this because i use data.table\n# \n# tic()\n# p_function <- function(model, data) predict(model, newdata = data, type = \"prob\")\n# shap_values <- individual_variable_effect(rf, data = X_pred, predict_function = p_function,\n#                                      new_observation = X_pred, nsamples = 150)\n# \n# \n# toc()\ntic()\n\ntic()\n\nshap_list <- foreach(i = 1:nrow(X_pred)) %do%{\n    shap <- Shapley$new(predictor,  x.interest = X_pred[i, ], sample.size = 150)\n    shap_import <-shap$results %>% data.table()\n    shap_import <- shap_import[class == \"M\"]\n    shap_import[,id := pat_id[i]]\n    \n}\ntoc()\n\n\ntoc()\n\nshap_values <- rbindlist(shap_list)\nwrite.csv(shap_values, file =\"shap_values3.csv\", row.names = F )"
  },
  {
    "objectID": "posts/breast_cancer_prediction/shap_vals.html#shap-plots",
    "href": "posts/breast_cancer_prediction/shap_vals.html#shap-plots",
    "title": "Shap Calculation R",
    "section": "Shap plots",
    "text": "Shap plots\n\nlibrary(ggforce)\nshap_values <-  fread(\"shap_values3.csv\")\n\nsetnames(shap_values, c(\"_attribution_\", \"_vname_\"), c(\"phi\",\"feature\" ))\nshap_values[, phi2 := abs( phi )]\nshap_imp <- shap_values[, .(Med = median(phi2),\n                            Mean = mean(phi2)), by = feature] %>%\n    setorder(-Med)\nshap_imp <- shap_imp[1:30, ]\n\nshap_values <- shap_values[feature %in%shap_imp$feature]\n\nshap_values[, feature := factor(feature, levels = rev(shap_imp$feature) )]\n\nggplot(shap_values, aes(feature, phi,  color = abs(phi)))+\n  geom_sina()+\n  geom_hline(yintercept = 0) +\n  scale_color_gradient(name = \"\",low=\"#2187E3\", high=\"#F32858\", \n                       breaks=c(0,.2), labels=c(\"Low\",\"High\"),\n                       limits = c(0,.2))+ \n  theme_bw() + \n    theme(axis.line.y = element_blank(), \n          axis.ticks.y = element_blank(), # remove axis line\n          legend.position=\"bottom\") +\n  coord_flip()"
  },
  {
    "objectID": "posts/breast_cancer_prediction/shap_vals.html#fit-models-using-caret-ensembles",
    "href": "posts/breast_cancer_prediction/shap_vals.html#fit-models-using-caret-ensembles",
    "title": "Shap Calculation R",
    "section": "Fit models using caret ensembles",
    "text": "Fit models using caret ensembles\n\n## Ignore; just a simpler if you want to compare models perfomance\n\n# cl <- makeCluster(3)\n# registerDoParallel(cl)\n# tic()\n# set.seed(100)\n# model_list <- caretList(\n#    diagnosis~.,\n#     data= train_data,\n#     trControl=train_ctrl,\n#     metric = \"ROC\",\n#     tuneList = list(caretModelSpec(method=\"xgbTree\",  tuneGrid= xgb_grid),\n#                     caretModelSpec(method = \"svmRadial\", tuneGrid = svm_grid),\n#                     caretModelSpec(method=\"ranger\", tuneGrid= ranger_grid)\n# \n#                    \n#                     \n#                     )\n# )\n# \n# toc()\n# \n# registerDoSEQ()\n# \n# model_list"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "notes/iml/intro.html",
    "href": "notes/iml/intro.html",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "",
    "text": "Code is from here\nMachine learning models usually perform really well for predictions, but are not interpretable. The iml package provides tools for analysing any black box machine learning model:\nThis document shows you how to use the iml package to analyse machine learning models.\nIf you want to learn more about the technical details of all the methods, read chapters from: https://christophm.github.io/interpretable-ml-book/agnostic.html"
  },
  {
    "objectID": "notes/iml/intro.html#data-boston-housing",
    "href": "notes/iml/intro.html#data-boston-housing",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Data: Boston Housing",
    "text": "Data: Boston Housing\nWe’ll use the MASS::Boston dataset to demonstrate the abilities of the iml package. This dataset contains median house values from Boston neighbourhoods.\n\ndata(\"Boston\", package = \"MASS\")\nhead(Boston)\n\n#>      crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n#> 1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n#> 2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n#> 3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n#> 4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n#> 5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n#> 6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n#>   medv\n#> 1 24.0\n#> 2 21.6\n#> 3 34.7\n#> 4 33.4\n#> 5 36.2\n#> 6 28.7"
  },
  {
    "objectID": "notes/iml/intro.html#fitting-the-machine-learning-model",
    "href": "notes/iml/intro.html#fitting-the-machine-learning-model",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Fitting the machine learning model",
    "text": "Fitting the machine learning model\nFirst we train a randomForest to predict the Boston median housing value:\n\nset.seed(42)\nlibrary(\"iml\")\nlibrary(\"randomForest\")\ndata(\"Boston\", package = \"MASS\")\nrf <- randomForest(medv ~ ., data = Boston, ntree = 50)"
  },
  {
    "objectID": "notes/iml/intro.html#using-the-iml-predictor-container",
    "href": "notes/iml/intro.html#using-the-iml-predictor-container",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Using the iml Predictor() container",
    "text": "Using the iml Predictor() container\nWe create a Predictor object, that holds the model and the data. The iml package uses R6 classes: New objects can be created by calling Predictor$new().\n\nX <- Boston[which(names(Boston) != \"medv\")]\npredictor <- Predictor$new(rf, data = X, y = Boston$medv)"
  },
  {
    "objectID": "notes/iml/intro.html#feature-importance",
    "href": "notes/iml/intro.html#feature-importance",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Feature importance",
    "text": "Feature importance\nWe can measure how important each feature was for the predictions with FeatureImp. The feature importance measure works by shuffling each feature and measuring how much the performance drops. For this regression task we choose to measure the loss in performance with the mean absolute error (‘mae’), another choice would be the mean squared error (‘mse’).\nOnce we create a new object of FeatureImp, the importance is automatically computed. We can call the plot() function of the object or look at the results in a data.frame.\n\nimp <- FeatureImp$new(predictor, loss = \"mae\")\nlibrary(\"ggplot2\")\nplot(imp)\n\n\n\n\n\n\n\nimp$results\n\n#>    feature importance.05 importance importance.95 permutation.error\n#> 1    lstat      4.394480   4.459282      4.740041          4.394662\n#> 2       rm      3.349928   3.620192      3.715445          3.567732\n#> 3      nox      1.772047   1.796058      1.833039          1.770032\n#> 4     crim      1.661024   1.703701      1.738660          1.679013\n#> 5      dis      1.670726   1.690742      1.694655          1.666242\n#> 6  ptratio      1.423169   1.426636      1.443533          1.405963\n#> 7    indus      1.399432   1.416183      1.434735          1.395661\n#> 8      age      1.346887   1.387961      1.423804          1.367848\n#> 9      tax      1.352382   1.376506      1.384641          1.356559\n#> 10   black      1.227135   1.234735      1.235719          1.216842\n#> 11     rad      1.097253   1.109357      1.131675          1.093281\n#> 12      zn      1.035018   1.042360      1.043935          1.027256\n#> 13    chas      1.032211   1.035745      1.047224          1.020736"
  },
  {
    "objectID": "notes/iml/intro.html#feature-effects",
    "href": "notes/iml/intro.html#feature-effects",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Feature effects",
    "text": "Feature effects\nBesides knowing which features were important, we are interested in how the features influence the predicted outcome. The FeatureEffect class implements accumulated local effect plots, partial dependence plots and individual conditional expectation curves. The following plot shows the accumulated local effects (ALE) for the feature ‘lstat’. ALE shows how the prediction changes locally, when the feature is varied. The marks on the x-axis indicates the distribution of the ‘lstat’ feature, showing how relevant a region is for interpretation (little or no points mean that we should not over-interpret this region).\n\nale <- FeatureEffect$new(predictor, feature = \"lstat\")\nale$plot()\n\n\n\n\n\n\n\n\nIf we want to compute the partial dependence curves on another feature, we can simply reset the feature:\n\nale$set.feature(\"rm\")\nale$plot()"
  },
  {
    "objectID": "notes/iml/intro.html#measure-interactions",
    "href": "notes/iml/intro.html#measure-interactions",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Measure interactions",
    "text": "Measure interactions\nWe can also measure how strongly features interact with each other. The interaction measure regards how much of the variance of \\(f(x)\\) is explained by the interaction. The measure is between 0 (no interaction) and 1 (= 100% of variance of \\(f(x)\\) due to interactions). For each feature, we measure how much they interact with any other feature:\n\ninteract <- Interaction$new(predictor)\n\n#> \n#> Attaching package: 'withr'\n\n\n#> The following objects are masked from 'package:rlang':\n#> \n#>     local_options, with_options\n\n\n#> The following object is masked from 'package:tools':\n#> \n#>     makevars_user\n\nplot(interact)\n\n\n\n\n\n\n\n\nWe can also specify a feature and measure all it’s 2-way interactions with all other features:\n\ninteract <- Interaction$new(predictor, feature = \"crim\")\nplot(interact)\n\n\n\n\n\n\n\n\nYou can also plot the feature effects for all features at once:\n\neffs <- FeatureEffects$new(predictor)\nplot(effs)"
  },
  {
    "objectID": "notes/iml/intro.html#surrogate-model",
    "href": "notes/iml/intro.html#surrogate-model",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Surrogate model",
    "text": "Surrogate model\nAnother way to make the models more interpretable is to replace the black box with a simpler model - a decision tree. We take the predictions of the black box model (in our case the random forest) and train a decision tree on the original features and the predicted outcome. The plot shows the terminal nodes of the fitted tree. The maxdepth parameter controls how deep the tree can grow and therefore how interpretable it is.\n\ntree <- TreeSurrogate$new(predictor, maxdepth = 2)\n\n#> Loading required package: partykit\n\n\n#> Loading required package: libcoin\n\n\n#> Loading required package: mvtnorm\n\nplot(tree)\n\n\n\n\n\n\n\n\nWe can use the tree to make predictions:\n\nhead(tree$predict(Boston))\n\n#> Warning in self$predictor$data$match_cols(data.frame(newdata)): Dropping\n#> additional columns: medv\n\n\n#>     .y.hat\n#> 1 27.09989\n#> 2 27.09989\n#> 3 27.09989\n#> 4 27.09989\n#> 5 27.09989\n#> 6 27.09989"
  },
  {
    "objectID": "notes/iml/intro.html#explain-single-predictions-with-a-local-model",
    "href": "notes/iml/intro.html#explain-single-predictions-with-a-local-model",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Explain single predictions with a local model",
    "text": "Explain single predictions with a local model\nGlobal surrogate model can improve the understanding of the global model behaviour. We can also fit a model locally to understand an individual prediction better. The local model fitted by LocalModel is a linear regression model and the data points are weighted by how close they are to the data point for wich we want to explain the prediction.\n\nlime.explain <- LocalModel$new(predictor, x.interest = X[1, ])\n\n#> Loading required package: glmnet\n\n\n#> Loading required package: Matrix\n\n\n#> Loaded glmnet 4.1-6\n\n\n#> Loading required package: gower\n\nlime.explain$results\n\n#>               beta x.recoded    effect x.original feature feature.value\n#> rm       4.4836483     6.575 29.479987      6.575      rm      rm=6.575\n#> ptratio -0.5244767    15.300 -8.024493       15.3 ptratio  ptratio=15.3\n#> lstat   -0.4348698     4.980 -2.165652       4.98   lstat    lstat=4.98\n\nplot(lime.explain)\n\n\n\n\n\n\n\n\n\nlime.explain$explain(X[2, ])\nplot(lime.explain)"
  },
  {
    "objectID": "notes/iml/intro.html#explain-single-predictions-with-game-theory",
    "href": "notes/iml/intro.html#explain-single-predictions-with-game-theory",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Explain single predictions with game theory",
    "text": "Explain single predictions with game theory\nAn alternative for explaining individual predictions is a method from coalitional game theory named Shapley value. Assume that for one data point, the feature values play a game together, in which they get the prediction as a payout. The Shapley value tells us how to fairly distribute the payout among the feature values.\n\nshapley <- Shapley$new(predictor, x.interest = X[1, ])\nshapley$plot()\n\n\n\n\n\n\n\n\nWe can reuse the object to explain other data points:\n\nshapley$explain(x.interest = X[2, ])\nshapley$plot()\n\n\n\n\n\n\n\n\nThe results in data.frame form can be extracted like this:\n\nresults <- shapley$results\nhead(results)\n\n#>   feature          phi    phi.var feature.value\n#> 1    crim -0.030772464 0.88726982  crim=0.02731\n#> 2      zn -0.009163333 0.01266404          zn=0\n#> 3   indus -0.412309000 0.65608174    indus=7.07\n#> 4    chas -0.065604772 0.04865024        chas=0\n#> 5     nox  0.067133048 0.37635155     nox=0.469\n#> 6      rm -0.354769984 7.26420349      rm=6.421"
  },
  {
    "objectID": "notes/hdx_data/hdx_data.html",
    "href": "notes/hdx_data/hdx_data.html",
    "title": "World Bank kenya data",
    "section": "",
    "text": "library(rhdx)\nlibrary(tidyverse)\nlibrary(data.table)\nset_rhdx_config(hdx_site = \"prod\")\nkenya_growth <- search_datasets(\"Kenya - Economy and Growth\", rows = 1) %>%\n    nth(1) %>%\n  get_resource(1) %>%\n  read_resource(filename = \"world-bank-economy-and-growth-indicators-for-kenya.csv\", hxl = TRUE)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mburu Blog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nMburu\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe trend of Earth surface temperatures in Kenyan towns\n\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2022\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeart Failure Prediction\n\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShap Calculation R\n\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMalware prediction\n\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature Selection Comparison\n\n\n\n\n\n\n\n\n\n\n\n\nSep 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeart Disease Explainable ML\n\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMalaysian Tourist Sites\n\n\n\n\n\n\n\n\n\n\n\n\nJul 17, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew york Airbnb\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJul 17, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMNIST Digits\n\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKenya Household Assets\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInflation Kenya\n\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKenya Census Data\n\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTanzanian Water Pumps\n\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoan Prediction Zindi\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2020\n\n\nmburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssociation analysis\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2018\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUSD-KES Hisotrical Data\n\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEast Africa Poverty Indicators\n\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKenya Inflation\n\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorld Health 2020 STATS\n\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering Fashion mnist\n\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2020\n\n\nMburu\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to advanced dimensionality reduction\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBusara Data Analysis\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2019\n\n\nmburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCancer Data\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2019\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCancer Data\n\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2019\n\n\nMburu\n\n\n\n\n\n\nNo matching items"
  }
]