[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Personal Blog",
    "section": "",
    "text": "Kenya Government Income & Expenditure from 2001 to r data.table::year(Sys.Date())-1\n\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2023\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiabetes Prediction using Tidymodels\n\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2023\n\n\nMburu\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShap Calculation R\n\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting loan defaults\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2020\n\n\nmburu\n\n\n\n\n\n\n  \n\n\n\n\nPredict whether the cancer is benign or malignant\n\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2019\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssociation analysis\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2018\n\n\nMburu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "kenya_population/household_assets_2019census.html",
    "href": "kenya_population/household_assets_2019census.html",
    "title": "Kenya Household Assets",
    "section": "",
    "text": "Kenya selected households assets 2019 census\nI figured that it will be important for me to do this to show why it is impossible for online learning to be adopted in Kenya.\nI used 2019 census data set which can be found here and the counties shape file can be found here\n\n#Packages used\n\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sf)\nlibrary(DT)\nlibrary(tmap)\nlibrary(ggthemes)\nlibrary(ggiraph)\n\n\n\nRead data set\n\nhousehold_assets <- fread(\"percentage-distribution-of-conventional-households-by-ownership-of-selected-household-assets-201.csv\")\n\n#\nkenya_shapefile <- st_read(\"County\") %>% setDT()\n\nReading layer `County' from data source \n  `/home/mburu/r_projects/m-mburu.github.io/kenya_population/County' \n  using driver `ESRI Shapefile'\nSimple feature collection with 47 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 33.91182 ymin: -4.702271 xmax: 41.90626 ymax: 5.430648\nGeodetic CRS:  WGS 84\n\nkenya_shapefile[, county := tolower(COUNTY)]\n\n# rename column\nsetnames(household_assets, \n         c(\"County / Sub-County\", \"Conventional Households\"  ), \n         c(\"county\", \"households\"))\n\n\n\nSome minor cleaning\n\n# remove commas \nhousehold_assets[, households := as.numeric(gsub(\",|AR K    \", \"\", households))]\n\nhousehold_assets[,  county := tolower(county)]\n\nsub_county <- household_assets %>% \n    group_by(county) %>%\n    filter(households == max(households)) %>% setDT()\n\n\nsub_county_melt <- melt(sub_county, \n                        id.vars = c(\"county\", \"households\"))\n\n\n\n% of households with various assets 2019 census\n\nThis is overall data set for the whole country computer devices is ownership about 8.8% this just means that about 91% of the students can’t access online learning. This is is just a naive estimation the number could be higher.\n\n\nkenya_dat <- sub_county_melt[county == \"kenya\"]\n\np <- ggplot(kenya_dat, aes(variable, value, tooltip = paste(variable, \" : \", value))) +\n    geom_bar_interactive(stat = \"identity\", width = 0.5, fill =\"slategray2\"  ) +\n    geom_text_interactive(aes(variable, value, label = paste0(value, \"%\")),\n              position = position_dodge(width = 0.5),\n              vjust = 0.001, size = 3)+\n    labs(x = \"Household assets\", y = \"%\",\n         title = \"% of households with various assets 2019 census\",\n         caption = \"twitter\\n@mmburu_w\")+\n    theme_fivethirtyeight()+\n  theme(\n    axis.text = element_text(size = 7, angle = 45, vjust = 1, hjust =1),\n    plot.title = element_text_interactive(size =11)\n  )\np1 <- girafe(ggobj = p, width_svg = 7, height_svg = 4.5, \n  options = list(\n    opts_sizing(rescale = T) )\n  )\n\np1\n\n\n\n\n\n\n\n\nMerge shapefile with asset data sets\n\nsub_county_melt[county == \"elgeyo/marakwet\", county := \"keiyo-marakwet\"]\nsub_county_melt[county == \"tharaka-nithi\", county := \"tharaka\"]\nsub_county_melt[county ==  \"taita/taveta\", county := \"taita taveta\"]\nsub_county_melt[county ==  \"nairobi city\", county := \"nairobi\"]\n\ncounty_shapes <- merge(kenya_shapefile, sub_county_melt, by = \"county\") \n\nsetnames(county_shapes, \"value\", \"Percentage\")\n\n\n\nComputer devices data\n\ncomputer <- county_shapes[variable  %in% c(\"Desk Top\\nComputer/\\nLaptop/ Tablet\")]\n\n# this converts to sf object\ncomputer <- st_set_geometry(computer, \"geometry\")\n\n\n\nPercentage of households with computer devices per county\n\nThat is if a household owns a tablet, laptop or a desktop\n\n\n#ttm()\ntm_shape(computer)+\n    tm_borders(col = \"grey\")+\n    tm_fill(col = \"Percentage\")+\n    tm_layout(title = \"% of households with laptop/tablet/desk top \\n 2019 census\",\n              title.size = 1, title.position = c(0.3, 0.95))\n\n\n\n#ttm()\n\n\n\n% of households that can access internet\n\nThis looks like is internet access through mobile phones\n\n\ninternet <- county_shapes[variable == \"Internet\"]\n\ninternet <- st_set_geometry(internet, \"geometry\")\n\n#ttm()\ntm_shape(internet)+\n    tm_borders(col = \"grey\")+\n    tm_fill(col = \"Percentage\")+\n    tm_layout(title = \"% of households with internet acess\",\n              title.size = 1, title.position = c(0.3, 0.95))"
  },
  {
    "objectID": "Intro_Python_Data/Intro_datascience.html",
    "href": "Intro_Python_Data/Intro_datascience.html",
    "title": "Personal Blog",
    "section": "",
    "text": "Modules (sometimes called packages or libraries) help group together related sets of tools in Python. In this exercise, we’ll examine two modules that are frequently used by Data Scientists:\nstatsmodels: used in machine learning; usually aliased as sm seaborn: a visualization library; usually aliased as sns Note that each module has a standard alias, which allows you to access the tools inside of the module without typing as many characters. For example, aliasing lets us shorten seaborn.scatterplot() to sns.scatterplot().\n\nimport statsmodels as sm\nimport seaborn as sns\nimport numpy as np\n\n\n\n\nBefore we start looking for Bayes’ kidnapper, we need to fill out a Missing Puppy Report with details of the case. Each piece of information will be stored as a variable.\nWe define a variable using an equals sign (=). For instance, we would define the variable height:\nheight = 24 In this exercise, we’ll be defining bayes_age to be 4.0 months old. The data type for this variable will be float, meaning that it is a number.\n\nbayes_age = 4.0\nbayes_age\n\n4.0\n\n\n\n\n\nLet’s continue to fill out the Missing Puppy Report for Bayes. In the previous exercise, we defined bayes_age, which was a float, which represents a number.\nIn this exercise, we’ll define favorite_toy and owner, which will both be strings. A string represents text. A string is surrounded by quotation marks (’ or “) and can contain letters, numbers, and special characters. It doesn’t matter if you use single (’) or double (”) quotes, but it’s important to be consistent throughout your code.\n\nfavorite_toy = \"Mr. Squeaky\"\nowner = \"DataCamp\"\n# Display variables\nprint(favorite_toy)\nprint(owner)\n\nMr. Squeaky\nDataCamp\n\n\n\n\n\nIt’s easy to make errors when you’re trying to type strings quickly.\nDon’t forget to use quotes! Without quotes, you’ll get a name error. owner = DataCamp Use the same type of quotation mark. If you start with a single quote, and end with a double quote, you’ll get a syntax error. fur_color = “blonde’ Someone at the police station made an error when filling out the final lines of Bayes’ Missing Puppy Report. In this exercise, you will correct the errors.\n\n# One or more of the following lines contains an error\n# Correct it so that it runs without producing syntax errors\nbirthday = '2017-07-14'\ncase_id = 'DATACAMP!123-456?'"
  },
  {
    "objectID": "datacamp.html",
    "href": "datacamp.html",
    "title": "Personal Blog",
    "section": "",
    "text": "Introduction to Python\n\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2023\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntermediate Regression in R\n\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nMburu\n\n\n\n\n\n\n  \n\n\n\n\nSurvival Analysis R\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFoundations of Probability in R\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Statistics with R\n\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunctions in R\n\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2022\n\n\n\n\n\n\n  \n\n\n\n\nHandling Missing Data with Imputations in R\n\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnomaly Detection in R\n\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2022\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling in R\n\n\n\n\n\n\n\n\n\n\n\n\nSep 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to advanced dimensionality reduction\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2022\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCensus data in r with tidycensus\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\nModeling with tidymodels in R\n\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunicating with Data in the Tidyverse\n\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2021\n\n\nMburu\n\n\n\n\n\n\n  \n\n\n\n\nThe reduction in weekly working hours in Europe\n\n\nLooking at the development between 1996 and 2006\n\n\n\n\n\n\n\n\n\nNov 8, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWriting Efficient R Code\n\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScalable Data Processing in R\n\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Linear and Logistic Regression in R\n\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork analysis in R\n\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nString manipulation with stringr in r\n\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML with tree based models in r\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2020\n\n\nMburu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "datacamp/intermidiate_regression_R/intermediate_regression.html",
    "href": "datacamp/intermidiate_regression_R/intermediate_regression.html",
    "title": "Intermediate Regression in R",
    "section": "",
    "text": "In Introduction to Regression in R, you learned to fit linear regression models with a single explanatory variable. In many cases, using only one explanatory variable limits the accuracy of predictions. That means that to truly master linear regression, you need to be able to include multiple explanatory variables.\nThe case when there is one numeric explanatory variable and one categorical explanatory variable is sometimes called a “parallel slopes” linear regression due to the shape of the predictions—more on that in the next exercise.\nHere, you’ll revisit the Taiwan real estate dataset. Recall the meaning of each variable.\n\n\n\nlibrary(mTools)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(fst)\nlibrary(broom)\ntaiwan_real_estate <- read_fst(here(\"data\", \"taiwan_real_estate2.fst\"))\n\n# Fit a linear regr'n of price_twd_msq vs. n_convenience\nmdl_price_vs_conv <- lm(price_twd_msq ~ n_convenience, data = taiwan_real_estate)\n\n# See the result\nDT_tidy_model(mdl_price_vs_conv)\n\n\n\n\n\n\n\n\n\n\n# Fit a linear regr'n of price_twd_msq vs. house_age_years, no intercept\nmdl_price_vs_age <- lm(price_twd_msq ~ house_age_years -1, data = taiwan_real_estate )\n\n# See the result\nDT_tidy_model(mdl_price_vs_age)\n\n\n\n\n\n\n\n\n\n\n# Fit a linear regr'n of price_twd_msq vs. n_convenience plus house_age_years, no intercept\nmdl_price_vs_both <- lm(price_twd_msq ~ n_convenience \n                        + house_age_years - 1, \n                        data = taiwan_real_estate)\n\n# See the result\nDT_tidy_model(mdl_price_vs_both)\n\n\n\n\n\n\n\n\n\n\nFor linear regression with a single numeric explanatory variable, there is an intercept coefficient and a slope coefficient. For linear regression with a single categorical explanatory variable, there is an intercept coefficient for each category.\nIn the “parallel slopes” case, where you have a numeric and a categorical explanatory variable, what do the coefficients mean?\ntaiwan_real_estate and mdl_price_vs_both are available.\n\nFor each additional nearby convenience store, the expected house price, in TWD per square meter, increases by 0.79.\n\n\n\n\nBeing able to see the predictions made by a model makes it easier to understand. In the case where there is only one explanatory variable, ggplot lets you do this without any manual calculation or messing about.\nTo visualize the relationship between a numeric explanatory variable and the numeric response, you can draw a scatter plot with a linear trend line.\nTo visualize the relationship between a categorical explanatory variable and the numeric response, you can draw a box plot.\ntaiwan_real_estate is available and ggplot2 is loaded.\n\nUsing the taiwan_real_estate dataset, plot the house price versus the number of nearby convenience stores.\nMake it a scatter plot.\nAdd a smooth linear regression trend line without a standard error ribbon.\n\n\n# Using taiwan_real_estate, plot price_twd_msq vs. n_convenience\nggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq)) +\n    # Add a point layer\n    geom_point() +\n    # Add a smooth trend line using linear regr'n, no ribbon\n    geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\nUsing the taiwan_real_estate dataset, plot the house price versus the house age.\nMake it a box plot.\n\n\n# Using taiwan_real_estate, plot price_twd_msq vs. house_age_years\nggplot(taiwan_real_estate, aes( house_age_years, price_twd_msq)) +\n    # Add a box plot layer\n    geom_boxplot()\n\n\n\n\n\n\n\nThe two plots in the previous exercise gave very different predictions: one gave a predicted response that increased linearly with a numeric variable; the other gave a fixed response for each category. The only sensible way to reconcile these two conflicting predictions is to incorporate both explanatory variables in the model at once.\nWhen it comes to a linear regression model with a numeric and a categorical explanatory variable, ggplot2 doesn’t have an easy, “out of the box” way to show the predictions. Fortunately, the moderndive package includes an extra geom, geom_parallel_slopes() to make it simple.\ntaiwan_real_estate is available; ggplot2 and moderndive are loaded.\n\nUsing the taiwan_real_estate dataset, plot house prices versus the number of nearby convenience stores, colored by house age.\nMake it a scatter plot.\nAdd parallel slopes, without a standard error ribbon.\n\n\nlibrary(moderndive)\n# Using taiwan_real_estate, plot price_twd_msq vs. n_convenience colored by house_age_years\nggplot(taiwan_real_estate, aes( n_convenience , price_twd_msq, color = house_age_years))  +\n    # Add a point layer\n    geom_point() +\n    # Add parallel slopes, no ribbon\n    geom_parallel_slopes(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\nWhile ggplot can automatically show you model predictions, in order to get those values to program with, you’ll need to do the calculations yourself.\nJust as with the case of a single explanatory variable, the workflow has two steps: create a data frame of explanatory variables, then add a column of predictions. To make sure you’ve got the right answer, you can add your predictions to the ggplot with the geom_parallel_slopes() lines.\ntaiwan_real_estate and mdl_price_vs_both are available; dplyr, tidyr, and ggplot2 are loaded.\n\n\n\nn_convenience should take the numbers zero to ten.\nhouse_age_years should take the unique values of the house_age_years column of taiwan_real_estate.\n\n\n# Make a grid of explanatory data\nexplanatory_data <- expand.grid(\n    # Set n_convenience to zero to ten\n    n_convenience = 0:10,\n    # Set house_age_years to the unique values of that variable\n    house_age_years = unique(taiwan_real_estate$house_age_years)\n)\n\n# See the result\nexplanatory_data[1:10,] %>% data_table()\n\n\n\n\n\n\n\nAdd a column to the explanatory_data named for the response variable, assigning to prediction_data.\nThe response column contain predictions made using mdl_price_vs_both and explanatory_data.\n\n\n# Add predictions to the data frame\nprediction_data <- explanatory_data %>% \n    mutate(price_twd_msq = predict(mdl_price_vs_both, explanatory_data) )\n\n# See the result\nprediction_data[1:10,] %>% data_table()\n\n\n\n\n\n\n\nUpdate the plot to add a point layer of predictions. Use the prediction_data, set the point size to 5, and the point shape to 15.\n\n\ntaiwan_real_estate %>% \n    ggplot(aes(n_convenience, price_twd_msq, color = house_age_years)) +\n    geom_point() +\n    geom_parallel_slopes(se = FALSE) +\n    # Add points using prediction_data, with size 5 and shape 15\n    geom_point(data =prediction_data, \n               aes(n_convenience, price_twd_msq),\n               size = 5, shape = 15)\n\n\n\n\n\n\n\n\nAs with simple linear regression, you can manually calculate the predictions from the model coefficients. The only change for the parallel slopes case is that the intercept is different for each category of the categorical explanatory variable. That means you need to consider the case when each each category occurs separately.\ntaiwan_real_estate, mdl_price_vs_both, and explanatory_data are available; dplyr is loaded.\n\nGet the coefficients from mdl_price_vs_both, assigning to coeffs.\nAssign each of the elements of coeffs to the appropriate variable.\n\n\n# Get the coefficients from mdl_price_vs_both\ncoeffs <- coefficients(mdl_price_vs_both)\n\n# Extract the slope coefficient\nslope <- coeffs[1]\n\n# Extract the intercept coefficient for 0 to 15\nintercept_0_15 <- coeffs[2]\n\n# Extract the intercept coefficient for 15 to 30\nintercept_15_30 <- coeffs[3]\n\n# Extract the intercept coefficient for 30 to 45\nintercept_30_45 <- coeffs[4]\n\n\n\n\nTo choose the intercept, in the case when house_age_years is “0 to 15”, choose intercept_0_15. In the case when house_age_years is “15 to 30”, choose intercept_15_30. Do likewise for “30 to 45”.\nManually calculate the predictions as the intercept plus the slope times n_convenience.\n\n\nprediction_data <- explanatory_data %>% \n    mutate(\n        # Consider the 3 cases to choose the intercept\n        intercept = case_when(\n            house_age_years == \"0 to 15\" ~ intercept_0_15,\n            house_age_years ==  \"15 to 30\" ~ intercept_15_30,\n            house_age_years ==  \"30 to 45\" ~ intercept_30_45\n        ),\n        \n        # Manually calculate the predictions\n        price_twd_msq = slope*n_convenience + intercept\n    )\n\n# See the results\nprediction_data[1:10,] %>% data_table()\n\n\n\n\n\n\n\n\n\n\nRecall that the coefficient of determination is a measure of how well the linear regression line fits the observed values. An important motivation for including several explanatory variables in a linear regression is that you can improve the fit compared to considering only a single explanatory variable.\nHere you’ll compare the coefficient of determination for the three Taiwan house price models, to see which gives the best result.\nmdl_price_vs_conv, mdl_price_vs_age, and mdl_price_vs_both are available; dplyr and broom are loaded.\n\nGet the unadjusted and adjusted coefficients of determination for mdl_price_vs_conv by glancing at the model, then selecting the r.squared and adj.r.squared values.\nDo the same for mdl_price_vs_age and mdl_price_vs_both.\n\n\nmy_models <- list(mdl_price_vs_conv = mdl_price_vs_conv,\n                  mdl_price_vs_age = mdl_price_vs_age,\n                  mdl_price_vs_both = mdl_price_vs_both)\nnms_ms <- names(my_models)\nmy_dfs <- lapply(seq_along(my_models), function(x){\n    \n    my_models[[x]] %>% \n        glance() %>% \n        mutate(lm_model =nms_ms[x] ) %>%\n        select(lm_model, r.squared, adj.r.squared, sigma) \n    \n    \n    \n})\n\nmodel_glance <- data.table::rbindlist(my_dfs)\nmodel_glance %>% \n    round_all_num_cols() %>%\n    data_table()\n\n\n\n\n\n\n\nWhich model does the adjusted coefficient of determination suggest gives a better fit?\nmdl_price_vs_both\n\n\n\n\nThe other common metric for assessing model fit is the residual standard error (RSE), which measures the typical size of the residuals.\nIn the last exercise you saw how including both explanatory variables into the model increased the coefficient of determination. How do you think using both explanatory variables will change the RSE?\nmdl_price_vs_conv, mdl_price_vs_age, and mdl_price_vs_both are available; dplyr and broom are loaded.\n\nShown as sigma below\n\n\nmodel_glance %>% \n    round_all_num_cols() %>%\n    data_table()\n\n\n\n\n\n\n\nThe model with the list sigma mdl_price_vs_both"
  },
  {
    "objectID": "datacamp/intermidiate_regression_R/intermediate_regression.html#interactions",
    "href": "datacamp/intermidiate_regression_R/intermediate_regression.html#interactions",
    "title": "Intermediate Regression in R",
    "section": "Interactions",
    "text": "Interactions"
  },
  {
    "objectID": "notes/gtsumary_test.html",
    "href": "notes/gtsumary_test.html",
    "title": "gtsumary_test",
    "section": "",
    "text": "library(gtsummary)\nlibrary(tidyverse)\ntrial2 <- trial %>% select(trt, age, grade)\n\n\ntab <- trial2 %>%\n    tbl_summary(by = trt) %>%\n    add_p()\n\n\ntab2 <- trial2 %>%\n    tbl_summary(\n        by = trt,\n        type = all_continuous() ~ \"continuous2\",\n        statistic = all_continuous() ~ c(\n            \"{N_nonmiss}\",\n            \"{median} ({p25}, {p75})\",\n            \"{mean} ({sd})\",\n            \"{min}, {max}\"\n        ),\n        missing = \"ifany\"\n    ) %>%\n    add_p(pvalue_fun = ~ style_pvalue(.x, digits = 2))\n\ntab_df = as.data.frame(tab2)\n\nmTools::data_table(tab_df)"
  },
  {
    "objectID": "posts/Diabetes/predict_diabetes_tidymodels.html",
    "href": "posts/Diabetes/predict_diabetes_tidymodels.html",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(data.table)\nlibrary(gtsummary)\nlibrary(mTools)\nDiabetes data"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html",
    "href": "datacamp/tidymodels/tidymodels.html",
    "title": "Modeling with tidymodels in R",
    "section": "",
    "text": "The rsample package is designed to create training and test datasets. Creating a test dataset is important for estimating how a trained model will likely perform on new data. It also guards against overfitting, where a model memorizes patterns that exist only in the training data and performs poorly on new data.\nIn this exercise, you will create training and test datasets from the home_sales data. This data contains information on homes sold in the Seattle, Washington area between 2015 and 2016.\nThe outcome variable in this data is selling_price.\nThe tidymodels package will be pre-loaded in every exercise in the course. The home_sales tibble has also been loaded for you.\n\n\n\nTidy model packages\n\n\n\nhome_sales <- readRDS(\"home_sales.rds\")\n\n# Create a data split object\nhome_split <- initial_split(home_sales, \n                            prop = 0.7, \n                            strata = selling_price)\n\n# Create the training data\nhome_training <- home_split %>%\n  training()\n\n# Create the test data\nhome_test <- home_split %>% \n  testing()\n\n# Check number of rows in each dataset\nnrow(home_training)\n\n[1] 1042\n\nnrow(home_test)\n\n[1] 450\n\n\nDistribution of outcome variable values Stratifying by the outcome variable when generating training and test datasets ensures that the outcome variable values have a similar range in both datasets.\nSince the original data is split at random, stratification avoids placing all the expensive homes in home_sales into the test dataset, for example. In this case, your model would most likely perform poorly because it was trained on less expensive homes.\nIn this exercise, you will calculate summary statistics for the selling_price variable in the training and test datasets. The home_training and home_test tibbles have been loaded from the previous exercise.\n\n# Distribution of selling_price in training data\nlibrary(knitr)\nsummary_func <- function(df){\n      df %>% summarize(min_sell_price = min(selling_price),\n            max_sell_price = max(selling_price),\n            mean_sell_price = mean(selling_price),\n            sd_sell_price = sd(selling_price)) %>%\n    kable()\n\n}\nhome_training %>% \n    summary_func()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n479358.7\n81534.16\n\n\n\n\nhome_test %>% \n  summary_func()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n478449.5\n79764.72"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#fitting-a-linear-regression-model",
    "href": "datacamp/tidymodels/tidymodels.html#fitting-a-linear-regression-model",
    "title": "Modeling with tidymodels in R",
    "section": "Fitting a linear regression model",
    "text": "Fitting a linear regression model\nThe parsnip package provides a unified syntax for the model fitting process in R.\nWith parsnip, it is easy to define models using the various packages, or engines, that exist in the R ecosystem.\nIn this exercise, you will define a parsnip linear regression object and train your model to predict selling_price using home_age and sqft_living as predictor variables from the home_sales data.\nThe home_training and home_test tibbles that you created in the previous lesson have been loaded into this session.\n\n# Specify a linear regression model, linear_model\nlinear_model <- linear_reg() %>% \n  # Set the model engine\n  set_engine('lm') %>% \n  # Set the model mode\n  set_mode('regression')\n\n# Train the model with the training data\nlm_fit <- linear_model %>% \n  fit(selling_price~ home_age + sqft_living,\n      data = home_training)\n\n# Print lm_fit to view model information\ntidy(lm_fit) %>%\n    kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n292350.3425\n7625.56910\n38.338167\n0\n\n\nhome_age\n-1616.2020\n176.93455\n-9.134463\n0\n\n\nsqft_living\n103.2897\n2.77595\n37.208781\n0"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#predicting-home-selling-prices",
    "href": "datacamp/tidymodels/tidymodels.html#predicting-home-selling-prices",
    "title": "Modeling with tidymodels in R",
    "section": "Predicting home selling prices",
    "text": "Predicting home selling prices\nAfter fitting a model using the training data, the next step is to use it to make predictions on the test dataset. The test dataset acts as a new source of data for the model and will allow you to evaluate how well it performs.\nBefore you can evaluate model performance, you must add your predictions to the test dataset.\nIn this exercise, you will use your trained model, lm_fit, to predict selling_price in the home_test dataset.\nYour trained model, lm_fit, as well as the test dataset, home_test have been loaded into your session.\n\n# Predict selling_price\nhome_predictions <- predict(lm_fit,\n                        new_data = home_test)\n\n# View predicted selling prices\n#home_predictions\n\n# Combine test data with predictions\nhome_test_results <- home_test %>% \n  select(selling_price, home_age, sqft_living) %>% \n  bind_cols(home_predictions)\n\n# View results\nhome_test_results %>% \n    head()\n\n# A tibble: 6 × 4\n  selling_price home_age sqft_living   .pred\n          <dbl>    <dbl>       <dbl>   <dbl>\n1        487000       10        2540 538544.\n2        465000       10        1530 434222.\n3        411000       18        1130 379976.\n4        635000        4        3350 631906.\n5        464950       19        2190 487847.\n6        425000       11        1920 472888."
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#model-performance-metrics",
    "href": "datacamp/tidymodels/tidymodels.html#model-performance-metrics",
    "title": "Modeling with tidymodels in R",
    "section": "Model performance metrics",
    "text": "Model performance metrics\nEvaluating model results is an important step in the modeling process. Model evaluation should be done on the test dataset in order to see how well a model will generalize to new datasets.\nIn the previous exercise, you trained a linear regression model to predict selling_price using home_age and sqft_living as predictor variables. You then created the home_test_results tibble using your trained model on the home_test data.\nIn this exercise, you will calculate the RMSE and R squared metrics using your results in home_test_results.\nThe home_test_results tibble has been loaded into your session.\n\n# Print home_test_results\n#home_test_results\n\n# Caculate the RMSE metric\nhome_test_results %>% \n  rmse(truth = selling_price, estimate =.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      45930.\n\n# Calculate the R squared metric\nhome_test_results %>% \n  rsq(truth = selling_price, estimate =.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.670"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#r-squared-plot",
    "href": "datacamp/tidymodels/tidymodels.html#r-squared-plot",
    "title": "Modeling with tidymodels in R",
    "section": "R squared plot",
    "text": "R squared plot\nIn the previous exercise, you got an R squared value of 0.651. The R squared metric ranges from 0 to 1, 0 being the worst and 1 the best.\nCalculating the R squared value is only the first step in studying your model’s predictions.\nMaking an R squared plot is extremely important because it will uncover potential problems with your model, such as non-linear patterns or regions where your model is either over or under-predicting the outcome variable.\nIn this exercise, you will create an R squared plot of your model’s performance.\nThe home_test_results tibble has been loaded into your session.\n\n# Create an R squared plot of model performance\nggplot(home_test_results, aes(x = selling_price, y = .pred)) +\n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred()  +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#complete-model-fitting-process-with-last_fit",
    "href": "datacamp/tidymodels/tidymodels.html#complete-model-fitting-process-with-last_fit",
    "title": "Modeling with tidymodels in R",
    "section": "Complete model fitting process with last_fit()",
    "text": "Complete model fitting process with last_fit()\nIn this exercise, you will train and evaluate the performance of a linear regression model that predicts selling_price using all the predictors available in the home_sales tibble.\nThis exercise will give you a chance to perform the entire model fitting process with tidymodels, from defining your model object to evaluating its performance on the test data.\nEarlier in the chapter, you created an rsample object called home_split by passing the home_sales tibble into initial_split(). The home_split object contains the instructions for randomly splitting home_sales into training and test sets.\nThe home_sales tibble, and home_split object have been loaded into this session.\n\n# Define a linear regression model\nlinear_model <- linear_reg() %>% \n  set_engine('lm') %>% \n  set_mode('regression')\n\n# Train linear_model with last_fit()\nlinear_fit <- linear_model %>% \n  last_fit(selling_price ~ ., split = home_split)\n\n# Collect predictions and view results\npredictions_df <- linear_fit %>% collect_predictions()\npredictions_df %>% head()\n\n# A tibble: 6 × 5\n  id                 .pred  .row selling_price .config             \n  <chr>              <dbl> <int>         <dbl> <chr>               \n1 train/test split 527338.     1        487000 Preprocessor1_Model1\n2 train/test split 421637.     2        465000 Preprocessor1_Model1\n3 train/test split 397998.     3        411000 Preprocessor1_Model1\n4 train/test split 694181.     4        635000 Preprocessor1_Model1\n5 train/test split 475255.     8        464950 Preprocessor1_Model1\n6 train/test split 436889.     9        425000 Preprocessor1_Model1\n\n# Make an R squared plot using predictions_df\nggplot(predictions_df, aes(x = selling_price, y = .pred)) + \n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred() +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#data-resampling",
    "href": "datacamp/tidymodels/tidymodels.html#data-resampling",
    "title": "Modeling with tidymodels in R",
    "section": "Data resampling",
    "text": "Data resampling\nThe first step in a machine learning project is to create training and test datasets for model fitting and evaluation. The test dataset provides an estimate of how your model will perform on new data and helps to guard against overfitting.\nYou will be working with the telecom_df dataset which contains information on customers of a telecommunications company. The outcome variable is canceled_service and it records whether a customer canceled their contract with the company. The predictor variables contain information about customers’ cell phone and internet usage as well as their contract type and monthly charges.\nThe telecom_df tibble has been loaded into your session.\n\ntelecom_df <- readRDS(\"telecom_df.rds\")\n# Create data split object\ntelecom_split <- initial_split(telecom_df, prop = 0.75,\n                     strata = canceled_service)\n\n# Create the training data\ntelecom_training <- telecom_split %>% \n  training()\n\n# Create the test data\ntelecom_test <- telecom_split %>% \n  testing()\n\n# Check the number of rows\nnrow(telecom_training)\n\n[1] 731\n\nnrow(telecom_test)\n\n[1] 244"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#fitting-a-logistic-regression-model",
    "href": "datacamp/tidymodels/tidymodels.html#fitting-a-logistic-regression-model",
    "title": "Modeling with tidymodels in R",
    "section": "Fitting a logistic regression model",
    "text": "Fitting a logistic regression model\nIn addition to regression models, the parsnip package also provides a general interface to classification models in R.\nIn this exercise, you will define a parsnip logistic regression object and train your model to predict canceled_service using avg_call_mins, avg_intl_mins, and monthly_charges as predictor variables from the telecom_df data.\nThe telecom_training and telecom_test tibbles that you created in the previous lesson have been loaded into this session.\n\n# Specify a logistic regression model\nlogistic_model <- logistic_reg() %>% \n  # Set the engine\n  set_engine('glm') %>% \n  # Set the mode\n  set_mode('classification')\n\n# Fit to training data\nlogistic_fit <- logistic_model %>% \n  fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n      data = telecom_training)\n\n# Print model fit object\nlogistic_fit %>% tidy()\n\n# A tibble: 4 × 5\n  term             estimate std.error statistic  p.value\n  <chr>               <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)      1.99       0.584      3.41   6.43e- 4\n2 avg_call_mins   -0.0107     0.00128   -8.40   4.50e-17\n3 avg_intl_mins    0.0236     0.00311    7.59   3.16e-14\n4 monthly_charges  0.000293   0.00477    0.0615 9.51e- 1"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#combining-test-dataset-results",
    "href": "datacamp/tidymodels/tidymodels.html#combining-test-dataset-results",
    "title": "Modeling with tidymodels in R",
    "section": "Combining test dataset results",
    "text": "Combining test dataset results\nEvaluating your model’s performance on the test dataset gives insights into how well your model predicts on new data sources. These insights will help you communicate your model’s value in solving problems or improving decision making.\nBefore you can calculate classification metrics such as sensitivity or specificity, you must create a results tibble with the required columns for yardstick metric functions.\nIn this exercise, you will use your trained model to predict the outcome variable in the telecom_test dataset and combine it with the true outcome values in the canceled_service column.\nYour trained model, logistic_fit, and test dataset, telecom_test, have been loaded from the previous exercise.\n\n# Predict outcome categories\nclass_preds <- predict(logistic_fit, new_data = telecom_test,\n                       type = 'class')\n\n# Obtain estimated probabilities for each outcome value\nprob_preds <- predict(logistic_fit, new_data = telecom_test, \n                      type = 'prob')\n\n# Combine test set results\ntelecom_results <- telecom_test %>% \n  select(canceled_service) %>% \n  bind_cols(class_preds, prob_preds)\n\n# View results tibble\ntelecom_results %>%\n    head()\n\n# A tibble: 6 × 4\n  canceled_service .pred_class .pred_yes .pred_no\n  <fct>            <fct>           <dbl>    <dbl>\n1 yes              no              0.136    0.864\n2 yes              yes             0.792    0.208\n3 no               no              0.171    0.829\n4 no               yes             0.508    0.492\n5 yes              no              0.371    0.629\n6 no               no              0.139    0.861"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#evaluating-performance-with-yardstick",
    "href": "datacamp/tidymodels/tidymodels.html#evaluating-performance-with-yardstick",
    "title": "Modeling with tidymodels in R",
    "section": "Evaluating performance with yardstick",
    "text": "Evaluating performance with yardstick\nIn the previous exercise, you calculated classification metrics from a sample confusion matrix. The yardstick package was designed to automate this process.\nFor classification models, yardstick functions require a tibble of model results as the first argument. This should include the actual outcome values, predicted outcome values, and estimated probabilities for each value of the outcome variable.\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate performance metrics.\nThe telecom_results tibble has been loaded into your session.\n\n# Calculate the confusion matrix\nconf_mat(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n          Truth\nPrediction yes  no\n       yes  31  22\n       no   51 140\n\n# Calculate the accuracy\naccuracy(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.701\n\n# Calculate the sensitivity\n\nsens(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.378\n\n# Calculate the specificity\n\nspec(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 spec    binary         0.864"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#creating-custom-metric-sets",
    "href": "datacamp/tidymodels/tidymodels.html#creating-custom-metric-sets",
    "title": "Modeling with tidymodels in R",
    "section": "Creating custom metric sets",
    "text": "Creating custom metric sets\nThe yardstick package also provides the ability to create custom sets of model metrics. In cases where the cost of obtaining false negative errors is different from the cost of false positive errors, it may be important to examine a specific set of performance metrics.\nInstead of calculating accuracy, sensitivity, and specificity separately, you can create your own metric function that calculates all three at the same time.\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate a custom set of performance metrics. You will also use a confusion matrix to calculate all available binary classification metrics in tidymodelsall at once.\nThe telecom_results tibble has been loaded into your session.\n\n# Create a custom metric function\ntelecom_metrics <- metric_set(accuracy, sens, spec)\n\n# Calculate metrics using model results tibble\ntelecom_metrics(telecom_results, \n                truth = canceled_service,\n                estimate = .pred_class)\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.701\n2 sens     binary         0.378\n3 spec     binary         0.864\n\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class) %>% \n  # Pass to the summary() function\n  summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary         0.701\n 2 kap                  binary         0.265\n 3 sens                 binary         0.378\n 4 spec                 binary         0.864\n 5 ppv                  binary         0.585\n 6 npv                  binary         0.733\n 7 mcc                  binary         0.278\n 8 j_index              binary         0.242\n 9 bal_accuracy         binary         0.621\n10 detection_prevalence binary         0.217\n11 precision            binary         0.585\n12 recall               binary         0.378\n13 f_meas               binary         0.459"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#plotting-the-confusion-matrix",
    "href": "datacamp/tidymodels/tidymodels.html#plotting-the-confusion-matrix",
    "title": "Modeling with tidymodels in R",
    "section": "Plotting the confusion matrix",
    "text": "Plotting the confusion matrix\nCalculating performance metrics with the yardstick package provides insight into how well a classification model is performing on the test dataset. Most yardstick functions return a single number that summarizes classification performance.\nMany times, it is helpful to create visualizations of the confusion matrix to more easily communicate your results.\nIn this exercise, you will make a heat map and mosaic plot of the confusion matrix from your logistic regression model on the telecom_df dataset.\nYour model results tibble, telecom_results, has been loaded into your session.\n\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a heat map\n  autoplot(type = \"heatmap\")\n\n\n\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a mosaic plot\n  autoplot(type = \"mosaic\")"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#roc-curves-and-area-under-the-roc-curve",
    "href": "datacamp/tidymodels/tidymodels.html#roc-curves-and-area-under-the-roc-curve",
    "title": "Modeling with tidymodels in R",
    "section": "ROC curves and area under the ROC curve",
    "text": "ROC curves and area under the ROC curve\nROC curves are used to visualize the performance of a classification model across a range of probability thresholds. An ROC curve with the majority of points near the upper left corner of the plot indicates that a classification model is able to correctly predict both the positive and negative outcomes correctly across a wide range of probability thresholds.\nThe area under this curve provides a letter grade summary of model performance.\nIn this exercise, you will create an ROC curve from your logistic regression model results and calculate the area under the ROC curve with yardstick.\nYour model results tibble, telecom_results has been loaded into your session.\n\n# Calculate metrics across thresholds\nthreshold_df <- telecom_results %>% \n  roc_curve(truth = canceled_service, .pred_yes)\n\n# View results\nthreshold_df %>%\n  head()\n\n# A tibble: 6 × 3\n  .threshold specificity sensitivity\n       <dbl>       <dbl>       <dbl>\n1  -Inf          0             1    \n2     0.0110     0             1    \n3     0.0350     0             0.988\n4     0.0416     0.00617       0.988\n5     0.0435     0.0123        0.988\n6     0.0487     0.0185        0.988\n\n# Plot ROC curve\nthreshold_df %>% \n  autoplot()\n\n\n\n# Calculate ROC AUC\nroc_auc(telecom_results, truth = canceled_service, .pred_yes)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.725\n\n\nStreamlining the modeling process The last_fit() function is designed to streamline the modeling workflow in tidymodels. Instead of training your model on the training data and building a results tibble using the test data, last_fit() accomplishes this with one function.\nIn this exercise, you will train the same logistic regression model as you fit in the previous exercises, except with the last_fit() function.\nYour data split object, telecom_split, and model specification, logistic_model, have been loaded into your session.\n\n# Train model with last_fit()\ntelecom_last_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n           split = telecom_split)\n\n# View test set metrics\ntelecom_last_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.701 Preprocessor1_Model1\n2 roc_auc  binary         0.725 Preprocessor1_Model1"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#collecting-predictions-and-creating-custom-metrics",
    "href": "datacamp/tidymodels/tidymodels.html#collecting-predictions-and-creating-custom-metrics",
    "title": "Modeling with tidymodels in R",
    "section": "Collecting predictions and creating custom metrics",
    "text": "Collecting predictions and creating custom metrics\nUsing the last_fit() modeling workflow also saves time in collecting model predictions. Instead of manually creating a tibble of model results, there are helper functions that extract this information automatically.\nIn this exercise, you will use your trained model, telecom_last_fit, to create a tibble of model results on the test dataset as well as calculate custom performance metrics.\nYou trained model, telecom_last_fit, has been loaded into this session.\n\n# Collect predictions\nlast_fit_results <- telecom_last_fit %>% \n  collect_predictions()\n\n# View results\nlast_fit_results %>%\n  head()\n\n# A tibble: 6 × 7\n  id               .pred_yes .pred_no  .row .pred_class canceled_service .config\n  <chr>                <dbl>    <dbl> <int> <fct>       <fct>            <chr>  \n1 train/test split     0.136    0.864     4 no          yes              Prepro…\n2 train/test split     0.792    0.208     7 yes         yes              Prepro…\n3 train/test split     0.171    0.829    10 no          no               Prepro…\n4 train/test split     0.508    0.492    16 yes         no               Prepro…\n5 train/test split     0.371    0.629    17 no          yes              Prepro…\n6 train/test split     0.139    0.861    21 no          no               Prepro…\n\n# Custom metrics function\nlast_fit_metrics <- metric_set(accuracy, sens,\n                               spec, roc_auc)\n\n# Calculate metrics\nlast_fit_metrics(last_fit_results,\n                 truth = canceled_service,\n                 estimate = .pred_class,\n                 .pred_yes)\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.701\n2 sens     binary         0.378\n3 spec     binary         0.864\n4 roc_auc  binary         0.725"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#complete-modeling-workflow",
    "href": "datacamp/tidymodels/tidymodels.html#complete-modeling-workflow",
    "title": "Modeling with tidymodels in R",
    "section": "Complete modeling workflow",
    "text": "Complete modeling workflow\nIn this exercise, you will use the last_fit() function to train a logistic regression model and evaluate its performance on the test data by assessing the ROC curve and the area under the ROC curve.\nSimilar to previous exercises, you will predict canceled_service in the telecom_df data, but with an additional predictor variable to see if you can improve model performance.\nThe telecom_df tibble, telecom_split, and logistic_model objects from the previous exercises have been loaded into your workspace. The telecom_split object contains the instructions for randomly splitting the telecom_df tibble into training and test sets. The logistic_model object is a parsnip specification of a logistic regression model.\n\n# Train a logistic regression model\nlogistic_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins + avg_intl_mins + monthly_charges + months_with_company, \n           split = telecom_split)\n\n# Collect metrics\nlogistic_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.742 Preprocessor1_Model1\n2 roc_auc  binary         0.797 Preprocessor1_Model1\n\n# Collect model predictions\nlogistic_fit %>% \n  collect_predictions() %>% \n  # Plot ROC curve\n  roc_curve(truth = canceled_service, .pred_yes) %>% \n  autoplot()"
  },
  {
    "objectID": "posts/Diabetes/predict_diabetes_tidymodels.html#data-processing",
    "href": "posts/Diabetes/predict_diabetes_tidymodels.html#data-processing",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "Data processing",
    "text": "Data processing\n\ndiabetes_df[, diabetes_char := factor(diabetes, \n                                      levels = c(0, 1),\n                                      labels = c(\"Non diabetic\", \"Diabetic\"))]"
  },
  {
    "objectID": "posts/Diabetes/predict_diabetes_tidymodels.html#summary-stats",
    "href": "posts/Diabetes/predict_diabetes_tidymodels.html#summary-stats",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "Summary Stats",
    "text": "Summary Stats\n\nlibrary(ggiraph)\ndb_perc <- diabetes_df[, .(freq = .N),\n                       by = diabetes_char][\n                           ,perc := round(freq/sum(freq) * 100, 1)]\n\n\nggplot(db_perc, aes(diabetes_char, freq, fill = diabetes_char))+\n    geom_bar_interactive(width = 0.5, stat = \"identity\")+\n    geom_text(aes(label = paste0(freq, \"(\", perc, \"%)\")),\n              position = position_dodge(width = 0.5),\n              vjust = 0.05)+\n    scale_fill_brewer(name = \"\", type = \"qual\", palette = \"Dark2\")+\n    theme_minimal()+\n    theme(\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\ntab2 <- diabetes_df %>%\n    tbl_summary(\n        by = diabetes_char,\n        type = all_continuous() ~ \"continuous2\",\n        statistic = all_continuous() ~ c(\n            \"{mean} ({sd})\",\n            \"{median} ({p25}, {p75})\",\n            \"[{min}, {max}]\"\n        ),\n        missing = \"ifany\"\n    ) %>%\n    add_p(pvalue_fun = ~ style_pvalue(.x, digits = 2))\n\ntab_df = as.data.frame(tab2)\nnms <- names(tab_df)\nnms <- gsub(\"\\\\*\", \"\", nms)\nnames(tab_df) <- nms\ndata_table(tab_df)"
  },
  {
    "objectID": "posts/Diabetes/predict_diabetes_tidymodels.html#model-fitting",
    "href": "posts/Diabetes/predict_diabetes_tidymodels.html#model-fitting",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "Model Fitting",
    "text": "Model Fitting\n\nset.seed(100)\ndiabetes_df[, diabetes:= as.factor(diabetes)]\ndiabetes_df_split <- initial_split(diabetes_df[,.SD, .SDcols = !\"diabetes_char\"], \n                                   strata = diabetes)\n\ndiabetes_df_train <- training(diabetes_df_split)\n\ndiabetes_df_test <- testing(diabetes_df_split)\n\n\n# Specify a logistic regression model\nlogistic_model <- logistic_reg() %>% \n  # Set the engine\n  set_engine('glm') %>% \n  # Set the mode\n  set_mode('classification')\n\n# Fit to training data\nlogistic_fit <- logistic_model %>% \n  fit(diabetes ~ .,\n      data = diabetes_df_train)\n\n# Print model fit object\nlogistic_fit %>% \n    DT_tidy_model()\n\n\n\n\n\n\n\nxgb_spec <- boost_tree(\n    trees = 2000,\n    tree_depth = tune(), \n    min_n = tune(),\n    loss_reduction = tune(),                     ## first three: model complexity\n    sample_size = tune(), \n    mtry = tune(),         ## randomness\n    learn_rate = tune()                          ## step size\n) %>%\n    set_engine(\"xgboost\") %>%\n    set_mode(\"classification\")\n\nxgb_spec\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 2000\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost"
  },
  {
    "objectID": "datacamp/foundation_prob_R/foundation_probability.html",
    "href": "datacamp/foundation_prob_R/foundation_probability.html",
    "title": "Foundations of Probability in R",
    "section": "",
    "text": "In these exercises, you’ll practice using the rbinom() function, which generates random “flips” that are either 1 (“heads”) or 0 (“tails”).\n\n# Generate 10 separate random flips with probability .3\nrbinom(10, 1, p = 0.3)\n\n [1] 0 0 1 0 1 0 0 0 1 0\n\n\n\n\n\nIn the last exercise, you simulated 10 separate coin flips, each with a 30% chance of heads. Thus, with rbinom(10, 1, .3) you ended up with 10 outcomes that were either 0 (“tails”) or 1 (“heads”).\nBut by changing the second argument of rbinom() (currently 1), you can flip multiple coins within each draw. Thus, each outcome will end up being a number between 0 and 10, showing the number of flips that were heads in that trial.\n\n# Generate 100 occurrences of flipping 10 coins, each with 30% probability\n\nrbinom(100, 10, p = 0.3)\n\n  [1] 2 4 4 5 3 1 5 4 4 1 6 6 4 2 2 0 3 6 1 1 4 1 3 1 3 4 7 3 0 4 0 3 5 1 4 3 2\n [38] 3 5 1 2 1 2 2 7 3 2 3 1 4 2 3 4 2 1 2 4 3 1 3 1 3 3 4 2 2 1 1 3 1 2 3 2 3\n [75] 1 2 2 0 6 1 2 2 4 5 2 3 5 3 4 4 3 4 3 1 4 3 3 3 2 3\n\n\n\n\n\nIf you flip 10 coins each with a 30% probability of coming up heads, what is the probability exactly 2 of them are heads?\n\nAnswer the above question using the dbinom() function. This function takes almost the same arguments as rbinom(). The second and third arguments are size and prob, but now the first argument is x instead of n. Use x to specify where you want to evaluate the binomial density.\nConfirm your answer using the rbinom() function by creating a simulation of 10,000 trials. Put this all on one line by wrapping the mean() function around the rbinom() function.\n\n\n# Calculate the probability that 2 are heads using dbinom\n\ndbinom(2, 10, .3)\n\n[1] 0.2334744\n\n# Confirm your answer with a simulation using rbinom\n\n#flips <- rbinom(10000, 10, .3)\n\nmean(rbinom(10000, 10, .3) == 2)\n\n[1] 0.2356\n\n\n\n\n\nIf you flip ten coins that each have a 30% probability of heads, what is the probability at least five are heads?\n\nAnswer the above question using the pbinom() function. (Note that you can compute the probability that the number of heads is less than or equal to 4, then take 1 - that probability).\nConfirm your answer with a simulation of 10,000 trials by finding the number of trials that result in 5 or more heads.\n\n\n# Calculate the probability that at least five coins are heads\n\n1 - pbinom(4, 10, .3)\n\n[1] 0.1502683\n\n# Confirm your answer with a simulation of 10,000 trials\n\nmean(rbinom(10000, 10, .3) >=  5)\n\n[1] 0.1484\n\n\n\n\n\nIn the last exercise you tried flipping ten coins with a 30% probability of heads to find the probability at least five are heads. You found that the exact answer was 1 - pbinom(4, 10, .3) = 0.1502683, then confirmed with 10,000 simulated trials.\n\nDid you need all 10,000 trials to get an accurate answer? Would your answer have been more accurate with more trials?\n\n\n# Here is how you computed the answer in the last problem\nmean(rbinom(10000, 10, .3) >= 5)\n\n[1] 0.1515\n\n# Try now with 100, 1000, 10,000, and 100,000 trials\n\nmean(rbinom(100, 10, .3) >= 5)\n\n[1] 0.18\n\nmean(rbinom(1000, 10, .3) >= 5)\n\n[1] 0.154\n\nmean(rbinom(10000, 10, .3) >= 5)\n\n[1] 0.1523\n\nmean(rbinom(100000, 10, .3) >= 5)\n\n[1] 0.14892"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html",
    "href": "posts/cbk_data/R/kenya_income_exp.html",
    "title": "Kenya Government Income & Expenditure from 2000 to Date",
    "section": "",
    "text": "Kenya Government Income & Expenditure from 2000 to Date\n\noptions(scipen = 999)\n\nexp_rev <- c(\"totalexpenditure\", \"totalrevenue\")\ngok_earnings_exp[, (exp_rev) := lapply(.SD, function(x) x/10000), .SDcols = exp_rev]\ngok_earnings_exp <- gok_earnings_exp[year < current_year]\nexp_plot <- plot_compare(df = gok_earnings_exp, \n                         id_vars = c(\"year\"),\n                         compare_vars = c(\"totalexpenditure\", \"totalrevenue\"),\n                         x_val = year,\n                         col_val = variable,\n                         y_val = sum_val,\n                         xlab = \"Year\",\n                         ylab = \"Kenya Shillings(Billions)\",\n                         title_lab = \"\")   \n\n[1] 2\n\ngirafe( ggobj =  exp_plot, pointsize =10)"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#difference",
    "href": "posts/cbk_data/R/kenya_income_exp.html#difference",
    "title": "Kenya Government Income & Expenditure from 2000 to 2022",
    "section": "% difference",
    "text": "% difference"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#what-type-of-expenditure-is-rising",
    "href": "posts/cbk_data/R/kenya_income_exp.html#what-type-of-expenditure-is-rising",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "What type of expenditure is rising",
    "text": "What type of expenditure is rising"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to2023",
    "href": "posts/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to2023",
    "title": "Kenya Government Income & Expenditure from 2000 to 2022",
    "section": "Income & Expenditure from 2000 to2023",
    "text": "Income & Expenditure from 2000 to2023\n\n\n[1] 2"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to-2023",
    "href": "posts/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to-2023",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "Income & Expenditure from 2000 to 2023",
    "text": "Income & Expenditure from 2000 to 2023\nA short blog on Kenyan Government income and expenditure. Due to the current discussion in Kenya. Decided to have a quick look of the Kenyan expenditure & income. The total expenditure is growing at a faster pace than the total revenue. This means that the government is spending more money than it is taking in.\nThere are a number of factors that could be contributing to this trend. One factor is the growth of the economy. As the economy grows, the government needs to spend more money on things like infrastructure, education, and healthcare. Another factor is the growth of the population. As the population grows, the government needs to spend more money on things like social welfare programs and security."
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#what-sub-type-of-reccurrent-expenditure-is-causing-the-rise",
    "href": "posts/cbk_data/R/kenya_income_exp.html#what-sub-type-of-reccurrent-expenditure-is-causing-the-rise",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "What Sub type of reccurrent expenditure is causing the rise",
    "text": "What Sub type of reccurrent expenditure is causing the rise"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#domestic-debt-composition",
    "href": "posts/cbk_data/R/kenya_income_exp.html#domestic-debt-composition",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "Domestic debt composition",
    "text": "Domestic debt composition"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to-2022",
    "href": "posts/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to-2022",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "Income & Expenditure from 2000 to 2022",
    "text": "Income & Expenditure from 2000 to 2022\nDecided to have a quick look of the Kenyan government expenditure & income due to the current cash crunch\nThere are a number of factors that could be contributing to this trend. One factor is the growth of the economy. As the economy grows, the government needs to spend more money on things like infrastructure, education, and healthcare. Another factor is the growth of the population. As the population grows, the government needs to spend more money on things like social welfare programs and security."
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html",
    "href": "datacamp/introduction_python/introduction_python.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python Basics\nThe Python Interface Hit Run Code to run your first Python code with Datacamp and see the output!\nNotice the script.py window; this is where you can type Python code to solve exercises. You can hit Run Code and Submit Answer as often as you want. If you’re stuck, you can click Get Hint, and ultimately Get Solution.\nYou can also use the IPython Shell interactively by typing commands and hitting Enter. Here, your code will not be checked for correctness so it is a great way to experiment."
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#any-comments",
    "href": "datacamp/introduction_python/introduction_python.html#any-comments",
    "title": "Introduction to Python",
    "section": "Any comments?",
    "text": "Any comments?\nYou can also add comments to your Python scripts. Comments are important to make sure that you and others can understand what your code is about and do not run as Python code.\nThey start with # tag. See the comment in the editor, # Division; now it’s your turn to add a comment!\n\n# Division\nprint(5 / 8)\n\n0.625\n\n# Addition\nprint(7 + 10)\n\n17"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#python-as-a-calculator",
    "href": "datacamp/introduction_python/introduction_python.html#python-as-a-calculator",
    "title": "Introduction to Python",
    "section": "Python as a calculator",
    "text": "Python as a calculator\nPython is perfectly suited to do basic calculations. It can do addition, subtraction, multiplication and division.\nThe code in the script gives some examples.\nNow it’s your turn to practice!\n\n# Addition, subtraction\nprint(5 + 5)\n\n10\n\nprint(5 - 5)\n\n0\n\n# Multiplication, division, modulo, and exponentiation\nprint(3 * 5)\n\n15\n\nprint(10 / 2)\n\n5.0\n\nprint(18 % 7)\n\n4\n\nprint(4 ** 2)\n\n16\n\n# How much is your $100 worth after 7 years?\nprint(100*1.1**7)\n\n194.87171000000012"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#variable-assignment",
    "href": "datacamp/introduction_python/introduction_python.html#variable-assignment",
    "title": "Introduction to Python",
    "section": "Variable Assignment",
    "text": "Variable Assignment\nIn Python, a variable allows you to refer to a value with a name. To create a variable x with a value of 5, you use =, like this example:\nx = 5 You can now use the name of this variable, x, instead of the actual value, 5.\nRemember, = in Python means assignment, it doesn’t test equality!\n\n#creating saving variable\nsavings=100\nprint(savings)\n\n100\n\n#checking out the type\ntype(savings)\n\n<class 'int'>"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#calculations-with-variables",
    "href": "datacamp/introduction_python/introduction_python.html#calculations-with-variables",
    "title": "Introduction to Python",
    "section": "Calculations with variables",
    "text": "Calculations with variables\nYou’ve now created a savings variable, so let’s start saving!\nInstead of calculating with the actual values, you can use variables instead. The savings variable you created in the previous exercise with a value of 100 is available to you.\nHow much money would you have saved four months from now, if you saved $10 each month?\n\n#creating variables\nsavings=100\ngrowth_multiplier=1.1\n#creating result\nresult=savings*growth_multiplier**7\n\nprint(result)\n\n194.87171000000012"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#other-variable-types",
    "href": "datacamp/introduction_python/introduction_python.html#other-variable-types",
    "title": "Introduction to Python",
    "section": "Other variable types",
    "text": "Other variable types\nIn the previous exercise, you worked with the integer Python data type:\nint, or integer: a number without a fractional part. savings, with the value 100, is an example of an integer. Next to numerical data types, there are three other very common data types:\nfloat, or floating point: a number that has both an integer and fractional part, separated by a point. 1.1, is an example of a float. str, or string: a type to represent text. You can use single or double quotes to build a string. bool, or boolean: a type to represent logical values. It can only be True or False (the capitalization is important!).\n\n#creating string\ndesc=\"compound interest\"\nprint(desc)\n\ncompound interest\n\n#creating boolean\nprofitable=True\nprint(profitable)\n\nTrue"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#operations-with-other-types",
    "href": "datacamp/introduction_python/introduction_python.html#operations-with-other-types",
    "title": "Introduction to Python",
    "section": "Operations with other types",
    "text": "Operations with other types\nHugo mentioned that different types behave differently in Python.\nWhen you sum two strings, for example, you’ll get different behavior than when you sum two integers or two booleans.\nIn the script some variables with different types have already been created. It’s up to you to use them.\n\nsavings = 100\ngrowth_multiplier = 1.1\ndesc = \"compound interest\"\n\n# Assign product of savings and growth_multiplier to year1\nyear1 = savings * growth_multiplier\n\n# Print the type of year1\nprint(type(year1))\n\n<class 'float'>\n\n# Assign sum of desc and desc to doubledesc\ndoubledesc = desc + desc\n\n# Print out doubledesc\nprint(doubledesc)\n\ncompound interestcompound interest"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#type-conversion",
    "href": "datacamp/introduction_python/introduction_python.html#type-conversion",
    "title": "Introduction to Python",
    "section": "Type conversion",
    "text": "Type conversion\nUsing the + operator to paste together two strings can be very useful in building custom messages.\nSuppose, for example, that you’ve calculated your savings want to summarize the results in a string.\nTo do this, you’ll need to explicitly convert the types of your variables. More specifically, you’ll need str(), to convert a value into a string. str(savings), for example, will convert the integer savings to a string.\nSimilar functions such as int(), float() and bool() will help you convert Python values into any type.\n\n# Definition of savings and result\nsavings = 100\nresult = 100 * 1.10 ** 7\n\n# Fix the printout\nprint(\"I started with $\" + str(savings) + \" and now have $\" + str(result) + \". Awesome!\")\n\nI started with $100 and now have $194.87171000000012. Awesome!\n\n\n# Definition of pi_string\npi_string = \"3.1415926\"\n\n# Convert pi_string into float: pi_float\npi_float = float(pi_string)"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#create-a-list",
    "href": "datacamp/introduction_python/introduction_python.html#create-a-list",
    "title": "Introduction to Python",
    "section": "Create a list",
    "text": "Create a list\nAs opposed to int, bool etc., a list is a compound data type; you can group values together:\na = “is” b = “nice” my_list = [“my”, “list”, a, b] After measuring the height of your family, you decide to collect some information on the house you’re living in. The areas of the different parts of your house are stored in separate variables for now, as shown in the script.\n\n# Area variables (in square meters)\nhall = 11.25\nkit = 18.0\nliv = 20.0\nbed = 10.75\nbath = 9.50\n\n# Create list areas\nareas = [hall, kit, liv, bed, bath]\n\n# Print areas\nprint(areas)\n\n[11.25, 18.0, 20.0, 10.75, 9.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#create-list-with-different-types",
    "href": "datacamp/introduction_python/introduction_python.html#create-list-with-different-types",
    "title": "Introduction to Python",
    "section": "Create list with different types",
    "text": "Create list with different types\nA list can contain any Python type. Although it’s not really common, a list can also contain a mix of Python types including strings, floats, booleans, etc.\nThe printout of the previous exercise wasn’t really satisfying. It’s just a list of numbers representing the areas, but you can’t tell which area corresponds to which part of your house.\nThe code in the editor is the start of a solution. For some of the areas, the name of the corresponding room is already placed in front. Pay attention here! “bathroom” is a string, while bath is a variable that represents the float 9.50 you specified earlier.\n\n# area variables (in square meters)\nhall = 11.25\nkit = 18.0\nliv = 20.0\nbed = 10.75\nbath = 9.50\n\n# Adapt list areas\nareas = [\"hallway\", hall, \"kitchen\", kit, \"living room\", liv, \"bedroom\", bed, \"bathroom\", bath]\n\n# Print areas\nprint(areas)\n\n['hallway', 11.25, 'kitchen', 18.0, 'living room', 20.0, 'bedroom', 10.75, 'bathroom', 9.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#list-of-lists",
    "href": "datacamp/introduction_python/introduction_python.html#list-of-lists",
    "title": "Introduction to Python",
    "section": "List of lists",
    "text": "List of lists\nAs a data scientist, you’ll often be dealing with a lot of data, and it will make sense to group some of this data.\nInstead of creating a flat list containing strings and floats, representing the names and areas of the rooms in your house, you can create a list of lists. The script in the editor can already give you an idea.\nDon’t get confused here: “hallway” is a string, while hall is a variable that represents the float 11.25 you specified earlier.\n\n# area variables (in square meters)\nhall = 11.25\nkit = 18.0\nliv = 20.0\nbed = 10.75\nbath = 9.50\n\n# house information as list of lists\nhouse = [[\"hallway\", hall],\n         [\"kitchen\", kit],\n         [\"living room\", liv],\n         [\"bedroom\", bed],\n         [\"bathroom\", bath]]\n\n# Print out house\nprint(house)\n\n[['hallway', 11.25], ['kitchen', 18.0], ['living room', 20.0], ['bedroom', 10.75], ['bathroom', 9.5]]\n\n# Print out the type of house\nprint(type(house))\n\n<class 'list'>"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#subset-and-conquer",
    "href": "datacamp/introduction_python/introduction_python.html#subset-and-conquer",
    "title": "Introduction to Python",
    "section": "Subset and conquer",
    "text": "Subset and conquer\nSubsetting Python lists is a piece of cake. Take the code sample below, which creates a list x and then selects “b” from it. Remember that this is the second element, so it has index 1. You can also use negative indexing.\nx = [“a”, “b”, “c”, “d”] x[1] x[-3] # same result! Remember the areas list from before, containing both strings and floats? Its definition is already in the script. Can you add the correct code to do some Python subsetting?\n\n# Create the areas list\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"living room\", 20.0, \"bedroom\", 10.75, \"bathroom\", 9.50]\n\n# Print out second element from areas\nprint(areas[1])\n\n11.25\n\n# Print out last element from areas\nprint(areas[-1])\n\n9.5\n\n# Print out the area of the living room\nprint(areas[5])\n\n20.0"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#subset-and-calculate",
    "href": "datacamp/introduction_python/introduction_python.html#subset-and-calculate",
    "title": "Introduction to Python",
    "section": "Subset and calculate",
    "text": "Subset and calculate\nAfter you’ve extracted values from a list, you can use them to perform additional calculations. Take this example, where the second and fourth element of a list x are extracted. The strings that result are pasted together using the + operator:\n\n# Create the areas list\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"living room\", 20.0, \"bedroom\", 10.75, \"bathroom\", 9.50]\n\n# Sum of kitchen and bedroom area: eat_sleep_area\neat_sleep_area=areas[3]+ areas[7]\n\n# Print the variable eat_sleep_area\nprint(eat_sleep_area)\n\n28.75"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#slicing-and-dicing",
    "href": "datacamp/introduction_python/introduction_python.html#slicing-and-dicing",
    "title": "Introduction to Python",
    "section": "Slicing and dicing",
    "text": "Slicing and dicing\nSelecting single values from a list is just one part of the story. It’s also possible to slice your list, which means selecting multiple elements from your list. Use the following syntax:\nmy_list[start:end] The start index will be included, while the end index is not.\nThe code sample below shows an example. A list with “b” and “c”, corresponding to indexes 1 and 2, are selected from a list x:\nx = [“a”, “b”, “c”, “d”] x[1:3] The elements with index 1 and 2 are included, while the element with index 3 is not.\n\n# Create the areas list\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"living room\", 20.0, \"bedroom\", 10.75, \"bathroom\", 9.50]\n\n# Use slicing to create downstairs\ndownstairs = areas[0:6]\n\n# Use slicing to create upstairs\nupstairs = areas[6:10]\n\n# Print out downstairs and upstairs\nprint(downstairs)\n\n['hallway', 11.25, 'kitchen', 18.0, 'living room', 20.0]\n\nprint(upstairs)\n\n['bedroom', 10.75, 'bathroom', 9.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#slicing-and-dicing-2",
    "href": "datacamp/introduction_python/introduction_python.html#slicing-and-dicing-2",
    "title": "Introduction to Python",
    "section": "Slicing and dicing (2)",
    "text": "Slicing and dicing (2)\nIn the video, Hugo first discussed the syntax where you specify both where to begin and end the slice of your list:\nmy_list[begin:end] However, it’s also possible not to specify these indexes. If you don’t specify the begin index, Python figures out that you want to start your slice at the beginning of your list. If you don’t specify the end index, the slice will go all the way to the last element of your list. To experiment with this, try the following commands in the IPython Shell:\nx = [“a”, “b”, “c”, “d”] x[:2] x[2:] x[:]\n\n# Create the areas list\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"living room\", 20.0, \"bedroom\", 10.75, \"bathroom\", 9.50]\n\ndownstairs = areas[:6]\nupstairs = areas[6:]\nprint(downstairs)\n\n['hallway', 11.25, 'kitchen', 18.0, 'living room', 20.0]\n\nprint(upstairs)\n\n['bedroom', 10.75, 'bathroom', 9.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#replace-list-elements",
    "href": "datacamp/introduction_python/introduction_python.html#replace-list-elements",
    "title": "Introduction to Python",
    "section": "Replace list elements",
    "text": "Replace list elements\nReplacing list elements is pretty easy. Simply subset the list and assign new values to the subset. You can select single elements or you can change entire list slices at once.\nUse the IPython Shell to experiment with the commands below. Can you tell what’s happening and why?\nx = [“a”, “b”, “c”, “d”] x[1] = “r” x[2:] = [“s”, “t”] For this and the following exercises, you’ll continue working on the areas list that contains the names and areas of different rooms in a house.\n\n#experimenting\nx = [\"a\", \"b\", \"c\", \"d\"]\nx[1] = \"r\"\nx[2:]=[\"s\",\"t\"]\nprint(x)\n\n['a', 'r', 's', 't']\n\n#original list\n# Create the areas list\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"living room\", 20.0, \"bedroom\", 10.75, \"bathroom\", 9.50]\n#updating the area of the bathroom\nareas[-1]=10.5\n\n#changing living room\nareas[4]= \"chill zone\"\nprint(areas)\n\n['hallway', 11.25, 'kitchen', 18.0, 'chill zone', 20.0, 'bedroom', 10.75, 'bathroom', 10.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#extend-a-list",
    "href": "datacamp/introduction_python/introduction_python.html#extend-a-list",
    "title": "Introduction to Python",
    "section": "Extend a list",
    "text": "Extend a list\nIf you can change elements in a list, you sure want to be able to add elements to it, right? You can use the + operator:\nx = [“a”, “b”, “c”, “d”] y = x + [“e”, “f”] You just won the lottery, awesome! You decide to build a poolhouse and a garage. Can you add the information to the areas list?\n\n# Create the areas list (updated version)\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"chill zone\", 20.0,\n         \"bedroom\", 10.75, \"bathroom\", 10.50]\n\n# Add poolhouse data to areas, new list is areas_1\nareas_1 = areas + [\"poolhouse\", 24.5]\nprint(areas_1)\n\n['hallway', 11.25, 'kitchen', 18.0, 'chill zone', 20.0, 'bedroom', 10.75, 'bathroom', 10.5, 'poolhouse', 24.5]\n\n# Add garage data to areas_1, new list is areas_2\nareas_2 = areas_1 + [\"garage\", 15.45]\n\nDelete list elements Finally, you can also remove elements from your list. You can do this with the del statement:\nx = [“a”, “b”, “c”, “d”] del(x[1]) Pay attention here: as soon as you remove an element from a list, the indexes of the elements that come after the deleted element all change!\nThe updated and extended version of areas that you’ve built in the previous exercises is coded below. You can copy and paste this into the IPython Shell to play around with the result.\n\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0,\n        \"chill zone\", 20.0, \"bedroom\", 10.75,\n         \"bathroom\", 10.50, \"poolhouse\", 24.5,\n         \"garage\", 15.45]\n         \nprint(areas)\n\n['hallway', 11.25, 'kitchen', 18.0, 'chill zone', 20.0, 'bedroom', 10.75, 'bathroom', 10.5, 'poolhouse', 24.5, 'garage', 15.45]\n\n\nThere was a mistake! The amount you won with the lottery is not that big after all and it looks like the poolhouse isn’t going to happen. You decide to remove the corresponding string and float from the areas list.\nThe ; sign is used to place commands on the same line. The following two code chunks are equivalent:\nWhich of the code chunks will do the job for us?\n\ndel(areas[-4:-2])\nprint(areas)\n\n['hallway', 11.25, 'kitchen', 18.0, 'chill zone', 20.0, 'bedroom', 10.75, 'bathroom', 10.5, 'garage', 15.45]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#inner-workings-of-lists",
    "href": "datacamp/introduction_python/introduction_python.html#inner-workings-of-lists",
    "title": "Introduction to Python",
    "section": "Inner workings of lists",
    "text": "Inner workings of lists\nAt the end of the video, Hugo explained how Python lists work behind the scenes. In this exercise you’ll get some hands-on experience with this.\nThe Python code in the script already creates a list with the name areas and a copy named areas_copy. Next, the first element in the areas_copy list is changed and the areas list is printed out. If you hit Run Code you’ll see that, although you’ve changed areas_copy, the change also takes effect in the areas list. That’s because areas and areas_copy point to the same list.\nIf you want to prevent changes in areas_copy from also taking effect in areas, you’ll have to do a more explicit copy of the areas list. You can do this with list() or by using [:].\n\n# Create list areas\nareas = [11.25, 18.0, 20.0, 10.75, 9.50]\n\n# Create areas_copy\nareas_copy = areas[:]\n# Change areas_copy\nareas_copy[0] = 5.0\n\n# Print areas\nprint(areas)\n\n[11.25, 18.0, 20.0, 10.75, 9.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#familiar-functions",
    "href": "datacamp/introduction_python/introduction_python.html#familiar-functions",
    "title": "Introduction to Python",
    "section": "Familiar functions",
    "text": "Familiar functions\nOut of the box, Python offers a bunch of built-in functions to make your life as a data scientist easier. You already know two such functions: print() and type(). You’ve also used the functions str(), int(), bool() and float() to switch between data types. These are built-in functions as well.\nCalling a function is easy. To get the type of 3.0 and store the output as a new variable, result, you can use the following:\nresult = type(3.0) The general recipe for calling functions and saving the result to a variable is thus:\noutput = function_name(input)\n\n# Create variables var1 and var2\nvar1 = [1, 2, 3, 4]\nvar2 = True\n\n# Print out type of var1\nprint(type(var1))\n\n<class 'list'>\n\n# Print out length of var1\nprint(len(var1))\n\n4\n\n\n# Convert var2 to an integer: out2\nout2 = int(var2)"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#help",
    "href": "datacamp/introduction_python/introduction_python.html#help",
    "title": "Introduction to Python",
    "section": "Help!",
    "text": "Help!\nMaybe you already know the name of a Python function, but you still have to figure out how to use it. Ironically, you have to ask for information about a function with another function: help(). In IPython specifically, you can also use ? before the function name.\nTo get help on the max() function, for example, you can use one of these calls:\n\nhelp(max)\n\nHelp on built-in function max in module builtins:\n\nmax(...)\n    max(iterable, *[, default=obj, key=func]) -> value\n    max(arg1, arg2, *args, *[, key=func]) -> value\n    \n    With a single iterable argument, return its biggest item. The\n    default keyword-only argument specifies an object to return if\n    the provided iterable is empty.\n    With two or more arguments, return the largest argument."
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#multiple-arguments",
    "href": "datacamp/introduction_python/introduction_python.html#multiple-arguments",
    "title": "Introduction to Python",
    "section": "Multiple arguments",
    "text": "Multiple arguments\nIn the previous exercise, you identified optional arguments by viewing the documentation with help(). You’ll now apply this to change the behavior of the sorted() function.\nHave a look at the documentation of sorted() by typing help(sorted) in the IPython Shell.\nYou’ll see that sorted() takes three arguments: iterable, key, and reverse.\nkey=None means that if you don’t specify the key argument, it will be None. reverse=False means that if you don’t specify the reverse argument, it will be False, by default.\nIn this exercise, you’ll only have to specify iterable and reverse, not key. The first input you pass to sorted() will be matched to the iterable argument, but what about the second input? To tell Python you want to specify reverse without changing anything about key, you can use = to assign it a new value:\nsorted(____, reverse=____) Two lists have been created for you. Can you paste them together and sort them in descending order?\nNote: For now, we can understand an iterable as being any collection of objects, e.g., a List.\n\n# Create lists first and second\nfirst = [11.25, 18.0, 20.0]\nsecond = [10.75, 9.50]\n\n# Paste together first and second: full\nfull=first + second\nprint(full)\n\n[11.25, 18.0, 20.0, 10.75, 9.5]\n\n# Sort full in descending order: full_sorted\nfull_sorted=(sorted(full,reverse=True))\n\n# Print out full_sorted\nprint(full_sorted)\n\n[20.0, 18.0, 11.25, 10.75, 9.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#string-methods",
    "href": "datacamp/introduction_python/introduction_python.html#string-methods",
    "title": "Introduction to Python",
    "section": "String Methods",
    "text": "String Methods\nStrings come with a bunch of methods. Follow the instructions closely to discover some of them. If you want to discover them in more detail, you can always type help(str) in the IPython Shell.\nA string place has already been created for you to experiment with.\n\n# string to experiment with: place\nplace = \"poolhouse\"\n\n# Use upper() on place: place_up\nplace_up = place.upper()\n\n# Print out place and place_up\nprint(place)\n\npoolhouse\n\nprint(place_up)\n\nPOOLHOUSE\n\n# Print out the number of o's in place\nprint(place.count('o'))\n\n3"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#list-methods",
    "href": "datacamp/introduction_python/introduction_python.html#list-methods",
    "title": "Introduction to Python",
    "section": "List Methods",
    "text": "List Methods\nStrings are not the only Python types that have methods associated with them. Lists, floats, integers and booleans are also types that come packaged with a bunch of useful methods. In this exercise, you’ll be experimenting with:\n\nindex(), to get the index of the first element of a list that matches its input and\ncount(), to get the number of times an element appears in a list. You’ll be working on the list with the area of different parts of a house: areas.\nappend(), that adds an element to the list it is called on,\nremove(), that removes the first element of a list that matches the input, and\nreverse(), that reverses the order of the elements in the list it is called on.\n\n\n# Create list areas\nareas = [11.25, 18.0, 20.0, 10.75, 9.50]\n\n# Print out the index of the element 20.0\nprint(areas.index(20.0))\n\n2\n\n# Print out how often 9.50 appears in areas\nprint(areas.count(9.50))\n\n1\n\n# Create list areas\nareas = [11.25, 18.0, 20.0, 10.75, 9.50]\n\n# Use append twice to add poolhouse and garage size\nareas.append(24.5)\nareas.append(15.45)\n\n# Print out areas\nprint(areas)\n\n[11.25, 18.0, 20.0, 10.75, 9.5, 24.5, 15.45]\n\n# Reverse the orders of the elements in areas\nareas.reverse()\n\n# Print out areas\nprint(areas)\n\n[15.45, 24.5, 9.5, 10.75, 20.0, 18.0, 11.25]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#import-package",
    "href": "datacamp/introduction_python/introduction_python.html#import-package",
    "title": "Introduction to Python",
    "section": "Import package",
    "text": "Import package\nAs a data scientist, some notions of geometry never hurt. Let’s refresh some of the basics.\nFor a fancy clustering algorithm, you want to find the circumference, , and area, , of a circle. When the radius of the circle is r, you can calculate and as:\nIn Python, the symbol for exponentiation is . This operator raises the number to its left to the power of the number to its right. For example 34 is 3 to the power of 4 and will give 81.\nTo use the constant pi, you’ll need the math package. A variable r is already coded in the script. Fill in the code to calculate C and A and see how the print() functions create some nice printouts.\n\n# Definition of radius\nr = 0.43\n\n# Import the math package\nimport math\n\n# Calculate C\nC = 2 * r * math.pi\n\n# Calculate A\nA = math.pi * r ** 2\n\n# Build printout\nprint(\"Circumference: \" + str(C))\n\nCircumference: 2.701769682087222\n\nprint(\"Area: \" + str(A))\n\nArea: 0.5808804816487527"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#selective-import",
    "href": "datacamp/introduction_python/introduction_python.html#selective-import",
    "title": "Introduction to Python",
    "section": "Selective import",
    "text": "Selective import\nGeneral imports, like import math, make all functionality from the math package available to you. However, if you decide to only use a specific part of a package, you can always make your import more selective:\nfrom math import pi Let’s say the Moon’s orbit around planet Earth is a perfect circle, with a radius r (in km) that is defined in the script.\n\n# Definition of radius\nr = 192500\n\n# Import radians function of math package\nfrom math import radians\n\n# Travel distance of Moon over 12 degrees. Store in dist.\n\ndist = r*  radians(12)\n\n# Print out dist\nprint(dist)\n\n40317.10572106901"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#different-ways-of-importing",
    "href": "datacamp/introduction_python/introduction_python.html#different-ways-of-importing",
    "title": "Introduction to Python",
    "section": "Different ways of importing",
    "text": "Different ways of importing\nThere are several ways to import packages and modules into Python. Depending on the import call, you’ll have to use different Python code.\nSuppose you want to use the function inv(), which is in the linalg subpackage of the scipy package. You want to be able to use this function as follows:\nmy_inv([[1,2], [3,4]]) Which import statement will you need in order to run the above code without an error?\n\nfrom scipy.linalg import inv as my_inv\n\nmy_inv([[1,2], [3,4]])\n\narray([[-2. ,  1. ],\n       [ 1.5, -0.5]])"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#your-first-numpy-array",
    "href": "datacamp/introduction_python/introduction_python.html#your-first-numpy-array",
    "title": "Introduction to Python",
    "section": "Your First NumPy Array",
    "text": "Your First NumPy Array\nIn this chapter, we’re going to dive into the world of baseball. Along the way, you’ll get comfortable with the basics of numpy, a powerful package to do data science.\nA list baseball has already been defined in the Python script, representing the height of some baseball players in centimeters. Can you add some code here and there to create a numpy array from it?\n\n# Create list baseball\nbaseball = [180, 215, 210, 210, 188, 176, 209, 200]\n\n# Import the numpy package as np\nimport numpy as np\n\n# Create a NumPy array from baseball: np_baseball\nnp_baseball = np.array(baseball)\nnp_baseball\n\narray([180, 215, 210, 210, 188, 176, 209, 200])\n\n# Print out type of np_baseball\nprint(type(np_baseball))\n\n<class 'numpy.ndarray'>"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#baseball-players-height",
    "href": "datacamp/introduction_python/introduction_python.html#baseball-players-height",
    "title": "Introduction to Python",
    "section": "Baseball players’ height",
    "text": "Baseball players’ height\nYou are a huge baseball fan. You decide to call the MLB (Major League Baseball) and ask around for some more statistics on the height of the main players. They pass along data on more than a thousand players, which is stored as a regular Python list: height_in. The height is expressed in inches. Can you make a numpy array out of it and convert the units to meters?\nheight_in is already available and the numpy package is loaded, so you can start straight away (Source: stat.ucla.edu).\n\n# height is available as a regular list\n\n# Import numpy\nimport numpy as np\n\nheight_in = np.array([74, 74, 72, 72, 73, 69, 69, 71, 76, 71])\n# Create a numpy array from height_in: np_height_in\nnp_height_in=np.array(height_in)\n# Print out np_height_in\nprint(np_height_in)\n\n[74 74 72 72 73 69 69 71 76 71]\n\n# Convert np_height_in to m: np_height_m\nnp_height_m = np_height_in*0.0254\n\n# Print np_height_m\nprint(np_height_m)\n\n[1.8796 1.8796 1.8288 1.8288 1.8542 1.7526 1.7526 1.8034 1.9304 1.8034]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#baseball-players-bmi",
    "href": "datacamp/introduction_python/introduction_python.html#baseball-players-bmi",
    "title": "Introduction to Python",
    "section": "Baseball player’s BMI",
    "text": "Baseball player’s BMI\nThe MLB also offers to let you analyze their weight data. Again, both are available as regular Python lists: height_in and weight_lb. height_in is in inches and weight_lb is in pounds.\nIt’s now possible to calculate the BMI of each baseball player. Python code to convert height_in to a numpy array with the correct units is already available in the workspace. Follow the instructions step by step and finish the game! height_in and weight_lb are available as regular lists.\n\n# height_in and weight_lb are available as regular lists\n\n# Import numpy\nimport numpy as np\n\n# Create array from height_in with metric units: np_height_m\nnp_height_m = np.array(height_in) * 0.0254\nweight_lb = np.array([180, 215, 210, 210, 188, 176, 209, 200, 231, 180])\n# Create array from weight_lb with metric units: np_weight_kg\nnp_weight_kg = np.array(weight_lb) * 0.453592\n\n# Calculate the BMI: bmi\nbmi = np_weight_kg / np_height_m ** 2\n\n# Print out bmi\nprint(bmi)\n\n[23.11037639 27.60406069 28.48080465 28.48080465 24.80333518 25.99036864\n 30.86356276 27.89402921 28.11789135 25.10462629]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#lightweight-baseball-players",
    "href": "datacamp/introduction_python/introduction_python.html#lightweight-baseball-players",
    "title": "Introduction to Python",
    "section": "Lightweight baseball players",
    "text": "Lightweight baseball players\nTo subset both regular Python lists and numpy arrays, you can use square brackets:\nx = [4 , 9 , 6, 3, 1] x[1] import numpy as np y = np.array(x) y[1] For numpy specifically, you can also use boolean numpy arrays:\nhigh = y > 5 y[high] The code that calculates the BMI of all baseball players is already included. Follow the instructions and reveal interesting things from the data! height_in and weight_lb are available as regular lists.\n\n# height and weight are available as a regular lists\n\n# Import numpy\nimport numpy as np\n\n# Calculate the BMI: bmi\nnp_height_m = np.array(height_in) * 0.0254\nnp_weight_kg = np.array(weight_lb) * 0.453592\nbmi = np_weight_kg / np_height_m ** 2\n\n# Create the light array\n\nlight = bmi < 21\n# Print out light\nprint(light)\n\n[False False False False False False False False False False]\n\n# Print out BMIs of all baseball players whose BMI is below 21\nprint(bmi[light])\n\n[]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#numpy-side-effects",
    "href": "datacamp/introduction_python/introduction_python.html#numpy-side-effects",
    "title": "Introduction to Python",
    "section": "NumPy Side Effects",
    "text": "NumPy Side Effects\nAs Hugo explained before, numpy is great for doing vector arithmetic. If you compare its functionality with regular Python lists, however, some things have changed.\nFirst of all, numpy arrays cannot contain elements with different types. If you try to build such a list, some of the elements’ types are changed to end up with a homogeneous list. This is known as type coercion.\nSecond, the typical arithmetic operators, such as +, -, * and / have a different meaning for regular Python lists and numpy arrays.\nHave a look at this line of code:\nnp.array([True, 1, 2]) + np.array([3, 4, False]) Can you tell which code chunk builds the exact same Python object? The numpy package is already imported as np, so you can start experimenting in the IPython Shell straight away!\n\nprint(np.array([True, 1, 2]) + np.array([3, 4, False]))\n\n[4 5 2]\n\nprint(np.array([4, 3, 0]) + np.array([0, 2, 2]))\n\n[4 5 2]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#subsetting-numpy-arrays",
    "href": "datacamp/introduction_python/introduction_python.html#subsetting-numpy-arrays",
    "title": "Introduction to Python",
    "section": "Subsetting NumPy Arrays",
    "text": "Subsetting NumPy Arrays\nYou’ve seen it with your own eyes: Python lists and numpy arrays sometimes behave differently. Luckily, there are still certainties in this world. For example, subsetting (using the square bracket notation on lists or arrays) works exactly the same. To see this for yourself, try the following lines of code in the IPython Shell:\nx = [“a”, “b”, “c”] x[1]\nnp_x = np.array(x) np_x[1] The script in the editor already contains code that imports numpy as np, and stores both the height and weight of the MLB players as numpy arrays. height_in and weight_lb are available as regular lists.\n\n# height and weight are available as a regular lists\n\n# Import numpy\nimport numpy as np\n\n# Store weight and height lists as numpy arrays\nnp_weight_lb = np.array(weight_lb)\nnp_height_in = np.array(height_in)\n\n# Print out the weight at index 5\n\nprint(np_weight_lb[5])\n\n176\n\n# Print out sub-array of np_height_in: index 1 up to and including index 3\n\nprint(np_height_in[1:3])\n\n[74 72]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#your-first-2d-numpy-array",
    "href": "datacamp/introduction_python/introduction_python.html#your-first-2d-numpy-array",
    "title": "Introduction to Python",
    "section": "Your First 2D NumPy Array",
    "text": "Your First 2D NumPy Array\nBefore working on the actual MLB data, let’s try to create a 2D numpy array from a small list of lists.\nIn this exercise, baseball is a list of lists. The main list contains 4 elements. Each of these elements is a list containing the height and the weight of 4 baseball players, in this order. baseball is already coded for you in the script.\n\n# Create baseball, a list of lists\nbaseball = [[180, 78.4],\n            [215, 102.7],\n            [210, 98.5],\n            [188, 75.2]]\n\n# Import numpy\nimport numpy as np\n\n# Create a 2D numpy array from baseball: np_baseball\nnp_baseball = np.array(baseball)\n\n# Print out the type of np_baseball\nprint(type(np_baseball))\n\n<class 'numpy.ndarray'>\n\n# Print out the shape of np_baseball\nprint(np_baseball.shape)\n\n(4, 2)"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#baseball-data-in-2d-form",
    "href": "datacamp/introduction_python/introduction_python.html#baseball-data-in-2d-form",
    "title": "Introduction to Python",
    "section": "Baseball data in 2D form",
    "text": "Baseball data in 2D form\nYou have another look at the MLB data and realize that it makes more sense to restructure all this information in a 2D numpy array. This array should have 1015 rows, corresponding to the 1015 baseball players you have information on, and 2 columns (for height and weight).\nThe MLB was, again, very helpful and passed you the data in a different structure, a Python list of lists. In this list of lists, each sublist represents the height and weight of a single baseball player. The name of this embedded list is baseball.\nCan you store the data as a 2D array to unlock numpy’s extra functionality? baseball is available as a regular list of lists.\n\n# baseball is available as a regular list of lists\n\n# Import numpy package\nimport numpy as np\n\n# Create a 2D numpy array from baseball: np_baseball\nnp_baseball = np.array(baseball)\n\n# Print out the shape of np_baseball\nprint(np_baseball.shape)\n\n(4, 2)"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#subsetting-2d-numpy-arrays",
    "href": "datacamp/introduction_python/introduction_python.html#subsetting-2d-numpy-arrays",
    "title": "Introduction to Python",
    "section": "Subsetting 2D NumPy Arrays",
    "text": "Subsetting 2D NumPy Arrays\nIf your 2D numpy array has a regular structure, i.e. each row and column has a fixed number of values, complicated ways of subsetting become very easy. Have a look at the code below where the elements “a” and “c” are extracted from a list of lists."
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#d-arithmetic",
    "href": "datacamp/introduction_python/introduction_python.html#d-arithmetic",
    "title": "Introduction to Python",
    "section": "2D Arithmetic",
    "text": "2D Arithmetic\nRemember how you calculated the Body Mass Index for all baseball players? numpy was able to perform all calculations element-wise (i.e. element by element). For 2D numpy arrays this isn’t any different! You can combine matrices with single numbers, with vectors, and with other matrices.\nExecute the code below in the IPython shell and see if you understand:\nimport numpy as np np_mat = np.array([[1, 2], [3, 4], [5, 6]]) np_mat * 2 np_mat + np.array([10, 10]) np_mat + np_mat np_baseball is coded for you; it’s again a 2D numpy array with 3 columns representing height (in inches), weight (in pounds) and age (in years). baseball is available as a regular list of lists and updated is available as 2D numpy array.\n\n# baseball is available as a regular list of lists\n# updated is available as 2D numpy array\n\n# Import numpy package\nimport numpy as np\n\n# Create np_baseball (3 cols)\nnp_baseball = np.array(baseball)\n\n# Print out addition of np_baseball and updated\nprint( np_baseball + updated)\n\n# Create numpy array: conversion\nconversion = np.array([0.0254, 0.453592, 1])\n\n# Print out product of np_baseball and conversion\nprint(np_baseball * conversion)"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#average-versus-median",
    "href": "datacamp/introduction_python/introduction_python.html#average-versus-median",
    "title": "Introduction to Python",
    "section": "Average versus median",
    "text": "Average versus median\nYou now know how to use numpy functions to get a better feeling for your data. It basically comes down to importing numpy and then calling several simple functions on the numpy arrays:\nimport numpy as np x = [1, 4, 8, 10, 12] np.mean(x) np.median(x) The baseball data is available as a 2D numpy array with 3 columns (height, weight, age) and 1015 rows. The name of this numpy array is np_baseball. After restructuring the data, however, you notice that some height values are abnormally high. Follow the instructions and discover which summary statistic is best suited if you’re dealing with so-called outliers. np_baseball is available.\n\n# np_baseball is available\n\n# Import numpy\nimport numpy as np\n\n# Create np_height_in from np_baseball\nnp_height_in = np_baseball[:,0]\n\n# Print out the mean of np_height_in\n\nprint(np.mean(np_height_in))\n\n198.25\n\n# Print out the median of np_height_in\nprint(np.median(np_height_in))\n\n199.0"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#explore-the-baseball-data",
    "href": "datacamp/introduction_python/introduction_python.html#explore-the-baseball-data",
    "title": "Introduction to Python",
    "section": "Explore the baseball data",
    "text": "Explore the baseball data\nBecause the mean and median are so far apart, you decide to complain to the MLB. They find the error and send the corrected data over to you. It’s again available as a 2D NumPy array np_baseball, with three columns.\nThe Python script in the editor already includes code to print out informative messages with the different summary statistics. Can you finish the job? np_baseball is available.\n\n# np_baseball is available\n\n# Import numpy\nimport numpy as np\n\n# Print mean height (first column)\navg = np.mean(np_baseball[:,0])\nprint(\"Average: \" + str(avg))\n\nAverage: 198.25\n\n# Print median height. Replace 'None'\nmed = np.median(np_baseball[:,0])\nprint(\"Median: \" + str(med))\n\nMedian: 199.0\n\n# Print out the standard deviation on height. Replace 'None'\nstddev = np.std(np_baseball[:,0])\nprint(\"Standard Deviation: \" + str(stddev))\n\nStandard Deviation: 14.635146053251399\n\n# Print out correlation between first and second column. Replace 'None'\ncorr = np.corrcoef(np_baseball[:, 0], np_baseball[:, 1])\nprint(\"Correlation: \" + str(corr))\n\nCorrelation: [[1.         0.95865738]\n [0.95865738 1.        ]]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#blend-it-all-together",
    "href": "datacamp/introduction_python/introduction_python.html#blend-it-all-together",
    "title": "Introduction to Python",
    "section": "Blend it all together",
    "text": "Blend it all together\nIn the last few exercises you’ve learned everything there is to know about heights and weights of baseball players. Now it’s time to dive into another sport: soccer.\nYou’ve contacted FIFA for some data and they handed you two lists. The lists are the following:\npositions = [‘GK’, ‘M’, ‘A’, ‘D’, …] heights = [191, 184, 185, 180, …] Each element in the lists corresponds to a player. The first list, positions, contains strings representing each player’s position. The possible positions are: ‘GK’ (goalkeeper), ‘M’ (midfield), ‘A’ (attack) and ‘D’ (defense). The second list, heights, contains integers representing the height of the player in cm. The first player in the lists is a goalkeeper and is pretty tall (191 cm).\nYou’re fairly confident that the median height of goalkeepers is higher than that of other players on the soccer field. Some of your friends don’t believe you, so you are determined to show them using the data you received from FIFA and your newly acquired Python skills. heights and positions are available as lists\n\nnp_positions = np.array(['GK', 'M', 'A', 'D', 'M', 'D', 'M', 'M', 'M', 'A'])\nnp_heights = np.array([191, 184, 185, 180, 181, 187, 170, 179, 183, 186])\n\n# Heights of the goalkeepers: gk_heights\ngk_heights = np_heights[np_positions == 'GK']\n\n# Heights of the other players: other_heights\nother_heights = np_heights[np_positions != 'GK']\n\n# Print out the median height of goalkeepers. Replace 'None'\nprint(\"Median height of goalkeepers: \" + str(np.median(gk_heights)))\n\nMedian height of goalkeepers: 191.0\n\n# Print out the median height of other players. Replace 'None'\nprint(\"Median height of other players: \" + str(np.median(other_heights)))\n\nMedian height of other players: 183.0"
  }
]