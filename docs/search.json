[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Personal Blog",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShap Calculation R\n\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting loan defaults\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2020\n\n\nmburu\n\n\n\n\n\n\n  \n\n\n\n\nPredict whether the cancer is benign or malignant\n\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2019\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssociation analysis\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2018\n\n\nMburu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "kenya_population/household_assets_2019census.html",
    "href": "kenya_population/household_assets_2019census.html",
    "title": "Kenya Household Assets",
    "section": "",
    "text": "Kenya selected households assets 2019 census\nI figured that it will be important for me to do this to show why it is impossible for online learning to be adopted in Kenya.\nI used 2019 census data set which can be found here and the counties shape file can be found here\n\n#Packages used\n\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sf)\nlibrary(DT)\nlibrary(tmap)\nlibrary(ggthemes)\nlibrary(ggiraph)\n\n\n\nRead data set\n\nhousehold_assets <- fread(\"percentage-distribution-of-conventional-households-by-ownership-of-selected-household-assets-201.csv\")\n\n#\nkenya_shapefile <- st_read(\"County\") %>% setDT()\n\nReading layer `County' from data source \n  `/home/mburu/r_projects/m-mburu.github.io/kenya_population/County' \n  using driver `ESRI Shapefile'\nSimple feature collection with 47 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 33.91182 ymin: -4.702271 xmax: 41.90626 ymax: 5.430648\nGeodetic CRS:  WGS 84\n\nkenya_shapefile[, county := tolower(COUNTY)]\n\n# rename column\nsetnames(household_assets, \n         c(\"County / Sub-County\", \"Conventional Households\"  ), \n         c(\"county\", \"households\"))\n\n\n\nSome minor cleaning\n\n# remove commas \nhousehold_assets[, households := as.numeric(gsub(\",|AR K    \", \"\", households))]\n\nhousehold_assets[,  county := tolower(county)]\n\nsub_county <- household_assets %>% \n    group_by(county) %>%\n    filter(households == max(households)) %>% setDT()\n\n\nsub_county_melt <- melt(sub_county, \n                        id.vars = c(\"county\", \"households\"))\n\n\n\n% of households with various assets 2019 census\n\nThis is overall data set for the whole country computer devices is ownership about 8.8% this just means that about 91% of the students can’t access online learning. This is is just a naive estimation the number could be higher.\n\n\nkenya_dat <- sub_county_melt[county == \"kenya\"]\n\np <- ggplot(kenya_dat, aes(variable, value, tooltip = paste(variable, \" : \", value))) +\n    geom_bar_interactive(stat = \"identity\", width = 0.5, fill =\"slategray2\"  ) +\n    geom_text_interactive(aes(variable, value, label = paste0(value, \"%\")),\n              position = position_dodge(width = 0.5),\n              vjust = 0.001, size = 3)+\n    labs(x = \"Household assets\", y = \"%\",\n         title = \"% of households with various assets 2019 census\",\n         caption = \"twitter\\n@mmburu_w\")+\n    theme_fivethirtyeight()+\n  theme(\n    axis.text = element_text(size = 7, angle = 45, vjust = 1, hjust =1),\n    plot.title = element_text_interactive(size =11)\n  )\np1 <- girafe(ggobj = p, width_svg = 7, height_svg = 4.5, \n  options = list(\n    opts_sizing(rescale = T) )\n  )\n\np1\n\n\n\n\n\n\n\n\nMerge shapefile with asset data sets\n\nsub_county_melt[county == \"elgeyo/marakwet\", county := \"keiyo-marakwet\"]\nsub_county_melt[county == \"tharaka-nithi\", county := \"tharaka\"]\nsub_county_melt[county ==  \"taita/taveta\", county := \"taita taveta\"]\nsub_county_melt[county ==  \"nairobi city\", county := \"nairobi\"]\n\ncounty_shapes <- merge(kenya_shapefile, sub_county_melt, by = \"county\") \n\nsetnames(county_shapes, \"value\", \"Percentage\")\n\n\n\nComputer devices data\n\ncomputer <- county_shapes[variable  %in% c(\"Desk Top\\nComputer/\\nLaptop/ Tablet\")]\n\n# this converts to sf object\ncomputer <- st_set_geometry(computer, \"geometry\")\n\n\n\nPercentage of households with computer devices per county\n\nThat is if a household owns a tablet, laptop or a desktop\n\n\n#ttm()\ntm_shape(computer)+\n    tm_borders(col = \"grey\")+\n    tm_fill(col = \"Percentage\")+\n    tm_layout(title = \"% of households with laptop/tablet/desk top \\n 2019 census\",\n              title.size = 1, title.position = c(0.3, 0.95))\n\n\n\n#ttm()\n\n\n\n% of households that can access internet\n\nThis looks like is internet access through mobile phones\n\n\ninternet <- county_shapes[variable == \"Internet\"]\n\ninternet <- st_set_geometry(internet, \"geometry\")\n\n#ttm()\ntm_shape(internet)+\n    tm_borders(col = \"grey\")+\n    tm_fill(col = \"Percentage\")+\n    tm_layout(title = \"% of households with internet acess\",\n              title.size = 1, title.position = c(0.3, 0.95))"
  },
  {
    "objectID": "Intro_Python_Data/Intro_datascience.html",
    "href": "Intro_Python_Data/Intro_datascience.html",
    "title": "Personal Blog",
    "section": "",
    "text": "Modules (sometimes called packages or libraries) help group together related sets of tools in Python. In this exercise, we’ll examine two modules that are frequently used by Data Scientists:\nstatsmodels: used in machine learning; usually aliased as sm seaborn: a visualization library; usually aliased as sns Note that each module has a standard alias, which allows you to access the tools inside of the module without typing as many characters. For example, aliasing lets us shorten seaborn.scatterplot() to sns.scatterplot().\n\nimport statsmodels as sm\nimport seaborn as sns\nimport numpy as np\n\n\n\n\nBefore we start looking for Bayes’ kidnapper, we need to fill out a Missing Puppy Report with details of the case. Each piece of information will be stored as a variable.\nWe define a variable using an equals sign (=). For instance, we would define the variable height:\nheight = 24 In this exercise, we’ll be defining bayes_age to be 4.0 months old. The data type for this variable will be float, meaning that it is a number.\n\nbayes_age = 4.0\nbayes_age\n\n4.0\n\n\n\n\n\nLet’s continue to fill out the Missing Puppy Report for Bayes. In the previous exercise, we defined bayes_age, which was a float, which represents a number.\nIn this exercise, we’ll define favorite_toy and owner, which will both be strings. A string represents text. A string is surrounded by quotation marks (’ or “) and can contain letters, numbers, and special characters. It doesn’t matter if you use single (’) or double (”) quotes, but it’s important to be consistent throughout your code.\n\nfavorite_toy = \"Mr. Squeaky\"\nowner = \"DataCamp\"\n# Display variables\nprint(favorite_toy)\nprint(owner)\n\nMr. Squeaky\nDataCamp\n\n\n\n\n\nIt’s easy to make errors when you’re trying to type strings quickly.\nDon’t forget to use quotes! Without quotes, you’ll get a name error. owner = DataCamp Use the same type of quotation mark. If you start with a single quote, and end with a double quote, you’ll get a syntax error. fur_color = “blonde’ Someone at the police station made an error when filling out the final lines of Bayes’ Missing Puppy Report. In this exercise, you will correct the errors.\n\n# One or more of the following lines contains an error\n# Correct it so that it runs without producing syntax errors\nbirthday = '2017-07-14'\ncase_id = 'DATACAMP!123-456?'"
  },
  {
    "objectID": "datacamp.html",
    "href": "datacamp.html",
    "title": "Personal Blog",
    "section": "",
    "text": "Data Manipulation with pandas\n\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntermediate Python\n\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Python\n\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2023\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntermediate Regression in R\n\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nMburu\n\n\n\n\n\n\n  \n\n\n\n\nSurvival Analysis R\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFoundations of Probability in R\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Statistics with R\n\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunctions in R\n\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2022\n\n\n\n\n\n\n  \n\n\n\n\nHandling Missing Data with Imputations in R\n\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnomaly Detection in R\n\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2022\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling in R\n\n\n\n\n\n\n\n\n\n\n\n\nSep 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to advanced dimensionality reduction\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2022\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCensus data in r with tidycensus\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\nModeling with tidymodels in R\n\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunicating with Data in the Tidyverse\n\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2021\n\n\nMburu\n\n\n\n\n\n\n  \n\n\n\n\nThe reduction in weekly working hours in Europe\n\n\nLooking at the development between 1996 and 2006\n\n\n\n\n\n\n\n\n\nNov 8, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWriting Efficient R Code\n\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScalable Data Processing in R\n\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Linear and Logistic Regression in R\n\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork analysis in R\n\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nString manipulation with stringr in r\n\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML with tree based models in r\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2020\n\n\nMburu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "datacamp/intermidiate_regression_R/intermediate_regression.html",
    "href": "datacamp/intermidiate_regression_R/intermediate_regression.html",
    "title": "Intermediate Regression in R",
    "section": "",
    "text": "In Introduction to Regression in R, you learned to fit linear regression models with a single explanatory variable. In many cases, using only one explanatory variable limits the accuracy of predictions. That means that to truly master linear regression, you need to be able to include multiple explanatory variables.\nThe case when there is one numeric explanatory variable and one categorical explanatory variable is sometimes called a “parallel slopes” linear regression due to the shape of the predictions—more on that in the next exercise.\nHere, you’ll revisit the Taiwan real estate dataset. Recall the meaning of each variable.\n\n\n\nlibrary(mTools)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(fst)\nlibrary(broom)\ntaiwan_real_estate <- read_fst(here(\"data\", \"taiwan_real_estate2.fst\"))\n\n# Fit a linear regr'n of price_twd_msq vs. n_convenience\nmdl_price_vs_conv <- lm(price_twd_msq ~ n_convenience, data = taiwan_real_estate)\n\n# See the result\nDT_tidy_model(mdl_price_vs_conv)\n\n\n\n\n\n\n\n\n\n\n# Fit a linear regr'n of price_twd_msq vs. house_age_years, no intercept\nmdl_price_vs_age <- lm(price_twd_msq ~ house_age_years -1, data = taiwan_real_estate )\n\n# See the result\nDT_tidy_model(mdl_price_vs_age)\n\n\n\n\n\n\n\n\n\n\n# Fit a linear regr'n of price_twd_msq vs. n_convenience plus house_age_years, no intercept\nmdl_price_vs_both <- lm(price_twd_msq ~ n_convenience \n                        + house_age_years - 1, \n                        data = taiwan_real_estate)\n\n# See the result\nDT_tidy_model(mdl_price_vs_both)\n\n\n\n\n\n\n\n\n\n\nFor linear regression with a single numeric explanatory variable, there is an intercept coefficient and a slope coefficient. For linear regression with a single categorical explanatory variable, there is an intercept coefficient for each category.\nIn the “parallel slopes” case, where you have a numeric and a categorical explanatory variable, what do the coefficients mean?\ntaiwan_real_estate and mdl_price_vs_both are available.\n\nFor each additional nearby convenience store, the expected house price, in TWD per square meter, increases by 0.79.\n\n\n\n\nBeing able to see the predictions made by a model makes it easier to understand. In the case where there is only one explanatory variable, ggplot lets you do this without any manual calculation or messing about.\nTo visualize the relationship between a numeric explanatory variable and the numeric response, you can draw a scatter plot with a linear trend line.\nTo visualize the relationship between a categorical explanatory variable and the numeric response, you can draw a box plot.\ntaiwan_real_estate is available and ggplot2 is loaded.\n\nUsing the taiwan_real_estate dataset, plot the house price versus the number of nearby convenience stores.\nMake it a scatter plot.\nAdd a smooth linear regression trend line without a standard error ribbon.\n\n\n# Using taiwan_real_estate, plot price_twd_msq vs. n_convenience\nggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq)) +\n    # Add a point layer\n    geom_point() +\n    # Add a smooth trend line using linear regr'n, no ribbon\n    geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\nUsing the taiwan_real_estate dataset, plot the house price versus the house age.\nMake it a box plot.\n\n\n# Using taiwan_real_estate, plot price_twd_msq vs. house_age_years\nggplot(taiwan_real_estate, aes( house_age_years, price_twd_msq)) +\n    # Add a box plot layer\n    geom_boxplot()\n\n\n\n\n\n\n\nThe two plots in the previous exercise gave very different predictions: one gave a predicted response that increased linearly with a numeric variable; the other gave a fixed response for each category. The only sensible way to reconcile these two conflicting predictions is to incorporate both explanatory variables in the model at once.\nWhen it comes to a linear regression model with a numeric and a categorical explanatory variable, ggplot2 doesn’t have an easy, “out of the box” way to show the predictions. Fortunately, the moderndive package includes an extra geom, geom_parallel_slopes() to make it simple.\ntaiwan_real_estate is available; ggplot2 and moderndive are loaded.\n\nUsing the taiwan_real_estate dataset, plot house prices versus the number of nearby convenience stores, colored by house age.\nMake it a scatter plot.\nAdd parallel slopes, without a standard error ribbon.\n\n\nlibrary(moderndive)\n# Using taiwan_real_estate, plot price_twd_msq vs. n_convenience colored by house_age_years\nggplot(taiwan_real_estate, aes( n_convenience , price_twd_msq, color = house_age_years))  +\n    # Add a point layer\n    geom_point() +\n    # Add parallel slopes, no ribbon\n    geom_parallel_slopes(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\nWhile ggplot can automatically show you model predictions, in order to get those values to program with, you’ll need to do the calculations yourself.\nJust as with the case of a single explanatory variable, the workflow has two steps: create a data frame of explanatory variables, then add a column of predictions. To make sure you’ve got the right answer, you can add your predictions to the ggplot with the geom_parallel_slopes() lines.\ntaiwan_real_estate and mdl_price_vs_both are available; dplyr, tidyr, and ggplot2 are loaded.\n\n\n\nn_convenience should take the numbers zero to ten.\nhouse_age_years should take the unique values of the house_age_years column of taiwan_real_estate.\n\n\n# Make a grid of explanatory data\nexplanatory_data <- expand.grid(\n    # Set n_convenience to zero to ten\n    n_convenience = 0:10,\n    # Set house_age_years to the unique values of that variable\n    house_age_years = unique(taiwan_real_estate$house_age_years)\n)\n\n# See the result\nexplanatory_data[1:10,] %>% data_table()\n\n\n\n\n\n\n\nAdd a column to the explanatory_data named for the response variable, assigning to prediction_data.\nThe response column contain predictions made using mdl_price_vs_both and explanatory_data.\n\n\n# Add predictions to the data frame\nprediction_data <- explanatory_data %>% \n    mutate(price_twd_msq = predict(mdl_price_vs_both, explanatory_data) )\n\n# See the result\nprediction_data[1:10,] %>% data_table()\n\n\n\n\n\n\n\nUpdate the plot to add a point layer of predictions. Use the prediction_data, set the point size to 5, and the point shape to 15.\n\n\ntaiwan_real_estate %>% \n    ggplot(aes(n_convenience, price_twd_msq, color = house_age_years)) +\n    geom_point() +\n    geom_parallel_slopes(se = FALSE) +\n    # Add points using prediction_data, with size 5 and shape 15\n    geom_point(data =prediction_data, \n               aes(n_convenience, price_twd_msq),\n               size = 5, shape = 15)\n\n\n\n\n\n\n\n\nAs with simple linear regression, you can manually calculate the predictions from the model coefficients. The only change for the parallel slopes case is that the intercept is different for each category of the categorical explanatory variable. That means you need to consider the case when each each category occurs separately.\ntaiwan_real_estate, mdl_price_vs_both, and explanatory_data are available; dplyr is loaded.\n\nGet the coefficients from mdl_price_vs_both, assigning to coeffs.\nAssign each of the elements of coeffs to the appropriate variable.\n\n\n# Get the coefficients from mdl_price_vs_both\ncoeffs <- coefficients(mdl_price_vs_both)\n\n# Extract the slope coefficient\nslope <- coeffs[1]\n\n# Extract the intercept coefficient for 0 to 15\nintercept_0_15 <- coeffs[2]\n\n# Extract the intercept coefficient for 15 to 30\nintercept_15_30 <- coeffs[3]\n\n# Extract the intercept coefficient for 30 to 45\nintercept_30_45 <- coeffs[4]\n\n\n\n\nTo choose the intercept, in the case when house_age_years is “0 to 15”, choose intercept_0_15. In the case when house_age_years is “15 to 30”, choose intercept_15_30. Do likewise for “30 to 45”.\nManually calculate the predictions as the intercept plus the slope times n_convenience.\n\n\nprediction_data <- explanatory_data %>% \n    mutate(\n        # Consider the 3 cases to choose the intercept\n        intercept = case_when(\n            house_age_years == \"0 to 15\" ~ intercept_0_15,\n            house_age_years ==  \"15 to 30\" ~ intercept_15_30,\n            house_age_years ==  \"30 to 45\" ~ intercept_30_45\n        ),\n        \n        # Manually calculate the predictions\n        price_twd_msq = slope*n_convenience + intercept\n    )\n\n# See the results\nprediction_data[1:10,] %>% data_table()\n\n\n\n\n\n\n\n\n\n\nRecall that the coefficient of determination is a measure of how well the linear regression line fits the observed values. An important motivation for including several explanatory variables in a linear regression is that you can improve the fit compared to considering only a single explanatory variable.\nHere you’ll compare the coefficient of determination for the three Taiwan house price models, to see which gives the best result.\nmdl_price_vs_conv, mdl_price_vs_age, and mdl_price_vs_both are available; dplyr and broom are loaded.\n\nGet the unadjusted and adjusted coefficients of determination for mdl_price_vs_conv by glancing at the model, then selecting the r.squared and adj.r.squared values.\nDo the same for mdl_price_vs_age and mdl_price_vs_both.\n\n\nmy_models <- list(mdl_price_vs_conv = mdl_price_vs_conv,\n                  mdl_price_vs_age = mdl_price_vs_age,\n                  mdl_price_vs_both = mdl_price_vs_both)\nnms_ms <- names(my_models)\nmy_dfs <- lapply(seq_along(my_models), function(x){\n    \n    my_models[[x]] %>% \n        glance() %>% \n        mutate(lm_model =nms_ms[x] ) %>%\n        select(lm_model, r.squared, adj.r.squared, sigma) \n    \n    \n    \n})\n\nmodel_glance <- data.table::rbindlist(my_dfs)\nmodel_glance %>% \n    round_all_num_cols() %>%\n    data_table()\n\n\n\n\n\n\n\nWhich model does the adjusted coefficient of determination suggest gives a better fit?\nmdl_price_vs_both\n\n\n\n\nThe other common metric for assessing model fit is the residual standard error (RSE), which measures the typical size of the residuals.\nIn the last exercise you saw how including both explanatory variables into the model increased the coefficient of determination. How do you think using both explanatory variables will change the RSE?\nmdl_price_vs_conv, mdl_price_vs_age, and mdl_price_vs_both are available; dplyr and broom are loaded.\n\nShown as sigma below\n\n\nmodel_glance %>% \n    round_all_num_cols() %>%\n    data_table()\n\n\n\n\n\n\n\nThe model with the list sigma mdl_price_vs_both"
  },
  {
    "objectID": "datacamp/intermidiate_regression_R/intermediate_regression.html#interactions",
    "href": "datacamp/intermidiate_regression_R/intermediate_regression.html#interactions",
    "title": "Intermediate Regression in R",
    "section": "Interactions",
    "text": "Interactions"
  },
  {
    "objectID": "notes/gtsumary_test.html",
    "href": "notes/gtsumary_test.html",
    "title": "gtsumary_test",
    "section": "",
    "text": "library(gtsummary)\nlibrary(tidyverse)\ntrial2 <- trial %>% select(trt, age, grade)\n\n\ntab <- trial2 %>%\n    tbl_summary(by = trt) %>%\n    add_p()\n\n\ntab2 <- trial2 %>%\n    tbl_summary(\n        by = trt,\n        type = all_continuous() ~ \"continuous2\",\n        statistic = all_continuous() ~ c(\n            \"{N_nonmiss}\",\n            \"{median} ({p25}, {p75})\",\n            \"{mean} ({sd})\",\n            \"{min}, {max}\"\n        ),\n        missing = \"ifany\"\n    ) %>%\n    add_p(pvalue_fun = ~ style_pvalue(.x, digits = 2))\n\ntab_df = as.data.frame(tab2)\n\nmTools::data_table(tab_df)"
  },
  {
    "objectID": "posts/Diabetes/predict_diabetes_tidymodels.html",
    "href": "posts/Diabetes/predict_diabetes_tidymodels.html",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(data.table)\nlibrary(gtsummary)\nlibrary(mTools)\nDiabetes data"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html",
    "href": "datacamp/tidymodels/tidymodels.html",
    "title": "Modeling with tidymodels in R",
    "section": "",
    "text": "The rsample package is designed to create training and test datasets. Creating a test dataset is important for estimating how a trained model will likely perform on new data. It also guards against overfitting, where a model memorizes patterns that exist only in the training data and performs poorly on new data.\nIn this exercise, you will create training and test datasets from the home_sales data. This data contains information on homes sold in the Seattle, Washington area between 2015 and 2016.\nThe outcome variable in this data is selling_price.\nThe tidymodels package will be pre-loaded in every exercise in the course. The home_sales tibble has also been loaded for you.\n\n\n\nTidy model packages\n\n\n\nhome_sales <- readRDS(\"home_sales.rds\")\n\n# Create a data split object\nhome_split <- initial_split(home_sales, \n                            prop = 0.7, \n                            strata = selling_price)\n\n# Create the training data\nhome_training <- home_split %>%\n  training()\n\n# Create the test data\nhome_test <- home_split %>% \n  testing()\n\n# Check number of rows in each dataset\nnrow(home_training)\n\n[1] 1042\n\nnrow(home_test)\n\n[1] 450\n\n\nDistribution of outcome variable values Stratifying by the outcome variable when generating training and test datasets ensures that the outcome variable values have a similar range in both datasets.\nSince the original data is split at random, stratification avoids placing all the expensive homes in home_sales into the test dataset, for example. In this case, your model would most likely perform poorly because it was trained on less expensive homes.\nIn this exercise, you will calculate summary statistics for the selling_price variable in the training and test datasets. The home_training and home_test tibbles have been loaded from the previous exercise.\n\n# Distribution of selling_price in training data\nlibrary(knitr)\nsummary_func <- function(df){\n      df %>% summarize(min_sell_price = min(selling_price),\n            max_sell_price = max(selling_price),\n            mean_sell_price = mean(selling_price),\n            sd_sell_price = sd(selling_price)) %>%\n    kable()\n\n}\nhome_training %>% \n    summary_func()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n479358.7\n81534.16\n\n\n\n\nhome_test %>% \n  summary_func()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n478449.5\n79764.72"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#fitting-a-linear-regression-model",
    "href": "datacamp/tidymodels/tidymodels.html#fitting-a-linear-regression-model",
    "title": "Modeling with tidymodels in R",
    "section": "Fitting a linear regression model",
    "text": "Fitting a linear regression model\nThe parsnip package provides a unified syntax for the model fitting process in R.\nWith parsnip, it is easy to define models using the various packages, or engines, that exist in the R ecosystem.\nIn this exercise, you will define a parsnip linear regression object and train your model to predict selling_price using home_age and sqft_living as predictor variables from the home_sales data.\nThe home_training and home_test tibbles that you created in the previous lesson have been loaded into this session.\n\n# Specify a linear regression model, linear_model\nlinear_model <- linear_reg() %>% \n  # Set the model engine\n  set_engine('lm') %>% \n  # Set the model mode\n  set_mode('regression')\n\n# Train the model with the training data\nlm_fit <- linear_model %>% \n  fit(selling_price~ home_age + sqft_living,\n      data = home_training)\n\n# Print lm_fit to view model information\ntidy(lm_fit) %>%\n    kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n292350.3425\n7625.56910\n38.338167\n0\n\n\nhome_age\n-1616.2020\n176.93455\n-9.134463\n0\n\n\nsqft_living\n103.2897\n2.77595\n37.208781\n0"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#predicting-home-selling-prices",
    "href": "datacamp/tidymodels/tidymodels.html#predicting-home-selling-prices",
    "title": "Modeling with tidymodels in R",
    "section": "Predicting home selling prices",
    "text": "Predicting home selling prices\nAfter fitting a model using the training data, the next step is to use it to make predictions on the test dataset. The test dataset acts as a new source of data for the model and will allow you to evaluate how well it performs.\nBefore you can evaluate model performance, you must add your predictions to the test dataset.\nIn this exercise, you will use your trained model, lm_fit, to predict selling_price in the home_test dataset.\nYour trained model, lm_fit, as well as the test dataset, home_test have been loaded into your session.\n\n# Predict selling_price\nhome_predictions <- predict(lm_fit,\n                        new_data = home_test)\n\n# View predicted selling prices\n#home_predictions\n\n# Combine test data with predictions\nhome_test_results <- home_test %>% \n  select(selling_price, home_age, sqft_living) %>% \n  bind_cols(home_predictions)\n\n# View results\nhome_test_results %>% \n    head()\n\n# A tibble: 6 × 4\n  selling_price home_age sqft_living   .pred\n          <dbl>    <dbl>       <dbl>   <dbl>\n1        487000       10        2540 538544.\n2        465000       10        1530 434222.\n3        411000       18        1130 379976.\n4        635000        4        3350 631906.\n5        464950       19        2190 487847.\n6        425000       11        1920 472888."
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#model-performance-metrics",
    "href": "datacamp/tidymodels/tidymodels.html#model-performance-metrics",
    "title": "Modeling with tidymodels in R",
    "section": "Model performance metrics",
    "text": "Model performance metrics\nEvaluating model results is an important step in the modeling process. Model evaluation should be done on the test dataset in order to see how well a model will generalize to new datasets.\nIn the previous exercise, you trained a linear regression model to predict selling_price using home_age and sqft_living as predictor variables. You then created the home_test_results tibble using your trained model on the home_test data.\nIn this exercise, you will calculate the RMSE and R squared metrics using your results in home_test_results.\nThe home_test_results tibble has been loaded into your session.\n\n# Print home_test_results\n#home_test_results\n\n# Caculate the RMSE metric\nhome_test_results %>% \n  rmse(truth = selling_price, estimate =.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      45930.\n\n# Calculate the R squared metric\nhome_test_results %>% \n  rsq(truth = selling_price, estimate =.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.670"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#r-squared-plot",
    "href": "datacamp/tidymodels/tidymodels.html#r-squared-plot",
    "title": "Modeling with tidymodels in R",
    "section": "R squared plot",
    "text": "R squared plot\nIn the previous exercise, you got an R squared value of 0.651. The R squared metric ranges from 0 to 1, 0 being the worst and 1 the best.\nCalculating the R squared value is only the first step in studying your model’s predictions.\nMaking an R squared plot is extremely important because it will uncover potential problems with your model, such as non-linear patterns or regions where your model is either over or under-predicting the outcome variable.\nIn this exercise, you will create an R squared plot of your model’s performance.\nThe home_test_results tibble has been loaded into your session.\n\n# Create an R squared plot of model performance\nggplot(home_test_results, aes(x = selling_price, y = .pred)) +\n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred()  +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#complete-model-fitting-process-with-last_fit",
    "href": "datacamp/tidymodels/tidymodels.html#complete-model-fitting-process-with-last_fit",
    "title": "Modeling with tidymodels in R",
    "section": "Complete model fitting process with last_fit()",
    "text": "Complete model fitting process with last_fit()\nIn this exercise, you will train and evaluate the performance of a linear regression model that predicts selling_price using all the predictors available in the home_sales tibble.\nThis exercise will give you a chance to perform the entire model fitting process with tidymodels, from defining your model object to evaluating its performance on the test data.\nEarlier in the chapter, you created an rsample object called home_split by passing the home_sales tibble into initial_split(). The home_split object contains the instructions for randomly splitting home_sales into training and test sets.\nThe home_sales tibble, and home_split object have been loaded into this session.\n\n# Define a linear regression model\nlinear_model <- linear_reg() %>% \n  set_engine('lm') %>% \n  set_mode('regression')\n\n# Train linear_model with last_fit()\nlinear_fit <- linear_model %>% \n  last_fit(selling_price ~ ., split = home_split)\n\n# Collect predictions and view results\npredictions_df <- linear_fit %>% collect_predictions()\npredictions_df %>% head()\n\n# A tibble: 6 × 5\n  id                 .pred  .row selling_price .config             \n  <chr>              <dbl> <int>         <dbl> <chr>               \n1 train/test split 527338.     1        487000 Preprocessor1_Model1\n2 train/test split 421637.     2        465000 Preprocessor1_Model1\n3 train/test split 397998.     3        411000 Preprocessor1_Model1\n4 train/test split 694181.     4        635000 Preprocessor1_Model1\n5 train/test split 475255.     8        464950 Preprocessor1_Model1\n6 train/test split 436889.     9        425000 Preprocessor1_Model1\n\n# Make an R squared plot using predictions_df\nggplot(predictions_df, aes(x = selling_price, y = .pred)) + \n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred() +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#data-resampling",
    "href": "datacamp/tidymodels/tidymodels.html#data-resampling",
    "title": "Modeling with tidymodels in R",
    "section": "Data resampling",
    "text": "Data resampling\nThe first step in a machine learning project is to create training and test datasets for model fitting and evaluation. The test dataset provides an estimate of how your model will perform on new data and helps to guard against overfitting.\nYou will be working with the telecom_df dataset which contains information on customers of a telecommunications company. The outcome variable is canceled_service and it records whether a customer canceled their contract with the company. The predictor variables contain information about customers’ cell phone and internet usage as well as their contract type and monthly charges.\nThe telecom_df tibble has been loaded into your session.\n\ntelecom_df <- readRDS(\"telecom_df.rds\")\n# Create data split object\ntelecom_split <- initial_split(telecom_df, prop = 0.75,\n                     strata = canceled_service)\n\n# Create the training data\ntelecom_training <- telecom_split %>% \n  training()\n\n# Create the test data\ntelecom_test <- telecom_split %>% \n  testing()\n\n# Check the number of rows\nnrow(telecom_training)\n\n[1] 731\n\nnrow(telecom_test)\n\n[1] 244"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#fitting-a-logistic-regression-model",
    "href": "datacamp/tidymodels/tidymodels.html#fitting-a-logistic-regression-model",
    "title": "Modeling with tidymodels in R",
    "section": "Fitting a logistic regression model",
    "text": "Fitting a logistic regression model\nIn addition to regression models, the parsnip package also provides a general interface to classification models in R.\nIn this exercise, you will define a parsnip logistic regression object and train your model to predict canceled_service using avg_call_mins, avg_intl_mins, and monthly_charges as predictor variables from the telecom_df data.\nThe telecom_training and telecom_test tibbles that you created in the previous lesson have been loaded into this session.\n\n# Specify a logistic regression model\nlogistic_model <- logistic_reg() %>% \n  # Set the engine\n  set_engine('glm') %>% \n  # Set the mode\n  set_mode('classification')\n\n# Fit to training data\nlogistic_fit <- logistic_model %>% \n  fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n      data = telecom_training)\n\n# Print model fit object\nlogistic_fit %>% tidy()\n\n# A tibble: 4 × 5\n  term             estimate std.error statistic  p.value\n  <chr>               <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)      1.99       0.584      3.41   6.43e- 4\n2 avg_call_mins   -0.0107     0.00128   -8.40   4.50e-17\n3 avg_intl_mins    0.0236     0.00311    7.59   3.16e-14\n4 monthly_charges  0.000293   0.00477    0.0615 9.51e- 1"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#combining-test-dataset-results",
    "href": "datacamp/tidymodels/tidymodels.html#combining-test-dataset-results",
    "title": "Modeling with tidymodels in R",
    "section": "Combining test dataset results",
    "text": "Combining test dataset results\nEvaluating your model’s performance on the test dataset gives insights into how well your model predicts on new data sources. These insights will help you communicate your model’s value in solving problems or improving decision making.\nBefore you can calculate classification metrics such as sensitivity or specificity, you must create a results tibble with the required columns for yardstick metric functions.\nIn this exercise, you will use your trained model to predict the outcome variable in the telecom_test dataset and combine it with the true outcome values in the canceled_service column.\nYour trained model, logistic_fit, and test dataset, telecom_test, have been loaded from the previous exercise.\n\n# Predict outcome categories\nclass_preds <- predict(logistic_fit, new_data = telecom_test,\n                       type = 'class')\n\n# Obtain estimated probabilities for each outcome value\nprob_preds <- predict(logistic_fit, new_data = telecom_test, \n                      type = 'prob')\n\n# Combine test set results\ntelecom_results <- telecom_test %>% \n  select(canceled_service) %>% \n  bind_cols(class_preds, prob_preds)\n\n# View results tibble\ntelecom_results %>%\n    head()\n\n# A tibble: 6 × 4\n  canceled_service .pred_class .pred_yes .pred_no\n  <fct>            <fct>           <dbl>    <dbl>\n1 yes              no              0.136    0.864\n2 yes              yes             0.792    0.208\n3 no               no              0.171    0.829\n4 no               yes             0.508    0.492\n5 yes              no              0.371    0.629\n6 no               no              0.139    0.861"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#evaluating-performance-with-yardstick",
    "href": "datacamp/tidymodels/tidymodels.html#evaluating-performance-with-yardstick",
    "title": "Modeling with tidymodels in R",
    "section": "Evaluating performance with yardstick",
    "text": "Evaluating performance with yardstick\nIn the previous exercise, you calculated classification metrics from a sample confusion matrix. The yardstick package was designed to automate this process.\nFor classification models, yardstick functions require a tibble of model results as the first argument. This should include the actual outcome values, predicted outcome values, and estimated probabilities for each value of the outcome variable.\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate performance metrics.\nThe telecom_results tibble has been loaded into your session.\n\n# Calculate the confusion matrix\nconf_mat(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n          Truth\nPrediction yes  no\n       yes  31  22\n       no   51 140\n\n# Calculate the accuracy\naccuracy(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.701\n\n# Calculate the sensitivity\n\nsens(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.378\n\n# Calculate the specificity\n\nspec(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 spec    binary         0.864"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#creating-custom-metric-sets",
    "href": "datacamp/tidymodels/tidymodels.html#creating-custom-metric-sets",
    "title": "Modeling with tidymodels in R",
    "section": "Creating custom metric sets",
    "text": "Creating custom metric sets\nThe yardstick package also provides the ability to create custom sets of model metrics. In cases where the cost of obtaining false negative errors is different from the cost of false positive errors, it may be important to examine a specific set of performance metrics.\nInstead of calculating accuracy, sensitivity, and specificity separately, you can create your own metric function that calculates all three at the same time.\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate a custom set of performance metrics. You will also use a confusion matrix to calculate all available binary classification metrics in tidymodelsall at once.\nThe telecom_results tibble has been loaded into your session.\n\n# Create a custom metric function\ntelecom_metrics <- metric_set(accuracy, sens, spec)\n\n# Calculate metrics using model results tibble\ntelecom_metrics(telecom_results, \n                truth = canceled_service,\n                estimate = .pred_class)\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.701\n2 sens     binary         0.378\n3 spec     binary         0.864\n\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class) %>% \n  # Pass to the summary() function\n  summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary         0.701\n 2 kap                  binary         0.265\n 3 sens                 binary         0.378\n 4 spec                 binary         0.864\n 5 ppv                  binary         0.585\n 6 npv                  binary         0.733\n 7 mcc                  binary         0.278\n 8 j_index              binary         0.242\n 9 bal_accuracy         binary         0.621\n10 detection_prevalence binary         0.217\n11 precision            binary         0.585\n12 recall               binary         0.378\n13 f_meas               binary         0.459"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#plotting-the-confusion-matrix",
    "href": "datacamp/tidymodels/tidymodels.html#plotting-the-confusion-matrix",
    "title": "Modeling with tidymodels in R",
    "section": "Plotting the confusion matrix",
    "text": "Plotting the confusion matrix\nCalculating performance metrics with the yardstick package provides insight into how well a classification model is performing on the test dataset. Most yardstick functions return a single number that summarizes classification performance.\nMany times, it is helpful to create visualizations of the confusion matrix to more easily communicate your results.\nIn this exercise, you will make a heat map and mosaic plot of the confusion matrix from your logistic regression model on the telecom_df dataset.\nYour model results tibble, telecom_results, has been loaded into your session.\n\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a heat map\n  autoplot(type = \"heatmap\")\n\n\n\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a mosaic plot\n  autoplot(type = \"mosaic\")"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#roc-curves-and-area-under-the-roc-curve",
    "href": "datacamp/tidymodels/tidymodels.html#roc-curves-and-area-under-the-roc-curve",
    "title": "Modeling with tidymodels in R",
    "section": "ROC curves and area under the ROC curve",
    "text": "ROC curves and area under the ROC curve\nROC curves are used to visualize the performance of a classification model across a range of probability thresholds. An ROC curve with the majority of points near the upper left corner of the plot indicates that a classification model is able to correctly predict both the positive and negative outcomes correctly across a wide range of probability thresholds.\nThe area under this curve provides a letter grade summary of model performance.\nIn this exercise, you will create an ROC curve from your logistic regression model results and calculate the area under the ROC curve with yardstick.\nYour model results tibble, telecom_results has been loaded into your session.\n\n# Calculate metrics across thresholds\nthreshold_df <- telecom_results %>% \n  roc_curve(truth = canceled_service, .pred_yes)\n\n# View results\nthreshold_df %>%\n  head()\n\n# A tibble: 6 × 3\n  .threshold specificity sensitivity\n       <dbl>       <dbl>       <dbl>\n1  -Inf          0             1    \n2     0.0110     0             1    \n3     0.0350     0             0.988\n4     0.0416     0.00617       0.988\n5     0.0435     0.0123        0.988\n6     0.0487     0.0185        0.988\n\n# Plot ROC curve\nthreshold_df %>% \n  autoplot()\n\n\n\n# Calculate ROC AUC\nroc_auc(telecom_results, truth = canceled_service, .pred_yes)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.725\n\n\nStreamlining the modeling process The last_fit() function is designed to streamline the modeling workflow in tidymodels. Instead of training your model on the training data and building a results tibble using the test data, last_fit() accomplishes this with one function.\nIn this exercise, you will train the same logistic regression model as you fit in the previous exercises, except with the last_fit() function.\nYour data split object, telecom_split, and model specification, logistic_model, have been loaded into your session.\n\n# Train model with last_fit()\ntelecom_last_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n           split = telecom_split)\n\n# View test set metrics\ntelecom_last_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.701 Preprocessor1_Model1\n2 roc_auc  binary         0.725 Preprocessor1_Model1"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#collecting-predictions-and-creating-custom-metrics",
    "href": "datacamp/tidymodels/tidymodels.html#collecting-predictions-and-creating-custom-metrics",
    "title": "Modeling with tidymodels in R",
    "section": "Collecting predictions and creating custom metrics",
    "text": "Collecting predictions and creating custom metrics\nUsing the last_fit() modeling workflow also saves time in collecting model predictions. Instead of manually creating a tibble of model results, there are helper functions that extract this information automatically.\nIn this exercise, you will use your trained model, telecom_last_fit, to create a tibble of model results on the test dataset as well as calculate custom performance metrics.\nYou trained model, telecom_last_fit, has been loaded into this session.\n\n# Collect predictions\nlast_fit_results <- telecom_last_fit %>% \n  collect_predictions()\n\n# View results\nlast_fit_results %>%\n  head()\n\n# A tibble: 6 × 7\n  id               .pred_yes .pred_no  .row .pred_class canceled_service .config\n  <chr>                <dbl>    <dbl> <int> <fct>       <fct>            <chr>  \n1 train/test split     0.136    0.864     4 no          yes              Prepro…\n2 train/test split     0.792    0.208     7 yes         yes              Prepro…\n3 train/test split     0.171    0.829    10 no          no               Prepro…\n4 train/test split     0.508    0.492    16 yes         no               Prepro…\n5 train/test split     0.371    0.629    17 no          yes              Prepro…\n6 train/test split     0.139    0.861    21 no          no               Prepro…\n\n# Custom metrics function\nlast_fit_metrics <- metric_set(accuracy, sens,\n                               spec, roc_auc)\n\n# Calculate metrics\nlast_fit_metrics(last_fit_results,\n                 truth = canceled_service,\n                 estimate = .pred_class,\n                 .pred_yes)\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.701\n2 sens     binary         0.378\n3 spec     binary         0.864\n4 roc_auc  binary         0.725"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#complete-modeling-workflow",
    "href": "datacamp/tidymodels/tidymodels.html#complete-modeling-workflow",
    "title": "Modeling with tidymodels in R",
    "section": "Complete modeling workflow",
    "text": "Complete modeling workflow\nIn this exercise, you will use the last_fit() function to train a logistic regression model and evaluate its performance on the test data by assessing the ROC curve and the area under the ROC curve.\nSimilar to previous exercises, you will predict canceled_service in the telecom_df data, but with an additional predictor variable to see if you can improve model performance.\nThe telecom_df tibble, telecom_split, and logistic_model objects from the previous exercises have been loaded into your workspace. The telecom_split object contains the instructions for randomly splitting the telecom_df tibble into training and test sets. The logistic_model object is a parsnip specification of a logistic regression model.\n\n# Train a logistic regression model\nlogistic_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins + avg_intl_mins + monthly_charges + months_with_company, \n           split = telecom_split)\n\n# Collect metrics\nlogistic_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.742 Preprocessor1_Model1\n2 roc_auc  binary         0.797 Preprocessor1_Model1\n\n# Collect model predictions\nlogistic_fit %>% \n  collect_predictions() %>% \n  # Plot ROC curve\n  roc_curve(truth = canceled_service, .pred_yes) %>% \n  autoplot()"
  },
  {
    "objectID": "posts/Diabetes/predict_diabetes_tidymodels.html#data-processing",
    "href": "posts/Diabetes/predict_diabetes_tidymodels.html#data-processing",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "Data processing",
    "text": "Data processing\n\ndiabetes_df[, diabetes_char := factor(diabetes, \n                                      levels = c(0, 1),\n                                      labels = c(\"Non diabetic\", \"Diabetic\"))]"
  },
  {
    "objectID": "posts/Diabetes/predict_diabetes_tidymodels.html#summary-stats",
    "href": "posts/Diabetes/predict_diabetes_tidymodels.html#summary-stats",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "Summary Stats",
    "text": "Summary Stats\n\nlibrary(ggiraph)\ndb_perc <- diabetes_df[, .(freq = .N),\n                       by = diabetes_char][\n                           ,perc := round(freq/sum(freq) * 100, 1)]\n\n\nggplot(db_perc, aes(diabetes_char, freq, fill = diabetes_char))+\n    geom_bar_interactive(width = 0.5, stat = \"identity\")+\n    geom_text(aes(label = paste0(freq, \"(\", perc, \"%)\")),\n              position = position_dodge(width = 0.5),\n              vjust = 0.05)+\n    scale_fill_brewer(name = \"\", type = \"qual\", palette = \"Dark2\")+\n    theme_minimal()+\n    theme(\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\ntab2 <- diabetes_df %>%\n    tbl_summary(\n        by = diabetes_char,\n        type = all_continuous() ~ \"continuous2\",\n        statistic = all_continuous() ~ c(\n            \"{mean} ({sd})\",\n            \"{median} ({p25}, {p75})\",\n            \"[{min}, {max}]\"\n        ),\n        missing = \"ifany\"\n    ) %>%\n    add_p(pvalue_fun = ~ style_pvalue(.x, digits = 2))\n\ntab_df = as.data.frame(tab2)\nnms <- names(tab_df)\nnms <- gsub(\"\\\\*\", \"\", nms)\nnames(tab_df) <- nms\ndata_table(tab_df)"
  },
  {
    "objectID": "posts/Diabetes/predict_diabetes_tidymodels.html#model-fitting",
    "href": "posts/Diabetes/predict_diabetes_tidymodels.html#model-fitting",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "Model Fitting",
    "text": "Model Fitting\n\nset.seed(100)\ndiabetes_df[, diabetes:= as.factor(diabetes)]\ndiabetes_df_split <- initial_split(diabetes_df[,.SD, .SDcols = !\"diabetes_char\"], \n                                   strata = diabetes)\n\ndiabetes_df_train <- training(diabetes_df_split)\n\ndiabetes_df_test <- testing(diabetes_df_split)\n\n\n# Specify a logistic regression model\nlogistic_model <- logistic_reg() %>% \n  # Set the engine\n  set_engine('glm') %>% \n  # Set the mode\n  set_mode('classification')\n\n# Fit to training data\nlogistic_fit <- logistic_model %>% \n  fit(diabetes ~ .,\n      data = diabetes_df_train)\n\n# Print model fit object\nlogistic_fit %>% \n    DT_tidy_model()\n\n\n\n\n\n\n\nxgb_spec <- boost_tree(\n    trees = 2000,\n    tree_depth = tune(), \n    min_n = tune(),\n    loss_reduction = tune(),                     ## first three: model complexity\n    sample_size = tune(), \n    mtry = tune(),         ## randomness\n    learn_rate = tune()                          ## step size\n) %>%\n    set_engine(\"xgboost\") %>%\n    set_mode(\"classification\")\n\nxgb_spec\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 2000\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost"
  },
  {
    "objectID": "datacamp/foundation_prob_R/foundation_probability.html",
    "href": "datacamp/foundation_prob_R/foundation_probability.html",
    "title": "Foundations of Probability in R",
    "section": "",
    "text": "In these exercises, you’ll practice using the rbinom() function, which generates random “flips” that are either 1 (“heads”) or 0 (“tails”).\n\n# Generate 10 separate random flips with probability .3\nrbinom(10, 1, p = 0.3)\n\n [1] 0 0 1 0 1 0 0 0 1 0\n\n\n\n\n\nIn the last exercise, you simulated 10 separate coin flips, each with a 30% chance of heads. Thus, with rbinom(10, 1, .3) you ended up with 10 outcomes that were either 0 (“tails”) or 1 (“heads”).\nBut by changing the second argument of rbinom() (currently 1), you can flip multiple coins within each draw. Thus, each outcome will end up being a number between 0 and 10, showing the number of flips that were heads in that trial.\n\n# Generate 100 occurrences of flipping 10 coins, each with 30% probability\n\nrbinom(100, 10, p = 0.3)\n\n  [1] 2 4 4 5 3 1 5 4 4 1 6 6 4 2 2 0 3 6 1 1 4 1 3 1 3 4 7 3 0 4 0 3 5 1 4 3 2\n [38] 3 5 1 2 1 2 2 7 3 2 3 1 4 2 3 4 2 1 2 4 3 1 3 1 3 3 4 2 2 1 1 3 1 2 3 2 3\n [75] 1 2 2 0 6 1 2 2 4 5 2 3 5 3 4 4 3 4 3 1 4 3 3 3 2 3\n\n\n\n\n\nIf you flip 10 coins each with a 30% probability of coming up heads, what is the probability exactly 2 of them are heads?\n\nAnswer the above question using the dbinom() function. This function takes almost the same arguments as rbinom(). The second and third arguments are size and prob, but now the first argument is x instead of n. Use x to specify where you want to evaluate the binomial density.\nConfirm your answer using the rbinom() function by creating a simulation of 10,000 trials. Put this all on one line by wrapping the mean() function around the rbinom() function.\n\n\n# Calculate the probability that 2 are heads using dbinom\n\ndbinom(2, 10, .3)\n\n[1] 0.2334744\n\n# Confirm your answer with a simulation using rbinom\n\n#flips <- rbinom(10000, 10, .3)\n\nmean(rbinom(10000, 10, .3) == 2)\n\n[1] 0.2356\n\n\n\n\n\nIf you flip ten coins that each have a 30% probability of heads, what is the probability at least five are heads?\n\nAnswer the above question using the pbinom() function. (Note that you can compute the probability that the number of heads is less than or equal to 4, then take 1 - that probability).\nConfirm your answer with a simulation of 10,000 trials by finding the number of trials that result in 5 or more heads.\n\n\n# Calculate the probability that at least five coins are heads\n\n1 - pbinom(4, 10, .3)\n\n[1] 0.1502683\n\n# Confirm your answer with a simulation of 10,000 trials\n\nmean(rbinom(10000, 10, .3) >=  5)\n\n[1] 0.1484\n\n\n\n\n\nIn the last exercise you tried flipping ten coins with a 30% probability of heads to find the probability at least five are heads. You found that the exact answer was 1 - pbinom(4, 10, .3) = 0.1502683, then confirmed with 10,000 simulated trials.\n\nDid you need all 10,000 trials to get an accurate answer? Would your answer have been more accurate with more trials?\n\n\n# Here is how you computed the answer in the last problem\nmean(rbinom(10000, 10, .3) >= 5)\n\n[1] 0.1515\n\n# Try now with 100, 1000, 10,000, and 100,000 trials\n\nmean(rbinom(100, 10, .3) >= 5)\n\n[1] 0.18\n\nmean(rbinom(1000, 10, .3) >= 5)\n\n[1] 0.154\n\nmean(rbinom(10000, 10, .3) >= 5)\n\n[1] 0.1523\n\nmean(rbinom(100000, 10, .3) >= 5)\n\n[1] 0.14892"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html",
    "href": "posts/cbk_data/R/kenya_income_exp.html",
    "title": "Kenya Government Income & Expenditure from 2000 to Date",
    "section": "",
    "text": "Kenya Government Income & Expenditure from 2000 to Date\n\noptions(scipen = 999)\n\nexp_rev <- c(\"totalexpenditure\", \"totalrevenue\")\ngok_earnings_exp[, (exp_rev) := lapply(.SD, function(x) x/10000), .SDcols = exp_rev]\ngok_earnings_exp <- gok_earnings_exp[year < current_year]\nexp_plot <- plot_compare(df = gok_earnings_exp, \n                         id_vars = c(\"year\"),\n                         compare_vars = c(\"totalexpenditure\", \"totalrevenue\"),\n                         x_val = year,\n                         col_val = variable,\n                         y_val = sum_val,\n                         xlab = \"Year\",\n                         ylab = \"Kenya Shillings(Billions)\",\n                         title_lab = \"\")   \n\n[1] 2\n\ngirafe( ggobj =  exp_plot, pointsize =10)"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#difference",
    "href": "posts/cbk_data/R/kenya_income_exp.html#difference",
    "title": "Kenya Government Income & Expenditure from 2000 to 2022",
    "section": "% difference",
    "text": "% difference"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#what-type-of-expenditure-is-rising",
    "href": "posts/cbk_data/R/kenya_income_exp.html#what-type-of-expenditure-is-rising",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "What type of expenditure is rising",
    "text": "What type of expenditure is rising"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to2023",
    "href": "posts/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to2023",
    "title": "Kenya Government Income & Expenditure from 2000 to 2022",
    "section": "Income & Expenditure from 2000 to2023",
    "text": "Income & Expenditure from 2000 to2023\n\n\n[1] 2"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to-2023",
    "href": "posts/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to-2023",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "Income & Expenditure from 2000 to 2023",
    "text": "Income & Expenditure from 2000 to 2023\nA short blog on Kenyan Government income and expenditure. Due to the current discussion in Kenya. Decided to have a quick look of the Kenyan expenditure & income. The total expenditure is growing at a faster pace than the total revenue. This means that the government is spending more money than it is taking in.\nThere are a number of factors that could be contributing to this trend. One factor is the growth of the economy. As the economy grows, the government needs to spend more money on things like infrastructure, education, and healthcare. Another factor is the growth of the population. As the population grows, the government needs to spend more money on things like social welfare programs and security."
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#what-sub-type-of-reccurrent-expenditure-is-causing-the-rise",
    "href": "posts/cbk_data/R/kenya_income_exp.html#what-sub-type-of-reccurrent-expenditure-is-causing-the-rise",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "What Sub type of reccurrent expenditure is causing the rise",
    "text": "What Sub type of reccurrent expenditure is causing the rise"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#domestic-debt-composition",
    "href": "posts/cbk_data/R/kenya_income_exp.html#domestic-debt-composition",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "Domestic debt composition",
    "text": "Domestic debt composition"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to-2022",
    "href": "posts/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to-2022",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "Income & Expenditure from 2000 to 2022",
    "text": "Income & Expenditure from 2000 to 2022\nDecided to have a quick look of the Kenyan government expenditure & income due to the current cash crunch\nThere are a number of factors that could be contributing to this trend. One factor is the growth of the economy. As the economy grows, the government needs to spend more money on things like infrastructure, education, and healthcare. Another factor is the growth of the population. As the population grows, the government needs to spend more money on things like social welfare programs and security."
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html",
    "href": "datacamp/introduction_python/introduction_python.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python Basics\nThe Python Interface Hit Run Code to run your first Python code with Datacamp and see the output!\nNotice the script.py window; this is where you can type Python code to solve exercises. You can hit Run Code and Submit Answer as often as you want. If you’re stuck, you can click Get Hint, and ultimately Get Solution.\nYou can also use the IPython Shell interactively by typing commands and hitting Enter. Here, your code will not be checked for correctness so it is a great way to experiment."
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#any-comments",
    "href": "datacamp/introduction_python/introduction_python.html#any-comments",
    "title": "Introduction to Python",
    "section": "Any comments?",
    "text": "Any comments?\nYou can also add comments to your Python scripts. Comments are important to make sure that you and others can understand what your code is about and do not run as Python code.\nThey start with # tag. See the comment in the editor, # Division; now it’s your turn to add a comment!\n\n# Division\nprint(5 / 8)\n\n0.625\n\n# Addition\nprint(7 + 10)\n\n17"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#python-as-a-calculator",
    "href": "datacamp/introduction_python/introduction_python.html#python-as-a-calculator",
    "title": "Introduction to Python",
    "section": "Python as a calculator",
    "text": "Python as a calculator\nPython is perfectly suited to do basic calculations. It can do addition, subtraction, multiplication and division.\nThe code in the script gives some examples.\nNow it’s your turn to practice!\n\n# Addition, subtraction\nprint(5 + 5)\n\n10\n\nprint(5 - 5)\n\n0\n\n# Multiplication, division, modulo, and exponentiation\nprint(3 * 5)\n\n15\n\nprint(10 / 2)\n\n5.0\n\nprint(18 % 7)\n\n4\n\nprint(4 ** 2)\n\n16\n\n# How much is your $100 worth after 7 years?\nprint(100*1.1**7)\n\n194.87171000000012"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#variable-assignment",
    "href": "datacamp/introduction_python/introduction_python.html#variable-assignment",
    "title": "Introduction to Python",
    "section": "Variable Assignment",
    "text": "Variable Assignment\nIn Python, a variable allows you to refer to a value with a name. To create a variable x with a value of 5, you use =, like this example:\nx = 5 You can now use the name of this variable, x, instead of the actual value, 5.\nRemember, = in Python means assignment, it doesn’t test equality!\n\n#creating saving variable\nsavings=100\nprint(savings)\n\n100\n\n#checking out the type\ntype(savings)\n\n<class 'int'>"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#calculations-with-variables",
    "href": "datacamp/introduction_python/introduction_python.html#calculations-with-variables",
    "title": "Introduction to Python",
    "section": "Calculations with variables",
    "text": "Calculations with variables\nYou’ve now created a savings variable, so let’s start saving!\nInstead of calculating with the actual values, you can use variables instead. The savings variable you created in the previous exercise with a value of 100 is available to you.\nHow much money would you have saved four months from now, if you saved $10 each month?\n\n#creating variables\nsavings=100\ngrowth_multiplier=1.1\n#creating result\nresult=savings*growth_multiplier**7\n\nprint(result)\n\n194.87171000000012"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#other-variable-types",
    "href": "datacamp/introduction_python/introduction_python.html#other-variable-types",
    "title": "Introduction to Python",
    "section": "Other variable types",
    "text": "Other variable types\nIn the previous exercise, you worked with the integer Python data type:\nint, or integer: a number without a fractional part. savings, with the value 100, is an example of an integer. Next to numerical data types, there are three other very common data types:\nfloat, or floating point: a number that has both an integer and fractional part, separated by a point. 1.1, is an example of a float. str, or string: a type to represent text. You can use single or double quotes to build a string. bool, or boolean: a type to represent logical values. It can only be True or False (the capitalization is important!).\n\n#creating string\ndesc=\"compound interest\"\nprint(desc)\n\ncompound interest\n\n#creating boolean\nprofitable=True\nprint(profitable)\n\nTrue"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#operations-with-other-types",
    "href": "datacamp/introduction_python/introduction_python.html#operations-with-other-types",
    "title": "Introduction to Python",
    "section": "Operations with other types",
    "text": "Operations with other types\nHugo mentioned that different types behave differently in Python.\nWhen you sum two strings, for example, you’ll get different behavior than when you sum two integers or two booleans.\nIn the script some variables with different types have already been created. It’s up to you to use them.\n\nsavings = 100\ngrowth_multiplier = 1.1\ndesc = \"compound interest\"\n\n# Assign product of savings and growth_multiplier to year1\nyear1 = savings * growth_multiplier\n\n# Print the type of year1\nprint(type(year1))\n\n<class 'float'>\n\n# Assign sum of desc and desc to doubledesc\ndoubledesc = desc + desc\n\n# Print out doubledesc\nprint(doubledesc)\n\ncompound interestcompound interest"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#type-conversion",
    "href": "datacamp/introduction_python/introduction_python.html#type-conversion",
    "title": "Introduction to Python",
    "section": "Type conversion",
    "text": "Type conversion\nUsing the + operator to paste together two strings can be very useful in building custom messages.\nSuppose, for example, that you’ve calculated your savings want to summarize the results in a string.\nTo do this, you’ll need to explicitly convert the types of your variables. More specifically, you’ll need str(), to convert a value into a string. str(savings), for example, will convert the integer savings to a string.\nSimilar functions such as int(), float() and bool() will help you convert Python values into any type.\n\n# Definition of savings and result\nsavings = 100\nresult = 100 * 1.10 ** 7\n\n# Fix the printout\nprint(\"I started with $\" + str(savings) + \" and now have $\" + str(result) + \". Awesome!\")\n\nI started with $100 and now have $194.87171000000012. Awesome!\n\n\n# Definition of pi_string\npi_string = \"3.1415926\"\n\n# Convert pi_string into float: pi_float\npi_float = float(pi_string)"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#create-a-list",
    "href": "datacamp/introduction_python/introduction_python.html#create-a-list",
    "title": "Introduction to Python",
    "section": "Create a list",
    "text": "Create a list\nAs opposed to int, bool etc., a list is a compound data type; you can group values together:\na = “is” b = “nice” my_list = [“my”, “list”, a, b] After measuring the height of your family, you decide to collect some information on the house you’re living in. The areas of the different parts of your house are stored in separate variables for now, as shown in the script.\n\n# Area variables (in square meters)\nhall = 11.25\nkit = 18.0\nliv = 20.0\nbed = 10.75\nbath = 9.50\n\n# Create list areas\nareas = [hall, kit, liv, bed, bath]\n\n# Print areas\nprint(areas)\n\n[11.25, 18.0, 20.0, 10.75, 9.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#create-list-with-different-types",
    "href": "datacamp/introduction_python/introduction_python.html#create-list-with-different-types",
    "title": "Introduction to Python",
    "section": "Create list with different types",
    "text": "Create list with different types\nA list can contain any Python type. Although it’s not really common, a list can also contain a mix of Python types including strings, floats, booleans, etc.\nThe printout of the previous exercise wasn’t really satisfying. It’s just a list of numbers representing the areas, but you can’t tell which area corresponds to which part of your house.\nThe code in the editor is the start of a solution. For some of the areas, the name of the corresponding room is already placed in front. Pay attention here! “bathroom” is a string, while bath is a variable that represents the float 9.50 you specified earlier.\n\n# area variables (in square meters)\nhall = 11.25\nkit = 18.0\nliv = 20.0\nbed = 10.75\nbath = 9.50\n\n# Adapt list areas\nareas = [\"hallway\", hall, \"kitchen\", kit, \"living room\", liv, \"bedroom\", bed, \"bathroom\", bath]\n\n# Print areas\nprint(areas)\n\n['hallway', 11.25, 'kitchen', 18.0, 'living room', 20.0, 'bedroom', 10.75, 'bathroom', 9.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#list-of-lists",
    "href": "datacamp/introduction_python/introduction_python.html#list-of-lists",
    "title": "Introduction to Python",
    "section": "List of lists",
    "text": "List of lists\nAs a data scientist, you’ll often be dealing with a lot of data, and it will make sense to group some of this data.\nInstead of creating a flat list containing strings and floats, representing the names and areas of the rooms in your house, you can create a list of lists. The script in the editor can already give you an idea.\nDon’t get confused here: “hallway” is a string, while hall is a variable that represents the float 11.25 you specified earlier.\n\n# area variables (in square meters)\nhall = 11.25\nkit = 18.0\nliv = 20.0\nbed = 10.75\nbath = 9.50\n\n# house information as list of lists\nhouse = [[\"hallway\", hall],\n         [\"kitchen\", kit],\n         [\"living room\", liv],\n         [\"bedroom\", bed],\n         [\"bathroom\", bath]]\n\n# Print out house\nprint(house)\n\n[['hallway', 11.25], ['kitchen', 18.0], ['living room', 20.0], ['bedroom', 10.75], ['bathroom', 9.5]]\n\n# Print out the type of house\nprint(type(house))\n\n<class 'list'>"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#subset-and-conquer",
    "href": "datacamp/introduction_python/introduction_python.html#subset-and-conquer",
    "title": "Introduction to Python",
    "section": "Subset and conquer",
    "text": "Subset and conquer\nSubsetting Python lists is a piece of cake. Take the code sample below, which creates a list x and then selects “b” from it. Remember that this is the second element, so it has index 1. You can also use negative indexing.\nx = [“a”, “b”, “c”, “d”] x[1] x[-3] # same result! Remember the areas list from before, containing both strings and floats? Its definition is already in the script. Can you add the correct code to do some Python subsetting?\n\n# Create the areas list\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"living room\", 20.0, \"bedroom\", 10.75, \"bathroom\", 9.50]\n\n# Print out second element from areas\nprint(areas[1])\n\n11.25\n\n# Print out last element from areas\nprint(areas[-1])\n\n9.5\n\n# Print out the area of the living room\nprint(areas[5])\n\n20.0"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#subset-and-calculate",
    "href": "datacamp/introduction_python/introduction_python.html#subset-and-calculate",
    "title": "Introduction to Python",
    "section": "Subset and calculate",
    "text": "Subset and calculate\nAfter you’ve extracted values from a list, you can use them to perform additional calculations. Take this example, where the second and fourth element of a list x are extracted. The strings that result are pasted together using the + operator:\n\n# Create the areas list\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"living room\", 20.0, \"bedroom\", 10.75, \"bathroom\", 9.50]\n\n# Sum of kitchen and bedroom area: eat_sleep_area\neat_sleep_area=areas[3]+ areas[7]\n\n# Print the variable eat_sleep_area\nprint(eat_sleep_area)\n\n28.75"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#slicing-and-dicing",
    "href": "datacamp/introduction_python/introduction_python.html#slicing-and-dicing",
    "title": "Introduction to Python",
    "section": "Slicing and dicing",
    "text": "Slicing and dicing\nSelecting single values from a list is just one part of the story. It’s also possible to slice your list, which means selecting multiple elements from your list. Use the following syntax:\nmy_list[start:end] The start index will be included, while the end index is not.\nThe code sample below shows an example. A list with “b” and “c”, corresponding to indexes 1 and 2, are selected from a list x:\nx = [“a”, “b”, “c”, “d”] x[1:3] The elements with index 1 and 2 are included, while the element with index 3 is not.\n\n# Create the areas list\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"living room\", 20.0, \"bedroom\", 10.75, \"bathroom\", 9.50]\n\n# Use slicing to create downstairs\ndownstairs = areas[0:6]\n\n# Use slicing to create upstairs\nupstairs = areas[6:10]\n\n# Print out downstairs and upstairs\nprint(downstairs)\n\n['hallway', 11.25, 'kitchen', 18.0, 'living room', 20.0]\n\nprint(upstairs)\n\n['bedroom', 10.75, 'bathroom', 9.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#slicing-and-dicing-2",
    "href": "datacamp/introduction_python/introduction_python.html#slicing-and-dicing-2",
    "title": "Introduction to Python",
    "section": "Slicing and dicing (2)",
    "text": "Slicing and dicing (2)\nIn the video, Hugo first discussed the syntax where you specify both where to begin and end the slice of your list:\nmy_list[begin:end] However, it’s also possible not to specify these indexes. If you don’t specify the begin index, Python figures out that you want to start your slice at the beginning of your list. If you don’t specify the end index, the slice will go all the way to the last element of your list. To experiment with this, try the following commands in the IPython Shell:\nx = [“a”, “b”, “c”, “d”] x[:2] x[2:] x[:]\n\n# Create the areas list\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"living room\", 20.0, \"bedroom\", 10.75, \"bathroom\", 9.50]\n\ndownstairs = areas[:6]\nupstairs = areas[6:]\nprint(downstairs)\n\n['hallway', 11.25, 'kitchen', 18.0, 'living room', 20.0]\n\nprint(upstairs)\n\n['bedroom', 10.75, 'bathroom', 9.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#replace-list-elements",
    "href": "datacamp/introduction_python/introduction_python.html#replace-list-elements",
    "title": "Introduction to Python",
    "section": "Replace list elements",
    "text": "Replace list elements\nReplacing list elements is pretty easy. Simply subset the list and assign new values to the subset. You can select single elements or you can change entire list slices at once.\nUse the IPython Shell to experiment with the commands below. Can you tell what’s happening and why?\nx = [“a”, “b”, “c”, “d”] x[1] = “r” x[2:] = [“s”, “t”] For this and the following exercises, you’ll continue working on the areas list that contains the names and areas of different rooms in a house.\n\n#experimenting\nx = [\"a\", \"b\", \"c\", \"d\"]\nx[1] = \"r\"\nx[2:]=[\"s\",\"t\"]\nprint(x)\n\n['a', 'r', 's', 't']\n\n#original list\n# Create the areas list\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"living room\", 20.0, \"bedroom\", 10.75, \"bathroom\", 9.50]\n#updating the area of the bathroom\nareas[-1]=10.5\n\n#changing living room\nareas[4]= \"chill zone\"\nprint(areas)\n\n['hallway', 11.25, 'kitchen', 18.0, 'chill zone', 20.0, 'bedroom', 10.75, 'bathroom', 10.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#extend-a-list",
    "href": "datacamp/introduction_python/introduction_python.html#extend-a-list",
    "title": "Introduction to Python",
    "section": "Extend a list",
    "text": "Extend a list\nIf you can change elements in a list, you sure want to be able to add elements to it, right? You can use the + operator:\nx = [“a”, “b”, “c”, “d”] y = x + [“e”, “f”] You just won the lottery, awesome! You decide to build a poolhouse and a garage. Can you add the information to the areas list?\n\n# Create the areas list (updated version)\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"chill zone\", 20.0,\n         \"bedroom\", 10.75, \"bathroom\", 10.50]\n\n# Add poolhouse data to areas, new list is areas_1\nareas_1 = areas + [\"poolhouse\", 24.5]\nprint(areas_1)\n\n['hallway', 11.25, 'kitchen', 18.0, 'chill zone', 20.0, 'bedroom', 10.75, 'bathroom', 10.5, 'poolhouse', 24.5]\n\n# Add garage data to areas_1, new list is areas_2\nareas_2 = areas_1 + [\"garage\", 15.45]\n\nDelete list elements Finally, you can also remove elements from your list. You can do this with the del statement:\nx = [“a”, “b”, “c”, “d”] del(x[1]) Pay attention here: as soon as you remove an element from a list, the indexes of the elements that come after the deleted element all change!\nThe updated and extended version of areas that you’ve built in the previous exercises is coded below. You can copy and paste this into the IPython Shell to play around with the result.\n\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0,\n        \"chill zone\", 20.0, \"bedroom\", 10.75,\n         \"bathroom\", 10.50, \"poolhouse\", 24.5,\n         \"garage\", 15.45]\n         \nprint(areas)\n\n['hallway', 11.25, 'kitchen', 18.0, 'chill zone', 20.0, 'bedroom', 10.75, 'bathroom', 10.5, 'poolhouse', 24.5, 'garage', 15.45]\n\n\nThere was a mistake! The amount you won with the lottery is not that big after all and it looks like the poolhouse isn’t going to happen. You decide to remove the corresponding string and float from the areas list.\nThe ; sign is used to place commands on the same line. The following two code chunks are equivalent:\nWhich of the code chunks will do the job for us?\n\ndel(areas[-4:-2])\nprint(areas)\n\n['hallway', 11.25, 'kitchen', 18.0, 'chill zone', 20.0, 'bedroom', 10.75, 'bathroom', 10.5, 'garage', 15.45]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#inner-workings-of-lists",
    "href": "datacamp/introduction_python/introduction_python.html#inner-workings-of-lists",
    "title": "Introduction to Python",
    "section": "Inner workings of lists",
    "text": "Inner workings of lists\nAt the end of the video, Hugo explained how Python lists work behind the scenes. In this exercise you’ll get some hands-on experience with this.\nThe Python code in the script already creates a list with the name areas and a copy named areas_copy. Next, the first element in the areas_copy list is changed and the areas list is printed out. If you hit Run Code you’ll see that, although you’ve changed areas_copy, the change also takes effect in the areas list. That’s because areas and areas_copy point to the same list.\nIf you want to prevent changes in areas_copy from also taking effect in areas, you’ll have to do a more explicit copy of the areas list. You can do this with list() or by using [:].\n\n# Create list areas\nareas = [11.25, 18.0, 20.0, 10.75, 9.50]\n\n# Create areas_copy\nareas_copy = areas[:]\n# Change areas_copy\nareas_copy[0] = 5.0\n\n# Print areas\nprint(areas)\n\n[11.25, 18.0, 20.0, 10.75, 9.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#familiar-functions",
    "href": "datacamp/introduction_python/introduction_python.html#familiar-functions",
    "title": "Introduction to Python",
    "section": "Familiar functions",
    "text": "Familiar functions\nOut of the box, Python offers a bunch of built-in functions to make your life as a data scientist easier. You already know two such functions: print() and type(). You’ve also used the functions str(), int(), bool() and float() to switch between data types. These are built-in functions as well.\nCalling a function is easy. To get the type of 3.0 and store the output as a new variable, result, you can use the following:\nresult = type(3.0) The general recipe for calling functions and saving the result to a variable is thus:\noutput = function_name(input)\n\n# Create variables var1 and var2\nvar1 = [1, 2, 3, 4]\nvar2 = True\n\n# Print out type of var1\nprint(type(var1))\n\n<class 'list'>\n\n# Print out length of var1\nprint(len(var1))\n\n4\n\n\n# Convert var2 to an integer: out2\nout2 = int(var2)"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#help",
    "href": "datacamp/introduction_python/introduction_python.html#help",
    "title": "Introduction to Python",
    "section": "Help!",
    "text": "Help!\nMaybe you already know the name of a Python function, but you still have to figure out how to use it. Ironically, you have to ask for information about a function with another function: help(). In IPython specifically, you can also use ? before the function name.\nTo get help on the max() function, for example, you can use one of these calls:\n\nhelp(max)\n\nHelp on built-in function max in module builtins:\n\nmax(...)\n    max(iterable, *[, default=obj, key=func]) -> value\n    max(arg1, arg2, *args, *[, key=func]) -> value\n    \n    With a single iterable argument, return its biggest item. The\n    default keyword-only argument specifies an object to return if\n    the provided iterable is empty.\n    With two or more arguments, return the largest argument."
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#multiple-arguments",
    "href": "datacamp/introduction_python/introduction_python.html#multiple-arguments",
    "title": "Introduction to Python",
    "section": "Multiple arguments",
    "text": "Multiple arguments\nIn the previous exercise, you identified optional arguments by viewing the documentation with help(). You’ll now apply this to change the behavior of the sorted() function.\nHave a look at the documentation of sorted() by typing help(sorted) in the IPython Shell.\nYou’ll see that sorted() takes three arguments: iterable, key, and reverse.\nkey=None means that if you don’t specify the key argument, it will be None. reverse=False means that if you don’t specify the reverse argument, it will be False, by default.\nIn this exercise, you’ll only have to specify iterable and reverse, not key. The first input you pass to sorted() will be matched to the iterable argument, but what about the second input? To tell Python you want to specify reverse without changing anything about key, you can use = to assign it a new value:\nsorted(____, reverse=____) Two lists have been created for you. Can you paste them together and sort them in descending order?\nNote: For now, we can understand an iterable as being any collection of objects, e.g., a List.\n\n# Create lists first and second\nfirst = [11.25, 18.0, 20.0]\nsecond = [10.75, 9.50]\n\n# Paste together first and second: full\nfull=first + second\nprint(full)\n\n[11.25, 18.0, 20.0, 10.75, 9.5]\n\n# Sort full in descending order: full_sorted\nfull_sorted=(sorted(full,reverse=True))\n\n# Print out full_sorted\nprint(full_sorted)\n\n[20.0, 18.0, 11.25, 10.75, 9.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#string-methods",
    "href": "datacamp/introduction_python/introduction_python.html#string-methods",
    "title": "Introduction to Python",
    "section": "String Methods",
    "text": "String Methods\nStrings come with a bunch of methods. Follow the instructions closely to discover some of them. If you want to discover them in more detail, you can always type help(str) in the IPython Shell.\nA string place has already been created for you to experiment with.\n\n# string to experiment with: place\nplace = \"poolhouse\"\n\n# Use upper() on place: place_up\nplace_up = place.upper()\n\n# Print out place and place_up\nprint(place)\n\npoolhouse\n\nprint(place_up)\n\nPOOLHOUSE\n\n# Print out the number of o's in place\nprint(place.count('o'))\n\n3"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#list-methods",
    "href": "datacamp/introduction_python/introduction_python.html#list-methods",
    "title": "Introduction to Python",
    "section": "List Methods",
    "text": "List Methods\nStrings are not the only Python types that have methods associated with them. Lists, floats, integers and booleans are also types that come packaged with a bunch of useful methods. In this exercise, you’ll be experimenting with:\n\nindex(), to get the index of the first element of a list that matches its input and\ncount(), to get the number of times an element appears in a list. You’ll be working on the list with the area of different parts of a house: areas.\nappend(), that adds an element to the list it is called on,\nremove(), that removes the first element of a list that matches the input, and\nreverse(), that reverses the order of the elements in the list it is called on.\n\n\n# Create list areas\nareas = [11.25, 18.0, 20.0, 10.75, 9.50]\n\n# Print out the index of the element 20.0\nprint(areas.index(20.0))\n\n2\n\n# Print out how often 9.50 appears in areas\nprint(areas.count(9.50))\n\n1\n\n# Create list areas\nareas = [11.25, 18.0, 20.0, 10.75, 9.50]\n\n# Use append twice to add poolhouse and garage size\nareas.append(24.5)\nareas.append(15.45)\n\n# Print out areas\nprint(areas)\n\n[11.25, 18.0, 20.0, 10.75, 9.5, 24.5, 15.45]\n\n# Reverse the orders of the elements in areas\nareas.reverse()\n\n# Print out areas\nprint(areas)\n\n[15.45, 24.5, 9.5, 10.75, 20.0, 18.0, 11.25]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#import-package",
    "href": "datacamp/introduction_python/introduction_python.html#import-package",
    "title": "Introduction to Python",
    "section": "Import package",
    "text": "Import package\nAs a data scientist, some notions of geometry never hurt. Let’s refresh some of the basics.\nFor a fancy clustering algorithm, you want to find the circumference, , and area, , of a circle. When the radius of the circle is r, you can calculate and as:\nIn Python, the symbol for exponentiation is . This operator raises the number to its left to the power of the number to its right. For example 34 is 3 to the power of 4 and will give 81.\nTo use the constant pi, you’ll need the math package. A variable r is already coded in the script. Fill in the code to calculate C and A and see how the print() functions create some nice printouts.\n\n# Definition of radius\nr = 0.43\n\n# Import the math package\nimport math\n\n# Calculate C\nC = 2 * r * math.pi\n\n# Calculate A\nA = math.pi * r ** 2\n\n# Build printout\nprint(\"Circumference: \" + str(C))\n\nCircumference: 2.701769682087222\n\nprint(\"Area: \" + str(A))\n\nArea: 0.5808804816487527"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#selective-import",
    "href": "datacamp/introduction_python/introduction_python.html#selective-import",
    "title": "Introduction to Python",
    "section": "Selective import",
    "text": "Selective import\nGeneral imports, like import math, make all functionality from the math package available to you. However, if you decide to only use a specific part of a package, you can always make your import more selective:\nfrom math import pi Let’s say the Moon’s orbit around planet Earth is a perfect circle, with a radius r (in km) that is defined in the script.\n\n# Definition of radius\nr = 192500\n\n# Import radians function of math package\nfrom math import radians\n\n# Travel distance of Moon over 12 degrees. Store in dist.\n\ndist = r*  radians(12)\n\n# Print out dist\nprint(dist)\n\n40317.10572106901"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#different-ways-of-importing",
    "href": "datacamp/introduction_python/introduction_python.html#different-ways-of-importing",
    "title": "Introduction to Python",
    "section": "Different ways of importing",
    "text": "Different ways of importing\nThere are several ways to import packages and modules into Python. Depending on the import call, you’ll have to use different Python code.\nSuppose you want to use the function inv(), which is in the linalg subpackage of the scipy package. You want to be able to use this function as follows:\nmy_inv([[1,2], [3,4]]) Which import statement will you need in order to run the above code without an error?\n\nfrom scipy.linalg import inv as my_inv\n\nmy_inv([[1,2], [3,4]])\n\narray([[-2. ,  1. ],\n       [ 1.5, -0.5]])"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#your-first-numpy-array",
    "href": "datacamp/introduction_python/introduction_python.html#your-first-numpy-array",
    "title": "Introduction to Python",
    "section": "Your First NumPy Array",
    "text": "Your First NumPy Array\nIn this chapter, we’re going to dive into the world of baseball. Along the way, you’ll get comfortable with the basics of numpy, a powerful package to do data science.\nA list baseball has already been defined in the Python script, representing the height of some baseball players in centimeters. Can you add some code here and there to create a numpy array from it?\n\n# Create list baseball\nbaseball = [180, 215, 210, 210, 188, 176, 209, 200]\n\n# Import the numpy package as np\nimport numpy as np\n\n# Create a NumPy array from baseball: np_baseball\nnp_baseball = np.array(baseball)\nnp_baseball\n\narray([180, 215, 210, 210, 188, 176, 209, 200])\n\n# Print out type of np_baseball\nprint(type(np_baseball))\n\n<class 'numpy.ndarray'>"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#baseball-players-height",
    "href": "datacamp/introduction_python/introduction_python.html#baseball-players-height",
    "title": "Introduction to Python",
    "section": "Baseball players’ height",
    "text": "Baseball players’ height\nYou are a huge baseball fan. You decide to call the MLB (Major League Baseball) and ask around for some more statistics on the height of the main players. They pass along data on more than a thousand players, which is stored as a regular Python list: height_in. The height is expressed in inches. Can you make a numpy array out of it and convert the units to meters?\nheight_in is already available and the numpy package is loaded, so you can start straight away (Source: stat.ucla.edu).\n\n# height is available as a regular list\n\n# Import numpy\nimport numpy as np\n\nheight_in = np.array([74, 74, 72, 72, 73, 69, 69, 71, 76, 71])\n# Create a numpy array from height_in: np_height_in\nnp_height_in=np.array(height_in)\n# Print out np_height_in\nprint(np_height_in)\n\n[74 74 72 72 73 69 69 71 76 71]\n\n# Convert np_height_in to m: np_height_m\nnp_height_m = np_height_in*0.0254\n\n# Print np_height_m\nprint(np_height_m)\n\n[1.8796 1.8796 1.8288 1.8288 1.8542 1.7526 1.7526 1.8034 1.9304 1.8034]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#baseball-players-bmi",
    "href": "datacamp/introduction_python/introduction_python.html#baseball-players-bmi",
    "title": "Introduction to Python",
    "section": "Baseball player’s BMI",
    "text": "Baseball player’s BMI\nThe MLB also offers to let you analyze their weight data. Again, both are available as regular Python lists: height_in and weight_lb. height_in is in inches and weight_lb is in pounds.\nIt’s now possible to calculate the BMI of each baseball player. Python code to convert height_in to a numpy array with the correct units is already available in the workspace. Follow the instructions step by step and finish the game! height_in and weight_lb are available as regular lists.\n\n# height_in and weight_lb are available as regular lists\n\n# Import numpy\nimport numpy as np\n\n# Create array from height_in with metric units: np_height_m\nnp_height_m = np.array(height_in) * 0.0254\nweight_lb = np.array([180, 215, 210, 210, 188, 176, 209, 200, 231, 180])\n# Create array from weight_lb with metric units: np_weight_kg\nnp_weight_kg = np.array(weight_lb) * 0.453592\n\n# Calculate the BMI: bmi\nbmi = np_weight_kg / np_height_m ** 2\n\n# Print out bmi\nprint(bmi)\n\n[23.11037639 27.60406069 28.48080465 28.48080465 24.80333518 25.99036864\n 30.86356276 27.89402921 28.11789135 25.10462629]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#lightweight-baseball-players",
    "href": "datacamp/introduction_python/introduction_python.html#lightweight-baseball-players",
    "title": "Introduction to Python",
    "section": "Lightweight baseball players",
    "text": "Lightweight baseball players\nTo subset both regular Python lists and numpy arrays, you can use square brackets:\nx = [4 , 9 , 6, 3, 1] x[1] import numpy as np y = np.array(x) y[1] For numpy specifically, you can also use boolean numpy arrays:\nhigh = y > 5 y[high] The code that calculates the BMI of all baseball players is already included. Follow the instructions and reveal interesting things from the data! height_in and weight_lb are available as regular lists.\n\n# height and weight are available as a regular lists\n\n# Import numpy\nimport numpy as np\n\n# Calculate the BMI: bmi\nnp_height_m = np.array(height_in) * 0.0254\nnp_weight_kg = np.array(weight_lb) * 0.453592\nbmi = np_weight_kg / np_height_m ** 2\n\n# Create the light array\n\nlight = bmi < 21\n# Print out light\nprint(light)\n\n[False False False False False False False False False False]\n\n# Print out BMIs of all baseball players whose BMI is below 21\nprint(bmi[light])\n\n[]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#numpy-side-effects",
    "href": "datacamp/introduction_python/introduction_python.html#numpy-side-effects",
    "title": "Introduction to Python",
    "section": "NumPy Side Effects",
    "text": "NumPy Side Effects\nAs Hugo explained before, numpy is great for doing vector arithmetic. If you compare its functionality with regular Python lists, however, some things have changed.\nFirst of all, numpy arrays cannot contain elements with different types. If you try to build such a list, some of the elements’ types are changed to end up with a homogeneous list. This is known as type coercion.\nSecond, the typical arithmetic operators, such as +, -, * and / have a different meaning for regular Python lists and numpy arrays.\nHave a look at this line of code:\nnp.array([True, 1, 2]) + np.array([3, 4, False]) Can you tell which code chunk builds the exact same Python object? The numpy package is already imported as np, so you can start experimenting in the IPython Shell straight away!\n\nprint(np.array([True, 1, 2]) + np.array([3, 4, False]))\n\n[4 5 2]\n\nprint(np.array([4, 3, 0]) + np.array([0, 2, 2]))\n\n[4 5 2]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#subsetting-numpy-arrays",
    "href": "datacamp/introduction_python/introduction_python.html#subsetting-numpy-arrays",
    "title": "Introduction to Python",
    "section": "Subsetting NumPy Arrays",
    "text": "Subsetting NumPy Arrays\nYou’ve seen it with your own eyes: Python lists and numpy arrays sometimes behave differently. Luckily, there are still certainties in this world. For example, subsetting (using the square bracket notation on lists or arrays) works exactly the same. To see this for yourself, try the following lines of code in the IPython Shell:\nx = [“a”, “b”, “c”] x[1]\nnp_x = np.array(x) np_x[1] The script in the editor already contains code that imports numpy as np, and stores both the height and weight of the MLB players as numpy arrays. height_in and weight_lb are available as regular lists.\n\n# height and weight are available as a regular lists\n\n# Import numpy\nimport numpy as np\n\n# Store weight and height lists as numpy arrays\nnp_weight_lb = np.array(weight_lb)\nnp_height_in = np.array(height_in)\n\n# Print out the weight at index 5\n\nprint(np_weight_lb[5])\n\n176\n\n# Print out sub-array of np_height_in: index 1 up to and including index 3\n\nprint(np_height_in[1:3])\n\n[74 72]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#your-first-2d-numpy-array",
    "href": "datacamp/introduction_python/introduction_python.html#your-first-2d-numpy-array",
    "title": "Introduction to Python",
    "section": "Your First 2D NumPy Array",
    "text": "Your First 2D NumPy Array\nBefore working on the actual MLB data, let’s try to create a 2D numpy array from a small list of lists.\nIn this exercise, baseball is a list of lists. The main list contains 4 elements. Each of these elements is a list containing the height and the weight of 4 baseball players, in this order. baseball is already coded for you in the script.\n\n# Create baseball, a list of lists\nbaseball = [[180, 78.4],\n            [215, 102.7],\n            [210, 98.5],\n            [188, 75.2]]\n\n# Import numpy\nimport numpy as np\n\n# Create a 2D numpy array from baseball: np_baseball\nnp_baseball = np.array(baseball)\n\n# Print out the type of np_baseball\nprint(type(np_baseball))\n\n<class 'numpy.ndarray'>\n\n# Print out the shape of np_baseball\nprint(np_baseball.shape)\n\n(4, 2)"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#baseball-data-in-2d-form",
    "href": "datacamp/introduction_python/introduction_python.html#baseball-data-in-2d-form",
    "title": "Introduction to Python",
    "section": "Baseball data in 2D form",
    "text": "Baseball data in 2D form\nYou have another look at the MLB data and realize that it makes more sense to restructure all this information in a 2D numpy array. This array should have 1015 rows, corresponding to the 1015 baseball players you have information on, and 2 columns (for height and weight).\nThe MLB was, again, very helpful and passed you the data in a different structure, a Python list of lists. In this list of lists, each sublist represents the height and weight of a single baseball player. The name of this embedded list is baseball.\nCan you store the data as a 2D array to unlock numpy’s extra functionality? baseball is available as a regular list of lists.\n\n# baseball is available as a regular list of lists\n\n# Import numpy package\nimport numpy as np\n\n# Create a 2D numpy array from baseball: np_baseball\nnp_baseball = np.array(baseball)\n\n# Print out the shape of np_baseball\nprint(np_baseball.shape)\n\n(4, 2)"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#subsetting-2d-numpy-arrays",
    "href": "datacamp/introduction_python/introduction_python.html#subsetting-2d-numpy-arrays",
    "title": "Introduction to Python",
    "section": "Subsetting 2D NumPy Arrays",
    "text": "Subsetting 2D NumPy Arrays\nIf your 2D numpy array has a regular structure, i.e. each row and column has a fixed number of values, complicated ways of subsetting become very easy. Have a look at the code below where the elements “a” and “c” are extracted from a list of lists."
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#d-arithmetic",
    "href": "datacamp/introduction_python/introduction_python.html#d-arithmetic",
    "title": "Introduction to Python",
    "section": "2D Arithmetic",
    "text": "2D Arithmetic\nRemember how you calculated the Body Mass Index for all baseball players? numpy was able to perform all calculations element-wise (i.e. element by element). For 2D numpy arrays this isn’t any different! You can combine matrices with single numbers, with vectors, and with other matrices.\nExecute the code below in the IPython shell and see if you understand:\nimport numpy as np np_mat = np.array([[1, 2], [3, 4], [5, 6]]) np_mat * 2 np_mat + np.array([10, 10]) np_mat + np_mat np_baseball is coded for you; it’s again a 2D numpy array with 3 columns representing height (in inches), weight (in pounds) and age (in years). baseball is available as a regular list of lists and updated is available as 2D numpy array.\n\n# baseball is available as a regular list of lists\n# updated is available as 2D numpy array\n\n# Import numpy package\nimport numpy as np\n\n# Create np_baseball (3 cols)\nnp_baseball = np.array(baseball)\n\n# Print out addition of np_baseball and updated\nprint( np_baseball + updated)\n\n# Create numpy array: conversion\nconversion = np.array([0.0254, 0.453592, 1])\n\n# Print out product of np_baseball and conversion\nprint(np_baseball * conversion)"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#average-versus-median",
    "href": "datacamp/introduction_python/introduction_python.html#average-versus-median",
    "title": "Introduction to Python",
    "section": "Average versus median",
    "text": "Average versus median\nYou now know how to use numpy functions to get a better feeling for your data. It basically comes down to importing numpy and then calling several simple functions on the numpy arrays:\nimport numpy as np x = [1, 4, 8, 10, 12] np.mean(x) np.median(x) The baseball data is available as a 2D numpy array with 3 columns (height, weight, age) and 1015 rows. The name of this numpy array is np_baseball. After restructuring the data, however, you notice that some height values are abnormally high. Follow the instructions and discover which summary statistic is best suited if you’re dealing with so-called outliers. np_baseball is available.\n\n# np_baseball is available\n\n# Import numpy\nimport numpy as np\n\n# Create np_height_in from np_baseball\nnp_height_in = np_baseball[:,0]\n\n# Print out the mean of np_height_in\n\nprint(np.mean(np_height_in))\n\n198.25\n\n# Print out the median of np_height_in\nprint(np.median(np_height_in))\n\n199.0"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#explore-the-baseball-data",
    "href": "datacamp/introduction_python/introduction_python.html#explore-the-baseball-data",
    "title": "Introduction to Python",
    "section": "Explore the baseball data",
    "text": "Explore the baseball data\nBecause the mean and median are so far apart, you decide to complain to the MLB. They find the error and send the corrected data over to you. It’s again available as a 2D NumPy array np_baseball, with three columns.\nThe Python script in the editor already includes code to print out informative messages with the different summary statistics. Can you finish the job? np_baseball is available.\n\n# np_baseball is available\n\n# Import numpy\nimport numpy as np\n\n# Print mean height (first column)\navg = np.mean(np_baseball[:,0])\nprint(\"Average: \" + str(avg))\n\nAverage: 198.25\n\n# Print median height. Replace 'None'\nmed = np.median(np_baseball[:,0])\nprint(\"Median: \" + str(med))\n\nMedian: 199.0\n\n# Print out the standard deviation on height. Replace 'None'\nstddev = np.std(np_baseball[:,0])\nprint(\"Standard Deviation: \" + str(stddev))\n\nStandard Deviation: 14.635146053251399\n\n# Print out correlation between first and second column. Replace 'None'\ncorr = np.corrcoef(np_baseball[:, 0], np_baseball[:, 1])\nprint(\"Correlation: \" + str(corr))\n\nCorrelation: [[1.         0.95865738]\n [0.95865738 1.        ]]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#blend-it-all-together",
    "href": "datacamp/introduction_python/introduction_python.html#blend-it-all-together",
    "title": "Introduction to Python",
    "section": "Blend it all together",
    "text": "Blend it all together\nIn the last few exercises you’ve learned everything there is to know about heights and weights of baseball players. Now it’s time to dive into another sport: soccer.\nYou’ve contacted FIFA for some data and they handed you two lists. The lists are the following:\npositions = [‘GK’, ‘M’, ‘A’, ‘D’, …] heights = [191, 184, 185, 180, …] Each element in the lists corresponds to a player. The first list, positions, contains strings representing each player’s position. The possible positions are: ‘GK’ (goalkeeper), ‘M’ (midfield), ‘A’ (attack) and ‘D’ (defense). The second list, heights, contains integers representing the height of the player in cm. The first player in the lists is a goalkeeper and is pretty tall (191 cm).\nYou’re fairly confident that the median height of goalkeepers is higher than that of other players on the soccer field. Some of your friends don’t believe you, so you are determined to show them using the data you received from FIFA and your newly acquired Python skills. heights and positions are available as lists\n\nnp_positions = np.array(['GK', 'M', 'A', 'D', 'M', 'D', 'M', 'M', 'M', 'A'])\nnp_heights = np.array([191, 184, 185, 180, 181, 187, 170, 179, 183, 186])\n\n# Heights of the goalkeepers: gk_heights\ngk_heights = np_heights[np_positions == 'GK']\n\n# Heights of the other players: other_heights\nother_heights = np_heights[np_positions != 'GK']\n\n# Print out the median height of goalkeepers. Replace 'None'\nprint(\"Median height of goalkeepers: \" + str(np.median(gk_heights)))\n\nMedian height of goalkeepers: 191.0\n\n# Print out the median height of other players. Replace 'None'\nprint(\"Median height of other players: \" + str(np.median(other_heights)))\n\nMedian height of other players: 183.0"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html",
    "href": "datacamp/intermediatePython/IntermediatePython.html",
    "title": "Intermediate Python",
    "section": "",
    "text": "With matplotlib, you can create a bunch of different plots in Python. The most basic plot is the line plot. A general recipe is given here.\nimport matplotlib.pyplot as plt plt.plot(x,y) plt.show() In the video, you already saw how much the world population has grown over the past years. Will it continue to do so? The world bank has estimates of the world population for the years 1950 up to 2100. The years are loaded in your workspace as a list called year, and the corresponding populations as a list called pop.\nThis course touches on a lot of concepts you may have forgotten, so if you ever need a quick refresher, download the Python For Data Science Cheat Sheet and keep it handy!\n\n# Print the last item from year and pop\n\nyear = [1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, \n        1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, \n        1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, \n        1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, \n        1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, \n        2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, \n        2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, \n        2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, \n        2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, \n        2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, \n        2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, \n        2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, \n        2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, \n        2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100]\n\npop = [2.53, 2.57, 2.62, 2.67, 2.71, 2.76, 2.81, 2.86, 2.92, 2.97, 3.03, \n      3.08, 3.14, 3.2, 3.26, 3.33, 3.4, 3.47, 3.54, 3.62, 3.69, 3.77,\n      3.84, 3.92, 4, 4.07, 4.15, 4.22, 4.3, 4.37, 4.45, 4.53, 4.61, \n      4.69, 4.78, 4.86, 4.95, 5.05, 5.14, 5.23, 5.32, 5.41, 5.49, \n      5.58, 5.66, 5.74, 5.82, 5.9, 5.98, 6.05, 6.13, 6.2, 6.28, 6.36,\n      6.44, 6.51, 6.59, 6.67, 6.75, 6.83, 6.92, 7, 7.08, 7.16, 7.24, \n      7.32, 7.4, 7.48, 7.56, 7.64, 7.72, 7.79, 7.87, 7.94, 8.01, 8.08, \n      8.15, 8.22, 8.29, 8.36, 8.42, 8.49, 8.56, 8.62, 8.68, 8.74, 8.8, \n      8.86, 8.92, 8.98, 9.04, 9.09, 9.15, 9.2, 9.26, 9.31, 9.36, 9.41, \n      9.46, 9.5, 9.55, 9.6, 9.64, 9.68, 9.73, 9.77, 9.81, 9.85, 9.88, 9.92, \n      9.96, 9.99, 10.03, 10.06, 10.09, 10.13, 10.16, 10.19, 10.22, 10.25, \n      10.28, 10.31, 10.33, 10.36, 10.38, 10.41, 10.43, 10.46, 10.48, 10.5, \n      10.52, 10.55, 10.57, 10.59, 10.61, 10.63, 10.65, 10.66, 10.68, 10.7, \n      10.72, 10.73, 10.75, 10.77, 10.78, 10.79, 10.81, 10.82, 10.83, 10.84, 10.85]\n\nprint(year[-1])\n\n2100\n\nprint(pop[-1])\n\n10.85\n\n# Import matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\n\n# Make a line plot: year on the x-axis, pop on the y-axis\n\nplt.plot(year, pop)\n# Display the plot with plt.show()\n\nplt.show()\n\n\n\n\n\n\n\nNow that you’ve built your first line plot, let’s start working on the data that professor Hans Rosling used to build his beautiful bubble chart. It was collected in 2007. Two lists are available for you:\nlife_exp which contains the life expectancy for each country and gdp_cap, which contains the GDP per capita (i.e. per person) for each country expressed in US Dollars. GDP stands for Gross Domestic Product. It basically represents the size of the economy of a country. Divide this by the population and you get the GDP per capita.\nmatplotlib.pyplot is already imported as plt, so you can get started straight away.\n\n# Print the last item of gdp_cap and life_exp\n\ngdp_cap = [974.5803384, 5937.029526, 6223.367465, 4797.231267, 12779.37964, \n            34435.36744, 36126.4927, 29796.04834, 1391.253792, 33692.60508, \n            1441.284873, 3822.137084, 7446.298803, 12569.85177, 9065.800825, \n            10680.79282, 1217.032994, 430.0706916, 1713.778686, 2042.09524, \n            36319.23501, 706.016537, 1704.063724, 13171.63885, 4959.114854, \n            7006.580419, 986.1478792, 277.5518587, 3632.557798, 9645.06142, \n            1544.750112, 14619.22272, 8948.102923, 22833.30851, 35278.41874, \n            2082.481567, 6025.374752, 6873.262326, 5581.180998, 5728.353514, \n            12154.08975, 641.3695236, 690.8055759, 33207.0844, 30470.0167, \n            13206.48452, 752.7497265, 32170.37442, 1327.60891, 27538.41188, \n            5186.050003, 942.6542111, 579.231743, 1201.637154, 3548.330846, \n            39724.97867, 18008.94444, 36180.78919, 2452.210407, 3540.651564, \n            11605.71449, 4471.061906, 40675.99635, 25523.2771, 28569.7197, \n            7320.880262, 31656.06806, 4519.461171, 1463.249282, 1593.06548, \n            23348.13973, 47306.98978, 10461.05868, 1569.331442, 414.5073415, \n            12057.49928, 1044.770126, 759.3499101, 12451.6558, 1042.581557, \n            1803.151496, 10956.99112, 11977.57496, 3095.772271, 9253.896111, \n            3820.17523, 823.6856205, 944, 4811.060429, 1091.359778, 36797.93332, \n            25185.00911, 2749.320965, 619.6768924, 2013.977305, 49357.19017, \n            22316.19287, 2605.94758, 9809.185636, 4172.838464, 7408.905561, \n            3190.481016, 15389.92468, 20509.64777, 19328.70901, 7670.122558, \n            10808.47561, 863.0884639, 1598.435089, 21654.83194, 1712.472136, \n            9786.534714, 862.5407561, 47143.17964, 18678.31435, 25768.25759, \n            926.1410683, 9269.657808, 28821.0637, 3970.095407, 2602.394995, \n            4513.480643, 33859.74835, 37506.41907, 4184.548089, 28718.27684, \n            1107.482182, 7458.396327, 882.9699438, 18008.50924, 7092.923025, \n            8458.276384, 1056.380121, 33203.26128, 42951.65309, 10611.46299, \n            11415.80569, 2441.576404, 3025.349798, 2280.769906, 1271.211593, \n            469.7092981]\n            \n            \n\nlife_exp = [43.828, 76.423, 72.301, 42.731, 75.32, 81.235, 79.829, 75.635, \n             64.062, 79.441, 56.728, 65.554, 74.852, 50.728, 72.39, 73.005, \n             52.295, 49.58, 59.723, 50.43, 80.653, 44.741, 50.651, 78.553, \n             72.961, 72.889, 65.152, 46.462, 55.322, 78.782, 48.328, 75.748, \n             78.273, 76.486, 78.332, 54.791, 72.235, 74.994, 71.338, 71.878, \n             51.579, 58.04, 52.947, 79.313, 80.657, 56.735, 59.448, 79.406, \n             60.022, 79.483, 70.259, 56.007, 46.388, 60.916, 70.198, 82.208, \n             73.338, 81.757, 64.698, 70.65, 70.964, 59.545, 78.885, 80.745, \n             80.546, 72.567, 82.603, 72.535, 54.11, 67.297, 78.623, 77.588, \n             71.993, 42.592, 45.678, 73.952, 59.443, 48.303, 74.241, 54.467, \n             64.164, 72.801, 76.195, 66.803, 74.543, 71.164, 42.082, 62.069, \n             52.906, 63.785, 79.762, 80.204, 72.899, 56.867, 46.859, 80.196, \n             75.64, 65.483, 75.537, 71.752, 71.421, 71.688, 75.563, 78.098, \n             78.746, 76.442, 72.476, 46.242, 65.528, 72.777, 63.062, 74.002, \n             42.568, 79.972, 74.663, 77.926, 48.159, 49.339, 80.941, 72.396, \n             58.556, 39.613, 80.884, 81.701, 74.143, 78.4, 52.517, 70.616, \n             58.42, 69.819, 73.923, 71.777, 51.542, 79.425, 78.242, 76.384, \n             73.747, 74.249, 73.422, 62.698, 42.384, 43.487]\n\n\nprint(gdp_cap[-1])\n\n469.7092981\n\nprint(life_exp[-1])\n\n43.487\n\nplt.plot(gdp_cap, life_exp)\n\n# Display the plot\n\nplt.show()\n\n\n\n\n\n\n\nWhen you have a time scale along the horizontal axis, the line plot is your friend. But in many other cases, when you’re trying to assess if there’s a correlation between two variables, for example, the scatter plot is the better choice. Below is an example of how to build a scatter plot.\nimport matplotlib.pyplot as plt plt.scatter(x,y) plt.show() Let’s continue with the gdp_cap versus life_exp plot, the GDP and life expectancy data for different countries in 2007. Maybe a scatter plot will be a better alternative?\n\n# Change the line plot below to a scatter plot\nplt.scatter(gdp_cap, life_exp)\n\n# Put the x-axis on a logarithmic scale\nplt.xscale('log')\n# Show plot.\nplt.show()\n\n\n\n\n\n\n\nIn the previous exercise, you saw that the higher GDP usually corresponds to a higher life expectancy. In other words, there is a positive correlation.\nDo you think there’s a relationship between population and life expectancy of a country? The list life_exp from the previous exercise is already available. In addition, now also pop is available, listing the corresponding populations for the countries in 2007. The populations are in millions of people.\n\n# Build Scatter plot\npop = pop[0:142]\nplt.scatter(pop[0:142], life_exp)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\nlife_exp, the list containing data on the life expectancy for different countries in 2007, is available in your Python shell.\nTo see how life expectancy in different countries is distributed, let’s create a histogram of life_exp.\nmatplotlib.pyplot is already available as plt.\n\n# Create histogram of life_exp data\nplt.hist(life_exp)\n\n# Display histogram\nplt.show()\n\n\n\n\n\n\n\nIn the previous exercise, you didn’t specify the number of bins. By default, Python sets the number of bins to 10 in that case. The number of bins is pretty important. Too few bins will oversimplify reality and won’t show you the details. Too many bins will overcomplicate reality and won’t show the bigger picture.\nTo control the number of bins to divide your data in, you can set the bins argument.\nThat’s exactly what you’ll do in this exercise. You’ll be making two plots here. The code in the script already includes plt.show() and plt.clf() calls; plt.show() displays a plot; plt.clf() cleans it up again so you can start afresh.\nAs before, life_exp is available and matplotlib.pyplot is imported as plt.\n\n# Build histogram with 5 bins\nplt.hist(life_exp, bins= 5)\n\n# Show and clean up plot\nplt.show()\n\n\n\nplt.clf()\n\n# Build histogram with 20 bins\n\nplt.hist(life_exp, bins= 20)\n\n# Show and clean up again\nplt.show()\n\n\n\n#plt.clf()\n\n\n\n\nIn the video, you saw population pyramids for the present day and for the future. Because we were using a histogram, it was very easy to make a comparison.\nLet’s do a similar comparison. life_exp contains life expectancy data for different countries in 2007. You also have access to a second list now, life_exp1950, containing similar data for 1950. Can you make a histogram for both datasets?\nYou’ll again be making two plots. The plt.show() and plt.clf() commands to render everything nicely are already included. Also matplotlib.pyplot is imported for you, as plt.\n\n# Histogram of life_exp, 15 bins\n\nplt.hist(life_exp, bins = 15)\n# Show and clear plot\nplt.show()\n\n\n\nplt.clf()\n\n# Histogram of life_exp1950, 15 bins\nlife_exp1950 = [28.8, 55.23, 43.08, 30.02, 62.48, 69.12, 66.8, 50.94, 37.48, \n                 68, 38.22, 40.41, 53.82, 47.62, 50.92, 59.6, 31.98, 39.03, 39.42, \n                 38.52, 68.75, 35.46, 38.09, 54.74, 44, 50.64, 40.72, 39.14, 42.11, \n                 57.21, 40.48, 61.21, 59.42, 66.87, 70.78, 34.81, 45.93, 48.36, \n                 41.89, 45.26, 34.48, 35.93, 34.08, 66.55, 67.41, 37, 30, 67.5, \n                 43.15, 65.86, 42.02, 33.61, 32.5, 37.58, 41.91, 60.96, 64.03, \n                 72.49, 37.37, 37.47, 44.87, 45.32, 66.91, 65.39, 65.94, 58.53, \n                 63.03, 43.16, 42.27, 50.06, 47.45, 55.56, 55.93, 42.14, 38.48, \n                 42.72, 36.68, 36.26, 48.46, 33.68, 40.54, 50.99, 50.79, 42.24, \n                 59.16, 42.87, 31.29, 36.32, 41.72, 36.16, 72.13, 69.39, 42.31, \n                 37.44, 36.32, 72.67, 37.58, 43.44, 55.19, 62.65, 43.9, 47.75, \n                 61.31, 59.82, 64.28, 52.72, 61.05, 40, 46.47, 39.88, 37.28, 58, \n                 30.33, 60.4, 64.36, 65.57, 32.98, 45.01, 64.94, 57.59, 38.64, \n                 41.41, 71.86, 69.62, 45.88, 58.5, 41.22, 50.85, 38.6, 59.1, 44.6, \n                 43.58, 39.98, 69.18, 68.44, 66.07, 55.09, 40.41, 43.16, 32.55, \n                 42.04, 48.45]\nplt.hist(life_exp1950, bins = 15)\n# Show and clear plot again\nplt.show()\n\n\n\n#plt.clf()\n\n\n\n\nIt’s time to customize your own plot. This is the fun part, you will see your plot come to life!\nYou’re going to work on the scatter plot with world development data: GDP per capita on the x-axis (logarithmic scale), life expectancy on the y-axis. The code for this plot is available in the script.\nAs a first step, let’s add axis labels and a title to the plot. You can do this with the xlabel(), ylabel() and title() functions, available in matplotlib.pyplot. This sub-package is already imported as plt.\n\n# Basic scatter plot, log scale\nplt.scatter(gdp_cap, life_exp)\nplt.xscale('log') \n\n# Strings\nxlab = 'GDP per Capita [in USD]'\nylab = 'Life Expectancy [in years]'\ntitle = 'World Development in 2007'\n\n# Add axis labels\nplt.xlabel(xlab)\nplt.ylabel(ylab)\n\n\n# Add title\nplt.title(title)\n\n# After customizing, display the plot\nplt.show()\n\n\n\n\n\n\n\nThe customizations you’ve coded up to now are available in the script, in a more concise form.\nIn the video, Hugo has demonstrated how you could control the y-ticks by specifying two arguments:\nplt.yticks([0,1,2], [“one”,“two”,“three”]) In this example, the ticks corresponding to the numbers 0, 1 and 2 will be replaced by one, two and three, respectively.\nLet’s do a similar thing for the x-axis of your world development chart, with the xticks() function. The tick values 1000, 10000 and 100000 should be replaced by 1k, 10k and 100k. To this end, two lists have already been created for you: tick_val and tick_lab.\n\n# Scatter plot\nplt.scatter(gdp_cap, life_exp)\n\n# Previous customizations\nplt.xscale('log') \nplt.xlabel('GDP per Capita [in USD]')\nplt.ylabel('Life Expectancy [in years]')\nplt.title('World Development in 2007')\n\n# Definition of tick_val and tick_lab\ntick_val = [1000, 10000, 100000]\ntick_lab = ['1k', '10k', '100k']\n\n# Adapt the ticks on the x-axis\nplt.xticks(tick_val, tick_lab)\n\n([<matplotlib.axis.XTick object at 0x7f21a695e150>, <matplotlib.axis.XTick object at 0x7f21a81f2810>, <matplotlib.axis.XTick object at 0x7f21abb9db90>], [Text(1000, 0, '1k'), Text(10000, 0, '10k'), Text(100000, 0, '100k')])\n\n# After customizing, display the plot\nplt.show()\n\n\n\n\n\n\n\nRight now, the scatter plot is just a cloud of blue dots, indistinguishable from each other. Let’s change this. Wouldn’t it be nice if the size of the dots corresponds to the population?\nTo accomplish this, there is a list pop loaded in your workspace. It contains population numbers for each country expressed in millions. You can see that this list is added to the scatter method, as the argument s, for size.\n\n # Import numpy as np\n\nimport numpy as np\n# Store pop as a numpy array: np_pop\nnp_pop = np.array(pop)\n\n# Double np_pop\n\nnp_pop = np_pop*2\n# Update: set s argument to np_pop\nplt.scatter(gdp_cap, life_exp,  s = np_pop)\n\n# Previous customizations\nplt.xscale('log') \nplt.xlabel('GDP per Capita [in USD]')\nplt.ylabel('Life Expectancy [in years]')\nplt.title('World Development in 2007')\nplt.xticks([1000, 10000, 100000],['1k', '10k', '100k'])\n\n([<matplotlib.axis.XTick object at 0x7f21a8180e90>, <matplotlib.axis.XTick object at 0x7f21abb42810>, <matplotlib.axis.XTick object at 0x7f21a6922810>], [Text(1000, 0, '1k'), Text(10000, 0, '10k'), Text(100000, 0, '100k')])\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\nThe code you’ve written up to now is available in the script.\nThe next step is making the plot more colorful! To do this, a list col has been created for you. It’s a list with a color for each corresponding country, depending on the continent the country is part of.\nHow did we make the list col you ask? The Gapminder data contains a list continent with the continent each country belongs to. A dictionary is constructed that maps continents onto colors:\ndict = { ‘Asia’:‘red’, ‘Europe’:‘green’, ‘Africa’:‘blue’, ‘Americas’:‘yellow’, ‘Oceania’:‘black’ } Nothing to worry about now; you will learn about dictionaries in the next chapter.\n\n# Specify c and alpha inside plt.scatter()\ncol =  [\"red\", \"green\", \"blue\", \"blue\", \"yellow\", \"black\", \"green\", \n        \"red\", \"red\", \"green\", \"blue\", \"yellow\", \"green\", \"blue\", \"yellow\", \n        \"green\", \"blue\", \"blue\", \"red\", \"blue\", \"yellow\", \"blue\", \"blue\", \n        \"yellow\", \"red\", \"yellow\", \"blue\", \"blue\", \"blue\", \"yellow\", \n        \"blue\", \"green\", \"yellow\", \"green\", \"green\", \"blue\", \"yellow\", \n        \"yellow\", \"blue\", \"yellow\", \"blue\", \"blue\", \"blue\", \"green\", \n        \"green\", \"blue\", \"blue\", \"green\", \"blue\", \"green\", \"yellow\", \n        \"blue\", \"blue\", \"yellow\", \"yellow\", \"red\", \"green\", \"green\", \n        \"red\", \"red\", \"red\", \"red\", \"green\", \"red\", \"green\", \"yellow\", \n        \"red\", \"red\", \"blue\", \"red\", \"red\", \"red\", \"red\", \"blue\", \"blue\", \n        \"blue\", \"blue\", \"blue\", \"red\", \"blue\", \"blue\", \"blue\", \"yellow\", \n        \"red\", \"green\", \"blue\", \"blue\", \"red\", \"blue\", \"red\", \"green\", \n        \"black\", \"yellow\", \"blue\", \"blue\", \"green\", \"red\", \"red\", \"yellow\", \n        \"yellow\", \"yellow\", \"red\", \"green\", \"green\", \"yellow\", \"blue\", \n        \"green\", \"blue\", \"blue\", \"red\", \"blue\", \"green\", \"blue\", \"red\", \n        \"green\", \"green\", \"blue\", \"blue\", \"green\", \"red\", \"blue\", \"blue\", \n        \"green\", \"green\", \"red\", \"red\", \"blue\", \"red\", \"blue\", \"yellow\", \n        \"blue\", \"green\", \"blue\", \"green\", \"yellow\", \"yellow\", \"yellow\", \n        \"red\", \"red\", \"red\", \"blue\", \"blue\"]\nplt.scatter(x = gdp_cap, y = life_exp, s = np.array(pop) * 2, alpha = 0.8, c = col)\n\n# Previous customizations\nplt.xscale('log') \nplt.xlabel('GDP per Capita [in USD]')\nplt.ylabel('Life Expectancy [in years]')\nplt.title('World Development in 2007')\nplt.xticks([1000,10000,100000], ['1k','10k','100k'])\n\n([<matplotlib.axis.XTick object at 0x7f21a69598d0>, <matplotlib.axis.XTick object at 0x7f21a6390f50>, <matplotlib.axis.XTick object at 0x7f21a6312810>], [Text(1000, 0, '1k'), Text(10000, 0, '10k'), Text(100000, 0, '100k')])\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nIf you have another look at the script, under # Additional Customizations, you’ll see that there are two plt.text() functions now. They add the words “India” and “China” in the plot.\n\n# Scatter plot\nplt.scatter(x = gdp_cap, y = life_exp, s = np.array(pop) * 2, c = col, alpha = 0.8)\n\n# Previous customizations\nplt.xscale('log') \nplt.xlabel('GDP per Capita [in USD]')\nplt.ylabel('Life Expectancy [in years]')\nplt.title('World Development in 2007')\nplt.xticks([1000,10000,100000], ['1k','10k','100k'])\n\n([<matplotlib.axis.XTick object at 0x7f21a6912590>, <matplotlib.axis.XTick object at 0x7f21a63bd510>, <matplotlib.axis.XTick object at 0x7f21a622db90>], [Text(1000, 0, '1k'), Text(10000, 0, '10k'), Text(100000, 0, '100k')])\n\n# Additional customizations\nplt.text(1550, 71, 'India')\nplt.text(5700, 80, 'China')\n\n# Add grid() call\n\nplt.grid(True)\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#motivation-for-dictionaries",
    "href": "datacamp/intermediatePython/IntermediatePython.html#motivation-for-dictionaries",
    "title": "Intermediate Python",
    "section": "Motivation for dictionaries",
    "text": "Motivation for dictionaries\nTo see why dictionaries are useful, have a look at the two lists defined in the script. countries contains the names of some European countries. capitals lists the corresponding names of their capital.\n\n# Definition of countries and capital\ncountries = ['spain', 'france', 'germany', 'norway']\ncapitals = ['madrid', 'paris', 'berlin', 'oslo']\n\n# Get index of 'germany': ind_ger\nind_ger = countries.index(\"germany\")\n\n# Use ind_ger to print out capital of Germany\nprint(capitals[ind_ger])\n\nberlin"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#access-dictionary",
    "href": "datacamp/intermediatePython/IntermediatePython.html#access-dictionary",
    "title": "Intermediate Python",
    "section": "Access dictionary",
    "text": "Access dictionary\nIf the keys of a dictionary are chosen wisely, accessing the values in a dictionary is easy and intuitive. For example, to get the capital for France from europe you can use:\neurope[‘france’] Here, ‘france’ is the key and ‘paris’ the value is returned.\n\n# Definition of dictionary\neurope = {'spain':'madrid', 'france':'paris', 'germany':'berlin', 'norway':'oslo' }\n\n# Print out the keys in europe\nprint(europe.keys())\n\ndict_keys(['spain', 'france', 'germany', 'norway'])\n\n# Print out value that belongs to key 'norway'\n\nprint(europe['norway'])\n\noslo"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#create-dictionary",
    "href": "datacamp/intermediatePython/IntermediatePython.html#create-dictionary",
    "title": "Intermediate Python",
    "section": "Create dictionary",
    "text": "Create dictionary\nThe countries and capitals lists are again available in the script. It’s your job to convert this data to a dictionary where the country names are the keys and the capitals are the corresponding values. As a refresher, here is a recipe for creating a dictionary:\n\nmy_dict = {\n   \"key1\":\"value1\",\n   \"key2\":\"value2\",\n}\n\nIn this recipe, both the keys and the values are strings. This will also be the case for this exercise.\n\n# Definition of countries and capital\ncountries = ['spain', 'france', 'germany', 'norway']\ncapitals = ['madrid', 'paris', 'berlin', 'oslo']\n\n# From string in countries and capitals, create dictionary europe\neurope = { 'spain':'madrid', 'france': 'paris','germany' : 'berlin', 'norway' : 'oslo'}\n\n# Print europe\nprint(europe)\n\n{'spain': 'madrid', 'france': 'paris', 'germany': 'berlin', 'norway': 'oslo'}"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#dictionary-manipulation-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#dictionary-manipulation-1",
    "title": "Intermediate Python",
    "section": "Dictionary Manipulation (1)",
    "text": "Dictionary Manipulation (1)\nIf you know how to access a dictionary, you can also assign a new value to it. To add a new key-value pair to europe you can use something like this:\n\n# Definition of dictionary\neurope = {'spain':'madrid', 'france':'paris', 'germany':'berlin', 'norway':'oslo' }\n\n# Add italy to europe\n\neurope['italy'] = 'rome'\n# Print out italy in europe\n\nprint('italy' in europe)\n\nTrue\n\n# Add poland to europe\n\neurope['poland'] = 'warsaw'\n# Print europe\nprint(europe)\n\n{'spain': 'madrid', 'france': 'paris', 'germany': 'berlin', 'norway': 'oslo', 'italy': 'rome', 'poland': 'warsaw'}"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#dictionary-manipulation-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#dictionary-manipulation-2",
    "title": "Intermediate Python",
    "section": "Dictionary Manipulation (2)",
    "text": "Dictionary Manipulation (2)\nSomebody thought it would be funny to mess with your accurately generated dictionary. An adapted version of the europe dictionary is available in the script.\nCan you clean up? Do not do this by adapting the definition of europe, but by adding Python commands to the script to update and remove key:value pairs.\n\n# Definition of dictionary\neurope = {'spain':'madrid', 'france':'paris', 'germany':'bonn',\n          'norway':'oslo', 'italy':'rome', 'poland':'warsaw',\n          'australia':'vienna' }\n\n# Update capital of germany\n\neurope['germany'] = 'berlin'\n# Remove australia\n\ndel(europe['australia'])\n# Print europe\nprint(europe)\n\n{'spain': 'madrid', 'france': 'paris', 'germany': 'berlin', 'norway': 'oslo', 'italy': 'rome', 'poland': 'warsaw'}"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#dictionariception",
    "href": "datacamp/intermediatePython/IntermediatePython.html#dictionariception",
    "title": "Intermediate Python",
    "section": "Dictionariception",
    "text": "Dictionariception\nRemember lists? They could contain anything, even other lists. Well, for dictionaries the same holds. Dictionaries can contain key:value pairs where the values are again dictionaries.\nAs an example, have a look at the script where another version of europe - the dictionary you’ve been working with all along - is coded. The keys are still the country names, but the values are dictionaries that contain more information than just the capital.\nIt’s perfectly possible to chain square brackets to select elements. To fetch the population for Spain from europe, for example, you need:\n\n# Dictionary of dictionaries\neurope = { 'spain': { 'capital':'madrid', 'population':46.77 },\n           'france': { 'capital':'paris', 'population':66.03 },\n           'germany': { 'capital':'berlin', 'population':80.62 },\n           'norway': { 'capital':'oslo', 'population':5.084 } }\n\n\n# Print out the capital of France\n\nprint(europe['spain']['capital'])\n\nmadrid\n\n# Create sub-dictionary data\ndata = {'capital' : 'rome', 'population' : 59.83}\n\n# Add data to europe under key 'italy'\neurope['italy'] = data\n\n# Print europe\n\nprint(europe)\n\n{'spain': {'capital': 'madrid', 'population': 46.77}, 'france': {'capital': 'paris', 'population': 66.03}, 'germany': {'capital': 'berlin', 'population': 80.62}, 'norway': {'capital': 'oslo', 'population': 5.084}, 'italy': {'capital': 'rome', 'population': 59.83}}"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#dictionary-to-dataframe-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#dictionary-to-dataframe-1",
    "title": "Intermediate Python",
    "section": "Dictionary to DataFrame (1)",
    "text": "Dictionary to DataFrame (1)\nPandas is an open source library, providing high-performance, easy-to-use data structures and data analysis tools for Python. Sounds promising!\nThe DataFrame is one of Pandas’ most important data structures. It’s basically a way to store tabular data where you can label the rows and the columns. One way to build a DataFrame is from a dictionary.\nIn the exercises that follow you will be working with vehicle data from different countries. Each observation corresponds to a country and the columns give information about the number of vehicles per capita, whether people drive left or right, and so on.\nThree lists are defined in the script:\nnames, containing the country names for which data is available. dr, a list with booleans that tells whether people drive left or right in the corresponding country. cpc, the number of motor vehicles per 1000 people in the corresponding country. Each dictionary key is a column label and each value is a list which contains the column elements.\n\n# Pre-defined lists\nnames = ['United States', 'Australia', 'Japan', 'India', 'Russia', 'Morocco', 'Egypt']\ndr =  [True, False, False, False, True, True, True]\ncpc = [809, 731, 588, 18, 200, 70, 45]\n\n# Import pandas as pd\n\nimport pandas as pd\n# Create dictionary my_dict with three key:value pairs: my_dict\n\nmy_dict = {'country' :names, 'drives_right' : dr, 'cars_per_cap' : cpc }\n# Build a DataFrame cars from my_dict: cars\ncars = pd.DataFrame(my_dict)\n\n\nfrom IPython.display import HTML\nHTML(cars.to_html())\n\n\n\n  \n    \n      \n      country\n      drives_right\n      cars_per_cap\n    \n  \n  \n    \n      0\n      United States\n      True\n      809\n    \n    \n      1\n      Australia\n      False\n      731\n    \n    \n      2\n      Japan\n      False\n      588\n    \n    \n      3\n      India\n      False\n      18\n    \n    \n      4\n      Russia\n      True\n      200\n    \n    \n      5\n      Morocco\n      True\n      70\n    \n    \n      6\n      Egypt\n      True\n      45"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#dictionary-to-dataframe-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#dictionary-to-dataframe-2",
    "title": "Intermediate Python",
    "section": "Dictionary to DataFrame (2)",
    "text": "Dictionary to DataFrame (2)\nThe Python code that solves the previous exercise is included in the script. Have you noticed that the row labels (i.e. the labels for the different observations) were automatically set to integers from 0 up to 6?\nTo solve this a list row_labels has been created. You can use it to specify the row labels of the cars DataFrame. You do this by setting the index attribute of cars, that you can access as cars.index.\n\n# Build cars DataFrame\nnames = ['United States', 'Australia', 'Japan', 'India', 'Russia', 'Morocco', 'Egypt']\ndr =  [True, False, False, False, True, True, True]\ncpc = [809, 731, 588, 18, 200, 70, 45]\ncars_dict = { 'country':names, 'drives_right':dr, 'cars_per_cap':cpc }\ncars = pd.DataFrame(cars_dict)\nprint(cars)\n\n         country  drives_right  cars_per_cap\n0  United States          True           809\n1      Australia         False           731\n2          Japan         False           588\n3          India         False            18\n4         Russia          True           200\n5        Morocco          True            70\n6          Egypt          True            45\n\n# Definition of row_labels\nrow_labels = ['US', 'AUS', 'JPN', 'IN', 'RU', 'MOR', 'EG']\n\n# Specify row labels of cars\ncars.index = row_labels\n\n# Print cars again\nHTML(cars.to_html())\n\n\n\n  \n    \n      \n      country\n      drives_right\n      cars_per_cap\n    \n  \n  \n    \n      US\n      United States\n      True\n      809\n    \n    \n      AUS\n      Australia\n      False\n      731\n    \n    \n      JPN\n      Japan\n      False\n      588\n    \n    \n      IN\n      India\n      False\n      18\n    \n    \n      RU\n      Russia\n      True\n      200\n    \n    \n      MOR\n      Morocco\n      True\n      70\n    \n    \n      EG\n      Egypt\n      True\n      45"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#csv-to-dataframe-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#csv-to-dataframe-1",
    "title": "Intermediate Python",
    "section": "CSV to DataFrame (1)",
    "text": "CSV to DataFrame (1)\nPutting data in a dictionary and then building a DataFrame works, but it’s not very efficient. What if you’re dealing with millions of observations? In those cases, the data is typically available as files with a regular structure. One of those file types is the CSV file, which is short for “comma-separated values”.\nTo import CSV data into Python as a Pandas DataFrame you can use read_csv().\nLet’s explore this function with the same cars data from the previous exercises. This time, however, the data is available in a CSV file, named cars.csv. It is available in your current working directory, so the path to the file is simply ‘cars.csv’.\n\n# Import the cars.csv data: cars\n\ncars = pd.read_csv('data/cars.csv')\n# Print out cars\nHTML(cars.to_html())\n\n\n\n  \n    \n      \n      Unnamed: 0\n      cars_per_cap\n      country\n      drives_right\n    \n  \n  \n    \n      0\n      US\n      809\n      United States\n      True\n    \n    \n      1\n      AUS\n      731\n      Australia\n      False\n    \n    \n      2\n      JAP\n      588\n      Japan\n      False\n    \n    \n      3\n      IN\n      18\n      India\n      False\n    \n    \n      4\n      RU\n      200\n      Russia\n      True\n    \n    \n      5\n      MOR\n      70\n      Morocco\n      True\n    \n    \n      6\n      EG\n      45\n      Egypt\n      True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#csv-to-dataframe-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#csv-to-dataframe-2",
    "title": "Intermediate Python",
    "section": "CSV to DataFrame (2)",
    "text": "CSV to DataFrame (2)\nYour read_csv() call to import the CSV data didn’t generate an error, but the output is not entirely what we wanted. The row labels were imported as another column without a name.\nRemember index_col, an argument of read_csv(), that you can use to specify which column in the CSV file should be used as a row label? Well, that’s exactly what you need here!\nPython code that solves the previous exercise is already included; can you make the appropriate changes to fix the data import?\n\n# Fix import by including index_col\ncars = pd.read_csv('data/cars.csv', index_col = 0)\n\n# Print out cars\nHTML(cars.to_html())\n\n\n\n  \n    \n      \n      cars_per_cap\n      country\n      drives_right\n    \n  \n  \n    \n      US\n      809\n      United States\n      True\n    \n    \n      AUS\n      731\n      Australia\n      False\n    \n    \n      JAP\n      588\n      Japan\n      False\n    \n    \n      IN\n      18\n      India\n      False\n    \n    \n      RU\n      200\n      Russia\n      True\n    \n    \n      MOR\n      70\n      Morocco\n      True\n    \n    \n      EG\n      45\n      Egypt\n      True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#square-brackets-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#square-brackets-1",
    "title": "Intermediate Python",
    "section": "Square Brackets (1)",
    "text": "Square Brackets (1)\nIn the video, you saw that you can index and select Pandas DataFrames in many different ways. The simplest, but not the most powerful way, is to use square brackets.\nIn the sample code, the same cars data is imported from a CSV files as a Pandas DataFrame. To select only the cars_per_cap column from cars, you can use:\ncars[‘cars_per_cap’] cars[[‘cars_per_cap’]] The single bracket version gives a Pandas Series, the double bracket version gives a Pandas DataFrame.\n\n# Print out country column as Pandas Series\n\nprint(cars['country'])\n\nUS     United States\nAUS        Australia\nJAP            Japan\nIN             India\nRU            Russia\nMOR          Morocco\nEG             Egypt\nName: country, dtype: object\n\n# Print out country column as Pandas DataFrame\nprint(cars[['country']])\n\n           country\nUS   United States\nAUS      Australia\nJAP          Japan\nIN           India\nRU          Russia\nMOR        Morocco\nEG           Egypt\n\n# Print out DataFrame with country and drives_right columns\nprint(cars[['country', 'drives_right']])\n\n           country  drives_right\nUS   United States          True\nAUS      Australia         False\nJAP          Japan         False\nIN           India         False\nRU          Russia          True\nMOR        Morocco          True\nEG           Egypt          True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#square-brackets-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#square-brackets-2",
    "title": "Intermediate Python",
    "section": "Square Brackets (2)",
    "text": "Square Brackets (2)\nSquare brackets can do more than just selecting columns. You can also use them to get rows, or observations, from a DataFrame. The following call selects the first five rows from the cars DataFrame:\ncars[0:5] The result is another DataFrame containing only the rows you specified.\nPay attention: You can only select rows using square brackets if you specify a slice, like 0:4. Also, you’re using the integer indexes of the rows here, not the row labels!\n\n# Print out first 3 observations\nprint(cars[0:3])\n\n     cars_per_cap        country  drives_right\nUS            809  United States          True\nAUS           731      Australia         False\nJAP           588          Japan         False\n\n# Print out fourth, fifth and sixth observation\nprint(cars[3:6])\n\n     cars_per_cap  country  drives_right\nIN             18    India         False\nRU            200   Russia          True\nMOR            70  Morocco          True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#loc-and-iloc-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#loc-and-iloc-1",
    "title": "Intermediate Python",
    "section": "loc and iloc (1)",
    "text": "loc and iloc (1)\nWith loc and iloc you can do practically any data selection operation on DataFrames you can think of. loc is label-based, which means that you have to specify rows and columns based on their row and column labels. iloc is integer index based, so you have to specify rows and columns by their integer index like you did in the previous exercise.\nTry out the following commands in the IPython Shell to experiment with loc and iloc to select observations. Each pair of commands here gives the same result.\ncars.loc[‘RU’] cars.iloc[4]\ncars.loc[[‘RU’]] cars.iloc[[4]]\ncars.loc[[‘RU’, ‘AUS’]] cars.iloc[[4, 1]] As before, code is included that imports the cars data as a Pandas DataFrame.\n\n# Print out observation for Japan\nprint(cars.loc['JAP'])\n\ncars_per_cap      588\ncountry         Japan\ndrives_right    False\nName: JAP, dtype: object\n\n# Print out observations for Australia and Egypt\nprint(cars.loc[['AUS', 'EG']])\n\n     cars_per_cap    country  drives_right\nAUS           731  Australia         False\nEG             45      Egypt          True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#loc-and-iloc-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#loc-and-iloc-2",
    "title": "Intermediate Python",
    "section": "loc and iloc (2)",
    "text": "loc and iloc (2)\nloc and iloc also allow you to select both rows and columns from a DataFrame. To experiment, try out the following commands in the IPython Shell. Again, paired commands produce the same result.\n\n# Print out drives_right value of Morocco\nprint(cars.loc[['MOR'], 'drives_right'])\n\nMOR    True\nName: drives_right, dtype: bool\n\n# Print sub-DataFrame\nprint(cars.loc[['RU','MOR'], ['country','drives_right']])\n\n     country  drives_right\nRU    Russia          True\nMOR  Morocco          True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#loc-and-iloc-3",
    "href": "datacamp/intermediatePython/IntermediatePython.html#loc-and-iloc-3",
    "title": "Intermediate Python",
    "section": "loc and iloc (3)",
    "text": "loc and iloc (3)\nIt’s also possible to select only columns with loc and iloc. In both cases, you simply put a slice going from beginning to end in front of the comma:\n\n# Print out drives_right column as Series\nprint(cars.loc[:, 'drives_right'])\n\nUS      True\nAUS    False\nJAP    False\nIN     False\nRU      True\nMOR     True\nEG      True\nName: drives_right, dtype: bool\n\n# Print out drives_right column as DataFrame\n\nprint(cars.loc[:, ['drives_right']])\n\n     drives_right\nUS           True\nAUS         False\nJAP         False\nIN          False\nRU           True\nMOR          True\nEG           True\n\n# Print out cars_per_cap and drives_right as DataFrame\n\nprint(cars.loc[:, ['cars_per_cap','drives_right']])\n\n     cars_per_cap  drives_right\nUS            809          True\nAUS           731         False\nJAP           588         False\nIN             18         False\nRU            200          True\nMOR            70          True\nEG             45          True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#equality",
    "href": "datacamp/intermediatePython/IntermediatePython.html#equality",
    "title": "Intermediate Python",
    "section": "Equality",
    "text": "Equality\nTo check if two Python values, or variables, are equal you can use ==. To check for inequality, you need !=. As a refresher, have a look at the following examples that all result in True. Feel free to try them out in the IPython Shell.\n2 == (1 + 1) “intermediate” != “python” True != False “Python” != “python” When you write these comparisons in a script, you will need to wrap a print() function around them to see the output.\n\n# Comparison of booleans\nprint(True == False)\n\nFalse\n\n# Comparison of integers\nprint(-5*15 != 75)\n\nTrue\n\n# Comparison of strings\nprint(\"pyscript\" == \"PyScript\")\n\nFalse\n\n# Compare a boolean with an integer\nprint(True == 1)\n\nTrue"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#greater-and-less-than",
    "href": "datacamp/intermediatePython/IntermediatePython.html#greater-and-less-than",
    "title": "Intermediate Python",
    "section": "Greater and less than",
    "text": "Greater and less than\nIn the video, Hugo also talked about the less than and greater than signs, < and > in Python. You can combine them with an equals sign: <= and >=. Pay attention: <= is valid syntax, but =< is not.\nAll Python expressions in the following code chunk evaluate to True:\n\n# Comparison of integers\nx = -3 * 6\nprint(x >= -10)\n\nFalse\n\n# Comparison of strings\ny=\"test\"\nprint(\"test\" <= y)\n\nTrue\n\n# Comparison of booleans\nprint(True > False)\n\nTrue"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#compare-arrays",
    "href": "datacamp/intermediatePython/IntermediatePython.html#compare-arrays",
    "title": "Intermediate Python",
    "section": "Compare arrays",
    "text": "Compare arrays\nOut of the box, you can also use comparison operators with NumPy arrays.\nRemember areas, the list of area measurements for different rooms in your house from Introduction to Python? This time there’s two NumPy arrays: my_house and your_house. They both contain the areas for the kitchen, living room, bedroom and bathroom in the same order, so you can compare them.\n\n# Create arrays\nmy_house = np.array([18.0, 20.0, 10.75, 9.50])\nyour_house = np.array([14.0, 24.0, 14.25, 9.0])\n\n# my_house greater than or equal to 18\nprint(my_house>=18)\n\n[ True  True False False]\n\n# my_house less than your_house\nprint(my_house < your_house)\n\n[False  True  True False]"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#and-or-not-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#and-or-not-1",
    "title": "Intermediate Python",
    "section": "and, or, not (1)",
    "text": "and, or, not (1)\nA boolean is either 1 or 0, True or False. With boolean operators such as and, or and not, you can combine these booleans to perform more advanced queries on your data.\nIn the sample code, two variables are defined: my_kitchen and your_kitchen, representing areas\n\n# Define variables\nmy_kitchen = 18.0\nyour_kitchen = 14.0\n\n# my_kitchen bigger than 10 and smaller than 18?\nprint(my_kitchen> 10 and my_kitchen < 18)\n\nFalse\n\n# my_kitchen smaller than 14 or bigger than 17?\n\nprint(my_kitchen> 17 or my_kitchen < 14)\n\nTrue\n\n# Double my_kitchen smaller than triple your_kitchen?\n\nprint(my_kitchen * 2 <  your_kitchen * 3)\n\nTrue\n\n\nand, or, not (2) To see if you completely understood the boolean operators, have a look at the following piece of Python code:\n\nx = 8\ny = 9\nnot(not(x < 3) and not(y > 14 or y > 10))\n\nFalse\n\n\nWhat will the result be if you execute these three commands in the IPython Shell?\nNB: Notice that not has a higher priority than and and or, it is executed first."
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#boolean-operators-with-numpy",
    "href": "datacamp/intermediatePython/IntermediatePython.html#boolean-operators-with-numpy",
    "title": "Intermediate Python",
    "section": "Boolean operators with NumPy",
    "text": "Boolean operators with NumPy\nBefore, the operational operators like < and >= worked with NumPy arrays out of the box. Unfortunately, this is not true for the boolean operators and, or, and not.\nTo use these operators with NumPy, you will need np.logical_and(), np.logical_or() and np.logical_not(). Here’s an example on the my_house and your_house arrays from before to give you an idea:\n\n# Create arrays\nimport numpy as np\nmy_house = np.array([18.0, 20.0, 10.75, 9.50])\nyour_house = np.array([14.0, 24.0, 14.25, 9.0])\n\n# my_house greater than 18.5 or smaller than 10\nprint(np.logical_or(your_house > 18.5, \n               your_house < 10))\n\n[False  True False  True]\n\n# Both my_house and your_house smaller than 11\nprint(np.logical_and(my_house < 11, \n               your_house < 11))\n\n[False False False  True]"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#warmup",
    "href": "datacamp/intermediatePython/IntermediatePython.html#warmup",
    "title": "Intermediate Python",
    "section": "Warmup",
    "text": "Warmup\nTo experiment with if and else a bit, have a look at this code sample:\n\narea = 10.0\nif(area < 9) :\n    print(\"small\")\nelif(area < 12) :\n    print(\"medium\")\nelse :\n    print(\"large\")\n\nmedium"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#if",
    "href": "datacamp/intermediatePython/IntermediatePython.html#if",
    "title": "Intermediate Python",
    "section": "if",
    "text": "if\nIt’s time to take a closer look around in your house.\nTwo variables are defined in the sample code: room, a string that tells you which room of the house we’re looking at, and area, the area of that room.\n\n# Define variables\nroom = \"kit\"\narea = 14.0\n\n# if statement for room\nif room == \"kit\" :\n    print(\"looking around in the kitchen.\")\n\nlooking around in the kitchen.\n\n\n# if statement for area\nif area > 15 :\n    print(\"big place!\")"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#add-else",
    "href": "datacamp/intermediatePython/IntermediatePython.html#add-else",
    "title": "Intermediate Python",
    "section": "Add else",
    "text": "Add else\nIn the script, the if construct for room has been extended with an else statement so that “looking around elsewhere.” is printed if the condition room == “kit” evaluates to False.\nCan you do a similar thing to add more functionality to the if construct for area?\n\n# Define variables\nroom = \"kit\"\narea = 14.0\n\n# if-else construct for room\nif room == \"kit\" :\n    print(\"looking around in the kitchen.\")\nelse :\n    print(\"looking around elsewhere.\")\n\nlooking around in the kitchen.\n\n# if-else construct for area\nif area > 15 :\n    print(\"big place!\")\nelse:\n    print(\"pretty small.\")\n\npretty small."
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#customize-further-elif",
    "href": "datacamp/intermediatePython/IntermediatePython.html#customize-further-elif",
    "title": "Intermediate Python",
    "section": "Customize further: elif",
    "text": "Customize further: elif\nIt’s also possible to have a look around in the bedroom. The sample code contains an elif part that checks if room equals “bed”. In that case, “looking around in the bedroom.” is printed out.\nIt’s up to you now! Make a similar addition to the second control structure to further customize the messages for different values of area.\n\n# Define variables\nroom = \"bed\"\narea = 14.0\n\n# if-elif-else construct for room\nif room == \"kit\" :\n    print(\"looking around in the kitchen.\")\nelif room == \"bed\":\n    print(\"looking around in the bedroom.\")\nelse :\n    print(\"looking around elsewhere.\")\n\nlooking around in the bedroom.\n\n# if-elif-else construct for area\nif area > 15 :\n    print(\"big place!\")\nelif area > 10:\n    print(\"medium size, nice!\")\nelse :\n    print(\"pretty small.\")\n\nmedium size, nice!"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#driving-right-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#driving-right-1",
    "title": "Intermediate Python",
    "section": "Driving right (1)",
    "text": "Driving right (1)\nRemember that cars dataset, containing the cars per 1000 people (cars_per_cap) and whether people drive right (drives_right) for different countries (country)? The code that imports this data in CSV format into Python as a DataFrame is included in the script.\nIn the video, you saw a step-by-step approach to filter observations from a DataFrame based on boolean arrays. Let’s start simple and try to find all observations in cars where drives_right is True. drives_right is a boolean column, so you’ll have to extract it as a Series and then use this boolean Series to select observations from cars.\n\n# Extract drives_right column as Series: dr\ndr = cars['drives_right']\n\n# Use dr to subset cars: sel\nsel= cars[dr]\n\n# Print sel\nprint(sel)\n\n     cars_per_cap        country  drives_right\nUS            809  United States          True\nRU            200         Russia          True\nMOR            70        Morocco          True\nEG             45          Egypt          True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#driving-right-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#driving-right-2",
    "title": "Intermediate Python",
    "section": "Driving right (2)",
    "text": "Driving right (2)\nThe code in the previous example worked fine, but you actually unnecessarily created a new variable dr. You can achieve the same result without this intermediate variable. Put the code that computes dr straight into the square brackets that select observations from cars.\n\n# Convert code to a one-liner\n#dr = cars['drives_right']\nsel = cars[cars['drives_right']]\n\n# Print sel\nprint(sel)\n\n     cars_per_cap        country  drives_right\nUS            809  United States          True\nRU            200         Russia          True\nMOR            70        Morocco          True\nEG             45          Egypt          True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#cars-per-capita-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#cars-per-capita-1",
    "title": "Intermediate Python",
    "section": "Cars per capita (1)",
    "text": "Cars per capita (1)\nLet’s stick to the cars data some more. This time you want to find out which countries have a high cars per capita figure. In other words, in which countries do many people have a car, or maybe multiple cars.\nSimilar to the previous example, you’ll want to build up a boolean Series, that you can then use to subset the cars DataFrame to select certain observations. If you want to do this in a one-liner, that’s perfectly fine!\n\n# Create car_maniac: observations that have a cars_per_cap over 500\n\ncar_maniac = cars[cars['cars_per_cap'] > 500]\n\n\n# Print car_maniac\nprint(car_maniac)\n\n     cars_per_cap        country  drives_right\nUS            809  United States          True\nAUS           731      Australia         False\nJAP           588          Japan         False"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#cars-per-capita-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#cars-per-capita-2",
    "title": "Intermediate Python",
    "section": "Cars per capita (2)",
    "text": "Cars per capita (2)\nRemember about np.logical_and(), np.logical_or() and np.logical_not(), the NumPy variants of the and, or and not operators? You can also use them on Pandas Series to do more advanced filtering operations.\nTake this example that selects the observations that have a cars_per_cap between 10 and 80. Try out these lines of code step by step to see what’s happening.\n\n# Create medium: observations with cars_per_cap between 100 and 500\ncpc = cars['cars_per_cap']\nbetween = np.logical_and(cpc > 100, cpc < 500)\nmedium = cars[between]\n\n# Print medium\n\nprint(medium)\n\n    cars_per_cap country  drives_right\nRU           200  Russia          True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#while-warming-up",
    "href": "datacamp/intermediatePython/IntermediatePython.html#while-warming-up",
    "title": "Intermediate Python",
    "section": "while: warming up",
    "text": "while: warming up\nThe while loop is like a repeated if statement. The code is executed over and over again, as long as the condition is True. Have another look at its recipe.\nwhile condition : expression Can you tell how many printouts the following while loop will do?\n\nx = 1\nwhile x < 4 :\n    print(x)\n    x = x + 1\n\n1\n2\n3"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#basic-while-loop",
    "href": "datacamp/intermediatePython/IntermediatePython.html#basic-while-loop",
    "title": "Intermediate Python",
    "section": "Basic while loop",
    "text": "Basic while loop\nBelow you can find the example from the video where the error variable, initially equal to 50.0, is divided by 4 and printed out on every run:\nerror = 50.0\nwhile error > 1 :\n    error = error / 4\n    print(error)\n\nThis example will come in handy, because it’s time to build a while loop yourself! We’re going to code a while loop that implements a very basic control system for an inverted pendulum. If there’s an offset from standing perfectly straight, the while loop will incrementally fix this offset.\nNote that if your while loop takes too long to run, you might have made a mistake. In particular, remember to indent the contents of the loop using four spaces or auto-indentation!\n\n# Initialize offset\n\noffset = 8\n# Code the while loop\nwhile offset > 0:\n    print(\"correcting...\")\n    offset = offset - 1\n    print(offset)\n\ncorrecting...\n7\ncorrecting...\n6\ncorrecting...\n5\ncorrecting...\n4\ncorrecting...\n3\ncorrecting...\n2\ncorrecting...\n1\ncorrecting...\n0"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#add-conditionals",
    "href": "datacamp/intermediatePython/IntermediatePython.html#add-conditionals",
    "title": "Intermediate Python",
    "section": "Add conditionals",
    "text": "Add conditionals\nThe while loop that corrects the offset is a good start, but what if offset is negative? You can try to run the following code where offset is initialized to -6:\n# Initialize offset\noffset = -6\n\n# Code the while loop\n\nwhile offset != 0 :\n    print(\"correcting...\")\n    offset = offset - 1\n    print(offset)\n    \nbut your session will be disconnected. The while loop will never stop running, because offset will be further decreased on every run. offset != 0 will never become False and the while loop continues forever.\nFix things by putting an if-else statement inside the while loop. If your code is still taking too long to run, you probably made a mistake!\n\n# Initialize offset\noffset = -6\n\n# Code the while loop\nwhile offset != 0 :\n    print(\"correcting...\")\n    if offset > 0 :\n      offset = offset - 1\n\n    else : \n      offset = offset + 1    \n    print(offset)\n\ncorrecting...\n-5\ncorrecting...\n-4\ncorrecting...\n-3\ncorrecting...\n-2\ncorrecting...\n-1\ncorrecting...\n0"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#loop-over-a-list",
    "href": "datacamp/intermediatePython/IntermediatePython.html#loop-over-a-list",
    "title": "Intermediate Python",
    "section": "Loop over a list",
    "text": "Loop over a list\nHave another look at the for loop that Hugo showed in the video:\n\nfam = [1.73, 1.68, 1.71, 1.89]\nfor height in fam : \n    print(height)\n    \nAs usual, you simply have to indent the code with 4 spaces to tell Python which code should be executed in the for loop.\nThe areas variable, containing the area of different rooms in your house, is already defined.\n\n# areas list\nareas = [11.25, 18.0, 20.0, 10.75, 9.50]\n\n# Code the for loop\nfor area in areas:\n    print(area)\n\n11.25\n18.0\n20.0\n10.75\n9.5"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#indexes-and-values-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#indexes-and-values-1",
    "title": "Intermediate Python",
    "section": "Indexes and values (1)",
    "text": "Indexes and values (1)\nUsing a for loop to iterate over a list only gives you access to every list element in each run, one after the other. If you also want to access the index information, so where the list element you’re iterating over is located, you can use enumerate().\nAs an example, have a look at how the for loop from the video was converted:\n\nfam = [1.73, 1.68, 1.71, 1.89]\nfor index, height in enumerate(fam) :\n    print(\"person \" + str(index) + \": \" + str(height))\n\n\n# areas list\nareas = [11.25, 18.0, 20.0, 10.75, 9.50]\n\n# Change for loop to use enumerate() and update print()\nfor index, area in enumerate(areas):\n    print('room' + str(index)+ \":\" + str(area))\n\nroom0:11.25\nroom1:18.0\nroom2:20.0\nroom3:10.75\nroom4:9.5"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#indexes-and-values-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#indexes-and-values-2",
    "title": "Intermediate Python",
    "section": "Indexes and values (2)",
    "text": "Indexes and values (2)\nFor non-programmer folks, room 0: 11.25 is strange. Wouldn’t it be better if the count started at 1?\n\n# areas list\nareas = [11.25, 18.0, 20.0, 10.75, 9.50]\n\n# Code the for loop\nfor index, area in enumerate(areas) :\n    print(\"room \" + str(index+1) + \": \" + str(area))\n\nroom 1: 11.25\nroom 2: 18.0\nroom 3: 20.0\nroom 4: 10.75\nroom 5: 9.5"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#loop-over-list-of-lists",
    "href": "datacamp/intermediatePython/IntermediatePython.html#loop-over-list-of-lists",
    "title": "Intermediate Python",
    "section": "Loop over list of lists",
    "text": "Loop over list of lists\nRemember the house variable from the Intro to Python course? Have a look at its definition in the script. It’s basically a list of lists, where each sublist contains the name and area of a room in your house.\nIt’s up to you to build a for loop from scratch this time!\n# house list of lists\nhouse = [[\"hallway\", 11.25], \n         [\"kitchen\", 18.0], \n         [\"living room\", 20.0], \n         [\"bedroom\", 10.75], \n         [\"bathroom\", 9.50]]\n         \n# Build a for loop from scratch\nfor x in house:\n    print(\"- The \" + x[0] + \" is \" + str(x[1]) + \" sqm\")\n\nThe hallway is 11.25 sqm\nThe kitchen is 18.0 sqm\nThe living room is 20.0 sqm\nThe bedroom is 10.75 sqm\nThe bathroom is 9.5 sqm"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#loop-over-dictionary",
    "href": "datacamp/intermediatePython/IntermediatePython.html#loop-over-dictionary",
    "title": "Intermediate Python",
    "section": "Loop over dictionary",
    "text": "Loop over dictionary\nIn Python 3, you need the items() method to loop over a dictionary:\n\nworld = { \"afghanistan\":30.55, \n          \"albania\":2.77,\n          \"algeria\":39.21 }\n\nfor key, value in world.items() :\n    print(key + \" -- \" + str(value))\n    \nRemember the europe dictionary that contained the names of some European countries as key and their capitals as corresponding value? Go ahead and write a loop to iterate over it!\n# Definition of dictionary\neurope = {'spain':'madrid', 'france':'paris', 'germany':'berlin',\n          'norway':'oslo', 'italy':'rome', 'poland':'warsaw', 'austria':'vienna' }\n          \n# Iterate over europe\nfor k, v in europe.items():\n    print('- The ' + 'capital '+ 'of ' + str(k) + ' is '+ str(v))\n\nThe capital of spain is madrid\nThe capital of france is paris\nThe capital of germany is berlin\nThe capital of norway is oslo\nThe capital of italy is rome\nThe capital of poland is warsaw\nThe capital of austria is vienna"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#loop-over-numpy-array",
    "href": "datacamp/intermediatePython/IntermediatePython.html#loop-over-numpy-array",
    "title": "Intermediate Python",
    "section": "Loop over NumPy array",
    "text": "Loop over NumPy array\nIf you’re dealing with a 1D NumPy array, looping over all elements can be as simple as:\nfor x in my_array :\n    ...\nIf you’re dealing with a 2D NumPy array, it’s more complicated. A 2D array is built up of multiple 1D arrays. To explicitly iterate over all separate elements of a multi-dimensional array, you’ll need this syntax:\nfor x in np.nditer(my_array) :\n    ...\nTwo NumPy arrays that you might recognize from the intro course are available in your Python session: np_height, a NumPy array containing the heights of Major League Baseball players, and np_baseball, a 2D NumPy array that contains both the heights (first column) and weights (second column) of those players.\n\nimport numpy as np\nheight =[1.7, 1.6, 1.3, 1.4, 1.65]\nweight = [80, 75, 86, 72, 83]\nnp_height = np.array(height)\nnp_weight = np.array(weight)\nnp_baseball = np.array([weight, height])\n# For loop over np_height\nfor x in np_height:\n    print(str(x) + ' metres')\n\n1.7 metres\n1.6 metres\n1.3 metres\n1.4 metres\n1.65 metres\n\n# For loop over np_baseball\nfor x in np.nditer(np_baseball):\n    print(x)\n\n80.0\n75.0\n86.0\n72.0\n83.0\n1.7\n1.6\n1.3\n1.4\n1.65"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#loop-over-dataframe-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#loop-over-dataframe-1",
    "title": "Intermediate Python",
    "section": "Loop over DataFrame (1)",
    "text": "Loop over DataFrame (1)\nIterating over a Pandas DataFrame is typically done with the iterrows() method. Used in a for loop, every observation is iterated over and on every iteration the row label and actual row contents are available:\nfor lab, row in brics.iterrows() :\n    ...\nIn this and the following exercises you will be working on the cars DataFrame. It contains information on the cars per capita and whether people drive right or left for seven countries in the world.\n\n# Iterate over rows of cars\nfor lab, row in cars.iterrows():\n    print(lab)\n    print(row)\n\nUS\ncars_per_cap              809\ncountry         United States\ndrives_right             True\nName: US, dtype: object\nAUS\ncars_per_cap          731\ncountry         Australia\ndrives_right        False\nName: AUS, dtype: object\nJAP\ncars_per_cap      588\ncountry         Japan\ndrives_right    False\nName: JAP, dtype: object\nIN\ncars_per_cap       18\ncountry         India\ndrives_right    False\nName: IN, dtype: object\nRU\ncars_per_cap       200\ncountry         Russia\ndrives_right      True\nName: RU, dtype: object\nMOR\ncars_per_cap         70\ncountry         Morocco\ndrives_right       True\nName: MOR, dtype: object\nEG\ncars_per_cap       45\ncountry         Egypt\ndrives_right     True\nName: EG, dtype: object"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#loop-over-dataframe-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#loop-over-dataframe-2",
    "title": "Intermediate Python",
    "section": "Loop over DataFrame (2)",
    "text": "Loop over DataFrame (2)\nThe row data that’s generated by iterrows() on every run is a Pandas Series. This format is not very convenient to print out. Luckily, you can easily select variables from the Pandas Series using square brackets:\nfor lab, row in brics.iterrows() :\n    print(row['country'])\n\n\n# Adapt for loop\nfor lab, row in cars.iterrows() :\n    print(str(lab) + ': ' + str(row['cars_per_cap']))\n\nUS: 809\nAUS: 731\nJAP: 588\nIN: 18\nRU: 200\nMOR: 70\nEG: 45"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#add-column-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#add-column-1",
    "title": "Intermediate Python",
    "section": "Add column (1)",
    "text": "Add column (1)\nIn the video, Hugo showed you how to add the length of the country names of the brics DataFrame in a new column:\n\nfor lab, row in brics.iterrows() :\n    brics.loc[lab, \"name_length\"] = len(row[\"country\"])\n    \nYou can do similar things on the cars DataFrame.\n\n# Code for loop that adds COUNTRY column\nfor lab, row in cars.iterrows() :\n    cars.loc[lab, \"COUNTRY\"] = row[\"country\"].upper()\n\n\n# Print cars\nprint(cars)\n\n     cars_per_cap        country  drives_right        COUNTRY\nUS            809  United States          True  UNITED STATES\nAUS           731      Australia         False      AUSTRALIA\nJAP           588          Japan         False          JAPAN\nIN             18          India         False          INDIA\nRU            200         Russia          True         RUSSIA\nMOR            70        Morocco          True        MOROCCO\nEG             45          Egypt          True          EGYPT"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#add-column-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#add-column-2",
    "title": "Intermediate Python",
    "section": "Add column (2)",
    "text": "Add column (2)\nUsing iterrows() to iterate over every observation of a Pandas DataFrame is easy to understand, but not very efficient. On every iteration, you’re creating a new Pandas Series.\nIf you want to add a column to a DataFrame by calling a function on another column, the iterrows() method in combination with a for loop is not the preferred way to go. Instead, you’ll want to use apply().\nCompare the iterrows() version with the apply() version to get the same result in the brics DataFrame:\n\nfor lab, row in brics.iterrows() :\n    brics.loc[lab, \"name_length\"] = len(row[\"country\"])\n\nbrics[\"name_length\"] = brics[\"country\"].apply(len)\n\nWe can do a similar thing to call the upper() method on every name in the country column. However, upper() is a method, so we’ll need a slightly different approach:\n\n# Import cars data\nimport pandas as pd\ncars = pd.read_csv('data/cars.csv', index_col = 0)\n\n# Use .apply(str.upper)\n\ncars[\"COUNTRY\"] = cars[\"country\"].apply(str.upper)\nprint(cars)\n\n     cars_per_cap        country  drives_right        COUNTRY\nUS            809  United States          True  UNITED STATES\nAUS           731      Australia         False      AUSTRALIA\nJAP           588          Japan         False          JAPAN\nIN             18          India         False          INDIA\nRU            200         Russia          True         RUSSIA\nMOR            70        Morocco          True        MOROCCO\nEG             45          Egypt          True          EGYPT"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#random-float",
    "href": "datacamp/intermediatePython/IntermediatePython.html#random-float",
    "title": "Intermediate Python",
    "section": "Random float",
    "text": "Random float\nRandomness has many uses in science, art, statistics, cryptography, gaming, gambling, and other fields. You’re going to use randomness to simulate a game.\nAll the functionality you need is contained in the random package, a sub-package of numpy. In this exercise, you’ll be using two functions from this package:\n\nseed(): sets the random seed, so that your results are reproducible between simulations. As an argument, it takes an integer of your choosing. If you call the function, no output will be generated.\nrand(): if you don’t specify any arguments, it generates a random float between zero and one.\n\n\n# Set the seed\nnp.random.seed(123)\n\n# Generate and print random float\nprint(np.random.rand())\n\n0.6964691855978616"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#roll-the-dice",
    "href": "datacamp/intermediatePython/IntermediatePython.html#roll-the-dice",
    "title": "Intermediate Python",
    "section": "Roll the dice",
    "text": "Roll the dice\nIn the previous exercise, you used rand(), that generates a random float between 0 and 1.\nAs Hugo explained in the video you can just as well use randint(), also a function of the random package, to generate integers randomly. The following call generates the integer 4, 5, 6 or 7 randomly. 8 is not included.\n\nimport numpy as np\nnp.random.randint(4, 8)\n\nNumPy has already been imported as np and a seed has been set. Can you roll some dice?\n\nnp.random.seed(123)\n\n# Use randint() to simulate a dice\nprint(np.random.randint(1, 7))\n\n6\n\n# Use randint() again\nprint(np.random.randint(1, 7))\n\n3"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#determine-your-next-move",
    "href": "datacamp/intermediatePython/IntermediatePython.html#determine-your-next-move",
    "title": "Intermediate Python",
    "section": "Determine your next move",
    "text": "Determine your next move\nIn the Empire State Building bet, your next move depends on the number you get after throwing the dice. We can perfectly code this with an if-elif-else construct!\nThe sample code assumes that you’re currently at step 50. Can you fill in the missing pieces to finish the script? numpy is already imported as np and the seed has been set to 123, so you don’t have to worry about that anymore.\n\nnp.random.seed(100)\nstep = 50\n\n# Roll the dice\ndice = np.random.randint(1, 7)\n\n# Finish the control construct\nif dice <= 2 :\n    step = step - 1\nelif dice <= 5 :\n    step = step + 1\nelse :\n    step = step + np.random.randint(1,7)\n\n# Print out dice and step\nprint(dice)\n\n1\n\nprint(step)\n\n49"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#the-next-step",
    "href": "datacamp/intermediatePython/IntermediatePython.html#the-next-step",
    "title": "Intermediate Python",
    "section": "The next step",
    "text": "The next step\nBefore, you have already written Python code that determines the next step based on the previous step. Now it’s time to put this code inside a for loop so that we can simulate a random walk.\nnumpy has been imported as np.\n\n# Numpy is imported, seed is set\n\n# Initialize random_walk\nrandom_walk =[0,]\n\n# Complete the ___\nfor x in range(100) :\n    # Set step: last element in random_walk\n    step = random_walk[-1]\n\n    # Roll the dice\n    dice = np.random.randint(1,7)\n\n    # Determine next step\n    if dice <= 2:\n        step = step - 1\n    elif dice <= 5:\n        step = step + 1\n    else:\n        step = step + np.random.randint(1,7)\n\n    # append next_step to random_walk\n    random_walk.append(step)\n\n# Print random_walk\nprint(random_walk)\n\n[0, -1, 0, -1, 0, 1, 2, 5, 6, 7, 6, 5, 4, 5, 6, 7, 8, 7, 8, 7, 10, 11, 12, 13, 12, 18, 19, 20, 21, 22, 23, 24, 23, 22, 26, 25, 26, 25, 24, 25, 26, 30, 29, 28, 27, 32, 33, 32, 31, 32, 35, 34, 33, 36, 35, 40, 41, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 53, 52, 53, 54, 55, 58, 59, 60, 61, 66, 65, 66, 67, 66, 67, 68, 69, 74, 75, 76, 77, 79, 78, 77, 79, 82, 83, 84, 85, 86]"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#how-low-can-you-go",
    "href": "datacamp/intermediatePython/IntermediatePython.html#how-low-can-you-go",
    "title": "Intermediate Python",
    "section": "How low can you go?",
    "text": "How low can you go?\nThings are shaping up nicely! You already have code that calculates your location in the Empire State Building after 100 dice throws. However, there’s something we haven’t thought about - you can’t go below 0!\nA typical way to solve problems like this is by using max(). If you pass max() two arguments, the biggest one gets returned. For example, to make sure that a variable x never goes below 10 when you decrease it, you can use:\nx = max(10, x - 1)\n\n\n# Initialize random_walk\nrandom_walk = [0]\n\nfor x in range(100) :\n    step = random_walk[-1]\n    dice = np.random.randint(1,7)\n\n    if dice <= 2:\n        # Replace below: use max to make sure step can't go below 0\n        step = max(0, step - 1)\n    elif dice <= 5:\n        step = step + 1\n    else:\n        step = step + np.random.randint(1,7)\n\n    random_walk.append(step)\n\nprint(random_walk)\n\n[0, 0, 1, 2, 1, 0, 6, 5, 9, 8, 9, 10, 15, 16, 17, 18, 19, 18, 17, 16, 17, 20, 25, 26, 28, 27, 26, 25, 26, 27, 26, 30, 31, 32, 33, 38, 37, 38, 37, 38, 37, 36, 39, 40, 41, 42, 43, 42, 43, 42, 45, 46, 47, 48, 52, 53, 56, 55, 56, 58, 60, 61, 62, 63, 64, 69, 70, 69, 70, 69, 70, 73, 75, 74, 75, 80, 79, 80, 81, 82, 83, 84, 85, 84, 86, 87, 88, 89, 88, 87, 90, 89, 88, 87, 86, 89, 90, 91, 90, 95, 99]"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#visualize-the-walk",
    "href": "datacamp/intermediatePython/IntermediatePython.html#visualize-the-walk",
    "title": "Intermediate Python",
    "section": "Visualize the walk",
    "text": "Visualize the walk\nLet’s visualize this random walk! Remember how you could use matplotlib to build a line plot?\nimport matplotlib.pyplot as plt\nplt.plot(x, y)\nplt.show()\n\nThe first list you pass is mapped onto the x axis and the second list is mapped onto the y axis.\nIf you pass only one argument, Python will know what to do and will use the index of the list to map onto the x axis, and the values in the list onto the y axis.\n\n# Numpy is imported, seed is set\n\n# Initialization\nrandom_walk = [0]\n\nfor x in range(100) :\n    step = random_walk[-1]\n    dice = np.random.randint(1,7)\n\n    if dice <= 2:\n        step = max(0, step - 1)\n    elif dice <= 5:\n        step = step + 1\n    else:\n        step = step + np.random.randint(1,7)\n\n    random_walk.append(step)\n\n# Import matplotlib.pyplot as plt\n\nimport matplotlib.pyplot as plt\n\n# Plot random_walk\n\nplt.plot(random_walk)\n\n# Show the plot\n\nplt.show()"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#simulate-multiple-walks",
    "href": "datacamp/intermediatePython/IntermediatePython.html#simulate-multiple-walks",
    "title": "Intermediate Python",
    "section": "Simulate multiple walks",
    "text": "Simulate multiple walks\nA single random walk is one thing, but that doesn’t tell you if you have a good chance at winning the bet.\nTo get an idea about how big your chances are of reaching 60 steps, you can repeatedly simulate the random walk and collect the results. That’s exactly what you’ll do in this exercise.\nThe sample code already sets you off in the right direction. Another for loop is wrapped around the code you already wrote. It’s up to you to add some bits and pieces to make sure all of the results are recorded correctly.\nNote: Don’t change anything about the initialization of all_walks that is given. Setting any number inside the list will cause the exercise to crash!\n\n# Numpy is imported; seed is set\n\n# Initialize all_walks (don't change this line)\nall_walks = []\n\n# Simulate random walk 10 times\nfor i in range(10) :\n\n    # Code from before\n    random_walk = [0]\n    for x in range(100) :\n        step = random_walk[-1]\n        dice = np.random.randint(1,7)\n\n        if dice <= 2:\n            step = max(0, step - 1)\n        elif dice <= 5:\n            step = step + 1\n        else:\n            step = step + np.random.randint(1,7)\n        random_walk.append(step)\n\n    # Append random_walk to all_walks\n    all_walks.append(random_walk)\n\n# Print all_walks\nprint(all_walks)\n\n[[0, 0, 1, 2, 3, 2, 5, 4, 8, 7, 8, 12, 13, 14, 18, 17, 16, 18, 19, 20, 21, 22, 23, 24, 23, 25, 26, 27, 26, 25, 24, 25, 24, 25, 26, 27, 28, 29, 30, 31, 32, 31, 32, 33, 32, 33, 32, 35, 36, 37, 38, 37, 38, 43, 44, 45, 46, 45, 44, 45, 44, 45, 46, 47, 48, 47, 46, 47, 48, 49, 50, 51, 52, 54, 59, 58, 57, 63, 62, 63, 62, 61, 60, 61, 62, 63, 64, 65, 68, 69, 70, 71, 70, 72, 73, 74, 75, 76, 75, 76, 75], [0, 1, 0, 1, 4, 5, 4, 5, 4, 3, 2, 1, 2, 1, 0, 1, 2, 1, 2, 3, 4, 5, 6, 5, 4, 5, 6, 7, 6, 11, 10, 11, 13, 14, 18, 17, 16, 15, 14, 15, 14, 13, 12, 16, 21, 22, 21, 22, 21, 22, 21, 22, 23, 25, 26, 28, 29, 34, 35, 36, 37, 38, 39, 40, 41, 42, 46, 45, 44, 45, 47, 48, 49, 50, 56, 55, 54, 55, 56, 57, 56, 58, 59, 60, 61, 62, 63, 62, 63, 65, 67, 68, 72, 74, 79, 80, 79, 85, 86, 85, 86], [0, 1, 2, 1, 2, 6, 5, 6, 7, 11, 12, 11, 10, 9, 10, 11, 12, 11, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 20, 26, 27, 26, 27, 28, 29, 30, 31, 32, 31, 30, 31, 32, 31, 32, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 42, 43, 44, 45, 46, 47, 46, 47, 48, 49, 50, 49, 48, 49, 48, 49, 50, 51, 52, 51, 53, 54, 53, 52, 51, 52, 51, 50, 54, 53, 54, 55, 56, 55, 56, 57, 62, 61, 67, 68, 69, 70], [0, 1, 2, 3, 2, 1, 2, 3, 4, 5, 6, 7, 8, 7, 9, 8, 10, 11, 12, 11, 10, 11, 12, 11, 14, 15, 16, 17, 18, 19, 24, 25, 24, 23, 24, 25, 26, 27, 28, 29, 30, 29, 28, 27, 28, 27, 28, 27, 28, 29, 28, 27, 28, 27, 33, 32, 33, 32, 31, 32, 36, 37, 38, 37, 38, 39, 40, 41, 42, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 50, 51, 52, 53, 54, 55, 56, 55, 54, 53, 54, 53, 52, 51, 52, 53, 54, 56, 55, 56, 55], [0, 6, 7, 6, 7, 6, 7, 12, 11, 13, 14, 13, 14, 13, 14, 15, 14, 13, 14, 13, 14, 16, 15, 16, 15, 16, 17, 18, 17, 16, 17, 18, 19, 18, 17, 18, 19, 20, 21, 20, 21, 20, 19, 24, 27, 28, 27, 26, 27, 28, 29, 30, 29, 34, 35, 36, 37, 40, 39, 40, 42, 41, 43, 44, 45, 46, 45, 46, 45, 46, 45, 44, 45, 44, 45, 46, 45, 50, 51, 55, 60, 59, 60, 59, 60, 61, 67, 66, 65, 66, 65, 67, 68, 69, 74, 73, 78, 79, 80, 81, 80], [0, 1, 0, 1, 4, 5, 4, 5, 6, 5, 6, 7, 6, 7, 12, 13, 14, 13, 14, 13, 14, 15, 16, 15, 16, 17, 18, 19, 20, 19, 20, 25, 24, 25, 26, 31, 32, 31, 30, 29, 30, 31, 32, 33, 32, 33, 34, 33, 32, 36, 35, 36, 37, 36, 40, 39, 40, 42, 43, 44, 45, 46, 51, 52, 51, 52, 53, 52, 51, 52, 57, 58, 59, 60, 61, 67, 66, 67, 68, 67, 68, 67, 66, 65, 66, 67, 68, 71, 76, 75, 76, 77, 76, 77, 76, 77, 76, 77, 80, 79, 80], [0, 0, 0, 1, 2, 4, 5, 6, 7, 8, 7, 6, 5, 6, 7, 6, 11, 12, 14, 15, 16, 17, 16, 15, 14, 15, 16, 15, 16, 17, 22, 23, 24, 25, 26, 25, 24, 23, 22, 21, 22, 21, 22, 23, 24, 25, 26, 25, 26, 28, 29, 28, 29, 28, 29, 30, 31, 30, 29, 28, 33, 34, 40, 41, 42, 45, 46, 45, 44, 47, 46, 45, 46, 47, 48, 47, 46, 47, 50, 49, 50, 51, 52, 58, 57, 56, 57, 58, 59, 64, 68, 69, 70, 75, 76, 75, 74, 73, 72, 71, 72], [0, 1, 2, 3, 4, 5, 4, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 13, 14, 15, 14, 13, 14, 13, 12, 13, 14, 15, 16, 15, 14, 15, 14, 13, 14, 15, 14, 15, 14, 15, 16, 22, 25, 24, 23, 22, 23, 22, 21, 22, 25, 29, 30, 31, 36, 37, 36, 35, 36, 37, 36, 41, 40, 39, 38, 39, 40, 39, 40, 39, 40, 41, 42, 43, 46, 49, 54, 53, 52, 53, 54, 55, 56, 57, 58, 59, 58, 57, 56, 57, 58, 59, 58, 57, 58, 64, 65, 66, 67, 72, 73], [0, 1, 2, 1, 0, 0, 1, 0, 1, 2, 1, 2, 3, 6, 7, 8, 7, 10, 11, 10, 12, 13, 14, 13, 12, 13, 12, 13, 14, 13, 14, 18, 22, 23, 26, 27, 28, 31, 32, 31, 32, 33, 34, 35, 36, 35, 36, 35, 34, 37, 43, 42, 41, 42, 41, 42, 41, 40, 39, 40, 39, 40, 41, 40, 41, 40, 41, 42, 43, 44, 48, 54, 55, 56, 57, 58, 59, 60, 59, 60, 61, 60, 61, 60, 59, 60, 61, 62, 63, 62, 63, 64, 67, 66, 65, 64, 65, 66, 67, 68, 67], [0, 1, 0, 5, 6, 7, 8, 7, 8, 9, 8, 7, 8, 7, 6, 7, 8, 7, 8, 7, 8, 9, 14, 17, 16, 17, 18, 17, 18, 19, 20, 19, 20, 21, 20, 21, 20, 21, 26, 25, 24, 25, 26, 27, 26, 27, 28, 29, 30, 29, 30, 31, 33, 37, 36, 37, 38, 39, 40, 41, 40, 41, 40, 41, 40, 41, 42, 43, 44, 43, 44, 43, 42, 43, 42, 43, 42, 41, 46, 45, 44, 43, 42, 43, 44, 43, 44, 43, 44, 43, 42, 41, 40, 41, 42, 43, 44, 45, 44, 45, 46]]"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#visualize-all-walks",
    "href": "datacamp/intermediatePython/IntermediatePython.html#visualize-all-walks",
    "title": "Intermediate Python",
    "section": "Visualize all walks",
    "text": "Visualize all walks\nall_walks is a list of lists: every sub-list represents a single random walk. If you convert this list of lists to a NumPy array, you can start making interesting plots! matplotlib.pyplot is already imported as plt.\nThe nested for loop is already coded for you - don’t worry about it. For now, focus on the code that comes after this for loop.\n\n# numpy and matplotlib imported, seed set.\n\n# initialize and populate all_walks\nall_walks = []\nfor i in range(10) :\n    random_walk = [0]\n    for x in range(100) :\n        step = random_walk[-1]\n        dice = np.random.randint(1,7)\n        if dice <= 2:\n            step = max(0, step - 1)\n        elif dice <= 5:\n            step = step + 1\n        else:\n            step = step + np.random.randint(1,7)\n        random_walk.append(step)\n    all_walks.append(random_walk)\n\n# Convert all_walks to Numpy array: np_aw\nnp_aw = np.array(all_walks)\n\n# Plot np_aw and show\nplt.plot(np_aw)\nplt.show()\n\n\n\n# Clear the figure\nplt.clf()\n\n# Transpose np_aw: np_aw_t\n\nnp_aw_t = np.transpose(np_aw)\n# Plot np_aw_t and show\n\nplt.plot(np_aw_t)\nplt.show()"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#implement-clumsiness",
    "href": "datacamp/intermediatePython/IntermediatePython.html#implement-clumsiness",
    "title": "Intermediate Python",
    "section": "Implement clumsiness",
    "text": "Implement clumsiness\nWith this neatly written code of yours, changing the number of times the random walk should be simulated is super-easy. You simply update the range() function in the top-level for loop.\nThere’s still something we forgot! You’re a bit clumsy and you have a 0.5% chance of falling down. That calls for another random number generation. Basically, you can generate a random float between 0 and 1. If this value is less than or equal to 0.005, you should reset step to 0.\n\n# numpy and matplotlib imported, seed set\n\n# Simulate random walk 250 times\nall_walks = []\nfor i in range(250) :\n    random_walk = [0]\n    for x in range(100) :\n        step = random_walk[-1]\n        dice = np.random.randint(1,7)\n        if dice <= 2:\n            step = max(0, step - 1)\n        elif dice <= 5:\n            step = step + 1\n        else:\n            step = step + np.random.randint(1,7)\n\n        # Implement clumsiness\n        if np.random.rand() <= 0.001 :\n            step = 0\n\n        random_walk.append(step)\n    all_walks.append(random_walk)\n\n# Create and plot np_aw_t\nnp_aw_t = np.transpose(np.array(all_walks))\nplt.plot(np_aw_t)\nplt.show()"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#plot-the-distribution",
    "href": "datacamp/intermediatePython/IntermediatePython.html#plot-the-distribution",
    "title": "Intermediate Python",
    "section": "Plot the distribution",
    "text": "Plot the distribution\nAll these fancy visualizations have put us on a sidetrack. We still have to solve the million-dollar problem: What are the odds that you’ll reach 60 steps high on the Empire State Building?\nBasically, you want to know about the end points of all the random walks you’ve simulated. These end points have a certain distribution that you can visualize with a histogram.\nNote that if your code is taking too long to run, you might be plotting a histogram of the wrong data!\n\n# numpy and matplotlib imported, seed set\n\n# Simulate random walk 500 times\nall_walks = []\nfor i in range(500) :\n    random_walk = [0]\n    for x in range(100) :\n        step = random_walk[-1]\n        dice = np.random.randint(1,7)\n        if dice <= 2:\n            step = max(0, step - 1)\n        elif dice <= 5:\n            step = step + 1\n        else:\n            step = step + np.random.randint(1,7)\n        if np.random.rand() <= 0.001 :\n            step = 0\n        random_walk.append(step)\n    all_walks.append(random_walk)\n\n# Create and plot np_aw_t\nnp_aw_t = np.transpose(np.array(all_walks))\n\n# Select last row from np_aw_t: ends\nends = np_aw_t[100,:]\n\n# Plot histogram of ends, display plot\nplt.hist(ends)\nplt.show()"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#calculate-the-odds",
    "href": "datacamp/intermediatePython/IntermediatePython.html#calculate-the-odds",
    "title": "Intermediate Python",
    "section": "Calculate the odds",
    "text": "Calculate the odds\nThe histogram of the previous exercise was created from a NumPy array ends, that contains 500 integers. Each integer represents the end point of a random walk. To calculate the chance that this end point is greater than or equal to 60, you can count the number of integers in ends that are greater than or equal to 60 and divide that number by 500, the total number of simulations.\nWell then, what’s the estimated chance that you’ll reach at least 60 steps high if you play this Empire State Building game? The ends array is everything you need; it’s available in your Python session so you can make calculations in the IPython Shell.\n\nround(len(ends[ends>=60])/len(ends) * 100, 2)\n\n75.8"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html",
    "title": "Data Manipulation with pandas",
    "section": "",
    "text": "When you get a new DataFrame to work with, the first thing you need to do is explore it and see what it contains. There are several useful methods and attributes for this.\n\nhead() returns the first few rows (the “head” of the DataFrame).\ninfo() shows information on each of the columns, such as the data type and number of missing values.\nshape returns the number of rows and columns of the DataFrame.\ndescribe() calculates a few summary statistics for each column. homelessness is a DataFrame containing estimates of homelessness in each U.S. state in 2018. The individual column is the number of homeless individuals not part of a family with children. The family_members column is the number of homeless individuals part of a family with children. The state_pop column is the state’s total population.\n\n\nimport pandas as pd\n\nhomelessness = pd.read_csv('data/homelessness.csv', index_col = 0)\n\n\n# Print the head of the homelessness data\nprint(homelessness.head())\n\n               region       state  individuals  family_members  state_pop\n0  East South Central     Alabama       2570.0           864.0    4887681\n1             Pacific      Alaska       1434.0           582.0     735139\n2            Mountain     Arizona       7259.0          2606.0    7158024\n3  West South Central    Arkansas       2280.0           432.0    3009733\n4             Pacific  California     109008.0         20964.0   39461588\n\n# Print information about homelessness\nprint(homelessness.info())\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 51 entries, 0 to 50\nData columns (total 5 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   region          51 non-null     object \n 1   state           51 non-null     object \n 2   individuals     51 non-null     float64\n 3   family_members  51 non-null     float64\n 4   state_pop       51 non-null     int64  \ndtypes: float64(2), int64(1), object(2)\nmemory usage: 2.4+ KB\nNone\n\n# Print the shape of homelessness\nprint(homelessness.shape)\n\n(51, 5)\n\n# Print a description of homelessness\n\nprint(homelessness.describe())\n\n         individuals  family_members     state_pop\ncount      51.000000       51.000000  5.100000e+01\nmean     7225.784314     3504.882353  6.405637e+06\nstd     15991.025083     7805.411811  7.327258e+06\nmin       434.000000       75.000000  5.776010e+05\n25%      1446.500000      592.000000  1.777414e+06\n50%      3082.000000     1482.000000  4.461153e+06\n75%      6781.500000     3196.000000  7.340946e+06\nmax    109008.000000    52070.000000  3.946159e+07\n\n\n\n\n\nTo better understand DataFrame objects, it’s useful to know that they consist of three components, stored as attributes:\n\n.values: A two-dimensional NumPy array of values.\n.columns: An index of columns: the column names.\n.index: An index for the rows: either row numbers or row names. You can usually think of indexes as a list of strings or numbers, though the pandas Index data type allows for more sophisticated options. (These will be covered later in the course.)\n\n\n# Print the values of homelessness\n\nprint(homelessness.values)\n\n[['East South Central' 'Alabama' 2570.0 864.0 4887681]\n ['Pacific' 'Alaska' 1434.0 582.0 735139]\n ['Mountain' 'Arizona' 7259.0 2606.0 7158024]\n ['West South Central' 'Arkansas' 2280.0 432.0 3009733]\n ['Pacific' 'California' 109008.0 20964.0 39461588]\n ['Mountain' 'Colorado' 7607.0 3250.0 5691287]\n ['New England' 'Connecticut' 2280.0 1696.0 3571520]\n ['South Atlantic' 'Delaware' 708.0 374.0 965479]\n ['South Atlantic' 'District of Columbia' 3770.0 3134.0 701547]\n ['South Atlantic' 'Florida' 21443.0 9587.0 21244317]\n ['South Atlantic' 'Georgia' 6943.0 2556.0 10511131]\n ['Pacific' 'Hawaii' 4131.0 2399.0 1420593]\n ['Mountain' 'Idaho' 1297.0 715.0 1750536]\n ['East North Central' 'Illinois' 6752.0 3891.0 12723071]\n ['East North Central' 'Indiana' 3776.0 1482.0 6695497]\n ['West North Central' 'Iowa' 1711.0 1038.0 3148618]\n ['West North Central' 'Kansas' 1443.0 773.0 2911359]\n ['East South Central' 'Kentucky' 2735.0 953.0 4461153]\n ['West South Central' 'Louisiana' 2540.0 519.0 4659690]\n ['New England' 'Maine' 1450.0 1066.0 1339057]\n ['South Atlantic' 'Maryland' 4914.0 2230.0 6035802]\n ['New England' 'Massachusetts' 6811.0 13257.0 6882635]\n ['East North Central' 'Michigan' 5209.0 3142.0 9984072]\n ['West North Central' 'Minnesota' 3993.0 3250.0 5606249]\n ['East South Central' 'Mississippi' 1024.0 328.0 2981020]\n ['West North Central' 'Missouri' 3776.0 2107.0 6121623]\n ['Mountain' 'Montana' 983.0 422.0 1060665]\n ['West North Central' 'Nebraska' 1745.0 676.0 1925614]\n ['Mountain' 'Nevada' 7058.0 486.0 3027341]\n ['New England' 'New Hampshire' 835.0 615.0 1353465]\n ['Mid-Atlantic' 'New Jersey' 6048.0 3350.0 8886025]\n ['Mountain' 'New Mexico' 1949.0 602.0 2092741]\n ['Mid-Atlantic' 'New York' 39827.0 52070.0 19530351]\n ['South Atlantic' 'North Carolina' 6451.0 2817.0 10381615]\n ['West North Central' 'North Dakota' 467.0 75.0 758080]\n ['East North Central' 'Ohio' 6929.0 3320.0 11676341]\n ['West South Central' 'Oklahoma' 2823.0 1048.0 3940235]\n ['Pacific' 'Oregon' 11139.0 3337.0 4181886]\n ['Mid-Atlantic' 'Pennsylvania' 8163.0 5349.0 12800922]\n ['New England' 'Rhode Island' 747.0 354.0 1058287]\n ['South Atlantic' 'South Carolina' 3082.0 851.0 5084156]\n ['West North Central' 'South Dakota' 836.0 323.0 878698]\n ['East South Central' 'Tennessee' 6139.0 1744.0 6771631]\n ['West South Central' 'Texas' 19199.0 6111.0 28628666]\n ['Mountain' 'Utah' 1904.0 972.0 3153550]\n ['New England' 'Vermont' 780.0 511.0 624358]\n ['South Atlantic' 'Virginia' 3928.0 2047.0 8501286]\n ['Pacific' 'Washington' 16424.0 5880.0 7523869]\n ['South Atlantic' 'West Virginia' 1021.0 222.0 1804291]\n ['East North Central' 'Wisconsin' 2740.0 2167.0 5807406]\n ['Mountain' 'Wyoming' 434.0 205.0 577601]]\n\n# Print the column index of homelessness\n\nprint(homelessness.columns)\n\nIndex(['region', 'state', 'individuals', 'family_members', 'state_pop'], dtype='object')\n\n# Print the row index of homelessness\n\nprint(homelessness.index)\n\nInt64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n            34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n            50],\n           dtype='int64')\n\n\n\n\n\nFinding interesting bits of data in a DataFrame is often easier if you change the order of the rows. You can sort the rows by passing a column name to .sort_values().\nIn cases where rows have the same value (this is common if you sort on a categorical variable), you may wish to break the ties by sorting on another column. You can sort on multiple columns in this way by passing a list of column names.\n\n\n\none column df.sort_values(“breed”)\nmultiple columns df.sort_values([“breed”, “weight_kg”])\nBy combining .sort_values() with .head(), you can answer questions in the form, “What are the top cases where…?”.\n\n\n# Sort homelessness by individuals\nhomelessness_ind = homelessness.sort_values('individuals')\n\n# Print the top few rows\nprint(homelessness_ind.head())\n\n                region         state  individuals  family_members  state_pop\n50            Mountain       Wyoming        434.0           205.0     577601\n34  West North Central  North Dakota        467.0            75.0     758080\n7       South Atlantic      Delaware        708.0           374.0     965479\n39         New England  Rhode Island        747.0           354.0    1058287\n45         New England       Vermont        780.0           511.0     624358\n\n# Sort homelessness by descending family members\nhomelessness_fam = homelessness.sort_values('family_members', ascending = False)\n\n# Print the top few rows\n\nprint(homelessness_fam.head())\n\n                region          state  individuals  family_members  state_pop\n32        Mid-Atlantic       New York      39827.0         52070.0   19530351\n4              Pacific     California     109008.0         20964.0   39461588\n21         New England  Massachusetts       6811.0         13257.0    6882635\n9       South Atlantic        Florida      21443.0          9587.0   21244317\n43  West South Central          Texas      19199.0          6111.0   28628666\n\n# Sort homelessness by region, then descending family members\nhomelessness_reg_fam =  homelessness.sort_values(['region', 'family_members'], ascending = [True, False])\n\n# Print the top few rows\n\nprint(homelessness_reg_fam.head())\n\n                region      state  individuals  family_members  state_pop\n13  East North Central   Illinois       6752.0          3891.0   12723071\n35  East North Central       Ohio       6929.0          3320.0   11676341\n22  East North Central   Michigan       5209.0          3142.0    9984072\n49  East North Central  Wisconsin       2740.0          2167.0    5807406\n14  East North Central    Indiana       3776.0          1482.0    6695497\n\n\n\n\n\n\nWhen working with data, you may not need all of the variables in your dataset. Square brackets ([]) can be used to select only the columns that matter to you in an order that makes sense to you. To select only “col_a” of the DataFrame df, use\ndf[\"col_a\"]\n\nTo select “col_a” and “col_b” of df, use\ndf[[\"col_a\", \"col_b\"]]\n\n\n# Select the individuals column\nindividuals = homelessness['individuals']\n\n# Print the head of the result\nprint(individuals.head())\n\n0      2570.0\n1      1434.0\n2      7259.0\n3      2280.0\n4    109008.0\nName: individuals, dtype: float64\n\n# Select the state and family_members columns\nstate_fam = homelessness[['state', 'family_members']]\n\n# Print the head of the result\n\nprint(state_fam.head())\n\n        state  family_members\n0     Alabama           864.0\n1      Alaska           582.0\n2     Arizona          2606.0\n3    Arkansas           432.0\n4  California         20964.0\n\n# Select only the individuals and state columns, in that order\nind_state =  homelessness[['individuals', 'state']]\n\n# Print the head of the result\n\nprint(ind_state.head())\n\n   individuals       state\n0       2570.0     Alabama\n1       1434.0      Alaska\n2       7259.0     Arizona\n3       2280.0    Arkansas\n4     109008.0  California\n\n\n\n\n\nA large part of data science is about finding which bits of your dataset are interesting. One of the simplest techniques for this is to find a subset of rows that match some criteria. This is sometimes known as filtering rows or selecting rows.\nThere are many ways to subset a DataFrame, perhaps the most common is to use relational operators to return True or False for each row, then pass that inside square brackets.\ndogs[dogs[\"height_cm\"] > 60]\ndogs[dogs[\"color\"] == \"tan\"]\n\nYou can filter for multiple conditions at once by using the “bitwise and” operator, &.\ndogs[(dogs[\"height_cm\"] > 60) & (dogs[\"color\"] == \"tan\")]\n\n\n# Filter for rows where individuals is greater than 10000\nind_gt_10k = homelessness[homelessness['individuals'] > 10000]\n\n# See the result\nprint(ind_gt_10k)\n\n                region       state  individuals  family_members  state_pop\n4              Pacific  California     109008.0         20964.0   39461588\n9       South Atlantic     Florida      21443.0          9587.0   21244317\n32        Mid-Atlantic    New York      39827.0         52070.0   19530351\n37             Pacific      Oregon      11139.0          3337.0    4181886\n43  West South Central       Texas      19199.0          6111.0   28628666\n47             Pacific  Washington      16424.0          5880.0    7523869\n\n# Filter for rows where region is Mountain\nmountain_reg =  homelessness[homelessness['region'] == 'Mountain']\n\n# See the result\nprint(mountain_reg)\n\n      region       state  individuals  family_members  state_pop\n2   Mountain     Arizona       7259.0          2606.0    7158024\n5   Mountain    Colorado       7607.0          3250.0    5691287\n12  Mountain       Idaho       1297.0           715.0    1750536\n26  Mountain     Montana        983.0           422.0    1060665\n28  Mountain      Nevada       7058.0           486.0    3027341\n31  Mountain  New Mexico       1949.0           602.0    2092741\n44  Mountain        Utah       1904.0           972.0    3153550\n50  Mountain     Wyoming        434.0           205.0     577601\n\n# Filter for rows where family_members is less than 1000 \n# and region is Pacific\nfam_lt_1k_pac = homelessness[(homelessness['region'] == 'Pacific') & (homelessness['family_members']<1000 )]\n\n# See the result\nprint(fam_lt_1k_pac)\n\n    region   state  individuals  family_members  state_pop\n1  Pacific  Alaska       1434.0           582.0     735139\n\n\n\n\n\nSubsetting data based on a categorical variable often involves using the “or” operator (|) to select rows from multiple categories. This can get tedious when you want all states in one of three different regions, for example. Instead, use the .isin() method, which will allow you to tackle this problem by writing one condition instead of three separate ones.\ncolors = [\"brown\", \"black\", \"tan\"]\ncondition = dogs[\"color\"].isin(colors)\ndogs[condition]\n\n\n# Subset for rows in South Atlantic or Mid-Atlantic regions\nsouth_mid_atlantic =  homelessness[homelessness['region'].isin([\"South Atlantic\", \"Mid-Atlantic\"])]\n\n# See the result\nprint(south_mid_atlantic)\n\n            region                 state  ...  family_members  state_pop\n7   South Atlantic              Delaware  ...           374.0     965479\n8   South Atlantic  District of Columbia  ...          3134.0     701547\n9   South Atlantic               Florida  ...          9587.0   21244317\n10  South Atlantic               Georgia  ...          2556.0   10511131\n20  South Atlantic              Maryland  ...          2230.0    6035802\n30    Mid-Atlantic            New Jersey  ...          3350.0    8886025\n32    Mid-Atlantic              New York  ...         52070.0   19530351\n33  South Atlantic        North Carolina  ...          2817.0   10381615\n38    Mid-Atlantic          Pennsylvania  ...          5349.0   12800922\n40  South Atlantic        South Carolina  ...           851.0    5084156\n46  South Atlantic              Virginia  ...          2047.0    8501286\n48  South Atlantic         West Virginia  ...           222.0    1804291\n\n[12 rows x 5 columns]\n\n# The Mojave Desert states\ncanu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\n\n# Filter for rows in the Mojave Desert states\nmojave_homelessness = homelessness[homelessness['state'].isin(canu)]\n\n# See the result\nprint(mojave_homelessness)\n\n      region       state  individuals  family_members  state_pop\n2   Mountain     Arizona       7259.0          2606.0    7158024\n4    Pacific  California     109008.0         20964.0   39461588\n28  Mountain      Nevada       7058.0           486.0    3027341\n44  Mountain        Utah       1904.0           972.0    3153550\n\n\n\n\n\nYou aren’t stuck with just the data you are given. Instead, you can add new columns to a DataFrame. This has many names, such as transforming, mutating, and feature engineering.\nYou can create new columns from scratch, but it is also common to derive them from other columns, for example, by adding columns together or by changing their units.\nhomelessness is available and pandas is loaded as pd.\n\n# Add total col as sum of individuals and family_members\n\nhomelessness['total'] = homelessness['family_members'] + homelessness['individuals']\n\n# Add p_individuals col as proportion of total that are individuals\n\nhomelessness['p_individuals'] = homelessness['individuals']/homelessness['total'] \n\n\n# See the result\nprint(homelessness.head())\n\n               region       state  ...     total  p_individuals\n0  East South Central     Alabama  ...    3434.0       0.748398\n1             Pacific      Alaska  ...    2016.0       0.711310\n2            Mountain     Arizona  ...    9865.0       0.735834\n3  West South Central    Arkansas  ...    2712.0       0.840708\n4             Pacific  California  ...  129972.0       0.838704\n\n[5 rows x 7 columns]\n\n\n\n\n\nYou’ve seen the four most common types of data manipulation: sorting rows, subsetting columns, subsetting rows, and adding new columns. In a real-life data analysis, you can mix and match these four manipulations to answer a multitude of questions.\nIn this exercise, you’ll answer the question, “Which state has the highest number of homeless individuals per 10,000 people in the state?” Combine your new pandas skills to find out.\n\n# Create indiv_per_10k col as homeless individuals per 10k state pop\nhomelessness[\"indiv_per_10k\"] = 10000 * homelessness['individuals']/ homelessness['state_pop']\n\n# Subset rows for indiv_per_10k greater than 20\nhigh_homelessness = homelessness[homelessness[\"indiv_per_10k\"]  > 20]\n\n# Sort high_homelessness by descending indiv_per_10k\nhigh_homelessness_srt = high_homelessness.sort_values('indiv_per_10k', ascending = False)\n\n# From high_homelessness_srt, select the state and indiv_per_10k cols\nresult = high_homelessness_srt[['state', 'indiv_per_10k']]\n\n# See the result\nprint(result)\n\n                   state  indiv_per_10k\n8   District of Columbia      53.738381\n11                Hawaii      29.079406\n4             California      27.623825\n37                Oregon      26.636307\n28                Nevada      23.314189\n47            Washington      21.829195\n32              New York      20.392363"
  },
  {
    "objectID": "inProcess/Diabetes/predict_diabetes_tidymodels.html",
    "href": "inProcess/Diabetes/predict_diabetes_tidymodels.html",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(data.table)\nlibrary(gtsummary)\nlibrary(mTools)\nDiabetes data"
  },
  {
    "objectID": "inProcess/Diabetes/predict_diabetes_tidymodels.html#data-processing",
    "href": "inProcess/Diabetes/predict_diabetes_tidymodels.html#data-processing",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "Data processing",
    "text": "Data processing\n\ndiabetes_df_all[, diabetes_char := factor(diabetes, \n                                      levels = c(0, 1),\n                                      labels = c(\"Non diabetic\", \"Diabetic\"))]\n\nre_balance_class <- function(df, outcome_col = \"diabetes_char\", pos_class = \"Diabetic\", pos_class_perc = .4){\n    \n    pos_class_df = df[get(outcome_col) == pos_class]\n    neg_class = df[get(outcome_col) != pos_class]\n    pos_perc = nrow(pos_class_df)/nrow(df)\n    N = round(nrow(pos_class_df)/pos_class_perc)\n    Nneg = N - nrow(pos_class_df)\n    neg_class_df = neg_class[sample(1:.N, Nneg)]\n    rbind(pos_class_df,neg_class_df )\n    \n    \n}\n\ndiabetes_df = re_balance_class(df = diabetes_df_all)"
  },
  {
    "objectID": "inProcess/Diabetes/predict_diabetes_tidymodels.html#summary-stats",
    "href": "inProcess/Diabetes/predict_diabetes_tidymodels.html#summary-stats",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "Summary Stats",
    "text": "Summary Stats\n\nlibrary(ggiraph)\ndb_perc <- diabetes_df[, .(freq = .N),\n                       by = diabetes_char][\n                           ,perc := round(freq/sum(freq) * 100, 1)]\n\n\nggplot(db_perc, aes(diabetes_char, freq, fill = diabetes_char))+\n    geom_bar_interactive(width = 0.5, stat = \"identity\")+\n    geom_text(aes(label = paste0(freq, \"(\", perc, \"%)\")),\n              position = position_dodge(width = 0.5),\n              vjust = 0.05)+\n    scale_fill_brewer(name = \"\", type = \"qual\", palette = \"Dark2\")+\n    theme_minimal()+\n    theme(\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\ntab2 <- diabetes_df %>%\n    tbl_summary(\n        by = diabetes_char,\n        type = all_continuous() ~ \"continuous2\",\n        statistic = all_continuous() ~ c(\n            \"{mean} ({sd})\",\n            \"{median} ({p25}, {p75})\",\n            \"[{min}, {max}]\"\n        ),\n        missing = \"ifany\"\n    ) %>%\n    add_p(pvalue_fun = ~ style_pvalue(.x, digits = 2))\n\ntab_df = as.data.frame(tab2)\nnms <- names(tab_df)\nnms <- gsub(\"\\\\*\", \"\", nms)\nnames(tab_df) <- nms\ndata_table(tab_df)"
  },
  {
    "objectID": "inProcess/Diabetes/predict_diabetes_tidymodels.html#model-fitting",
    "href": "inProcess/Diabetes/predict_diabetes_tidymodels.html#model-fitting",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "Model Fitting",
    "text": "Model Fitting\n\nset.seed(100)\ndiabetes_df[, diabetes:= as.factor(diabetes)]\ndiabetes_df_split <- initial_split(diabetes_df[,.SD, .SDcols = !\"diabetes_char\"], \n                                   strata = diabetes)\n\ndiabetes_df_train <- training(diabetes_df_split)\n\ndiabetes_df_test <- testing(diabetes_df_split)\n\n\n# Specify a logistic regression model\nlogistic_model <- logistic_reg() %>% \n  # Set the engine\n  set_engine('glm') %>% \n  # Set the mode\n  set_mode('classification')\n\n# Fit to training data\nlogistic_fit <- logistic_model %>% \n  fit(diabetes ~ .,\n      data = diabetes_df_train)\n\n# Print model fit object\nlogistic_fit %>% \n    DT_tidy_model()\n\n\n\n\n\n\n\nxgb_spec <- boost_tree(\n    trees = 2000,\n    tree_depth = tune(), \n    min_n = tune(),\n    loss_reduction = tune(),                     ## first three: model complexity\n    sample_size = tune(), \n    mtry = tune(),         ## randomness\n    learn_rate = tune()                          ## step size\n) %>%\n    set_engine(\"xgboost\") %>%\n    set_mode(\"classification\")\n\nxgb_spec\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 2000\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost"
  },
  {
    "objectID": "inProcess/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to-2022",
    "href": "inProcess/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to-2022",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "Income & Expenditure from 2000 to 2022",
    "text": "Income & Expenditure from 2000 to 2022\nDecided to have a quick look of the Kenyan government expenditure & income due to the current cash crunch\nThere are a number of factors that could be contributing to this trend. One factor is the growth of the economy. As the economy grows, the government needs to spend more money on things like infrastructure, education, and healthcare. Another factor is the growth of the population. As the population grows, the government needs to spend more money on things like social welfare programs and security."
  },
  {
    "objectID": "inProcess/cbk_data/R/kenya_income_exp.html#what-type-of-expenditure-is-rising",
    "href": "inProcess/cbk_data/R/kenya_income_exp.html#what-type-of-expenditure-is-rising",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "What type of expenditure is rising",
    "text": "What type of expenditure is rising"
  },
  {
    "objectID": "inProcess/cbk_data/R/kenya_income_exp.html#what-sub-type-of-reccurrent-expenditure-is-causing-the-rise",
    "href": "inProcess/cbk_data/R/kenya_income_exp.html#what-sub-type-of-reccurrent-expenditure-is-causing-the-rise",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "What Sub type of reccurrent expenditure is causing the rise",
    "text": "What Sub type of reccurrent expenditure is causing the rise"
  },
  {
    "objectID": "inProcess/cbk_data/R/kenya_income_exp.html#domestic-debt-composition",
    "href": "inProcess/cbk_data/R/kenya_income_exp.html#domestic-debt-composition",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "Domestic debt composition",
    "text": "Domestic debt composition"
  }
]