[
  {
    "objectID": "myblog/loan_prediction/clustering_loans.html#reading-data",
    "href": "myblog/loan_prediction/clustering_loans.html#reading-data",
    "title": "clustering",
    "section": "Reading data",
    "text": "Reading data"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html",
    "href": "myblog/datacamp/regression_r/regression.html",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "",
    "text": "We use the lm() function to fit linear models to data. In this case, we want to understand how the price of MarioKart games sold at auction varies as a function of not only the number of wheels included in the package, but also whether the item is new or used. Obviously, it is expected that you might have to pay a premium to buy these new. But how much is that premium? Can we estimate its value after controlling for the number of wheels?\nWe will fit a parallel slopes model using lm(). In addition to the data argument, lm() needs to know which variables you want to include in your regression model, and how you want to include them. It accomplishes this using a formula argument. A simple linear regression formula looks like y ~ x, where y is the name of the response variable, and x is the name of the explanatory variable. Here, we will simply extend this formula to include multiple explanatory variables. A parallel slopes model has the form y ~ x + z, where z is a categorical explanatory variable, and x is a numerical explanatory variable.\nThe output from lm() is a model object, which when printed, will show the fitted coefficients.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(openintro)\nlibrary(broom)\nlibrary(pander)\ndata( mariokart, package = \"openintro\")\nmario_kart <- mariokart\n\nmario_kart <- mario_kart %>% mutate(total_pr := ifelse(total_pr > 100, NA, total_pr))\n# Explore the data\nglimpse(mario_kart)\n\nRows: 143\nColumns: 12\n$ id          <dbl> 150377422259, 260483376854, 320432342985, 280405224677, 17…\n$ duration    <int> 3, 7, 3, 3, 1, 3, 1, 1, 3, 7, 1, 1, 1, 1, 7, 7, 3, 3, 1, 7…\n$ n_bids      <int> 20, 13, 16, 18, 20, 19, 13, 15, 29, 8, 15, 15, 13, 16, 6, …\n$ cond        <fct> new, used, new, new, new, new, used, new, used, used, new,…\n$ start_pr    <dbl> 0.99, 0.99, 0.99, 0.99, 0.01, 0.99, 0.01, 1.00, 0.99, 19.9…\n$ ship_pr     <dbl> 4.00, 3.99, 3.50, 0.00, 0.00, 4.00, 0.00, 2.99, 4.00, 4.00…\n$ total_pr    <dbl> 51.55, 37.04, 45.50, 44.00, 71.00, 45.00, 37.02, 53.99, 47…\n$ ship_sp     <fct> standard, firstClass, firstClass, standard, media, standar…\n$ seller_rate <int> 1580, 365, 998, 7, 820, 270144, 7284, 4858, 27, 201, 4858,…\n$ stock_photo <fct> yes, yes, no, yes, yes, yes, yes, yes, yes, no, yes, yes, …\n$ wheels      <int> 1, 1, 1, 1, 2, 0, 0, 2, 1, 1, 2, 2, 2, 2, 1, 0, 1, 1, 2, 2…\n$ title       <fct> \"~~ Wii MARIO KART &amp; WHEEL ~ NINTENDO Wii ~ BRAND NEW …\n\n# fit parallel slopes\n\nmod_mario <- lm(total_pr ~ wheels + cond, data = mario_kart)"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#reasoning-about-two-intercepts",
    "href": "myblog/datacamp/regression_r/regression.html#reasoning-about-two-intercepts",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Reasoning about two intercepts",
    "text": "Reasoning about two intercepts\nThe mario_kart data contains several other variables. The totalPr, startPr, and shipPr variables are numeric, while the cond and stockPhoto variables are categorical.\nWhich formula will result in a parallel slopes model?\n\ntotalPr ~ shipPr + stockPhoto"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#using-geom_line-and-augment",
    "href": "myblog/datacamp/regression_r/regression.html#using-geom_line-and-augment",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Using geom_line() and augment()",
    "text": "Using geom_line() and augment()\nParallel slopes models are so-named because we can visualize these models in the data space as not one line, but two parallel lines. To do this, we’ll draw two things:\na scatterplot showing the data, with color separating the points into groups a line for each value of the categorical variable Our plotting strategy is to compute the fitted values, plot these, and connect the points to form a line. The augment() function from the broom package provides an easy way to add the fitted values to our data frame, and the geom_line() function can then use that data frame to plot the points and connect them.\nNote that this approach has the added benefit of automatically coloring the lines appropriately to match the data.\nYou already know how to use ggplot() and geom_point() to make the scatterplot. The only twist is that now you’ll pass your augment()-ed model as the data argument in your ggplot() call. When you add your geom_line(), instead of letting the y aesthetic inherit its values from the ggplot() call, you can set it to the .fitted column of the augment()-ed model. This has the advantage of automatically coloring the lines for you.\n\n# Augment the model\naugmented_mod <- augment(mod_mario)\nglimpse(augmented_mod)\n\nRows: 141\nColumns: 10\n$ .rownames  <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"1…\n$ total_pr   <dbl> 51.55, 37.04, 45.50, 44.00, 71.00, 45.00, 37.02, 53.99, 47.…\n$ wheels     <int> 1, 1, 1, 1, 2, 0, 0, 2, 1, 1, 2, 2, 2, 2, 1, 0, 1, 1, 2, 0,…\n$ cond       <fct> new, used, new, new, new, new, used, new, used, used, new, …\n$ .fitted    <dbl> 49.60260, 44.01777, 49.60260, 49.60260, 56.83544, 42.36976,…\n$ .resid     <dbl> 1.9473995, -6.9777674, -4.1026005, -5.6026005, 14.1645592, …\n$ .hat       <dbl> 0.02103158, 0.01250410, 0.02103158, 0.02103158, 0.01915635,…\n$ .sigma     <dbl> 4.902339, 4.868399, 4.892414, 4.881308, 4.750591, 4.899816,…\n$ .cooksd    <dbl> 1.161354e-03, 8.712334e-03, 5.154337e-03, 9.612441e-03, 5.5…\n$ .std.resid <dbl> 0.40270893, -1.43671086, -0.84838977, -1.15857953, 2.926332…\n\n# scatterplot, with color\ndata_space <- ggplot(augmented_mod, aes(x = wheels, y = total_pr , color = cond )) + \n  geom_point()\n  \n# single call to geom_line()\ndata_space + \n  geom_line(aes(y = .fitted))"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#intercept-interpretation",
    "href": "myblog/datacamp/regression_r/regression.html#intercept-interpretation",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Intercept interpretation",
    "text": "Intercept interpretation\nRecall that the cond variable is either new or used. Here are the fitted coefficients from your model:\nCall: lm(formula = totalPr ~ wheels + cond, data = mario_kart)\nCoefficients: (Intercept) wheels condused\n42.370 7.233 -5.585\nChoose the correct interpretation of the coefficient on condused:\n\nThe expected price of a used MarioKart is $5.58 less than that of a new one with the same number of wheels.\nFor each additional wheel, the expected price of a MarioKart increases by $7.23 regardless of whether it is new or used.\n\nSyntax from math The babies data set contains observations about the birthweight and other characteristics of children born in the San Francisco Bay area from 1960–1967.\nWe would like to build a model for birthweight as a function of the mother’s age and whether this child was her first (parity == 0). Use the mathematical specification below to code the model in R.\n\\[birthweight = \\beta_0 + \\beta_1 * age  + \\beta_2 * parity + \\epsilon\\]\n\ndata( babies, package = \"openintro\")\n\nmod <- lm(bwt~ age+parity, data = babies)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n118.3\n2.788\n42.43\n3.957e-243\n\n\nage\n0.06315\n0.09577\n0.6594\n0.5097\n\n\nparity\n-1.652\n1.271\n-1.3\n0.1937"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#syntax-from-plot",
    "href": "myblog/datacamp/regression_r/regression.html#syntax-from-plot",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Syntax from plot",
    "text": "Syntax from plot\nThis time, we’d like to build a model for birthweight as a function of the length of gestation and the mother’s smoking status. Use the plot to inform your model specification.\n\nggplot(babies, aes(gestation, bwt, color = factor(smoke)))+\n    geom_point()\n\n\n\nmod <- lm(bwt~ gestation + smoke, data = babies)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.9317\n8.152\n-0.1143\n0.909\n\n\ngestation\n0.4429\n0.02902\n15.26\n3.156e-48\n\n\nsmoke\n-8.088\n0.9527\n-8.49\n5.963e-17\n\n\n\n\n\nR-squared vs. adjusted R-squared Two common measures of how well a model fits to data are \\[R^2\\] (the coefficient of determination) and the adjusted \\[R^2\\] . The former measures the percentage of the variability in the response variable that is explained by the model. To compute this, we define\n\\[R^2 = 1 - \\frac{sse}{sst} \\] where SSE and SST are the sum of the squared residuals, and the total sum of the squares, respectively. One issue with this measure is that the can only decrease as new variable are added to the model, while the SST depends only on the response variable and therefore is not affected by changes to the model. This means that you can increase \\[R^2\\] by adding any additional variable to your model—even random noise.\nThe adjusted \\[R^2\\] includes a term that penalizes a model for each additional explanatory variable (where is the number of explanatory variables). We can see both measures in the output of the summary() function on our model object.\n\n# R^2 and adjusted R^2\nsummary(mod_mario)\n\n\nCall:\nlm(formula = total_pr ~ wheels + cond, data = mario_kart)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.0078  -3.0754  -0.8254   2.9822  14.1646 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  42.3698     1.0651  39.780  < 2e-16 ***\nwheels        7.2328     0.5419  13.347  < 2e-16 ***\ncondused     -5.5848     0.9245  -6.041 1.35e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.887 on 138 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7165,    Adjusted R-squared:  0.7124 \nF-statistic: 174.4 on 2 and 138 DF,  p-value: < 2.2e-16\n\n# add random noise\nmario_kart_noisy <- mario_kart %>% \nmutate(noise = rnorm(n = nrow(mario_kart)))\n  \n# compute new model\nmod2_mario2 <- lm(total_pr ~ wheels + cond+noise, data = mario_kart_noisy)\n\n# new R^2 and adjusted R^2\nsummary(mod2_mario2)\n\n\nCall:\nlm(formula = total_pr ~ wheels + cond + noise, data = mario_kart_noisy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.1221  -3.0761  -0.8246   2.8795  14.3721 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  42.4152     1.0787  39.321  < 2e-16 ***\nwheels        7.2197     0.5454  13.239  < 2e-16 ***\ncondused     -5.6279     0.9380  -6.000 1.67e-08 ***\nnoise         0.1404     0.4546   0.309    0.758    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.904 on 137 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7167,    Adjusted R-squared:  0.7105 \nF-statistic: 115.5 on 3 and 137 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#prediction",
    "href": "myblog/datacamp/regression_r/regression.html#prediction",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Prediction",
    "text": "Prediction\nOnce we have fit a regression model, we can use it to make predictions for unseen observations or retrieve the fitted values. Here, we explore two methods for doing the latter.\nA traditional way to return the fitted values (i.e. the y ’s) is to run the predict() function on the model object. This will return a vector of the fitted values. Note that predict() will take an optional newdata argument that will allow you to make predictions for observations that are not in the original data.\nA newer alternative is the augment() function from the broom package, which returns a data.frame with the response varible (), the relevant explanatory variables (the ’s), the fitted value ( ) and some information about the residuals (). augment() will also take a newdata argument that allows you to make predictions.\n\n# return a vector\nlibrary(knitr)\n\npredict(mod_mario)\n\n       1        2        3        4        5        6        7        8 \n49.60260 44.01777 49.60260 49.60260 56.83544 42.36976 36.78493 56.83544 \n       9       10       11       12       13       14       15       16 \n44.01777 44.01777 56.83544 56.83544 56.83544 56.83544 44.01777 36.78493 \n      17       18       19       21       22       23       24       25 \n49.60260 49.60260 56.83544 36.78493 56.83544 56.83544 56.83544 44.01777 \n      26       27       28       29       30       31       32       33 \n56.83544 36.78493 36.78493 36.78493 49.60260 36.78493 36.78493 44.01777 \n      34       35       36       37       38       39       40       41 \n51.25061 44.01777 44.01777 36.78493 44.01777 56.83544 56.83544 49.60260 \n      42       43       44       45       46       47       48       49 \n44.01777 51.25061 56.83544 56.83544 44.01777 56.83544 36.78493 36.78493 \n      50       51       52       53       54       55       56       57 \n44.01777 56.83544 36.78493 44.01777 42.36976 36.78493 36.78493 44.01777 \n      58       59       60       61       62       63       64       66 \n44.01777 36.78493 36.78493 56.83544 36.78493 56.83544 36.78493 51.25061 \n      67       68       69       70       71       72       73       74 \n56.83544 44.01777 58.48345 51.25061 49.60260 44.01777 49.60260 56.83544 \n      75       76       77       78       79       80       81       82 \n56.83544 51.25061 44.01777 36.78493 36.78493 36.78493 44.01777 56.83544 \n      83       84       85       86       87       88       89       90 \n44.01777 65.71629 44.01777 56.83544 36.78493 49.60260 49.60260 36.78493 \n      91       92       93       94       95       96       97       98 \n44.01777 36.78493 51.25061 44.01777 36.78493 51.25061 42.36976 56.83544 \n      99      100      101      102      103      104      105      106 \n51.25061 44.01777 51.25061 56.83544 56.83544 56.83544 36.78493 49.60260 \n     107      108      109      110      111      112      113      114 \n51.25061 44.01777 56.83544 49.60260 36.78493 44.01777 51.25061 56.83544 \n     115      116      117      118      119      120      121      122 \n64.06828 44.01777 49.60260 44.01777 49.60260 51.25061 42.36976 44.01777 \n     123      124      125      126      127      128      129      130 \n56.83544 44.01777 49.60260 44.01777 51.25061 56.83544 56.83544 49.60260 \n     131      132      133      134      135      136      137      138 \n56.83544 36.78493 44.01777 44.01777 36.78493 56.83544 36.78493 44.01777 \n     139      140      141      142      143 \n36.78493 51.25061 49.60260 36.78493 56.83544 \n\n# return a data frame\n\naugment(mod_mario)%>% head() %>% kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.rownames\ntotal_pr\nwheels\ncond\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n1\n51.55\n1\nnew\n49.60260\n1.947399\n0.0210316\n4.902340\n0.0011614\n0.4027089\n\n\n2\n37.04\n1\nused\n44.01777\n-6.977767\n0.0125041\n4.868399\n0.0087123\n-1.4367109\n\n\n3\n45.50\n1\nnew\n49.60260\n-4.102601\n0.0210316\n4.892414\n0.0051543\n-0.8483898\n\n\n4\n44.00\n1\nnew\n49.60260\n-5.602601\n0.0210316\n4.881308\n0.0096124\n-1.1585795\n\n\n5\n71.00\n2\nnew\n56.83544\n14.164559\n0.0191563\n4.750591\n0.0557493\n2.9263328\n\n\n6\n45.00\n0\nnew\n42.36976\n2.630240\n0.0474932\n4.899816\n0.0050537\n0.5514192"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#thought-experiments",
    "href": "myblog/datacamp/regression_r/regression.html#thought-experiments",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Thought experiments",
    "text": "Thought experiments\nSuppose that after going apple picking you have 12 apples left over. You decide to conduct an experiment to investigate how quickly they will rot under certain conditions. You place six apples in a cool spot in your basement, and leave the other six on the window sill in the kitchen. Every week, you estimate the percentage of the surface area of the apple that is rotten or moldy.\nConsider the following models:\n\\[rot = \\beta_0 + \\beta_1 *t + \\beta_2 * temp + \\epsilon \\]\nand\n\\[rot = \\beta_0 + \\beta_1 *t + \\beta_2 * temp + \\beta_2 * temp *t +  \\epsilon \\]\n\nThe rate at which apples rot will vary based on the temperature."
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#fitting-a-model-with-interaction",
    "href": "myblog/datacamp/regression_r/regression.html#fitting-a-model-with-interaction",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Fitting a model with interaction",
    "text": "Fitting a model with interaction\nIncluding an interaction term in a model is easy—we just have to tell lm() that we want to include that new variable. An expression of the form\nlm(y ~ x + z + x:z, data = mydata)\nwill do the trick. The use of the colon (:) here means that the interaction between and will be a third term in the model.\n\n# include interaction\n\nmod <- lm(total_pr ~cond + duration + cond:duration, data = mario_kart)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n58.27\n1.366\n42.64\n5.832e-81\n\n\ncondused\n-17.12\n2.178\n-7.86\n1.014e-12\n\n\nduration\n-1.966\n0.4488\n-4.38\n2.342e-05\n\n\ncondused:duration\n2.325\n0.5484\n4.239\n4.102e-05"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#visualizing-interaction-models",
    "href": "myblog/datacamp/regression_r/regression.html#visualizing-interaction-models",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Visualizing interaction models",
    "text": "Visualizing interaction models\nInteraction allows the slope of the regression line in each group to vary. In this case, this means that the relationship between the final price and the length of the auction is moderated by the condition of each item.\nInteraction models are easy to visualize in the data space with ggplot2 because they have the same coefficients as if the models were fit independently to each group defined by the level of the categorical variable. In this case, new and used MarioKarts each get their own regression line. To see this, we can set an aesthetic (e.g. color) to the categorical variable, and then add a geom_smooth() layer to overlay the regression line for each color.\n\n# interaction plot\nggplot(mario_kart, aes(duration, total_pr, color = cond)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = 0)"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#consequences-of-simpsons-paradox",
    "href": "myblog/datacamp/regression_r/regression.html#consequences-of-simpsons-paradox",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Consequences of Simpson’s paradox",
    "text": "Consequences of Simpson’s paradox\nIn the simple linear regression model for average SAT score, (total) as a function of average teacher salary (salary), the fitted coefficient was -5.02 points per thousand dollars. This suggests that for every additional thousand dollars of salary for teachers in a particular state, the expected SAT score for a student from that state is about 5 points lower.\nIn the model that includes the percentage of students taking the SAT, the coefficient on salary becomes 1.84 points per thousand dollars. Choose the correct interpretation of this slope coefficient.\n\nFor every additional thousand dollars of salary for teachers in a particular state, the expected SAT score for a student from that state is about 2 points higher, after controlling for the percentage of students taking the SAT."
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#simpsons-paradox-in-action",
    "href": "myblog/datacamp/regression_r/regression.html#simpsons-paradox-in-action",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Simpson’s paradox in action",
    "text": "Simpson’s paradox in action\nA mild version of Simpson’s paradox can be observed in the MarioKart auction data. Consider the relationship between the final auction price and the length of the auction. It seems reasonable to assume that longer auctions would result in higher prices, since—other things being equal—a longer auction gives more bidders more time to see the auction and bid on the item.\nHowever, a simple linear regression model reveals the opposite: longer auctions are associated with lower final prices. The problem is that all other things are not equal. In this case, the new MarioKarts—which people pay a premium for—were mostly sold in one-day auctions, while a plurality of the used MarioKarts were sold in the standard seven-day auctions.\nOur simple linear regression model is misleading, in that it suggests a negative relationship between final auction price and duration. However, for the used MarioKarts, the relationship is positive.\n\nslr <- ggplot(mario_kart, aes(y = total_pr, x = duration)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n# model with one slope\nmod <- lm(total_pr ~ duration, data = mario_kart)\n\n# plot with two slopes\nslr + aes(color = cond)"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#fitting-a-mlr-model",
    "href": "myblog/datacamp/regression_r/regression.html#fitting-a-mlr-model",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Fitting a MLR model",
    "text": "Fitting a MLR model\nIn terms of the R code, fitting a multiple linear regression model is easy: simply add variables to the model formula you specify in the lm() command.\nIn a parallel slopes model, we had two explanatory variables: one was numeric and one was categorical. Here, we will allow both explanatory variables to be numeric.\n\n# Fit the model using duration and startPr\n\nmod <- lm(total_pr~ start_pr + duration, data = mario_kart)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n51.03\n1.179\n43.28\n3.666e-82\n\n\nstart_pr\n0.233\n0.04364\n5.339\n3.756e-07\n\n\nduration\n-1.508\n0.2555\n-5.902\n2.645e-08"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#tiling-the-plane",
    "href": "myblog/datacamp/regression_r/regression.html#tiling-the-plane",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Tiling the plane",
    "text": "Tiling the plane\nOne method for visualizing a multiple linear regression model is to create a heatmap of the fitted values in the plane defined by the two explanatory variables. This heatmap will illustrate how the model output changes over different combinations of the explanatory variables.\nThis is a multistep process:\nFirst, create a grid of the possible pairs of values of the explanatory variables. The grid should be over the actual range of the data present in each variable. We’ve done this for you and stored the result as a data frame called grid. Use augment() with the newdata argument to find the ’s corresponding to the values in grid. Add these to the data_space plot by using the fill aesthetic and geom_tile().\n\n# add predictions to grid\nprice_hats <- augment(mod, newdata = grid)\n\n# tile the plane\ndata_space + \n  geom_tile(data = price_hats, aes(fill = .fitted), alpha = 0.5)"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#models-in-3d",
    "href": "myblog/datacamp/regression_r/regression.html#models-in-3d",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Models in 3D",
    "text": "Models in 3D\nAn alternative way to visualize a multiple regression model with two numeric explanatory variables is as a plane in three dimensions. This is possible in R using the plotly package.\nWe have created three objects that you will need:\nx: a vector of unique values of duration y: a vector of unique values of startPr plane: a matrix of the fitted values across all combinations of x and y Much like ggplot(), the plot_ly() function will allow you to create a plot object with variables mapped to x, y, and z aesthetics. The add_markers() function is similar to geom_point() in that it allows you to add points to your 3D plot.\nNote that plot_ly uses the pipe (%>%) operator to chain commands together.\n\n# draw the 3D scatterplot\np <- plot_ly(data = mario_kart, z = ~totalPr, x = ~duration, y = ~startPr, opacity = 0.6) %>%\n  add_markers() \n  \n# draw the plane\np %>%\n  add_surface(x = ~x, y = ~y, z = ~plane, showscale = FALSE)"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html",
    "href": "myblog/datacamp/tidymodels/tidymodels.html",
    "title": "Modeling with tidymodels in R",
    "section": "",
    "text": "The rsample package is designed to create training and test datasets. Creating a test dataset is important for estimating how a trained model will likely perform on new data. It also guards against overfitting, where a model memorizes patterns that exist only in the training data and performs poorly on new data.\nIn this exercise, you will create training and test datasets from the home_sales data. This data contains information on homes sold in the Seattle, Washington area between 2015 and 2016.\nThe outcome variable in this data is selling_price.\nThe tidymodels package will be pre-loaded in every exercise in the course. The home_sales tibble has also been loaded for you.\n\n\n\nTidy model packages\n\n\n\nhome_sales <- readRDS(\"home_sales.rds\")\n\n# Create a data split object\nhome_split <- initial_split(home_sales, \n                            prop = 0.7, \n                            strata = selling_price)\n\n# Create the training data\nhome_training <- home_split %>%\n  training()\n\n# Create the test data\nhome_test <- home_split %>% \n  testing()\n\n# Check number of rows in each dataset\nnrow(home_training)\n\n[1] 1042\n\nnrow(home_test)\n\n[1] 450\n\n\nDistribution of outcome variable values Stratifying by the outcome variable when generating training and test datasets ensures that the outcome variable values have a similar range in both datasets.\nSince the original data is split at random, stratification avoids placing all the expensive homes in home_sales into the test dataset, for example. In this case, your model would most likely perform poorly because it was trained on less expensive homes.\nIn this exercise, you will calculate summary statistics for the selling_price variable in the training and test datasets. The home_training and home_test tibbles have been loaded from the previous exercise.\n\n# Distribution of selling_price in training data\nlibrary(knitr)\nsummary_func <- function(df){\n      df %>% summarize(min_sell_price = min(selling_price),\n            max_sell_price = max(selling_price),\n            mean_sell_price = mean(selling_price),\n            sd_sell_price = sd(selling_price)) %>%\n    kable()\n\n}\nhome_training %>% \n    summary_func()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n479438.2\n80925.92\n\n\n\n\nhome_test %>% \n  summary_func()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n478265.5\n81185.76"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#fitting-a-linear-regression-model",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#fitting-a-linear-regression-model",
    "title": "Modeling with tidymodels in R",
    "section": "Fitting a linear regression model",
    "text": "Fitting a linear regression model\nThe parsnip package provides a unified syntax for the model fitting process in R.\nWith parsnip, it is easy to define models using the various packages, or engines, that exist in the R ecosystem.\nIn this exercise, you will define a parsnip linear regression object and train your model to predict selling_price using home_age and sqft_living as predictor variables from the home_sales data.\nThe home_training and home_test tibbles that you created in the previous lesson have been loaded into this session.\n\n# Specify a linear regression model, linear_model\nlinear_model <- linear_reg() %>% \n  # Set the model engine\n  set_engine('lm') %>% \n  # Set the model mode\n  set_mode('regression')\n\n# Train the model with the training data\nlm_fit <- linear_model %>% \n  fit(selling_price~ home_age + sqft_living,\n      data = home_training)\n\n# Print lm_fit to view model information\ntidy(lm_fit) %>%\n    kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n285035.0613\n7691.199132\n37.059899\n0\n\n\nhome_age\n-1231.5792\n178.152470\n-6.913063\n0\n\n\nsqft_living\n103.9248\n2.771416\n37.498823\n0"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#predicting-home-selling-prices",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#predicting-home-selling-prices",
    "title": "Modeling with tidymodels in R",
    "section": "Predicting home selling prices",
    "text": "Predicting home selling prices\nAfter fitting a model using the training data, the next step is to use it to make predictions on the test dataset. The test dataset acts as a new source of data for the model and will allow you to evaluate how well it performs.\nBefore you can evaluate model performance, you must add your predictions to the test dataset.\nIn this exercise, you will use your trained model, lm_fit, to predict selling_price in the home_test dataset.\nYour trained model, lm_fit, as well as the test dataset, home_test have been loaded into your session.\n\n# Predict selling_price\nhome_predictions <- predict(lm_fit,\n                        new_data = home_test)\n\n# View predicted selling prices\n#home_predictions\n\n# Combine test data with predictions\nhome_test_results <- home_test %>% \n  select(selling_price, home_age, sqft_living) %>% \n  bind_cols(home_predictions)\n\n# View results\nhome_test_results %>% \n    head()\n\n# A tibble: 6 × 4\n  selling_price home_age sqft_living   .pred\n          <dbl>    <dbl>       <dbl>   <dbl>\n1        635000        4        3350 628257.\n2        380000       24        2130 476837.\n3        495000       21        1650 430648.\n4        355000       19        1430 410248.\n5        464950       19        2190 489230.\n6        475000        0        2300 524062."
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#model-performance-metrics",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#model-performance-metrics",
    "title": "Modeling with tidymodels in R",
    "section": "Model performance metrics",
    "text": "Model performance metrics\nEvaluating model results is an important step in the modeling process. Model evaluation should be done on the test dataset in order to see how well a model will generalize to new datasets.\nIn the previous exercise, you trained a linear regression model to predict selling_price using home_age and sqft_living as predictor variables. You then created the home_test_results tibble using your trained model on the home_test data.\nIn this exercise, you will calculate the RMSE and R squared metrics using your results in home_test_results.\nThe home_test_results tibble has been loaded into your session.\n\n# Print home_test_results\n#home_test_results\n\n# Caculate the RMSE metric\nhome_test_results %>% \n  rmse(truth = selling_price, estimate =.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      46214.\n\n# Calculate the R squared metric\nhome_test_results %>% \n  rsq(truth = selling_price, estimate =.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.677"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#r-squared-plot",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#r-squared-plot",
    "title": "Modeling with tidymodels in R",
    "section": "R squared plot",
    "text": "R squared plot\nIn the previous exercise, you got an R squared value of 0.651. The R squared metric ranges from 0 to 1, 0 being the worst and 1 the best.\nCalculating the R squared value is only the first step in studying your model’s predictions.\nMaking an R squared plot is extremely important because it will uncover potential problems with your model, such as non-linear patterns or regions where your model is either over or under-predicting the outcome variable.\nIn this exercise, you will create an R squared plot of your model’s performance.\nThe home_test_results tibble has been loaded into your session.\n\n# Create an R squared plot of model performance\nggplot(home_test_results, aes(x = selling_price, y = .pred)) +\n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred()  +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#complete-model-fitting-process-with-last_fit",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#complete-model-fitting-process-with-last_fit",
    "title": "Modeling with tidymodels in R",
    "section": "Complete model fitting process with last_fit()",
    "text": "Complete model fitting process with last_fit()\nIn this exercise, you will train and evaluate the performance of a linear regression model that predicts selling_price using all the predictors available in the home_sales tibble.\nThis exercise will give you a chance to perform the entire model fitting process with tidymodels, from defining your model object to evaluating its performance on the test data.\nEarlier in the chapter, you created an rsample object called home_split by passing the home_sales tibble into initial_split(). The home_split object contains the instructions for randomly splitting home_sales into training and test sets.\nThe home_sales tibble, and home_split object have been loaded into this session.\n\n# Define a linear regression model\nlinear_model <- linear_reg() %>% \n  set_engine('lm') %>% \n  set_mode('regression')\n\n# Train linear_model with last_fit()\nlinear_fit <- linear_model %>% \n  last_fit(selling_price ~ ., split = home_split)\n\n# Collect predictions and view results\npredictions_df <- linear_fit %>% collect_predictions()\npredictions_df %>% head()\n\n# A tibble: 6 × 5\n  id                 .pred  .row selling_price .config             \n  <chr>              <dbl> <int>         <dbl> <chr>               \n1 train/test split 696151.     4        635000 Preprocessor1_Model1\n2 train/test split 408483.     5        380000 Preprocessor1_Model1\n3 train/test split 445489.     6        495000 Preprocessor1_Model1\n4 train/test split 399760.     7        355000 Preprocessor1_Model1\n5 train/test split 474256.     8        464950 Preprocessor1_Model1\n6 train/test split 456075.    16        475000 Preprocessor1_Model1\n\n# Make an R squared plot using predictions_df\nggplot(predictions_df, aes(x = selling_price, y = .pred)) + \n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred() +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#data-resampling",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#data-resampling",
    "title": "Modeling with tidymodels in R",
    "section": "Data resampling",
    "text": "Data resampling\nThe first step in a machine learning project is to create training and test datasets for model fitting and evaluation. The test dataset provides an estimate of how your model will perform on new data and helps to guard against overfitting.\nYou will be working with the telecom_df dataset which contains information on customers of a telecommunications company. The outcome variable is canceled_service and it records whether a customer canceled their contract with the company. The predictor variables contain information about customers’ cell phone and internet usage as well as their contract type and monthly charges.\nThe telecom_df tibble has been loaded into your session.\n\ntelecom_df <- readRDS(\"telecom_df.rds\")\n# Create data split object\ntelecom_split <- initial_split(telecom_df, prop = 0.75,\n                     strata = canceled_service)\n\n# Create the training data\ntelecom_training <- telecom_split %>% \n  training()\n\n# Create the test data\ntelecom_test <- telecom_split %>% \n  testing()\n\n# Check the number of rows\nnrow(telecom_training)\n\n[1] 731\n\nnrow(telecom_test)\n\n[1] 244"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#fitting-a-logistic-regression-model",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#fitting-a-logistic-regression-model",
    "title": "Modeling with tidymodels in R",
    "section": "Fitting a logistic regression model",
    "text": "Fitting a logistic regression model\nIn addition to regression models, the parsnip package also provides a general interface to classification models in R.\nIn this exercise, you will define a parsnip logistic regression object and train your model to predict canceled_service using avg_call_mins, avg_intl_mins, and monthly_charges as predictor variables from the telecom_df data.\nThe telecom_training and telecom_test tibbles that you created in the previous lesson have been loaded into this session.\n\n# Specify a logistic regression model\nlogistic_model <- logistic_reg() %>% \n  # Set the engine\n  set_engine('glm') %>% \n  # Set the mode\n  set_mode('classification')\n\n# Fit to training data\nlogistic_fit <- logistic_model %>% \n  fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n      data = telecom_training)\n\n# Print model fit object\nlogistic_fit %>% tidy()\n\n# A tibble: 4 × 5\n  term            estimate std.error statistic  p.value\n  <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)      2.46      0.590       4.17  3.02e- 5\n2 avg_call_mins   -0.0107    0.00129    -8.29  1.10e-16\n3 avg_intl_mins    0.0209    0.00307     6.82  9.07e-12\n4 monthly_charges -0.00144   0.00484    -0.298 7.65e- 1"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#combining-test-dataset-results",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#combining-test-dataset-results",
    "title": "Modeling with tidymodels in R",
    "section": "Combining test dataset results",
    "text": "Combining test dataset results\nEvaluating your model’s performance on the test dataset gives insights into how well your model predicts on new data sources. These insights will help you communicate your model’s value in solving problems or improving decision making.\nBefore you can calculate classification metrics such as sensitivity or specificity, you must create a results tibble with the required columns for yardstick metric functions.\nIn this exercise, you will use your trained model to predict the outcome variable in the telecom_test dataset and combine it with the true outcome values in the canceled_service column.\nYour trained model, logistic_fit, and test dataset, telecom_test, have been loaded from the previous exercise.\n\n# Predict outcome categories\nclass_preds <- predict(logistic_fit, new_data = telecom_test,\n                       type = 'class')\n\n# Obtain estimated probabilities for each outcome value\nprob_preds <- predict(logistic_fit, new_data = telecom_test, \n                      type = 'prob')\n\n# Combine test set results\ntelecom_results <- telecom_test %>% \n  select(canceled_service) %>% \n  bind_cols(class_preds, prob_preds)\n\n# View results tibble\ntelecom_results %>%\n    head()\n\n# A tibble: 6 × 4\n  canceled_service .pred_class .pred_yes .pred_no\n  <fct>            <fct>           <dbl>    <dbl>\n1 yes              no             0.364     0.636\n2 no               no             0.0166    0.983\n3 no               no             0.252     0.748\n4 yes              no             0.448     0.552\n5 no               no             0.209     0.791\n6 no               yes            0.585     0.415"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#evaluating-performance-with-yardstick",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#evaluating-performance-with-yardstick",
    "title": "Modeling with tidymodels in R",
    "section": "Evaluating performance with yardstick",
    "text": "Evaluating performance with yardstick\nIn the previous exercise, you calculated classification metrics from a sample confusion matrix. The yardstick package was designed to automate this process.\nFor classification models, yardstick functions require a tibble of model results as the first argument. This should include the actual outcome values, predicted outcome values, and estimated probabilities for each value of the outcome variable.\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate performance metrics.\nThe telecom_results tibble has been loaded into your session.\n\n# Calculate the confusion matrix\nconf_mat(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n          Truth\nPrediction yes  no\n       yes  25  11\n       no   57 151\n\n# Calculate the accuracy\naccuracy(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.721\n\n# Calculate the sensitivity\n\nsens(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.305\n\n# Calculate the specificity\n\nspec(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 spec    binary         0.932"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#creating-custom-metric-sets",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#creating-custom-metric-sets",
    "title": "Modeling with tidymodels in R",
    "section": "Creating custom metric sets",
    "text": "Creating custom metric sets\nThe yardstick package also provides the ability to create custom sets of model metrics. In cases where the cost of obtaining false negative errors is different from the cost of false positive errors, it may be important to examine a specific set of performance metrics.\nInstead of calculating accuracy, sensitivity, and specificity separately, you can create your own metric function that calculates all three at the same time.\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate a custom set of performance metrics. You will also use a confusion matrix to calculate all available binary classification metrics in tidymodelsall at once.\nThe telecom_results tibble has been loaded into your session.\n\n# Create a custom metric function\ntelecom_metrics <- metric_set(accuracy, sens, spec)\n\n# Calculate metrics using model results tibble\ntelecom_metrics(telecom_results, \n                truth = canceled_service,\n                estimate = .pred_class)\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.721\n2 sens     binary         0.305\n3 spec     binary         0.932\n\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class) %>% \n  # Pass to the summary() function\n  summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary         0.721\n 2 kap                  binary         0.275\n 3 sens                 binary         0.305\n 4 spec                 binary         0.932\n 5 ppv                  binary         0.694\n 6 npv                  binary         0.726\n 7 mcc                  binary         0.316\n 8 j_index              binary         0.237\n 9 bal_accuracy         binary         0.618\n10 detection_prevalence binary         0.148\n11 precision            binary         0.694\n12 recall               binary         0.305\n13 f_meas               binary         0.424"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#plotting-the-confusion-matrix",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#plotting-the-confusion-matrix",
    "title": "Modeling with tidymodels in R",
    "section": "Plotting the confusion matrix",
    "text": "Plotting the confusion matrix\nCalculating performance metrics with the yardstick package provides insight into how well a classification model is performing on the test dataset. Most yardstick functions return a single number that summarizes classification performance.\nMany times, it is helpful to create visualizations of the confusion matrix to more easily communicate your results.\nIn this exercise, you will make a heat map and mosaic plot of the confusion matrix from your logistic regression model on the telecom_df dataset.\nYour model results tibble, telecom_results, has been loaded into your session.\n\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a heat map\n  autoplot(type = \"heatmap\")\n\n\n\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a mosaic plot\n  autoplot(type = \"mosaic\")"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#roc-curves-and-area-under-the-roc-curve",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#roc-curves-and-area-under-the-roc-curve",
    "title": "Modeling with tidymodels in R",
    "section": "ROC curves and area under the ROC curve",
    "text": "ROC curves and area under the ROC curve\nROC curves are used to visualize the performance of a classification model across a range of probability thresholds. An ROC curve with the majority of points near the upper left corner of the plot indicates that a classification model is able to correctly predict both the positive and negative outcomes correctly across a wide range of probability thresholds.\nThe area under this curve provides a letter grade summary of model performance.\nIn this exercise, you will create an ROC curve from your logistic regression model results and calculate the area under the ROC curve with yardstick.\nYour model results tibble, telecom_results has been loaded into your session.\n\n# Calculate metrics across thresholds\nthreshold_df <- telecom_results %>% \n  roc_curve(truth = canceled_service, .pred_yes)\n\n# View results\nthreshold_df %>%\n  head()\n\n# A tibble: 6 × 3\n  .threshold specificity sensitivity\n       <dbl>       <dbl>       <dbl>\n1  -Inf          0             1    \n2     0.0109     0             1    \n3     0.0166     0             0.988\n4     0.0210     0.00617       0.988\n5     0.0371     0.0123        0.988\n6     0.0443     0.0185        0.988\n\n# Plot ROC curve\nthreshold_df %>% \n  autoplot()\n\n\n\n# Calculate ROC AUC\nroc_auc(telecom_results, truth = canceled_service, .pred_yes)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.751\n\n\nStreamlining the modeling process The last_fit() function is designed to streamline the modeling workflow in tidymodels. Instead of training your model on the training data and building a results tibble using the test data, last_fit() accomplishes this with one function.\nIn this exercise, you will train the same logistic regression model as you fit in the previous exercises, except with the last_fit() function.\nYour data split object, telecom_split, and model specification, logistic_model, have been loaded into your session.\n\n# Train model with last_fit()\ntelecom_last_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n           split = telecom_split)\n\n# View test set metrics\ntelecom_last_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.721 Preprocessor1_Model1\n2 roc_auc  binary         0.751 Preprocessor1_Model1"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#collecting-predictions-and-creating-custom-metrics",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#collecting-predictions-and-creating-custom-metrics",
    "title": "Modeling with tidymodels in R",
    "section": "Collecting predictions and creating custom metrics",
    "text": "Collecting predictions and creating custom metrics\nUsing the last_fit() modeling workflow also saves time in collecting model predictions. Instead of manually creating a tibble of model results, there are helper functions that extract this information automatically.\nIn this exercise, you will use your trained model, telecom_last_fit, to create a tibble of model results on the test dataset as well as calculate custom performance metrics.\nYou trained model, telecom_last_fit, has been loaded into this session.\n\n# Collect predictions\nlast_fit_results <- telecom_last_fit %>% \n  collect_predictions()\n\n# View results\nlast_fit_results %>%\n  head()\n\n# A tibble: 6 × 7\n  id               .pred_yes .pred_no  .row .pred_class canceled_service .config\n  <chr>                <dbl>    <dbl> <int> <fct>       <fct>            <chr>  \n1 train/test split    0.364     0.636     2 no          yes              Prepro…\n2 train/test split    0.0166    0.983    22 no          no               Prepro…\n3 train/test split    0.252     0.748    23 no          no               Prepro…\n4 train/test split    0.448     0.552    30 no          yes              Prepro…\n5 train/test split    0.209     0.791    33 no          no               Prepro…\n6 train/test split    0.585     0.415    43 yes         no               Prepro…\n\n# Custom metrics function\nlast_fit_metrics <- metric_set(accuracy, sens,\n                               spec, roc_auc)\n\n# Calculate metrics\nlast_fit_metrics(last_fit_results,\n                 truth = canceled_service,\n                 estimate = .pred_class,\n                 .pred_yes)\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.721\n2 sens     binary         0.305\n3 spec     binary         0.932\n4 roc_auc  binary         0.751"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#complete-modeling-workflow",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#complete-modeling-workflow",
    "title": "Modeling with tidymodels in R",
    "section": "Complete modeling workflow",
    "text": "Complete modeling workflow\nIn this exercise, you will use the last_fit() function to train a logistic regression model and evaluate its performance on the test data by assessing the ROC curve and the area under the ROC curve.\nSimilar to previous exercises, you will predict canceled_service in the telecom_df data, but with an additional predictor variable to see if you can improve model performance.\nThe telecom_df tibble, telecom_split, and logistic_model objects from the previous exercises have been loaded into your workspace. The telecom_split object contains the instructions for randomly splitting the telecom_df tibble into training and test sets. The logistic_model object is a parsnip specification of a logistic regression model.\n\n# Train a logistic regression model\nlogistic_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins + avg_intl_mins + monthly_charges + months_with_company, \n           split = telecom_split)\n\n# Collect metrics\nlogistic_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.811 Preprocessor1_Model1\n2 roc_auc  binary         0.865 Preprocessor1_Model1\n\n# Collect model predictions\nlogistic_fit %>% \n  collect_predictions() %>% \n  # Plot ROC curve\n  roc_curve(truth = canceled_service, .pred_yes) %>% \n  autoplot()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mburu",
    "section": "",
    "text": "Data Science projects I have been working on 😊"
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html",
    "title": "Writing Efficient R Code",
    "section": "",
    "text": "One of the relatively easy optimizations available is to use an up-to-date version of R. In general, R is very conservative, so upgrading doesn’t break existing code. However, a new version will often provide free speed boosts for key functions.\nThe version command returns a list that contains (among other things) the major and minor version of R currently being used.\n\n# Print the R version details using version\nversion\n\n               _                           \nplatform       x86_64-pc-linux-gnu         \narch           x86_64                      \nos             linux-gnu                   \nsystem         x86_64, linux-gnu           \nstatus                                     \nmajor          4                           \nminor          1.2                         \nyear           2021                        \nmonth          11                          \nday            01                          \nsvn rev        81115                       \nlanguage       R                           \nversion.string R version 4.1.2 (2021-11-01)\nnickname       Bird Hippie                 \n\n# Assign the variable major to the major component\nmajor <- version$major\n\n# Assign the variable minor to the minor component\nminor <- version$minor"
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#comparing-read-times-of-csv-and-rds-files",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#comparing-read-times-of-csv-and-rds-files",
    "title": "Writing Efficient R Code",
    "section": "Comparing read times of CSV and RDS files",
    "text": "Comparing read times of CSV and RDS files\nOne of the most common tasks we perform is reading in data from CSV files. However, for large CSV files this can be slow. One neat trick is to read in the data and save as an R binary file (rds) using saveRDS(). To read in the rds file, we use readRDS().\nNote: Since rds is R’s native format for storing single objects, you have not introduced any third-party dependencies that may change in the future.\nTo benchmark the two approaches, you can use system.time(). This function returns the time taken to evaluate any R expression. For example, to time how long it takes to calculate the square root of the numbers from one to ten million, you would write the following:\n\n# How long does it take to read movies from CSV?\nsystem.time(read.csv(\"movies.csv\"))\n\n   user  system elapsed \n  0.131   0.003   0.135 \n\n# How long does it take to read movies from RDS?\nsystem.time(readRDS(\"movies.rds\"))\n\n   user  system elapsed \n   0.03    0.00    0.03"
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#elapsed-time",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#elapsed-time",
    "title": "Writing Efficient R Code",
    "section": "Elapsed time",
    "text": "Elapsed time\nUsing system.time() is convenient, but it does have its drawbacks when comparing multiple function calls. The microbenchmark package solves this problem with the microbenchmark() function.\n\n# Load the microbenchmark package\nlibrary(microbenchmark)\n\n# Compare the two functions\ncompare <- microbenchmark(read.csv(\"movies.csv\"), \n                          readRDS(\"movies.rds\"), \n                          times = 100)\n\n# Print compare\ncompare\n\nUnit: milliseconds\n                   expr       min        lq     mean   median        uq\n read.csv(\"movies.csv\") 124.06143 132.66351 147.2988 139.0496 152.97855\n  readRDS(\"movies.rds\")  30.09878  31.21481  33.3113  32.3566  34.34812\n       max neval cld\n 240.20944   100   b\n  60.82679   100  a \n\n\nMy hardware For many problems your time is the expensive part. If having a faster computer makes you more productive, it can be cost effective to buy one. However, before you splash out on new toys for yourself, your boss/partner may want to see some numbers to justify the expense. Measuring the performance of your computer is called benchmarking, and you can do that with the benchmarkme package.\n\n# Load the benchmarkme package\nlibrary(benchmarkme)\n\n# Assign the variable ram to the amount of RAM on this machine\nram <- get_ram()\nram\n\n16.5 GB\n\n# Assign the variable cpu to the cpu specs\ncpu <- get_cpu()\ncpu\n\n$vendor_id\n[1] \"GenuineIntel\"\n\n$model_name\n[1] \"11th Gen Intel(R) Core(TM) i7-11370H @ 3.30GHz\"\n\n$no_of_cores\n[1] 8"
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#benchmark-datacamps-machine",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#benchmark-datacamps-machine",
    "title": "Writing Efficient R Code",
    "section": "Benchmark DataCamp’s machine",
    "text": "Benchmark DataCamp’s machine\nThe benchmarkme package allows you to run a set of standardized benchmarks and compare your results to other users. One set of benchmarks tests is reading and writing speeds.\nThe function call\nres = benchmark_io(runs = 1, size = 5) records the length of time it takes to read and write a 5MB file.\n\n# Run the io benchmark\nres <- benchmark_io(runs = 1, size = 50)\n\nPreparing read/write io\n\n\n# IO benchmarks (2 tests) for size 50 MB:\n\n\n     Writing a csv with 6250000 values: 3.97 (sec).\n\n\n     Reading a csv with 6250000 values: 1.43 (sec).\n\n# Plot the results\nplot(res)\n\nYou are ranked 1 out of 119 machines.\n\n\nPress return to get next plot \n\n\nYou are ranked 2 out of 119 machines."
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#benchmark-r-operations",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#benchmark-r-operations",
    "title": "Writing Efficient R Code",
    "section": "Benchmark r operations",
    "text": "Benchmark r operations\n\n# Run each benchmark 3 times\nres <- benchmark_std(runs = 10)\n\n# Programming benchmarks (5 tests):\n\n\n    3,500,000 Fibonacci numbers calculation (vector calc): 0.114 (sec).\n\n\n    Grand common divisors of 1,000,000 pairs (recursion): 0.358 (sec).\n\n\n    Creation of a 3,500 x 3,500 Hilbert matrix (matrix calc): 0.169 (sec).\n\n\n    Creation of a 3,000 x 3,000 Toeplitz matrix (loops): 0.782 (sec).\n\n\n    Escoufier's method on a 60 x 60 matrix (mixed): 0.554 (sec).\n\n\n# Matrix calculation benchmarks (5 tests):\n\n\n    Creation, transp., deformation of a 5,000 x 5,000 matrix: 0.278 (sec).\n\n\n    2,500 x 2,500 normal distributed random matrix^1,000: 0.146 (sec).\n\n\n    Sorting of 7,000,000 random values: 0.524 (sec).\n\n\n    2,500 x 2,500 cross-product matrix (b = a' * a): 0.967 (sec).\n\n\n    Linear regr. over a 5,000 x 500 matrix (c = a \\ b'): 0.0893 (sec).\n\n\n# Matrix function benchmarks (5 tests):\n\n\n    Cholesky decomposition of a 3,000 x 3,000 matrix: 0.633 (sec).\n\n\n    Determinant of a 2,500 x 2,500 random matrix: 0.693 (sec).\n\n\n    Eigenvalues of a 640 x 640 random matrix: 0.287 (sec).\n\n\n    FFT over 2,500,000 random values: 0.171 (sec).\n\n\n    Inverse of a 1,600 x 1,600 random matrix: 0.573 (sec).\n\nplot(res)\n\nYou are ranked 1 out of 749 machines.\n\n\nPress return to get next plot \n\n\nYou are ranked 2 out of 747 machines.\n\n\n\n\n\nPress return to get next plot \n\n\nYou are ranked 47 out of 747 machines."
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#timings---growing-a-vector",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#timings---growing-a-vector",
    "title": "Writing Efficient R Code",
    "section": "Timings - growing a vector",
    "text": "Timings - growing a vector\nGrowing a vector is one of the deadly sins in R; you should always avoid it.\nThe growing() function defined below generates n random standard normal numbers, but grows the size of the vector each time an element is added!\nNote: Standard normal numbers are numbers drawn from a normal distribution with mean 0 and standard deviation 1.\nn <- 30000 # Slow code growing <- function(n) { x <- NULL for(i in 1:n) x <- c(x, rnorm(1)) x }\n\ngrowing <- function(n) {\n    x = NULL\n    for(i in 1:n) \n        x = c(x, rnorm(1))\n    x\n}\n\n# Use <- with system.time() to store the result as res_grow\nsystem.time(res_grow <- growing(30000))\n\n   user  system elapsed \n  0.646   0.000   0.647 \n\n\nTimings - pre-allocation In the previous exercise, growing the vector took around 2 seconds. How long does it take when we pre-allocate the vector? The pre_allocate() function is defined below.\n\nn <- 30000\n# Fast code\npre_allocate <- function(n) {\n    x <- numeric(n) # Pre-allocate\n    for(i in 1:n) \n        x[i] <- rnorm(1)\n    x\n}\n\n\n# Use <- with system.time() to store the result as res_allocate\nn <- 30000\nsystem.time(res_allocate <- pre_allocate(n))\n\n   user  system elapsed \n  0.036   0.000   0.035"
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#vectorized-code-multiplication",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#vectorized-code-multiplication",
    "title": "Writing Efficient R Code",
    "section": "Vectorized code: multiplication",
    "text": "Vectorized code: multiplication\nThe following piece of code is written like traditional C or Fortran code. Instead of using the vectorized version of multiplication, it uses a for loop.\nYour job is to make this code more “R-like” by vectorizing it.\n\nx <- rnorm(10)\nx2 <- numeric(length(x))\nfor(i in 1:10)\n    x2[i] <- x[i] * x[i]\n# Store your answer as x2_imp\nx2_imp <- x*x"
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#vectorized-code-calculating-a-log-sum",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#vectorized-code-calculating-a-log-sum",
    "title": "Writing Efficient R Code",
    "section": "Vectorized code: calculating a log-sum",
    "text": "Vectorized code: calculating a log-sum\nA common operation in statistics is to calculate the sum of log probabilities. The following code calculates the log-sum (the sum of the logs)."
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#data-frames-and-matrices---column-selection",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#data-frames-and-matrices---column-selection",
    "title": "Writing Efficient R Code",
    "section": "Data frames and matrices - column selection",
    "text": "Data frames and matrices - column selection\nAll values in a matrix must have the same data type, which has efficiency implications when selecting rows and columns.\nSuppose we have two objects, mat (a matrix) and df (a data frame).\n\n# Which is faster, mat[, 1] or df[, 1]? \nmicrobenchmark(mat[, 1], df[, 1])"
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#row-timings",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#row-timings",
    "title": "Writing Efficient R Code",
    "section": "Row timings",
    "text": "Row timings\nSimilar to the previous example, the objects mat and df are a matrix and a data frame, respectively. Using microbenchmark(), how long does it take to select the first row from each of these objects?\nTo make the comparison fair, just use mat[1, ] and df[1, ].\n\nInteresting! Accessing a row of a data frame is much slower than accessing that of a matrix, more so than when accessing a column from each data type. This is because the values of a column of a data frame must be the same data type, whereas that of a row doesn’t have to be. Do you see the pattern here?\n\n\n# Which is faster, mat[, 1] or df[, 1]? \nmicrobenchmark(mat[1, ], df[1, ])"
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#profvis-in-action",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#profvis-in-action",
    "title": "Writing Efficient R Code",
    "section": "Profvis in action",
    "text": "Profvis in action\nExamine the code on the right that performs a standard data analysis. It loads and selects data, plots the data of interest, and adds in a regression line.\n\n# Load the data set\ndata(movies, package = \"ggplot2movies\") \n\n# Load the profvis package\nlibrary(profvis)\n\n# Profile the following code with the profvis function\nprofvis({\n  # Load and select data\n  comedies <- movies[movies$Comedy == 1, ]\n\n  # Plot data of interest\n  plot(comedies$year, comedies$rating)\n\n  # Loess regression line\n  model <- loess(rating ~ year, data = comedies)\n  j <- order(comedies$year)\n  \n  # Add fitted line to the plot\n  lines(comedies$year[j], model$fitted[j], col = \"red\")\n})    ## Remember the closing brackets!"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Mburu | ML/Viz/Spatial/Philosophy/Stats",
    "section": "",
    "text": "Description\nNotebook\nSource Code\n\n\n\n\nBreast cancer prediction\nlink\nR Markdown (Rmd)\n\n\nBusara task\nlink\nR Markdown (Rmd)\n\n\nZindi loan prediction\nlink\nR Markdown (Rmd)\n\n\nTZ Water Pumps\nlink\nR Markdown (Rmd)\n\n\nIntro to dimension reduction\nlink\nR Markdown (Rmd)\n\n\nClustering\nlink\nR Markdown (Rmd)\n\n\nKenya maps\nlink\nR Markdown (Rmd)\n\n\nKenya 2019 household assets\nlink\nR Markdown (Rmd)\n\n\nIntroduction to association analysis\nlink\nR Markdown (Rmd)\n\n\nDigit recognition MNIST\nlink\nR Markdown (Rmd)\n\n\nML with tree models\nlink\nR Markdown (Rmd))\n\n\nNetwork Analysis\nlink\nR Markdown (Rmd))\n\n\nAirbnb NYC\nlink\nR Markdown (Rmd)\n\n\nExplainable ML\nlink\nR Markdown (Rmd)\n\n\nIntroduction to deep learning\nlink\nPython notebook (ipnb)\n\n\nMburu CV\nlink\nRmarkdown (Rmd)\n\n\nMalnutrition Prevalence Africa\nShiny App\ngithub repo\n\n\nEast Africa poverty data\nlink\nRmarkdown (Rmd)\n\n\nKenya debt\nlink\nRmarkdown (Rmd)\n\n\nData camp imputation in R\nlink\nRmarkdown (Rmd)\n\n\nDatacamp Anomaly detection in R\nlink\nRmarkdown (Rmd)"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html",
    "href": "myblog/breast_cancer_prediction/cancer_data.html",
    "title": "Cancer Data",
    "section": "",
    "text": "In this tutorial I’m going to predict whether a breast cancer tumor is benign or malignant. Using Wiscosin breast cancer data set available on Kaggle. The 30 predictors are divided into three parts first is Mean ( variables 3-13), Standard Error(13-23) and Worst(23-32) and each contain 10 parameter (radius, texture,area, perimeter, smoothness,compactness,concavity,concave points,symmetry and fractal dimension of the tumor). When predicting cancer breast tumor types there two types of cost;\n\nThe cost of telling someone who has malignant tumor that they have benign these are the false negatives in this case someone might not seek medical help which is can cause death.\nTelling someone that they have malignant type of tumor but they don’t which is usually false positives. In this case you subject someone to unnecessary stress\n\nSo it’s highly desirable that our model has good accuracy $ f_1 score$ and high recall.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(xgboost)\nlibrary(knitr)\nlibrary(broom)\nlibrary(caret)\nlibrary(e1071)\nlibrary(kableExtra)\nlibrary(ggthemes)\nlibrary(DT)\nlibrary(glmnet)\n\noptions(scipen = 1, digits = 4)\ncancer <- fread(\"data.csv\")\n\ncancer[, V33 := NULL]\n\n\nhead(cancer)  %>%\n  datatable(options = list(scrollX = TRUE))"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#percentage-of-women-with-malignant-tumor",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#percentage-of-women-with-malignant-tumor",
    "title": "Cancer Data",
    "section": "Percentage of women with malignant tumor",
    "text": "Percentage of women with malignant tumor\nThe percentage of women with malignant tumor is 37.26%(212 out 569) while the rest 62.74%(357) had benign tumors.\n\ncancer[, .(freq = .N),\n       by = diagnosis] %>% \n    .[, perc := round(100 * freq/sum(freq), 2)] %>%\n  \nggplot(aes(x=diagnosis, y=perc, fill = diagnosis)) + \n    geom_bar(stat = \"identity\", width  = 0.5)+ theme_hc() +\n    geom_text(aes(x=diagnosis, y=perc, label = paste(perc, \"%\")),\n              position =  position_dodge(width = 0.5),\n              vjust = 0.05, hjust = 0.5, size = 5)+\n    scale_fill_hc(name = \"\")+\n    labs(x = \"Cancer Type\",\n         y = \"Percentage\",\n         title = \"Percentage of women with benign or malignant breast bancer\")+\n    theme(legend.position = \"none\",\n          axis.title = element_text(size =12))"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#boxplots",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#boxplots",
    "title": "Cancer Data",
    "section": "Boxplots",
    "text": "Boxplots\nFrom the boxplots we can identify variables where we expect there is a significance difference between the two groups of cancer tumors. When using a boxplot if two distributions do not averlap or more than 75% of two boxplot do not overlap then we expect that there is a significance difference in the mean/median between the two groups. Some of the variables where the distribution of two cancer tumors are significantly different are radius_mean, texture_mean etc. The visible differences between malignant tumors and benign tumors can be seen in means of all cells and worst means where worst means is the average of all the worst cells. The distribution of malignant tumors have higher scores than the benign tumors in this cases.\n\ncancerm <- melt(cancer[, -1, with = F], id.vars = \"diagnosis\")\n\nggplot(cancerm, aes(x = diagnosis, y = value))+\n    geom_boxplot() + facet_wrap(~variable, scales = \"free_y\")"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#features-scaling",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#features-scaling",
    "title": "Cancer Data",
    "section": "Features Scaling",
    "text": "Features Scaling\nWe find that some variables are highly correlated. We can use principle component analysis for dimension reduction. Since variables are correlated it’s evident that we can use a smaller set of features to build our models.\n\ncancer[, id := NULL]\npredictors <- names(cancer)[3:31]\ncancer[, (predictors) := lapply(.SD, function(x) scale(x)), .SDcols = predictors ]\ncancer[, diagnosis := as.factor(diagnosis)]"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#correlation-matrix",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#correlation-matrix",
    "title": "Cancer Data",
    "section": "Correlation matrix",
    "text": "Correlation matrix\n\ncor(cancer[, -(1:2), with = F]) %>%\n  datatable(options = list(scrollX = TRUE), style = \"bootstrap4\")"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#principle-component-analysis",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#principle-component-analysis",
    "title": "Cancer Data",
    "section": "Principle Component Analysis",
    "text": "Principle Component Analysis\nUsing the elbow rule we can use the first 5 principle components. Using 15 principle components we will have achieved al most 100% of the variance from the original data set.\n\npca <- prcomp(cancer[, predictors, with = F], scale. = F)"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#variance-explained",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#variance-explained",
    "title": "Cancer Data",
    "section": "Variance Explained",
    "text": "Variance Explained\nSince PCA forms new characteristics the variance explained plot shows the amount of variation of the original features captured by each principle component. The new features are simply linear combinations of the old features.\n\nstdpca <- pca$sdev\n\nvarpca <- stdpca^2\n\nprop_var <- varpca/sum(varpca)\nprop_var * 100\n\n [1] 43.706363 18.472237  9.716239  6.816736  5.676223  4.161723  2.292352\n [8]  1.643434  1.363238  1.191515  1.011032  0.897368  0.832105  0.539193\n[15]  0.323823  0.269517  0.198317  0.178851  0.153573  0.107095  0.102579\n[22]  0.093821  0.082603  0.058725  0.053331  0.027514  0.022985  0.005110\n[29]  0.002394\n\nsum(prop_var[1:15])\n\n[1] 0.9864"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#scree-plot",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#scree-plot",
    "title": "Cancer Data",
    "section": "Scree plot",
    "text": "Scree plot\nScree plot shows the variance explained by each principle component which reduces as the number of principle components increase.\n\nplot(prop_var, xlab = \"Principal Component\",\n     ylab = \"Proportion of Variance Explained\",\n     type = \"b\", xlim = c(0, 30))"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#cumulative-variance-explained",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#cumulative-variance-explained",
    "title": "Cancer Data",
    "section": "Cumulative Variance Explained",
    "text": "Cumulative Variance Explained\nThe cumulative of variance plot helps to choose the number of features based on the amount of variation from original data set you want captured. In this case, I wanted to use number of principle components that capture almost 100% of the variation. After trying with different number of principle components I found out that the accuracy of the models did not increase after the 15th principle components.\n\ncum_var <- cumsum(prop_var)\nplot(cum_var, xlab = \"Principal Component\",\n     ylab = \"Cumulative Proportion of Variance Explained\",\n     type = \"b\", xlim = c(0, 30))"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#construct-new-data-set",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#construct-new-data-set",
    "title": "Cancer Data",
    "section": "Construct new data set",
    "text": "Construct new data set\nWe use the first 15 principle components as our new predictors, then we randomly split data into training and test set in 7:3 ratio.\n\nset.seed(100)\ntrain_sample <- sample(1:nrow(cancer), round(0.7*nrow(cancer)))\n\npcadat <- data.table( label = cancer$diagnosis, pca$x[,1:15]) \npcadat[, label := factor(label, levels = c(\"M\", \"B\"))]\ntrain <- pcadat[train_sample,]\ntest <- pcadat[-train_sample,]"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#logistic-regression",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#logistic-regression",
    "title": "Cancer Data",
    "section": "Logistic regression",
    "text": "Logistic regression\nThis is one of generalized linear models which deals with binary data. There is a generalization of this model which is called multinomial regression where you can fit multi class data. The equation for logistic regression model is:\n\\[log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1*X_1 + ... \\beta_n * X_n\\] and using mle the cost function can be derived as: \\[J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m} y^i log(h_\\theta(x^i)) + (1-y^i) log(1 - h_\\theta(x^i))\\] Given that \\[y = 0\\] \\[y = 1\\] . Finding \\[\\beta\\] s we minimizing the cost function.\n\nfit_glm <- glm(label ~., data = train, family = binomial)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#regularization-in-logistic-regression",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#regularization-in-logistic-regression",
    "title": "Cancer Data",
    "section": "Regularization in logistic regression",
    "text": "Regularization in logistic regression\nThe warning “glm.fit: fitted probabilities numerically 0 or 1 occurred” shows that there is a perfect separation/over fitting. In this case you can load glmnet library and fit a regularized logistic regression. These can be achieved by adding a regularization term to the cost function.The L1 regularization(Lasso) adds a penalty equal to the sum of the absolute values of the coefficients.\n\\[J(\\theta) = -\\frac{1}{m}\\sum_{i=0}^{m} y^i log(h_\\theta(x^i)) + (1-y^i) log(1 - h_\\theta(x^i)) + \\frac {\\lambda}{2m}\\sum_{j=1}^{n} |\\theta^i|\\]\n\ntrainx <- train[,-1]\n\ny_train <- factor(train$label, levels = c(\"B\", \"M\"), labels = 0:1)\n#y <- as.numeric(as.character(y))\n\ny_test <- factor(test$label, levels = c(\"B\", \"M\"), labels = 0:1) %>% as.character() %>% as.numeric()\n#ytest <- as.numeric(as.character(ytest))\n\ntestx <- data.matrix(test[, -1]) \n\nTo find the optimal values \\(\\lambda\\) we use cross validation. We choose \\(\\lambda\\) which gives the highest cross validation accuracy.\n\ncv_fold <- createFolds(train$label, k = 10)\n\nmyControl <- trainControl(\n  method = \"cv\", \n  number = 10,\n  summaryFunction = twoClassSummary,\n  savePredictions = \"all\",\n  classProbs = TRUE,\n  verboseIter = FALSE,\n  index = cv_fold,\n  allowParallel = TRUE\n  \n)\n\ntuneGrid <-  expand.grid(\n    alpha = 0:1,\n    lambda = seq(0.001, 1, length.out = 10))\n    \nglmnet_model <- train(\n  label ~.,\n  data = train,\n  method = \"glmnet\",\n  metric = \"ROC\",\n  trControl = myControl,\n  tuneGrid = tuneGrid\n)\n\ns\n\nplot(glmnet_model) \n\n\n\n#lamda_min <- cv_glm$lambda.min\n\n\nresample_glmnet <- thresholder(glmnet_model, \n                              threshold = seq(.2, 1, by = 0.05), \n                              final = TRUE, \n                              statistics = \"all\")\n\nggplot(resample_glmnet , aes(x = prob_threshold, y = F1)) + \n  geom_point() + \n  geom_point(aes(y = Sensitivity), col = \"blue\")\n\n\n\n\n\nlibrary(caTools)\n\npred_glm <- predict(glmnet_model, test, type = \"prob\")\n\ncolAUC(pred_glm , test$label, plotROC = TRUE)\n\n\n\n\n             M      B\nM vs. B 0.9683 0.9683\n\npred_glm1 <- ifelse(pred_glm[, \"M\"] > 0.4, \"M\", \"B\")\n#pred_glm1 <- predict(glmnet_model, test, type = \"raw\")\n\n\npred_glm1 <- factor(pred_glm1, levels = levels(test$label))\n\n\nconfusionMatrix(pred_glm1, test$label,positive = \"M\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   M   B\n         M  53   7\n         B   7 104\n                                        \n               Accuracy : 0.918         \n                 95% CI : (0.866, 0.955)\n    No Information Rate : 0.649         \n    P-Value [Acc > NIR] : <2e-16        \n                                        \n                  Kappa : 0.82          \n                                        \n Mcnemar's Test P-Value : 1             \n                                        \n            Sensitivity : 0.883         \n            Specificity : 0.937         \n         Pos Pred Value : 0.883         \n         Neg Pred Value : 0.937         \n             Prevalence : 0.351         \n         Detection Rate : 0.310         \n   Detection Prevalence : 0.351         \n      Balanced Accuracy : 0.910         \n                                        \n       'Positive' Class : M"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#svm",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#svm",
    "title": "Cancer Data",
    "section": "SVM",
    "text": "SVM\nSupport Vector Machines is a type of supervised learning algorithm that is used for classification and regression. Most of the times however, it’s used for classification.\nTo understand how SVM works consider the following example of linearly separable data. It’s clear that we can separate the two classes using a straight line(decision boundary). Which is normally referred to a separating hyperplane.\n\n\n\n\n\nThe question is, since there exists many lines that can separate the red and the black classes which is the best one. This introduces us to the maximal margin classification, In short SVM finds the hyperplane/line that gives the biggest margin/gap between the two classes. In this case SVM will choose the solid line as the hyperplane while the margins are the dotted lines. The circled points that lie directly on the margin, or on the wrong side of the margin for their class, are known as support vectors. This shows that SVM uses this points to come up with a the decision boundary, the other points are not used. In this case since it’s a two dimensional space the equation of the separating line will be \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2\\]. Then when equations evaluates to more than 0 then 1 is predicted \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2 > 0, y = 1\\] and when it evaluates to less than zero then predicted class is -1 \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2 < 0, \\; y = -1\\] This becomes maximisation problem \\[width \\; of \\;the \\; margin = M \\] \\[\\sum_{j=1}^{n}\\beta_j = 1\\]\n\\[y_i(\\beta_0 + \\beta_1X_1 + \\beta_2X_2) >= M\\]\n\n\n\n\n\nThis is a best case scenario but in most cases the classes are noisy. Consider the plot below no matter which line you choose some points are bound to be on the wrong side of the desicion boundary. Thus maximal margin classification would not work.\n\n\n\n\n\nSVM then introduces what is called a soft margin. In naive explanation you can think of this as a margin that allows some points to be on the wrong side. By introducing an error term we allow for some slack. Thus in a two case the maximisation becomes \\[y_i(\\beta_0 + \\beta_1X_1 + \\beta_2X_2) >= M(1- \\epsilon)\\]\n\\[\\sum_{i=0}^{n} \\epsilon_i <= C\\] C is a tuning parameter which determines the width of the margin while \\[\\epsilon_i  \\;'s\\] are slack variables. that allow individual observations to fall on the wrong side of the margin. In some cases the decision boundary maybe non linear. In case your are dealing with logistic regression you will be forced to introduce polynomial terms which might result in a very large feature space. SVM then introduces what are called kernels"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#tuning-svm",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#tuning-svm",
    "title": "Cancer Data",
    "section": "Tuning SVM",
    "text": "Tuning SVM\n\nsvm_tune <-  expand.grid(\n    C =c(1 ,5 ,  10, 100, 150),\n    sigma = seq(0, .01, length.out = 5))\n    \nsvm_model <- train(\n  label ~.,\n  data = train,\n   metric=\"ROC\",\n  method = \"svmRadial\",\n  trControl = myControl,\n  tuneGrid = svm_tune,\n  verbose = FALSE\n)\n\n\nresample_svm <- thresholder(svm_model, \n                              threshold = seq(.0, 1, by = 0.05), \n                              final = TRUE, \n                              statistics = \"all\")\n\nggplot(resample_svm , aes(x = prob_threshold, y = F1, col = \"F1\")) + \n  geom_point() + \n  geom_point(aes(y = Sensitivity,  col = \"Sensitivity\"))+\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1))\n\n\n\n#mean(pred_svm == ytest)\n\n\npred_svm <-predict(svm_model, newdata = test, type = \"prob\")\n\npred_svm <- ifelse(pred_svm[, \"M\"] > 0.40, \"M\", \"B\")\n\npred_svm <- factor(pred_svm, levels = levels(test$label))\n\nconfusionMatrix(test$label, pred_svm, positive = \"M\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   M   B\n         M  58   2\n         B   2 109\n                                        \n               Accuracy : 0.977         \n                 95% CI : (0.941, 0.994)\n    No Information Rate : 0.649         \n    P-Value [Acc > NIR] : <2e-16        \n                                        \n                  Kappa : 0.949         \n                                        \n Mcnemar's Test P-Value : 1             \n                                        \n            Sensitivity : 0.967         \n            Specificity : 0.982         \n         Pos Pred Value : 0.967         \n         Neg Pred Value : 0.982         \n             Prevalence : 0.351         \n         Detection Rate : 0.339         \n   Detection Prevalence : 0.351         \n      Balanced Accuracy : 0.974         \n                                        \n       'Positive' Class : M"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#xgboost",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#xgboost",
    "title": "Cancer Data",
    "section": "Xgboost",
    "text": "Xgboost\nXGBoost is a type of an ensemble learner. Ensemble learning is where multiple machine learning algorithms are used at the same time for prediction. A good example will be Random Forests. In random Forest multiple decision trees are used together for prediction. There are two main types of ensemble learners, bagging and boosting. Random forest use the bagging approach. Trees are built from random subsets(rows and columns) of training set and then the final prediction is the weighted sum of all decision trees functions. Boosting methods are similar but in boosting samples are selected sequentially. For instance the first sample is selected and a decision tree is fitted, The model then picks the examples that were hard to learn and using this examples and a few others selected randomly from the training set the second model is fitted, Using the first model and the second model prediction is made, the model is evaluated and hard examples are picked and together with another randomly selected new examples from training set another model is trained. This is the process for boosting algorithms which continues for a specified number of n.\nIn gradient boosting the first model is fitted to the original training set. Let say your fitting a simple regression model for ease of explanation. Then your first model will be $ y = f(x) + $. When you find that the error is too large one of the things you might try to do is add more features, use another algorithm, tune your algorithm, look for more training data etc. But what if the error is not white noise and it has some relationship with output \\(y\\) . Then we can fit a second model. $ = f_1(x) + _1$. then this process can continue lets say until n times. Then the final model will be\n$ n = f*{n}(x) + _{n-1}$.\nThen the final step is to add this models together with some weighting criteria $ weights = ’s$ which gives us the final function used for prediction.\n\\(y = \\alpha * f(x) + \\alpha_1 * f_1(x) + \\alpha_2 * f_2(x)...+ \\alpha_n * f_n + \\epsilon\\)\n\n# \"subsample\" is the fraction of the training samples (randomly selected) that will be used to train each tree.\n# \"colsample_by_tree\" is the fraction of features (randomly selected) that will be used to train each tree.\n# \"colsample_bylevel\" is the fraction of features (randomly selected) that will be used in each node to train each tree.\n#eta learning rate\n\n\n\nxgb_ctrl <- trainControl(method = \"cv\",\n                        number = 5,\n                        summaryFunction = twoClassSummary,\n                        classProbs = TRUE,\n                        allowParallel=T,\n                        index = cv_fold,\n                        verboseIter = FALSE,\n                        savePredictions = TRUE,\n                        search = \"grid\")\n\nxgb_grid <- expand.grid(nrounds = c(10, 50, 100),\n                        eta = seq(0.06, .2, length.out = 3),\n                        max_depth = c(50, 80),\n                        gamma = c(0,.01, 0.1),\n                        colsample_bytree = c(0.6, 0.7,0.8),\n                        min_child_weight = 1,\n                        subsample =  .7\n                        \n    )\n\nxgb_model <-train(label~.,\n                 data=train,\n                 method=\"xgbTree\",\n                 trControl= xgb_ctrl,\n                 tuneGrid=xgb_grid,\n                 verbosity=0,\n                 metric=\"ROC\",\n                 nthread =4\n                     \n    )\n\nIncreasing cut of increases the precision. A greater fraction of those who will be predicted that they have cancer will turn out that they have, but the algorithm is likely to have lower recall. If we want to avoid too many cases of people cancer being predicted that they do not have cancer. It will be very bad to tell someone that they do not have cancer but they have. If we lower the probability let say to 0.3 then we want to make sure that even if there is a 30% chance you have cancer then you should be flagged.\n\nresample_xgb <- thresholder(xgb_model, \n                              threshold = seq(.0, 1, by = 0.01), \n                              final = TRUE, \n                              statistics = \"all\")\n\nggplot(resample_xgb , aes(x = prob_threshold, y = F1, col = \"F1\")) + \n  geom_point() + \n  geom_point(aes(y = Sensitivity, col = \"Sensitivity\"))+\n  scale_x_continuous(breaks = seq(0, 1, by =.1))\n\n\n\n\n\npred_xgb <-predict(xgb_model, newdata = test, type = \"prob\")\npred_xgb1 <- ifelse(pred_xgb[, \"M\"] > 0.4, \"M\", \"B\")\npred_xgb1 <- factor(pred_xgb1, levels = levels(test$label))\n\nconfusionMatrix(pred_xgb1,test$label,  positive = \"M\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   M   B\n         M  59   2\n         B   1 109\n                                       \n               Accuracy : 0.982        \n                 95% CI : (0.95, 0.996)\n    No Information Rate : 0.649        \n    P-Value [Acc > NIR] : <2e-16       \n                                       \n                  Kappa : 0.962        \n                                       \n Mcnemar's Test P-Value : 1            \n                                       \n            Sensitivity : 0.983        \n            Specificity : 0.982        \n         Pos Pred Value : 0.967        \n         Neg Pred Value : 0.991        \n             Prevalence : 0.351        \n         Detection Rate : 0.345        \n   Detection Prevalence : 0.357        \n      Balanced Accuracy : 0.983        \n                                       \n       'Positive' Class : M"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#learning-curves",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#learning-curves",
    "title": "Cancer Data",
    "section": "Learning Curves",
    "text": "Learning Curves\n\nsets <- seq(from = 50, to = nrow(train), by = 50)\nsets[length(sets)] <-nrow(train) \ntrain.err <- c()\ntest.err<- c()\ntune_grid <- expand.grid( nrounds = 50, max_depth = 50, eta = 0.06, gamma = 0.01, \n                         colsample_bytree = 0.6, min_child_weight = 1, subsample = 0.7)\nfor (i in 1:length(sets)) {\n    \n    traini = train[1:sets[i],]\n    fit_svm <- train(label ~., data = traini, metric=\"Accuracy\", method = \"svmRadial\",\n                 trControl = trainControl(method = \"none\", summaryFunction = twoClassSummary,\n                                          classProbs = TRUE),\n                 tuneGrid = expand_grid( sigma = 0.0075, C = 5),\n                 )\n    \n    # fit_svm <-train(label~.,\n    #              data=traini,\n    #              method=\"xgbTree\",\n    #              trControl= xgb_ctrl,\n    #              tuneGrid= tune_grid ,\n    #              verbose=T,\n    #              metric=\"ROC\",\n    #              nthread =3\n    #                  \n    # )\n    pred_train = predict(fit_svm, newdata = traini, type = \"prob\")\n    pred_train = ifelse(pred_train[[\"M\"]] > 0.4, \"M\", \"B\")\n    train.err[i] =1 -  mean(pred_train == traini$label)\n    pred_test = predict(fit_svm, newdata = test, type = 'prob')\n    pred_test = ifelse(pred_test[, \"M\"] > 0.4, \"M\", \"B\")\n    test.err[i] = 1 - mean(test$label == pred_test)\n    \n    cat(i,\" \")\n    \n}\n\n1  2  3  4  5  6  7  \n\ntrain.err\n\n[1] 0.00000 0.03000 0.03333 0.01500 0.02000 0.02000 0.02261\n\nmatplot(sets, cbind(test.err, train.err), pch = 19, col = c(\"red\", \"blue\"),\n        type = \"b\", ylab = \"Error\", xlab = \"Train sample size\", main = \"SVM Learning Curves\")\nlegend(\"topright\", legend = c(\"Test\", \"Train\"), pch = 19, col = c(\"red\", \"blue\"))"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#error-analysis",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#error-analysis",
    "title": "Cancer Data",
    "section": "Error Analysis",
    "text": "Error Analysis\nLook at the examples that the algorithm misclassified to see if there is a trend. Generally you are trying to find out the weak points of your algorithm. Checking why your algorithm is making those errors. For instance, from the boxplots below the malignant tumors that were misclassified had lower radius mean compared to mislassified benign tumors. This contrary to what we saw in the first boxplots graph.\n\ndf <- data.frame(cancer[-train_sample,], pred_svm) %>%\n    setDT()\n\n\ntest_mis_svm <- df[(diagnosis == \"M\" & pred_svm == 0) |( diagnosis == \"B\" & pred_svm == \"M\")]\n\n\n# test_mis_svm_m <- melt(test_mis_svm, \n#                 id.vars = c(\"diagnosis\", \"pred_svm\"))\n# \n# ggplot(test_mis_svm_m , aes(x = pred_svm, y = value))+\n#     geom_boxplot() + facet_wrap(~variable, scales = \"free_y\")"
  },
  {
    "objectID": "myblog/kenya_population/household_assets_2019census.html",
    "href": "myblog/kenya_population/household_assets_2019census.html",
    "title": "Kenya Household Assets",
    "section": "",
    "text": "Kenya selected households assets 2019 census\nI figured that it will be important for me to do this to show why it is impossible for online learning to be adopted in Kenya.\nI used 2019 census data set which can be found here and the counties shape file can be found here\n\n#Packages used\n\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sf)\nlibrary(DT)\nlibrary(tmap)\nlibrary(ggthemes)\nlibrary(ggiraph)\n\n\n\nRead data set\n\nhousehold_assets <- fread(\"percentage-distribution-of-conventional-households-by-ownership-of-selected-household-assets-201.csv\")\n\n#\nkenya_shapefile <- st_read(\"County\") %>% setDT()\n\nReading layer `County' from data source \n  `/home/mburu/personal_projects/github_blog/myblog/kenya_population/County' \n  using driver `ESRI Shapefile'\nSimple feature collection with 47 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 33.91182 ymin: -4.702271 xmax: 41.90626 ymax: 5.430648\nGeodetic CRS:  WGS 84\n\nkenya_shapefile[, county := tolower(COUNTY)]\n\n# rename column\nsetnames(household_assets, \n         c(\"County / Sub-County\", \"Conventional Households\"  ), \n         c(\"county\", \"households\"))\n\n\n\nSome minor cleaning\n\n# remove commas \nhousehold_assets[, households := as.numeric(gsub(\",|AR K    \", \"\", households))]\n\nhousehold_assets[,  county := tolower(county)]\n\nsub_county <- household_assets %>% \n    group_by(county) %>%\n    filter(households == max(households)) %>% setDT()\n\n\nsub_county_melt <- melt(sub_county, \n                        id.vars = c(\"county\", \"households\"))\n\n\n\n% of households with various assets 2019 census\n\nThis is overall data set for the whole country computer devices is ownership about 8.8% this just means that about 91% of the students can’t access online learning. This is is just a naive estimation the number could be higher.\n\n\nkenya_dat <- sub_county_melt[county == \"kenya\"]\n\np <- ggplot(kenya_dat, aes(variable, value, tooltip = paste(variable, \" : \", value))) +\n    geom_bar_interactive(stat = \"identity\", width = 0.5, fill =\"slategray2\"  ) +\n    geom_text_interactive(aes(variable, value, label = paste0(value, \"%\")),\n              position = position_dodge(width = 0.5),\n              vjust = 0.001, size = 3)+\n    labs(x = \"Household assets\", y = \"%\",\n         title = \"% of households with various assets 2019 census\",\n         caption = \"twitter\\n@mmburu_w\")+\n    theme_fivethirtyeight()+\n  theme(\n    axis.text = element_text(size = 7, angle = 45, vjust = 1, hjust =1),\n    plot.title = element_text_interactive(size =11)\n  )\np1 <- girafe(ggobj = p, width_svg = 7, height_svg = 4.5, \n  options = list(\n    opts_sizing(rescale = T) )\n  )\n\np1\n\n\n\n\n\np\n\n\n\n\n\n\nMerge shapefile with asset data sets\n\nsub_county_melt[county == \"elgeyo/marakwet\", county := \"keiyo-marakwet\"]\nsub_county_melt[county == \"tharaka-nithi\", county := \"tharaka\"]\nsub_county_melt[county ==  \"taita/taveta\", county := \"taita taveta\"]\nsub_county_melt[county ==  \"nairobi city\", county := \"nairobi\"]\n\ncounty_shapes <- merge(kenya_shapefile, sub_county_melt, by = \"county\") \n\nsetnames(county_shapes, \"value\", \"Percentage\")\n\n\n\nComputer devices data\n\ncomputer <- county_shapes[variable  %in% c(\"Desk Top\\nComputer/\\nLaptop/ Tablet\")]\n\n# this converts to sf object\ncomputer <- st_set_geometry(computer, \"geometry\")\n\n\n\nPercentage of households with computer devices per county\n\nThat is if a household owns a tablet, laptop or a desktop\n\n\n#ttm()\ntm_shape(computer)+\n    tm_borders(col = \"grey\")+\n    tm_fill(col = \"Percentage\")+\n    tm_layout(title = \"% of households with laptop/tablet/desk top \\n 2019 census\",\n              title.size = 1, title.position = c(0.3, 0.95))\n\n\n\n#ttm()\n\n\n\n% of households that can access internet\n\nThis looks like is internet access through mobile phones\n\n\ninternet <- county_shapes[variable == \"Internet\"]\n\ninternet <- st_set_geometry(internet, \"geometry\")\n\n#ttm()\ntm_shape(internet)+\n    tm_borders(col = \"grey\")+\n    tm_fill(col = \"Percentage\")+\n    tm_layout(title = \"% of households with internet acess\",\n              title.size = 1, title.position = c(0.3, 0.95))"
  }
]