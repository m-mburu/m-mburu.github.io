[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Personal Blog",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShap Calculation R\n\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting loan defaults\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2020\n\n\nmburu\n\n\n\n\n\n\n  \n\n\n\n\nPredict whether the cancer is benign or malignant\n\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2019\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssociation analysis\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2018\n\n\nMburu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "kenya_population/household_assets_2019census.html",
    "href": "kenya_population/household_assets_2019census.html",
    "title": "Kenya Household Assets",
    "section": "",
    "text": "Kenya selected households assets 2019 census\nI figured that it will be important for me to do this to show why it is impossible for online learning to be adopted in Kenya.\nI used 2019 census data set which can be found here and the counties shape file can be found here\n\n#Packages used\n\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sf)\nlibrary(DT)\nlibrary(tmap)\nlibrary(ggthemes)\nlibrary(ggiraph)\n\n\n\nRead data set\n\nhousehold_assets <- fread(\"percentage-distribution-of-conventional-households-by-ownership-of-selected-household-assets-201.csv\")\n\n#\nkenya_shapefile <- st_read(\"County\") %>% setDT()\n\nReading layer `County' from data source \n  `/home/mburu/r_projects/m-mburu.github.io/kenya_population/County' \n  using driver `ESRI Shapefile'\nSimple feature collection with 47 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 33.91182 ymin: -4.702271 xmax: 41.90626 ymax: 5.430648\nGeodetic CRS:  WGS 84\n\nkenya_shapefile[, county := tolower(COUNTY)]\n\n# rename column\nsetnames(household_assets, \n         c(\"County / Sub-County\", \"Conventional Households\"  ), \n         c(\"county\", \"households\"))\n\n\n\nSome minor cleaning\n\n# remove commas \nhousehold_assets[, households := as.numeric(gsub(\",|AR K    \", \"\", households))]\n\nhousehold_assets[,  county := tolower(county)]\n\nsub_county <- household_assets %>% \n    group_by(county) %>%\n    filter(households == max(households)) %>% setDT()\n\n\nsub_county_melt <- melt(sub_county, \n                        id.vars = c(\"county\", \"households\"))\n\n\n\n% of households with various assets 2019 census\n\nThis is overall data set for the whole country computer devices is ownership about 8.8% this just means that about 91% of the students can’t access online learning. This is is just a naive estimation the number could be higher.\n\n\nkenya_dat <- sub_county_melt[county == \"kenya\"]\n\np <- ggplot(kenya_dat, aes(variable, value, tooltip = paste(variable, \" : \", value))) +\n    geom_bar_interactive(stat = \"identity\", width = 0.5, fill =\"slategray2\"  ) +\n    geom_text_interactive(aes(variable, value, label = paste0(value, \"%\")),\n              position = position_dodge(width = 0.5),\n              vjust = 0.001, size = 3)+\n    labs(x = \"Household assets\", y = \"%\",\n         title = \"% of households with various assets 2019 census\",\n         caption = \"twitter\\n@mmburu_w\")+\n    theme_fivethirtyeight()+\n  theme(\n    axis.text = element_text(size = 7, angle = 45, vjust = 1, hjust =1),\n    plot.title = element_text_interactive(size =11)\n  )\np1 <- girafe(ggobj = p, width_svg = 7, height_svg = 4.5, \n  options = list(\n    opts_sizing(rescale = T) )\n  )\n\np1\n\n\n\n\n\n\n\n\nMerge shapefile with asset data sets\n\nsub_county_melt[county == \"elgeyo/marakwet\", county := \"keiyo-marakwet\"]\nsub_county_melt[county == \"tharaka-nithi\", county := \"tharaka\"]\nsub_county_melt[county ==  \"taita/taveta\", county := \"taita taveta\"]\nsub_county_melt[county ==  \"nairobi city\", county := \"nairobi\"]\n\ncounty_shapes <- merge(kenya_shapefile, sub_county_melt, by = \"county\") \n\nsetnames(county_shapes, \"value\", \"Percentage\")\n\n\n\nComputer devices data\n\ncomputer <- county_shapes[variable  %in% c(\"Desk Top\\nComputer/\\nLaptop/ Tablet\")]\n\n# this converts to sf object\ncomputer <- st_set_geometry(computer, \"geometry\")\n\n\n\nPercentage of households with computer devices per county\n\nThat is if a household owns a tablet, laptop or a desktop\n\n\n#ttm()\ntm_shape(computer)+\n    tm_borders(col = \"grey\")+\n    tm_fill(col = \"Percentage\")+\n    tm_layout(title = \"% of households with laptop/tablet/desk top \\n 2019 census\",\n              title.size = 1, title.position = c(0.3, 0.95))\n\n\n\n#ttm()\n\n\n\n% of households that can access internet\n\nThis looks like is internet access through mobile phones\n\n\ninternet <- county_shapes[variable == \"Internet\"]\n\ninternet <- st_set_geometry(internet, \"geometry\")\n\n#ttm()\ntm_shape(internet)+\n    tm_borders(col = \"grey\")+\n    tm_fill(col = \"Percentage\")+\n    tm_layout(title = \"% of households with internet acess\",\n              title.size = 1, title.position = c(0.3, 0.95))"
  },
  {
    "objectID": "Intro_Python_Data/Intro_datascience.html",
    "href": "Intro_Python_Data/Intro_datascience.html",
    "title": "Personal Blog",
    "section": "",
    "text": "Modules (sometimes called packages or libraries) help group together related sets of tools in Python. In this exercise, we’ll examine two modules that are frequently used by Data Scientists:\nstatsmodels: used in machine learning; usually aliased as sm seaborn: a visualization library; usually aliased as sns Note that each module has a standard alias, which allows you to access the tools inside of the module without typing as many characters. For example, aliasing lets us shorten seaborn.scatterplot() to sns.scatterplot().\n\nimport statsmodels as sm\nimport seaborn as sns\nimport numpy as np\n\n\n\n\nBefore we start looking for Bayes’ kidnapper, we need to fill out a Missing Puppy Report with details of the case. Each piece of information will be stored as a variable.\nWe define a variable using an equals sign (=). For instance, we would define the variable height:\nheight = 24 In this exercise, we’ll be defining bayes_age to be 4.0 months old. The data type for this variable will be float, meaning that it is a number.\n\nbayes_age = 4.0\nbayes_age\n\n4.0\n\n\n\n\n\nLet’s continue to fill out the Missing Puppy Report for Bayes. In the previous exercise, we defined bayes_age, which was a float, which represents a number.\nIn this exercise, we’ll define favorite_toy and owner, which will both be strings. A string represents text. A string is surrounded by quotation marks (’ or “) and can contain letters, numbers, and special characters. It doesn’t matter if you use single (’) or double (”) quotes, but it’s important to be consistent throughout your code.\n\nfavorite_toy = \"Mr. Squeaky\"\nowner = \"DataCamp\"\n# Display variables\nprint(favorite_toy)\nprint(owner)\n\nMr. Squeaky\nDataCamp\n\n\n\n\n\nIt’s easy to make errors when you’re trying to type strings quickly.\nDon’t forget to use quotes! Without quotes, you’ll get a name error. owner = DataCamp Use the same type of quotation mark. If you start with a single quote, and end with a double quote, you’ll get a syntax error. fur_color = “blonde’ Someone at the police station made an error when filling out the final lines of Bayes’ Missing Puppy Report. In this exercise, you will correct the errors.\n\n# One or more of the following lines contains an error\n# Correct it so that it runs without producing syntax errors\nbirthday = '2017-07-14'\ncase_id = 'DATACAMP!123-456?'"
  },
  {
    "objectID": "datacamp.html",
    "href": "datacamp.html",
    "title": "Personal Blog",
    "section": "",
    "text": "Introduction to the OpenAI API\n\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Shell\n\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Data Science Toolbox (Part 1)\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Manipulation with pandas\n\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntermediate Python\n\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Python\n\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2023\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntermediate Regression in R\n\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nMburu\n\n\n\n\n\n\n  \n\n\n\n\nSurvival Analysis R\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFoundations of Probability in R\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Statistics with R\n\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunctions in R\n\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2022\n\n\n\n\n\n\n  \n\n\n\n\nHandling Missing Data with Imputations in R\n\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnomaly Detection in R\n\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2022\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling in R\n\n\n\n\n\n\n\n\n\n\n\n\nSep 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to advanced dimensionality reduction\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2022\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCensus data in r with tidycensus\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\nModeling with tidymodels in R\n\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunicating with Data in the Tidyverse\n\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2021\n\n\nMburu\n\n\n\n\n\n\n  \n\n\n\n\nThe reduction in weekly working hours in Europe\n\n\nLooking at the development between 1996 and 2006\n\n\n\n\n\n\n\n\n\nNov 8, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWriting Efficient R Code\n\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScalable Data Processing in R\n\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Linear and Logistic Regression in R\n\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork analysis in R\n\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nString manipulation with stringr in r\n\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML with tree based models in r\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2020\n\n\nMburu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "datacamp/intermidiate_regression_R/intermediate_regression.html",
    "href": "datacamp/intermidiate_regression_R/intermediate_regression.html",
    "title": "Intermediate Regression in R",
    "section": "",
    "text": "In Introduction to Regression in R, you learned to fit linear regression models with a single explanatory variable. In many cases, using only one explanatory variable limits the accuracy of predictions. That means that to truly master linear regression, you need to be able to include multiple explanatory variables.\nThe case when there is one numeric explanatory variable and one categorical explanatory variable is sometimes called a “parallel slopes” linear regression due to the shape of the predictions—more on that in the next exercise.\nHere, you’ll revisit the Taiwan real estate dataset. Recall the meaning of each variable.\n\n\n\nlibrary(mTools)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(fst)\nlibrary(broom)\ntaiwan_real_estate <- read_fst(here(\"data\", \"taiwan_real_estate2.fst\"))\n\n# Fit a linear regr'n of price_twd_msq vs. n_convenience\nmdl_price_vs_conv <- lm(price_twd_msq ~ n_convenience, data = taiwan_real_estate)\n\n# See the result\nDT_tidy_model(mdl_price_vs_conv)\n\n\n\n\n\n\n\n\n\n\n# Fit a linear regr'n of price_twd_msq vs. house_age_years, no intercept\nmdl_price_vs_age <- lm(price_twd_msq ~ house_age_years -1, data = taiwan_real_estate )\n\n# See the result\nDT_tidy_model(mdl_price_vs_age)\n\n\n\n\n\n\n\n\n\n\n# Fit a linear regr'n of price_twd_msq vs. n_convenience plus house_age_years, no intercept\nmdl_price_vs_both <- lm(price_twd_msq ~ n_convenience \n                        + house_age_years - 1, \n                        data = taiwan_real_estate)\n\n# See the result\nDT_tidy_model(mdl_price_vs_both)\n\n\n\n\n\n\n\n\n\n\nFor linear regression with a single numeric explanatory variable, there is an intercept coefficient and a slope coefficient. For linear regression with a single categorical explanatory variable, there is an intercept coefficient for each category.\nIn the “parallel slopes” case, where you have a numeric and a categorical explanatory variable, what do the coefficients mean?\ntaiwan_real_estate and mdl_price_vs_both are available.\n\nFor each additional nearby convenience store, the expected house price, in TWD per square meter, increases by 0.79.\n\n\n\n\nBeing able to see the predictions made by a model makes it easier to understand. In the case where there is only one explanatory variable, ggplot lets you do this without any manual calculation or messing about.\nTo visualize the relationship between a numeric explanatory variable and the numeric response, you can draw a scatter plot with a linear trend line.\nTo visualize the relationship between a categorical explanatory variable and the numeric response, you can draw a box plot.\ntaiwan_real_estate is available and ggplot2 is loaded.\n\nUsing the taiwan_real_estate dataset, plot the house price versus the number of nearby convenience stores.\nMake it a scatter plot.\nAdd a smooth linear regression trend line without a standard error ribbon.\n\n\n# Using taiwan_real_estate, plot price_twd_msq vs. n_convenience\nggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq)) +\n    # Add a point layer\n    geom_point() +\n    # Add a smooth trend line using linear regr'n, no ribbon\n    geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\nUsing the taiwan_real_estate dataset, plot the house price versus the house age.\nMake it a box plot.\n\n\n# Using taiwan_real_estate, plot price_twd_msq vs. house_age_years\nggplot(taiwan_real_estate, aes( house_age_years, price_twd_msq)) +\n    # Add a box plot layer\n    geom_boxplot()\n\n\n\n\n\n\n\nThe two plots in the previous exercise gave very different predictions: one gave a predicted response that increased linearly with a numeric variable; the other gave a fixed response for each category. The only sensible way to reconcile these two conflicting predictions is to incorporate both explanatory variables in the model at once.\nWhen it comes to a linear regression model with a numeric and a categorical explanatory variable, ggplot2 doesn’t have an easy, “out of the box” way to show the predictions. Fortunately, the moderndive package includes an extra geom, geom_parallel_slopes() to make it simple.\ntaiwan_real_estate is available; ggplot2 and moderndive are loaded.\n\nUsing the taiwan_real_estate dataset, plot house prices versus the number of nearby convenience stores, colored by house age.\nMake it a scatter plot.\nAdd parallel slopes, without a standard error ribbon.\n\n\nlibrary(moderndive)\n# Using taiwan_real_estate, plot price_twd_msq vs. n_convenience colored by house_age_years\nggplot(taiwan_real_estate, aes( n_convenience , price_twd_msq, color = house_age_years))  +\n    # Add a point layer\n    geom_point() +\n    # Add parallel slopes, no ribbon\n    geom_parallel_slopes(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\nWhile ggplot can automatically show you model predictions, in order to get those values to program with, you’ll need to do the calculations yourself.\nJust as with the case of a single explanatory variable, the workflow has two steps: create a data frame of explanatory variables, then add a column of predictions. To make sure you’ve got the right answer, you can add your predictions to the ggplot with the geom_parallel_slopes() lines.\ntaiwan_real_estate and mdl_price_vs_both are available; dplyr, tidyr, and ggplot2 are loaded.\n\n\n\nn_convenience should take the numbers zero to ten.\nhouse_age_years should take the unique values of the house_age_years column of taiwan_real_estate.\n\n\n# Make a grid of explanatory data\nexplanatory_data <- expand.grid(\n    # Set n_convenience to zero to ten\n    n_convenience = 0:10,\n    # Set house_age_years to the unique values of that variable\n    house_age_years = unique(taiwan_real_estate$house_age_years)\n)\n\n# See the result\nexplanatory_data[1:10,] %>% data_table()\n\n\n\n\n\n\n\nAdd a column to the explanatory_data named for the response variable, assigning to prediction_data.\nThe response column contain predictions made using mdl_price_vs_both and explanatory_data.\n\n\n# Add predictions to the data frame\nprediction_data <- explanatory_data %>% \n    mutate(price_twd_msq = predict(mdl_price_vs_both, explanatory_data) )\n\n# See the result\nprediction_data[1:10,] %>% data_table()\n\n\n\n\n\n\n\nUpdate the plot to add a point layer of predictions. Use the prediction_data, set the point size to 5, and the point shape to 15.\n\n\ntaiwan_real_estate %>% \n    ggplot(aes(n_convenience, price_twd_msq, color = house_age_years)) +\n    geom_point() +\n    geom_parallel_slopes(se = FALSE) +\n    # Add points using prediction_data, with size 5 and shape 15\n    geom_point(data =prediction_data, \n               aes(n_convenience, price_twd_msq),\n               size = 5, shape = 15)\n\n\n\n\n\n\n\n\nAs with simple linear regression, you can manually calculate the predictions from the model coefficients. The only change for the parallel slopes case is that the intercept is different for each category of the categorical explanatory variable. That means you need to consider the case when each each category occurs separately.\ntaiwan_real_estate, mdl_price_vs_both, and explanatory_data are available; dplyr is loaded.\n\nGet the coefficients from mdl_price_vs_both, assigning to coeffs.\nAssign each of the elements of coeffs to the appropriate variable.\n\n\n# Get the coefficients from mdl_price_vs_both\ncoeffs <- coefficients(mdl_price_vs_both)\n\n# Extract the slope coefficient\nslope <- coeffs[1]\n\n# Extract the intercept coefficient for 0 to 15\nintercept_0_15 <- coeffs[2]\n\n# Extract the intercept coefficient for 15 to 30\nintercept_15_30 <- coeffs[3]\n\n# Extract the intercept coefficient for 30 to 45\nintercept_30_45 <- coeffs[4]\n\n\n\n\nTo choose the intercept, in the case when house_age_years is “0 to 15”, choose intercept_0_15. In the case when house_age_years is “15 to 30”, choose intercept_15_30. Do likewise for “30 to 45”.\nManually calculate the predictions as the intercept plus the slope times n_convenience.\n\n\nprediction_data <- explanatory_data %>% \n    mutate(\n        # Consider the 3 cases to choose the intercept\n        intercept = case_when(\n            house_age_years == \"0 to 15\" ~ intercept_0_15,\n            house_age_years ==  \"15 to 30\" ~ intercept_15_30,\n            house_age_years ==  \"30 to 45\" ~ intercept_30_45\n        ),\n        \n        # Manually calculate the predictions\n        price_twd_msq = slope*n_convenience + intercept\n    )\n\n# See the results\nprediction_data[1:10,] %>% data_table()\n\n\n\n\n\n\n\n\n\n\nRecall that the coefficient of determination is a measure of how well the linear regression line fits the observed values. An important motivation for including several explanatory variables in a linear regression is that you can improve the fit compared to considering only a single explanatory variable.\nHere you’ll compare the coefficient of determination for the three Taiwan house price models, to see which gives the best result.\nmdl_price_vs_conv, mdl_price_vs_age, and mdl_price_vs_both are available; dplyr and broom are loaded.\n\nGet the unadjusted and adjusted coefficients of determination for mdl_price_vs_conv by glancing at the model, then selecting the r.squared and adj.r.squared values.\nDo the same for mdl_price_vs_age and mdl_price_vs_both.\n\n\nmy_models <- list(mdl_price_vs_conv = mdl_price_vs_conv,\n                  mdl_price_vs_age = mdl_price_vs_age,\n                  mdl_price_vs_both = mdl_price_vs_both)\nnms_ms <- names(my_models)\nmy_dfs <- lapply(seq_along(my_models), function(x){\n    \n    my_models[[x]] %>% \n        glance() %>% \n        mutate(lm_model =nms_ms[x] ) %>%\n        select(lm_model, r.squared, adj.r.squared, sigma) \n    \n    \n    \n})\n\nmodel_glance <- data.table::rbindlist(my_dfs)\nmodel_glance %>% \n    round_all_num_cols() %>%\n    data_table()\n\n\n\n\n\n\n\nWhich model does the adjusted coefficient of determination suggest gives a better fit?\nmdl_price_vs_both\n\n\n\n\nThe other common metric for assessing model fit is the residual standard error (RSE), which measures the typical size of the residuals.\nIn the last exercise you saw how including both explanatory variables into the model increased the coefficient of determination. How do you think using both explanatory variables will change the RSE?\nmdl_price_vs_conv, mdl_price_vs_age, and mdl_price_vs_both are available; dplyr and broom are loaded.\n\nShown as sigma below\n\n\nmodel_glance %>% \n    round_all_num_cols() %>%\n    data_table()\n\n\n\n\n\n\n\nThe model with the list sigma mdl_price_vs_both"
  },
  {
    "objectID": "datacamp/intermidiate_regression_R/intermediate_regression.html#interactions",
    "href": "datacamp/intermidiate_regression_R/intermediate_regression.html#interactions",
    "title": "Intermediate Regression in R",
    "section": "Interactions",
    "text": "Interactions"
  },
  {
    "objectID": "notes/gtsumary_test.html",
    "href": "notes/gtsumary_test.html",
    "title": "gtsumary_test",
    "section": "",
    "text": "library(gtsummary)\nlibrary(tidyverse)\ntrial2 <- trial %>% select(trt, age, grade)\n\n\ntab <- trial2 %>%\n    tbl_summary(by = trt) %>%\n    add_p()\n\n\ntab2 <- trial2 %>%\n    tbl_summary(\n        by = trt,\n        type = all_continuous() ~ \"continuous2\",\n        statistic = all_continuous() ~ c(\n            \"{N_nonmiss}\",\n            \"{median} ({p25}, {p75})\",\n            \"{mean} ({sd})\",\n            \"{min}, {max}\"\n        ),\n        missing = \"ifany\"\n    ) %>%\n    add_p(pvalue_fun = ~ style_pvalue(.x, digits = 2))\n\ntab_df = as.data.frame(tab2)\n\nmTools::data_table(tab_df)"
  },
  {
    "objectID": "posts/Diabetes/predict_diabetes_tidymodels.html",
    "href": "posts/Diabetes/predict_diabetes_tidymodels.html",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(data.table)\nlibrary(gtsummary)\nlibrary(mTools)\nDiabetes data"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html",
    "href": "datacamp/tidymodels/tidymodels.html",
    "title": "Modeling with tidymodels in R",
    "section": "",
    "text": "The rsample package is designed to create training and test datasets. Creating a test dataset is important for estimating how a trained model will likely perform on new data. It also guards against overfitting, where a model memorizes patterns that exist only in the training data and performs poorly on new data.\nIn this exercise, you will create training and test datasets from the home_sales data. This data contains information on homes sold in the Seattle, Washington area between 2015 and 2016.\nThe outcome variable in this data is selling_price.\nThe tidymodels package will be pre-loaded in every exercise in the course. The home_sales tibble has also been loaded for you.\n\n\n\nTidy model packages\n\n\n\nhome_sales <- readRDS(\"home_sales.rds\")\n\n# Create a data split object\nhome_split <- initial_split(home_sales, \n                            prop = 0.7, \n                            strata = selling_price)\n\n# Create the training data\nhome_training <- home_split %>%\n  training()\n\n# Create the test data\nhome_test <- home_split %>% \n  testing()\n\n# Check number of rows in each dataset\nnrow(home_training)\n\n[1] 1042\n\nnrow(home_test)\n\n[1] 450\n\n\nDistribution of outcome variable values Stratifying by the outcome variable when generating training and test datasets ensures that the outcome variable values have a similar range in both datasets.\nSince the original data is split at random, stratification avoids placing all the expensive homes in home_sales into the test dataset, for example. In this case, your model would most likely perform poorly because it was trained on less expensive homes.\nIn this exercise, you will calculate summary statistics for the selling_price variable in the training and test datasets. The home_training and home_test tibbles have been loaded from the previous exercise.\n\n# Distribution of selling_price in training data\nlibrary(knitr)\nsummary_func <- function(df){\n      df %>% summarize(min_sell_price = min(selling_price),\n            max_sell_price = max(selling_price),\n            mean_sell_price = mean(selling_price),\n            sd_sell_price = sd(selling_price)) %>%\n    kable()\n\n}\nhome_training %>% \n    summary_func()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n479358.7\n81534.16\n\n\n\n\nhome_test %>% \n  summary_func()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n478449.5\n79764.72"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#fitting-a-linear-regression-model",
    "href": "datacamp/tidymodels/tidymodels.html#fitting-a-linear-regression-model",
    "title": "Modeling with tidymodels in R",
    "section": "Fitting a linear regression model",
    "text": "Fitting a linear regression model\nThe parsnip package provides a unified syntax for the model fitting process in R.\nWith parsnip, it is easy to define models using the various packages, or engines, that exist in the R ecosystem.\nIn this exercise, you will define a parsnip linear regression object and train your model to predict selling_price using home_age and sqft_living as predictor variables from the home_sales data.\nThe home_training and home_test tibbles that you created in the previous lesson have been loaded into this session.\n\n# Specify a linear regression model, linear_model\nlinear_model <- linear_reg() %>% \n  # Set the model engine\n  set_engine('lm') %>% \n  # Set the model mode\n  set_mode('regression')\n\n# Train the model with the training data\nlm_fit <- linear_model %>% \n  fit(selling_price~ home_age + sqft_living,\n      data = home_training)\n\n# Print lm_fit to view model information\ntidy(lm_fit) %>%\n    kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n292350.3425\n7625.56910\n38.338167\n0\n\n\nhome_age\n-1616.2020\n176.93455\n-9.134463\n0\n\n\nsqft_living\n103.2897\n2.77595\n37.208781\n0"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#predicting-home-selling-prices",
    "href": "datacamp/tidymodels/tidymodels.html#predicting-home-selling-prices",
    "title": "Modeling with tidymodels in R",
    "section": "Predicting home selling prices",
    "text": "Predicting home selling prices\nAfter fitting a model using the training data, the next step is to use it to make predictions on the test dataset. The test dataset acts as a new source of data for the model and will allow you to evaluate how well it performs.\nBefore you can evaluate model performance, you must add your predictions to the test dataset.\nIn this exercise, you will use your trained model, lm_fit, to predict selling_price in the home_test dataset.\nYour trained model, lm_fit, as well as the test dataset, home_test have been loaded into your session.\n\n# Predict selling_price\nhome_predictions <- predict(lm_fit,\n                        new_data = home_test)\n\n# View predicted selling prices\n#home_predictions\n\n# Combine test data with predictions\nhome_test_results <- home_test %>% \n  select(selling_price, home_age, sqft_living) %>% \n  bind_cols(home_predictions)\n\n# View results\nhome_test_results %>% \n    head()\n\n# A tibble: 6 × 4\n  selling_price home_age sqft_living   .pred\n          <dbl>    <dbl>       <dbl>   <dbl>\n1        487000       10        2540 538544.\n2        465000       10        1530 434222.\n3        411000       18        1130 379976.\n4        635000        4        3350 631906.\n5        464950       19        2190 487847.\n6        425000       11        1920 472888."
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#model-performance-metrics",
    "href": "datacamp/tidymodels/tidymodels.html#model-performance-metrics",
    "title": "Modeling with tidymodels in R",
    "section": "Model performance metrics",
    "text": "Model performance metrics\nEvaluating model results is an important step in the modeling process. Model evaluation should be done on the test dataset in order to see how well a model will generalize to new datasets.\nIn the previous exercise, you trained a linear regression model to predict selling_price using home_age and sqft_living as predictor variables. You then created the home_test_results tibble using your trained model on the home_test data.\nIn this exercise, you will calculate the RMSE and R squared metrics using your results in home_test_results.\nThe home_test_results tibble has been loaded into your session.\n\n# Print home_test_results\n#home_test_results\n\n# Caculate the RMSE metric\nhome_test_results %>% \n  rmse(truth = selling_price, estimate =.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      45930.\n\n# Calculate the R squared metric\nhome_test_results %>% \n  rsq(truth = selling_price, estimate =.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.670"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#r-squared-plot",
    "href": "datacamp/tidymodels/tidymodels.html#r-squared-plot",
    "title": "Modeling with tidymodels in R",
    "section": "R squared plot",
    "text": "R squared plot\nIn the previous exercise, you got an R squared value of 0.651. The R squared metric ranges from 0 to 1, 0 being the worst and 1 the best.\nCalculating the R squared value is only the first step in studying your model’s predictions.\nMaking an R squared plot is extremely important because it will uncover potential problems with your model, such as non-linear patterns or regions where your model is either over or under-predicting the outcome variable.\nIn this exercise, you will create an R squared plot of your model’s performance.\nThe home_test_results tibble has been loaded into your session.\n\n# Create an R squared plot of model performance\nggplot(home_test_results, aes(x = selling_price, y = .pred)) +\n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred()  +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#complete-model-fitting-process-with-last_fit",
    "href": "datacamp/tidymodels/tidymodels.html#complete-model-fitting-process-with-last_fit",
    "title": "Modeling with tidymodels in R",
    "section": "Complete model fitting process with last_fit()",
    "text": "Complete model fitting process with last_fit()\nIn this exercise, you will train and evaluate the performance of a linear regression model that predicts selling_price using all the predictors available in the home_sales tibble.\nThis exercise will give you a chance to perform the entire model fitting process with tidymodels, from defining your model object to evaluating its performance on the test data.\nEarlier in the chapter, you created an rsample object called home_split by passing the home_sales tibble into initial_split(). The home_split object contains the instructions for randomly splitting home_sales into training and test sets.\nThe home_sales tibble, and home_split object have been loaded into this session.\n\n# Define a linear regression model\nlinear_model <- linear_reg() %>% \n  set_engine('lm') %>% \n  set_mode('regression')\n\n# Train linear_model with last_fit()\nlinear_fit <- linear_model %>% \n  last_fit(selling_price ~ ., split = home_split)\n\n# Collect predictions and view results\npredictions_df <- linear_fit %>% collect_predictions()\npredictions_df %>% head()\n\n# A tibble: 6 × 5\n  id                 .pred  .row selling_price .config             \n  <chr>              <dbl> <int>         <dbl> <chr>               \n1 train/test split 527338.     1        487000 Preprocessor1_Model1\n2 train/test split 421637.     2        465000 Preprocessor1_Model1\n3 train/test split 397998.     3        411000 Preprocessor1_Model1\n4 train/test split 694181.     4        635000 Preprocessor1_Model1\n5 train/test split 475255.     8        464950 Preprocessor1_Model1\n6 train/test split 436889.     9        425000 Preprocessor1_Model1\n\n# Make an R squared plot using predictions_df\nggplot(predictions_df, aes(x = selling_price, y = .pred)) + \n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred() +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#data-resampling",
    "href": "datacamp/tidymodels/tidymodels.html#data-resampling",
    "title": "Modeling with tidymodels in R",
    "section": "Data resampling",
    "text": "Data resampling\nThe first step in a machine learning project is to create training and test datasets for model fitting and evaluation. The test dataset provides an estimate of how your model will perform on new data and helps to guard against overfitting.\nYou will be working with the telecom_df dataset which contains information on customers of a telecommunications company. The outcome variable is canceled_service and it records whether a customer canceled their contract with the company. The predictor variables contain information about customers’ cell phone and internet usage as well as their contract type and monthly charges.\nThe telecom_df tibble has been loaded into your session.\n\ntelecom_df <- readRDS(\"telecom_df.rds\")\n# Create data split object\ntelecom_split <- initial_split(telecom_df, prop = 0.75,\n                     strata = canceled_service)\n\n# Create the training data\ntelecom_training <- telecom_split %>% \n  training()\n\n# Create the test data\ntelecom_test <- telecom_split %>% \n  testing()\n\n# Check the number of rows\nnrow(telecom_training)\n\n[1] 731\n\nnrow(telecom_test)\n\n[1] 244"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#fitting-a-logistic-regression-model",
    "href": "datacamp/tidymodels/tidymodels.html#fitting-a-logistic-regression-model",
    "title": "Modeling with tidymodels in R",
    "section": "Fitting a logistic regression model",
    "text": "Fitting a logistic regression model\nIn addition to regression models, the parsnip package also provides a general interface to classification models in R.\nIn this exercise, you will define a parsnip logistic regression object and train your model to predict canceled_service using avg_call_mins, avg_intl_mins, and monthly_charges as predictor variables from the telecom_df data.\nThe telecom_training and telecom_test tibbles that you created in the previous lesson have been loaded into this session.\n\n# Specify a logistic regression model\nlogistic_model <- logistic_reg() %>% \n  # Set the engine\n  set_engine('glm') %>% \n  # Set the mode\n  set_mode('classification')\n\n# Fit to training data\nlogistic_fit <- logistic_model %>% \n  fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n      data = telecom_training)\n\n# Print model fit object\nlogistic_fit %>% tidy()\n\n# A tibble: 4 × 5\n  term             estimate std.error statistic  p.value\n  <chr>               <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)      1.99       0.584      3.41   6.43e- 4\n2 avg_call_mins   -0.0107     0.00128   -8.40   4.50e-17\n3 avg_intl_mins    0.0236     0.00311    7.59   3.16e-14\n4 monthly_charges  0.000293   0.00477    0.0615 9.51e- 1"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#combining-test-dataset-results",
    "href": "datacamp/tidymodels/tidymodels.html#combining-test-dataset-results",
    "title": "Modeling with tidymodels in R",
    "section": "Combining test dataset results",
    "text": "Combining test dataset results\nEvaluating your model’s performance on the test dataset gives insights into how well your model predicts on new data sources. These insights will help you communicate your model’s value in solving problems or improving decision making.\nBefore you can calculate classification metrics such as sensitivity or specificity, you must create a results tibble with the required columns for yardstick metric functions.\nIn this exercise, you will use your trained model to predict the outcome variable in the telecom_test dataset and combine it with the true outcome values in the canceled_service column.\nYour trained model, logistic_fit, and test dataset, telecom_test, have been loaded from the previous exercise.\n\n# Predict outcome categories\nclass_preds <- predict(logistic_fit, new_data = telecom_test,\n                       type = 'class')\n\n# Obtain estimated probabilities for each outcome value\nprob_preds <- predict(logistic_fit, new_data = telecom_test, \n                      type = 'prob')\n\n# Combine test set results\ntelecom_results <- telecom_test %>% \n  select(canceled_service) %>% \n  bind_cols(class_preds, prob_preds)\n\n# View results tibble\ntelecom_results %>%\n    head()\n\n# A tibble: 6 × 4\n  canceled_service .pred_class .pred_yes .pred_no\n  <fct>            <fct>           <dbl>    <dbl>\n1 yes              no              0.136    0.864\n2 yes              yes             0.792    0.208\n3 no               no              0.171    0.829\n4 no               yes             0.508    0.492\n5 yes              no              0.371    0.629\n6 no               no              0.139    0.861"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#evaluating-performance-with-yardstick",
    "href": "datacamp/tidymodels/tidymodels.html#evaluating-performance-with-yardstick",
    "title": "Modeling with tidymodels in R",
    "section": "Evaluating performance with yardstick",
    "text": "Evaluating performance with yardstick\nIn the previous exercise, you calculated classification metrics from a sample confusion matrix. The yardstick package was designed to automate this process.\nFor classification models, yardstick functions require a tibble of model results as the first argument. This should include the actual outcome values, predicted outcome values, and estimated probabilities for each value of the outcome variable.\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate performance metrics.\nThe telecom_results tibble has been loaded into your session.\n\n# Calculate the confusion matrix\nconf_mat(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n          Truth\nPrediction yes  no\n       yes  31  22\n       no   51 140\n\n# Calculate the accuracy\naccuracy(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.701\n\n# Calculate the sensitivity\n\nsens(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.378\n\n# Calculate the specificity\n\nspec(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 spec    binary         0.864"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#creating-custom-metric-sets",
    "href": "datacamp/tidymodels/tidymodels.html#creating-custom-metric-sets",
    "title": "Modeling with tidymodels in R",
    "section": "Creating custom metric sets",
    "text": "Creating custom metric sets\nThe yardstick package also provides the ability to create custom sets of model metrics. In cases where the cost of obtaining false negative errors is different from the cost of false positive errors, it may be important to examine a specific set of performance metrics.\nInstead of calculating accuracy, sensitivity, and specificity separately, you can create your own metric function that calculates all three at the same time.\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate a custom set of performance metrics. You will also use a confusion matrix to calculate all available binary classification metrics in tidymodelsall at once.\nThe telecom_results tibble has been loaded into your session.\n\n# Create a custom metric function\ntelecom_metrics <- metric_set(accuracy, sens, spec)\n\n# Calculate metrics using model results tibble\ntelecom_metrics(telecom_results, \n                truth = canceled_service,\n                estimate = .pred_class)\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.701\n2 sens     binary         0.378\n3 spec     binary         0.864\n\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class) %>% \n  # Pass to the summary() function\n  summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary         0.701\n 2 kap                  binary         0.265\n 3 sens                 binary         0.378\n 4 spec                 binary         0.864\n 5 ppv                  binary         0.585\n 6 npv                  binary         0.733\n 7 mcc                  binary         0.278\n 8 j_index              binary         0.242\n 9 bal_accuracy         binary         0.621\n10 detection_prevalence binary         0.217\n11 precision            binary         0.585\n12 recall               binary         0.378\n13 f_meas               binary         0.459"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#plotting-the-confusion-matrix",
    "href": "datacamp/tidymodels/tidymodels.html#plotting-the-confusion-matrix",
    "title": "Modeling with tidymodels in R",
    "section": "Plotting the confusion matrix",
    "text": "Plotting the confusion matrix\nCalculating performance metrics with the yardstick package provides insight into how well a classification model is performing on the test dataset. Most yardstick functions return a single number that summarizes classification performance.\nMany times, it is helpful to create visualizations of the confusion matrix to more easily communicate your results.\nIn this exercise, you will make a heat map and mosaic plot of the confusion matrix from your logistic regression model on the telecom_df dataset.\nYour model results tibble, telecom_results, has been loaded into your session.\n\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a heat map\n  autoplot(type = \"heatmap\")\n\n\n\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a mosaic plot\n  autoplot(type = \"mosaic\")"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#roc-curves-and-area-under-the-roc-curve",
    "href": "datacamp/tidymodels/tidymodels.html#roc-curves-and-area-under-the-roc-curve",
    "title": "Modeling with tidymodels in R",
    "section": "ROC curves and area under the ROC curve",
    "text": "ROC curves and area under the ROC curve\nROC curves are used to visualize the performance of a classification model across a range of probability thresholds. An ROC curve with the majority of points near the upper left corner of the plot indicates that a classification model is able to correctly predict both the positive and negative outcomes correctly across a wide range of probability thresholds.\nThe area under this curve provides a letter grade summary of model performance.\nIn this exercise, you will create an ROC curve from your logistic regression model results and calculate the area under the ROC curve with yardstick.\nYour model results tibble, telecom_results has been loaded into your session.\n\n# Calculate metrics across thresholds\nthreshold_df <- telecom_results %>% \n  roc_curve(truth = canceled_service, .pred_yes)\n\n# View results\nthreshold_df %>%\n  head()\n\n# A tibble: 6 × 3\n  .threshold specificity sensitivity\n       <dbl>       <dbl>       <dbl>\n1  -Inf          0             1    \n2     0.0110     0             1    \n3     0.0350     0             0.988\n4     0.0416     0.00617       0.988\n5     0.0435     0.0123        0.988\n6     0.0487     0.0185        0.988\n\n# Plot ROC curve\nthreshold_df %>% \n  autoplot()\n\n\n\n# Calculate ROC AUC\nroc_auc(telecom_results, truth = canceled_service, .pred_yes)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.725\n\n\nStreamlining the modeling process The last_fit() function is designed to streamline the modeling workflow in tidymodels. Instead of training your model on the training data and building a results tibble using the test data, last_fit() accomplishes this with one function.\nIn this exercise, you will train the same logistic regression model as you fit in the previous exercises, except with the last_fit() function.\nYour data split object, telecom_split, and model specification, logistic_model, have been loaded into your session.\n\n# Train model with last_fit()\ntelecom_last_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n           split = telecom_split)\n\n# View test set metrics\ntelecom_last_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.701 Preprocessor1_Model1\n2 roc_auc  binary         0.725 Preprocessor1_Model1"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#collecting-predictions-and-creating-custom-metrics",
    "href": "datacamp/tidymodels/tidymodels.html#collecting-predictions-and-creating-custom-metrics",
    "title": "Modeling with tidymodels in R",
    "section": "Collecting predictions and creating custom metrics",
    "text": "Collecting predictions and creating custom metrics\nUsing the last_fit() modeling workflow also saves time in collecting model predictions. Instead of manually creating a tibble of model results, there are helper functions that extract this information automatically.\nIn this exercise, you will use your trained model, telecom_last_fit, to create a tibble of model results on the test dataset as well as calculate custom performance metrics.\nYou trained model, telecom_last_fit, has been loaded into this session.\n\n# Collect predictions\nlast_fit_results <- telecom_last_fit %>% \n  collect_predictions()\n\n# View results\nlast_fit_results %>%\n  head()\n\n# A tibble: 6 × 7\n  id               .pred_yes .pred_no  .row .pred_class canceled_service .config\n  <chr>                <dbl>    <dbl> <int> <fct>       <fct>            <chr>  \n1 train/test split     0.136    0.864     4 no          yes              Prepro…\n2 train/test split     0.792    0.208     7 yes         yes              Prepro…\n3 train/test split     0.171    0.829    10 no          no               Prepro…\n4 train/test split     0.508    0.492    16 yes         no               Prepro…\n5 train/test split     0.371    0.629    17 no          yes              Prepro…\n6 train/test split     0.139    0.861    21 no          no               Prepro…\n\n# Custom metrics function\nlast_fit_metrics <- metric_set(accuracy, sens,\n                               spec, roc_auc)\n\n# Calculate metrics\nlast_fit_metrics(last_fit_results,\n                 truth = canceled_service,\n                 estimate = .pred_class,\n                 .pred_yes)\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.701\n2 sens     binary         0.378\n3 spec     binary         0.864\n4 roc_auc  binary         0.725"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#complete-modeling-workflow",
    "href": "datacamp/tidymodels/tidymodels.html#complete-modeling-workflow",
    "title": "Modeling with tidymodels in R",
    "section": "Complete modeling workflow",
    "text": "Complete modeling workflow\nIn this exercise, you will use the last_fit() function to train a logistic regression model and evaluate its performance on the test data by assessing the ROC curve and the area under the ROC curve.\nSimilar to previous exercises, you will predict canceled_service in the telecom_df data, but with an additional predictor variable to see if you can improve model performance.\nThe telecom_df tibble, telecom_split, and logistic_model objects from the previous exercises have been loaded into your workspace. The telecom_split object contains the instructions for randomly splitting the telecom_df tibble into training and test sets. The logistic_model object is a parsnip specification of a logistic regression model.\n\n# Train a logistic regression model\nlogistic_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins + avg_intl_mins + monthly_charges + months_with_company, \n           split = telecom_split)\n\n# Collect metrics\nlogistic_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.742 Preprocessor1_Model1\n2 roc_auc  binary         0.797 Preprocessor1_Model1\n\n# Collect model predictions\nlogistic_fit %>% \n  collect_predictions() %>% \n  # Plot ROC curve\n  roc_curve(truth = canceled_service, .pred_yes) %>% \n  autoplot()"
  },
  {
    "objectID": "posts/Diabetes/predict_diabetes_tidymodels.html#data-processing",
    "href": "posts/Diabetes/predict_diabetes_tidymodels.html#data-processing",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "Data processing",
    "text": "Data processing\n\ndiabetes_df[, diabetes_char := factor(diabetes, \n                                      levels = c(0, 1),\n                                      labels = c(\"Non diabetic\", \"Diabetic\"))]"
  },
  {
    "objectID": "posts/Diabetes/predict_diabetes_tidymodels.html#summary-stats",
    "href": "posts/Diabetes/predict_diabetes_tidymodels.html#summary-stats",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "Summary Stats",
    "text": "Summary Stats\n\nlibrary(ggiraph)\ndb_perc <- diabetes_df[, .(freq = .N),\n                       by = diabetes_char][\n                           ,perc := round(freq/sum(freq) * 100, 1)]\n\n\nggplot(db_perc, aes(diabetes_char, freq, fill = diabetes_char))+\n    geom_bar_interactive(width = 0.5, stat = \"identity\")+\n    geom_text(aes(label = paste0(freq, \"(\", perc, \"%)\")),\n              position = position_dodge(width = 0.5),\n              vjust = 0.05)+\n    scale_fill_brewer(name = \"\", type = \"qual\", palette = \"Dark2\")+\n    theme_minimal()+\n    theme(\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\ntab2 <- diabetes_df %>%\n    tbl_summary(\n        by = diabetes_char,\n        type = all_continuous() ~ \"continuous2\",\n        statistic = all_continuous() ~ c(\n            \"{mean} ({sd})\",\n            \"{median} ({p25}, {p75})\",\n            \"[{min}, {max}]\"\n        ),\n        missing = \"ifany\"\n    ) %>%\n    add_p(pvalue_fun = ~ style_pvalue(.x, digits = 2))\n\ntab_df = as.data.frame(tab2)\nnms <- names(tab_df)\nnms <- gsub(\"\\\\*\", \"\", nms)\nnames(tab_df) <- nms\ndata_table(tab_df)"
  },
  {
    "objectID": "posts/Diabetes/predict_diabetes_tidymodels.html#model-fitting",
    "href": "posts/Diabetes/predict_diabetes_tidymodels.html#model-fitting",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "Model Fitting",
    "text": "Model Fitting\n\nset.seed(100)\ndiabetes_df[, diabetes:= as.factor(diabetes)]\ndiabetes_df_split <- initial_split(diabetes_df[,.SD, .SDcols = !\"diabetes_char\"], \n                                   strata = diabetes)\n\ndiabetes_df_train <- training(diabetes_df_split)\n\ndiabetes_df_test <- testing(diabetes_df_split)\n\n\n# Specify a logistic regression model\nlogistic_model <- logistic_reg() %>% \n  # Set the engine\n  set_engine('glm') %>% \n  # Set the mode\n  set_mode('classification')\n\n# Fit to training data\nlogistic_fit <- logistic_model %>% \n  fit(diabetes ~ .,\n      data = diabetes_df_train)\n\n# Print model fit object\nlogistic_fit %>% \n    DT_tidy_model()\n\n\n\n\n\n\n\nxgb_spec <- boost_tree(\n    trees = 2000,\n    tree_depth = tune(), \n    min_n = tune(),\n    loss_reduction = tune(),                     ## first three: model complexity\n    sample_size = tune(), \n    mtry = tune(),         ## randomness\n    learn_rate = tune()                          ## step size\n) %>%\n    set_engine(\"xgboost\") %>%\n    set_mode(\"classification\")\n\nxgb_spec\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 2000\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost"
  },
  {
    "objectID": "datacamp/foundation_prob_R/foundation_probability.html",
    "href": "datacamp/foundation_prob_R/foundation_probability.html",
    "title": "Foundations of Probability in R",
    "section": "",
    "text": "In these exercises, you’ll practice using the rbinom() function, which generates random “flips” that are either 1 (“heads”) or 0 (“tails”).\n\n# Generate 10 separate random flips with probability .3\nrbinom(10, 1, p = 0.3)\n\n [1] 0 0 1 0 1 0 0 0 1 0\n\n\n\n\n\nIn the last exercise, you simulated 10 separate coin flips, each with a 30% chance of heads. Thus, with rbinom(10, 1, .3) you ended up with 10 outcomes that were either 0 (“tails”) or 1 (“heads”).\nBut by changing the second argument of rbinom() (currently 1), you can flip multiple coins within each draw. Thus, each outcome will end up being a number between 0 and 10, showing the number of flips that were heads in that trial.\n\n# Generate 100 occurrences of flipping 10 coins, each with 30% probability\n\nrbinom(100, 10, p = 0.3)\n\n  [1] 2 4 4 5 3 1 5 4 4 1 6 6 4 2 2 0 3 6 1 1 4 1 3 1 3 4 7 3 0 4 0 3 5 1 4 3 2\n [38] 3 5 1 2 1 2 2 7 3 2 3 1 4 2 3 4 2 1 2 4 3 1 3 1 3 3 4 2 2 1 1 3 1 2 3 2 3\n [75] 1 2 2 0 6 1 2 2 4 5 2 3 5 3 4 4 3 4 3 1 4 3 3 3 2 3\n\n\n\n\n\nIf you flip 10 coins each with a 30% probability of coming up heads, what is the probability exactly 2 of them are heads?\n\nAnswer the above question using the dbinom() function. This function takes almost the same arguments as rbinom(). The second and third arguments are size and prob, but now the first argument is x instead of n. Use x to specify where you want to evaluate the binomial density.\nConfirm your answer using the rbinom() function by creating a simulation of 10,000 trials. Put this all on one line by wrapping the mean() function around the rbinom() function.\n\n\n# Calculate the probability that 2 are heads using dbinom\n\ndbinom(2, 10, .3)\n\n[1] 0.2334744\n\n# Confirm your answer with a simulation using rbinom\n\n#flips <- rbinom(10000, 10, .3)\n\nmean(rbinom(10000, 10, .3) == 2)\n\n[1] 0.2356\n\n\n\n\n\nIf you flip ten coins that each have a 30% probability of heads, what is the probability at least five are heads?\n\nAnswer the above question using the pbinom() function. (Note that you can compute the probability that the number of heads is less than or equal to 4, then take 1 - that probability).\nConfirm your answer with a simulation of 10,000 trials by finding the number of trials that result in 5 or more heads.\n\n\n# Calculate the probability that at least five coins are heads\n\n1 - pbinom(4, 10, .3)\n\n[1] 0.1502683\n\n# Confirm your answer with a simulation of 10,000 trials\n\nmean(rbinom(10000, 10, .3) >=  5)\n\n[1] 0.1484\n\n\n\n\n\nIn the last exercise you tried flipping ten coins with a 30% probability of heads to find the probability at least five are heads. You found that the exact answer was 1 - pbinom(4, 10, .3) = 0.1502683, then confirmed with 10,000 simulated trials.\n\nDid you need all 10,000 trials to get an accurate answer? Would your answer have been more accurate with more trials?\n\n\n# Here is how you computed the answer in the last problem\nmean(rbinom(10000, 10, .3) >= 5)\n\n[1] 0.1515\n\n# Try now with 100, 1000, 10,000, and 100,000 trials\n\nmean(rbinom(100, 10, .3) >= 5)\n\n[1] 0.18\n\nmean(rbinom(1000, 10, .3) >= 5)\n\n[1] 0.154\n\nmean(rbinom(10000, 10, .3) >= 5)\n\n[1] 0.1523\n\nmean(rbinom(100000, 10, .3) >= 5)\n\n[1] 0.14892"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html",
    "href": "posts/cbk_data/R/kenya_income_exp.html",
    "title": "Kenya Government Income & Expenditure from 2000 to Date",
    "section": "",
    "text": "Kenya Government Income & Expenditure from 2000 to Date\n\noptions(scipen = 999)\n\nexp_rev <- c(\"totalexpenditure\", \"totalrevenue\")\ngok_earnings_exp[, (exp_rev) := lapply(.SD, function(x) x/10000), .SDcols = exp_rev]\ngok_earnings_exp <- gok_earnings_exp[year < current_year]\nexp_plot <- plot_compare(df = gok_earnings_exp, \n                         id_vars = c(\"year\"),\n                         compare_vars = c(\"totalexpenditure\", \"totalrevenue\"),\n                         x_val = year,\n                         col_val = variable,\n                         y_val = sum_val,\n                         xlab = \"Year\",\n                         ylab = \"Kenya Shillings(Billions)\",\n                         title_lab = \"\")   \n\n[1] 2\n\ngirafe( ggobj =  exp_plot, pointsize =10)"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#difference",
    "href": "posts/cbk_data/R/kenya_income_exp.html#difference",
    "title": "Kenya Government Income & Expenditure from 2000 to 2022",
    "section": "% difference",
    "text": "% difference"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#what-type-of-expenditure-is-rising",
    "href": "posts/cbk_data/R/kenya_income_exp.html#what-type-of-expenditure-is-rising",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "What type of expenditure is rising",
    "text": "What type of expenditure is rising"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to2023",
    "href": "posts/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to2023",
    "title": "Kenya Government Income & Expenditure from 2000 to 2022",
    "section": "Income & Expenditure from 2000 to2023",
    "text": "Income & Expenditure from 2000 to2023\n\n\n[1] 2"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to-2023",
    "href": "posts/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to-2023",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "Income & Expenditure from 2000 to 2023",
    "text": "Income & Expenditure from 2000 to 2023\nA short blog on Kenyan Government income and expenditure. Due to the current discussion in Kenya. Decided to have a quick look of the Kenyan expenditure & income. The total expenditure is growing at a faster pace than the total revenue. This means that the government is spending more money than it is taking in.\nThere are a number of factors that could be contributing to this trend. One factor is the growth of the economy. As the economy grows, the government needs to spend more money on things like infrastructure, education, and healthcare. Another factor is the growth of the population. As the population grows, the government needs to spend more money on things like social welfare programs and security."
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#what-sub-type-of-reccurrent-expenditure-is-causing-the-rise",
    "href": "posts/cbk_data/R/kenya_income_exp.html#what-sub-type-of-reccurrent-expenditure-is-causing-the-rise",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "What Sub type of reccurrent expenditure is causing the rise",
    "text": "What Sub type of reccurrent expenditure is causing the rise"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#domestic-debt-composition",
    "href": "posts/cbk_data/R/kenya_income_exp.html#domestic-debt-composition",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "Domestic debt composition",
    "text": "Domestic debt composition"
  },
  {
    "objectID": "posts/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to-2022",
    "href": "posts/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to-2022",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "Income & Expenditure from 2000 to 2022",
    "text": "Income & Expenditure from 2000 to 2022\nDecided to have a quick look of the Kenyan government expenditure & income due to the current cash crunch\nThere are a number of factors that could be contributing to this trend. One factor is the growth of the economy. As the economy grows, the government needs to spend more money on things like infrastructure, education, and healthcare. Another factor is the growth of the population. As the population grows, the government needs to spend more money on things like social welfare programs and security."
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html",
    "href": "datacamp/introduction_python/introduction_python.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python Basics\nThe Python Interface Hit Run Code to run your first Python code with Datacamp and see the output!\nNotice the script.py window; this is where you can type Python code to solve exercises. You can hit Run Code and Submit Answer as often as you want. If you’re stuck, you can click Get Hint, and ultimately Get Solution.\nYou can also use the IPython Shell interactively by typing commands and hitting Enter. Here, your code will not be checked for correctness so it is a great way to experiment."
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#any-comments",
    "href": "datacamp/introduction_python/introduction_python.html#any-comments",
    "title": "Introduction to Python",
    "section": "Any comments?",
    "text": "Any comments?\nYou can also add comments to your Python scripts. Comments are important to make sure that you and others can understand what your code is about and do not run as Python code.\nThey start with # tag. See the comment in the editor, # Division; now it’s your turn to add a comment!\n\n# Division\nprint(5 / 8)\n\n0.625\n\n# Addition\nprint(7 + 10)\n\n17"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#python-as-a-calculator",
    "href": "datacamp/introduction_python/introduction_python.html#python-as-a-calculator",
    "title": "Introduction to Python",
    "section": "Python as a calculator",
    "text": "Python as a calculator\nPython is perfectly suited to do basic calculations. It can do addition, subtraction, multiplication and division.\nThe code in the script gives some examples.\nNow it’s your turn to practice!\n\n# Addition, subtraction\nprint(5 + 5)\n\n10\n\nprint(5 - 5)\n\n0\n\n# Multiplication, division, modulo, and exponentiation\nprint(3 * 5)\n\n15\n\nprint(10 / 2)\n\n5.0\n\nprint(18 % 7)\n\n4\n\nprint(4 ** 2)\n\n16\n\n# How much is your $100 worth after 7 years?\nprint(100*1.1**7)\n\n194.87171000000012"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#variable-assignment",
    "href": "datacamp/introduction_python/introduction_python.html#variable-assignment",
    "title": "Introduction to Python",
    "section": "Variable Assignment",
    "text": "Variable Assignment\nIn Python, a variable allows you to refer to a value with a name. To create a variable x with a value of 5, you use =, like this example:\nx = 5 You can now use the name of this variable, x, instead of the actual value, 5.\nRemember, = in Python means assignment, it doesn’t test equality!\n\n#creating saving variable\nsavings=100\nprint(savings)\n\n100\n\n#checking out the type\ntype(savings)\n\n<class 'int'>"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#calculations-with-variables",
    "href": "datacamp/introduction_python/introduction_python.html#calculations-with-variables",
    "title": "Introduction to Python",
    "section": "Calculations with variables",
    "text": "Calculations with variables\nYou’ve now created a savings variable, so let’s start saving!\nInstead of calculating with the actual values, you can use variables instead. The savings variable you created in the previous exercise with a value of 100 is available to you.\nHow much money would you have saved four months from now, if you saved $10 each month?\n\n#creating variables\nsavings=100\ngrowth_multiplier=1.1\n#creating result\nresult=savings*growth_multiplier**7\n\nprint(result)\n\n194.87171000000012"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#other-variable-types",
    "href": "datacamp/introduction_python/introduction_python.html#other-variable-types",
    "title": "Introduction to Python",
    "section": "Other variable types",
    "text": "Other variable types\nIn the previous exercise, you worked with the integer Python data type:\nint, or integer: a number without a fractional part. savings, with the value 100, is an example of an integer. Next to numerical data types, there are three other very common data types:\nfloat, or floating point: a number that has both an integer and fractional part, separated by a point. 1.1, is an example of a float. str, or string: a type to represent text. You can use single or double quotes to build a string. bool, or boolean: a type to represent logical values. It can only be True or False (the capitalization is important!).\n\n#creating string\ndesc=\"compound interest\"\nprint(desc)\n\ncompound interest\n\n#creating boolean\nprofitable=True\nprint(profitable)\n\nTrue"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#operations-with-other-types",
    "href": "datacamp/introduction_python/introduction_python.html#operations-with-other-types",
    "title": "Introduction to Python",
    "section": "Operations with other types",
    "text": "Operations with other types\nHugo mentioned that different types behave differently in Python.\nWhen you sum two strings, for example, you’ll get different behavior than when you sum two integers or two booleans.\nIn the script some variables with different types have already been created. It’s up to you to use them.\n\nsavings = 100\ngrowth_multiplier = 1.1\ndesc = \"compound interest\"\n\n# Assign product of savings and growth_multiplier to year1\nyear1 = savings * growth_multiplier\n\n# Print the type of year1\nprint(type(year1))\n\n<class 'float'>\n\n# Assign sum of desc and desc to doubledesc\ndoubledesc = desc + desc\n\n# Print out doubledesc\nprint(doubledesc)\n\ncompound interestcompound interest"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#type-conversion",
    "href": "datacamp/introduction_python/introduction_python.html#type-conversion",
    "title": "Introduction to Python",
    "section": "Type conversion",
    "text": "Type conversion\nUsing the + operator to paste together two strings can be very useful in building custom messages.\nSuppose, for example, that you’ve calculated your savings want to summarize the results in a string.\nTo do this, you’ll need to explicitly convert the types of your variables. More specifically, you’ll need str(), to convert a value into a string. str(savings), for example, will convert the integer savings to a string.\nSimilar functions such as int(), float() and bool() will help you convert Python values into any type.\n\n# Definition of savings and result\nsavings = 100\nresult = 100 * 1.10 ** 7\n\n# Fix the printout\nprint(\"I started with $\" + str(savings) + \" and now have $\" + str(result) + \". Awesome!\")\n\nI started with $100 and now have $194.87171000000012. Awesome!\n\n\n# Definition of pi_string\npi_string = \"3.1415926\"\n\n# Convert pi_string into float: pi_float\npi_float = float(pi_string)"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#create-a-list",
    "href": "datacamp/introduction_python/introduction_python.html#create-a-list",
    "title": "Introduction to Python",
    "section": "Create a list",
    "text": "Create a list\nAs opposed to int, bool etc., a list is a compound data type; you can group values together:\na = “is” b = “nice” my_list = [“my”, “list”, a, b] After measuring the height of your family, you decide to collect some information on the house you’re living in. The areas of the different parts of your house are stored in separate variables for now, as shown in the script.\n\n# Area variables (in square meters)\nhall = 11.25\nkit = 18.0\nliv = 20.0\nbed = 10.75\nbath = 9.50\n\n# Create list areas\nareas = [hall, kit, liv, bed, bath]\n\n# Print areas\nprint(areas)\n\n[11.25, 18.0, 20.0, 10.75, 9.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#create-list-with-different-types",
    "href": "datacamp/introduction_python/introduction_python.html#create-list-with-different-types",
    "title": "Introduction to Python",
    "section": "Create list with different types",
    "text": "Create list with different types\nA list can contain any Python type. Although it’s not really common, a list can also contain a mix of Python types including strings, floats, booleans, etc.\nThe printout of the previous exercise wasn’t really satisfying. It’s just a list of numbers representing the areas, but you can’t tell which area corresponds to which part of your house.\nThe code in the editor is the start of a solution. For some of the areas, the name of the corresponding room is already placed in front. Pay attention here! “bathroom” is a string, while bath is a variable that represents the float 9.50 you specified earlier.\n\n# area variables (in square meters)\nhall = 11.25\nkit = 18.0\nliv = 20.0\nbed = 10.75\nbath = 9.50\n\n# Adapt list areas\nareas = [\"hallway\", hall, \"kitchen\", kit, \"living room\", liv, \"bedroom\", bed, \"bathroom\", bath]\n\n# Print areas\nprint(areas)\n\n['hallway', 11.25, 'kitchen', 18.0, 'living room', 20.0, 'bedroom', 10.75, 'bathroom', 9.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#list-of-lists",
    "href": "datacamp/introduction_python/introduction_python.html#list-of-lists",
    "title": "Introduction to Python",
    "section": "List of lists",
    "text": "List of lists\nAs a data scientist, you’ll often be dealing with a lot of data, and it will make sense to group some of this data.\nInstead of creating a flat list containing strings and floats, representing the names and areas of the rooms in your house, you can create a list of lists. The script in the editor can already give you an idea.\nDon’t get confused here: “hallway” is a string, while hall is a variable that represents the float 11.25 you specified earlier.\n\n# area variables (in square meters)\nhall = 11.25\nkit = 18.0\nliv = 20.0\nbed = 10.75\nbath = 9.50\n\n# house information as list of lists\nhouse = [[\"hallway\", hall],\n         [\"kitchen\", kit],\n         [\"living room\", liv],\n         [\"bedroom\", bed],\n         [\"bathroom\", bath]]\n\n# Print out house\nprint(house)\n\n[['hallway', 11.25], ['kitchen', 18.0], ['living room', 20.0], ['bedroom', 10.75], ['bathroom', 9.5]]\n\n# Print out the type of house\nprint(type(house))\n\n<class 'list'>"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#subset-and-conquer",
    "href": "datacamp/introduction_python/introduction_python.html#subset-and-conquer",
    "title": "Introduction to Python",
    "section": "Subset and conquer",
    "text": "Subset and conquer\nSubsetting Python lists is a piece of cake. Take the code sample below, which creates a list x and then selects “b” from it. Remember that this is the second element, so it has index 1. You can also use negative indexing.\nx = [“a”, “b”, “c”, “d”] x[1] x[-3] # same result! Remember the areas list from before, containing both strings and floats? Its definition is already in the script. Can you add the correct code to do some Python subsetting?\n\n# Create the areas list\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"living room\", 20.0, \"bedroom\", 10.75, \"bathroom\", 9.50]\n\n# Print out second element from areas\nprint(areas[1])\n\n11.25\n\n# Print out last element from areas\nprint(areas[-1])\n\n9.5\n\n# Print out the area of the living room\nprint(areas[5])\n\n20.0"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#subset-and-calculate",
    "href": "datacamp/introduction_python/introduction_python.html#subset-and-calculate",
    "title": "Introduction to Python",
    "section": "Subset and calculate",
    "text": "Subset and calculate\nAfter you’ve extracted values from a list, you can use them to perform additional calculations. Take this example, where the second and fourth element of a list x are extracted. The strings that result are pasted together using the + operator:\n\n# Create the areas list\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"living room\", 20.0, \"bedroom\", 10.75, \"bathroom\", 9.50]\n\n# Sum of kitchen and bedroom area: eat_sleep_area\neat_sleep_area=areas[3]+ areas[7]\n\n# Print the variable eat_sleep_area\nprint(eat_sleep_area)\n\n28.75"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#slicing-and-dicing",
    "href": "datacamp/introduction_python/introduction_python.html#slicing-and-dicing",
    "title": "Introduction to Python",
    "section": "Slicing and dicing",
    "text": "Slicing and dicing\nSelecting single values from a list is just one part of the story. It’s also possible to slice your list, which means selecting multiple elements from your list. Use the following syntax:\nmy_list[start:end] The start index will be included, while the end index is not.\nThe code sample below shows an example. A list with “b” and “c”, corresponding to indexes 1 and 2, are selected from a list x:\nx = [“a”, “b”, “c”, “d”] x[1:3] The elements with index 1 and 2 are included, while the element with index 3 is not.\n\n# Create the areas list\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"living room\", 20.0, \"bedroom\", 10.75, \"bathroom\", 9.50]\n\n# Use slicing to create downstairs\ndownstairs = areas[0:6]\n\n# Use slicing to create upstairs\nupstairs = areas[6:10]\n\n# Print out downstairs and upstairs\nprint(downstairs)\n\n['hallway', 11.25, 'kitchen', 18.0, 'living room', 20.0]\n\nprint(upstairs)\n\n['bedroom', 10.75, 'bathroom', 9.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#slicing-and-dicing-2",
    "href": "datacamp/introduction_python/introduction_python.html#slicing-and-dicing-2",
    "title": "Introduction to Python",
    "section": "Slicing and dicing (2)",
    "text": "Slicing and dicing (2)\nIn the video, Hugo first discussed the syntax where you specify both where to begin and end the slice of your list:\nmy_list[begin:end] However, it’s also possible not to specify these indexes. If you don’t specify the begin index, Python figures out that you want to start your slice at the beginning of your list. If you don’t specify the end index, the slice will go all the way to the last element of your list. To experiment with this, try the following commands in the IPython Shell:\nx = [“a”, “b”, “c”, “d”] x[:2] x[2:] x[:]\n\n# Create the areas list\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"living room\", 20.0, \"bedroom\", 10.75, \"bathroom\", 9.50]\n\ndownstairs = areas[:6]\nupstairs = areas[6:]\nprint(downstairs)\n\n['hallway', 11.25, 'kitchen', 18.0, 'living room', 20.0]\n\nprint(upstairs)\n\n['bedroom', 10.75, 'bathroom', 9.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#replace-list-elements",
    "href": "datacamp/introduction_python/introduction_python.html#replace-list-elements",
    "title": "Introduction to Python",
    "section": "Replace list elements",
    "text": "Replace list elements\nReplacing list elements is pretty easy. Simply subset the list and assign new values to the subset. You can select single elements or you can change entire list slices at once.\nUse the IPython Shell to experiment with the commands below. Can you tell what’s happening and why?\nx = [“a”, “b”, “c”, “d”] x[1] = “r” x[2:] = [“s”, “t”] For this and the following exercises, you’ll continue working on the areas list that contains the names and areas of different rooms in a house.\n\n#experimenting\nx = [\"a\", \"b\", \"c\", \"d\"]\nx[1] = \"r\"\nx[2:]=[\"s\",\"t\"]\nprint(x)\n\n['a', 'r', 's', 't']\n\n#original list\n# Create the areas list\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"living room\", 20.0, \"bedroom\", 10.75, \"bathroom\", 9.50]\n#updating the area of the bathroom\nareas[-1]=10.5\n\n#changing living room\nareas[4]= \"chill zone\"\nprint(areas)\n\n['hallway', 11.25, 'kitchen', 18.0, 'chill zone', 20.0, 'bedroom', 10.75, 'bathroom', 10.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#extend-a-list",
    "href": "datacamp/introduction_python/introduction_python.html#extend-a-list",
    "title": "Introduction to Python",
    "section": "Extend a list",
    "text": "Extend a list\nIf you can change elements in a list, you sure want to be able to add elements to it, right? You can use the + operator:\nx = [“a”, “b”, “c”, “d”] y = x + [“e”, “f”] You just won the lottery, awesome! You decide to build a poolhouse and a garage. Can you add the information to the areas list?\n\n# Create the areas list (updated version)\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"chill zone\", 20.0,\n         \"bedroom\", 10.75, \"bathroom\", 10.50]\n\n# Add poolhouse data to areas, new list is areas_1\nareas_1 = areas + [\"poolhouse\", 24.5]\nprint(areas_1)\n\n['hallway', 11.25, 'kitchen', 18.0, 'chill zone', 20.0, 'bedroom', 10.75, 'bathroom', 10.5, 'poolhouse', 24.5]\n\n# Add garage data to areas_1, new list is areas_2\nareas_2 = areas_1 + [\"garage\", 15.45]\n\nDelete list elements Finally, you can also remove elements from your list. You can do this with the del statement:\nx = [“a”, “b”, “c”, “d”] del(x[1]) Pay attention here: as soon as you remove an element from a list, the indexes of the elements that come after the deleted element all change!\nThe updated and extended version of areas that you’ve built in the previous exercises is coded below. You can copy and paste this into the IPython Shell to play around with the result.\n\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0,\n        \"chill zone\", 20.0, \"bedroom\", 10.75,\n         \"bathroom\", 10.50, \"poolhouse\", 24.5,\n         \"garage\", 15.45]\n         \nprint(areas)\n\n['hallway', 11.25, 'kitchen', 18.0, 'chill zone', 20.0, 'bedroom', 10.75, 'bathroom', 10.5, 'poolhouse', 24.5, 'garage', 15.45]\n\n\nThere was a mistake! The amount you won with the lottery is not that big after all and it looks like the poolhouse isn’t going to happen. You decide to remove the corresponding string and float from the areas list.\nThe ; sign is used to place commands on the same line. The following two code chunks are equivalent:\nWhich of the code chunks will do the job for us?\n\ndel(areas[-4:-2])\nprint(areas)\n\n['hallway', 11.25, 'kitchen', 18.0, 'chill zone', 20.0, 'bedroom', 10.75, 'bathroom', 10.5, 'garage', 15.45]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#inner-workings-of-lists",
    "href": "datacamp/introduction_python/introduction_python.html#inner-workings-of-lists",
    "title": "Introduction to Python",
    "section": "Inner workings of lists",
    "text": "Inner workings of lists\nAt the end of the video, Hugo explained how Python lists work behind the scenes. In this exercise you’ll get some hands-on experience with this.\nThe Python code in the script already creates a list with the name areas and a copy named areas_copy. Next, the first element in the areas_copy list is changed and the areas list is printed out. If you hit Run Code you’ll see that, although you’ve changed areas_copy, the change also takes effect in the areas list. That’s because areas and areas_copy point to the same list.\nIf you want to prevent changes in areas_copy from also taking effect in areas, you’ll have to do a more explicit copy of the areas list. You can do this with list() or by using [:].\n\n# Create list areas\nareas = [11.25, 18.0, 20.0, 10.75, 9.50]\n\n# Create areas_copy\nareas_copy = areas[:]\n# Change areas_copy\nareas_copy[0] = 5.0\n\n# Print areas\nprint(areas)\n\n[11.25, 18.0, 20.0, 10.75, 9.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#familiar-functions",
    "href": "datacamp/introduction_python/introduction_python.html#familiar-functions",
    "title": "Introduction to Python",
    "section": "Familiar functions",
    "text": "Familiar functions\nOut of the box, Python offers a bunch of built-in functions to make your life as a data scientist easier. You already know two such functions: print() and type(). You’ve also used the functions str(), int(), bool() and float() to switch between data types. These are built-in functions as well.\nCalling a function is easy. To get the type of 3.0 and store the output as a new variable, result, you can use the following:\nresult = type(3.0) The general recipe for calling functions and saving the result to a variable is thus:\noutput = function_name(input)\n\n# Create variables var1 and var2\nvar1 = [1, 2, 3, 4]\nvar2 = True\n\n# Print out type of var1\nprint(type(var1))\n\n<class 'list'>\n\n# Print out length of var1\nprint(len(var1))\n\n4\n\n\n# Convert var2 to an integer: out2\nout2 = int(var2)"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#help",
    "href": "datacamp/introduction_python/introduction_python.html#help",
    "title": "Introduction to Python",
    "section": "Help!",
    "text": "Help!\nMaybe you already know the name of a Python function, but you still have to figure out how to use it. Ironically, you have to ask for information about a function with another function: help(). In IPython specifically, you can also use ? before the function name.\nTo get help on the max() function, for example, you can use one of these calls:\n\nhelp(max)\n\nHelp on built-in function max in module builtins:\n\nmax(...)\n    max(iterable, *[, default=obj, key=func]) -> value\n    max(arg1, arg2, *args, *[, key=func]) -> value\n    \n    With a single iterable argument, return its biggest item. The\n    default keyword-only argument specifies an object to return if\n    the provided iterable is empty.\n    With two or more arguments, return the largest argument."
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#multiple-arguments",
    "href": "datacamp/introduction_python/introduction_python.html#multiple-arguments",
    "title": "Introduction to Python",
    "section": "Multiple arguments",
    "text": "Multiple arguments\nIn the previous exercise, you identified optional arguments by viewing the documentation with help(). You’ll now apply this to change the behavior of the sorted() function.\nHave a look at the documentation of sorted() by typing help(sorted) in the IPython Shell.\nYou’ll see that sorted() takes three arguments: iterable, key, and reverse.\nkey=None means that if you don’t specify the key argument, it will be None. reverse=False means that if you don’t specify the reverse argument, it will be False, by default.\nIn this exercise, you’ll only have to specify iterable and reverse, not key. The first input you pass to sorted() will be matched to the iterable argument, but what about the second input? To tell Python you want to specify reverse without changing anything about key, you can use = to assign it a new value:\nsorted(____, reverse=____) Two lists have been created for you. Can you paste them together and sort them in descending order?\nNote: For now, we can understand an iterable as being any collection of objects, e.g., a List.\n\n# Create lists first and second\nfirst = [11.25, 18.0, 20.0]\nsecond = [10.75, 9.50]\n\n# Paste together first and second: full\nfull=first + second\nprint(full)\n\n[11.25, 18.0, 20.0, 10.75, 9.5]\n\n# Sort full in descending order: full_sorted\nfull_sorted=(sorted(full,reverse=True))\n\n# Print out full_sorted\nprint(full_sorted)\n\n[20.0, 18.0, 11.25, 10.75, 9.5]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#string-methods",
    "href": "datacamp/introduction_python/introduction_python.html#string-methods",
    "title": "Introduction to Python",
    "section": "String Methods",
    "text": "String Methods\nStrings come with a bunch of methods. Follow the instructions closely to discover some of them. If you want to discover them in more detail, you can always type help(str) in the IPython Shell.\nA string place has already been created for you to experiment with.\n\n# string to experiment with: place\nplace = \"poolhouse\"\n\n# Use upper() on place: place_up\nplace_up = place.upper()\n\n# Print out place and place_up\nprint(place)\n\npoolhouse\n\nprint(place_up)\n\nPOOLHOUSE\n\n# Print out the number of o's in place\nprint(place.count('o'))\n\n3"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#list-methods",
    "href": "datacamp/introduction_python/introduction_python.html#list-methods",
    "title": "Introduction to Python",
    "section": "List Methods",
    "text": "List Methods\nStrings are not the only Python types that have methods associated with them. Lists, floats, integers and booleans are also types that come packaged with a bunch of useful methods. In this exercise, you’ll be experimenting with:\n\nindex(), to get the index of the first element of a list that matches its input and\ncount(), to get the number of times an element appears in a list. You’ll be working on the list with the area of different parts of a house: areas.\nappend(), that adds an element to the list it is called on,\nremove(), that removes the first element of a list that matches the input, and\nreverse(), that reverses the order of the elements in the list it is called on.\n\n\n# Create list areas\nareas = [11.25, 18.0, 20.0, 10.75, 9.50]\n\n# Print out the index of the element 20.0\nprint(areas.index(20.0))\n\n2\n\n# Print out how often 9.50 appears in areas\nprint(areas.count(9.50))\n\n1\n\n# Create list areas\nareas = [11.25, 18.0, 20.0, 10.75, 9.50]\n\n# Use append twice to add poolhouse and garage size\nareas.append(24.5)\nareas.append(15.45)\n\n# Print out areas\nprint(areas)\n\n[11.25, 18.0, 20.0, 10.75, 9.5, 24.5, 15.45]\n\n# Reverse the orders of the elements in areas\nareas.reverse()\n\n# Print out areas\nprint(areas)\n\n[15.45, 24.5, 9.5, 10.75, 20.0, 18.0, 11.25]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#import-package",
    "href": "datacamp/introduction_python/introduction_python.html#import-package",
    "title": "Introduction to Python",
    "section": "Import package",
    "text": "Import package\nAs a data scientist, some notions of geometry never hurt. Let’s refresh some of the basics.\nFor a fancy clustering algorithm, you want to find the circumference, , and area, , of a circle. When the radius of the circle is r, you can calculate and as:\nIn Python, the symbol for exponentiation is . This operator raises the number to its left to the power of the number to its right. For example 34 is 3 to the power of 4 and will give 81.\nTo use the constant pi, you’ll need the math package. A variable r is already coded in the script. Fill in the code to calculate C and A and see how the print() functions create some nice printouts.\n\n# Definition of radius\nr = 0.43\n\n# Import the math package\nimport math\n\n# Calculate C\nC = 2 * r * math.pi\n\n# Calculate A\nA = math.pi * r ** 2\n\n# Build printout\nprint(\"Circumference: \" + str(C))\n\nCircumference: 2.701769682087222\n\nprint(\"Area: \" + str(A))\n\nArea: 0.5808804816487527"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#selective-import",
    "href": "datacamp/introduction_python/introduction_python.html#selective-import",
    "title": "Introduction to Python",
    "section": "Selective import",
    "text": "Selective import\nGeneral imports, like import math, make all functionality from the math package available to you. However, if you decide to only use a specific part of a package, you can always make your import more selective:\nfrom math import pi Let’s say the Moon’s orbit around planet Earth is a perfect circle, with a radius r (in km) that is defined in the script.\n\n# Definition of radius\nr = 192500\n\n# Import radians function of math package\nfrom math import radians\n\n# Travel distance of Moon over 12 degrees. Store in dist.\n\ndist = r*  radians(12)\n\n# Print out dist\nprint(dist)\n\n40317.10572106901"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#different-ways-of-importing",
    "href": "datacamp/introduction_python/introduction_python.html#different-ways-of-importing",
    "title": "Introduction to Python",
    "section": "Different ways of importing",
    "text": "Different ways of importing\nThere are several ways to import packages and modules into Python. Depending on the import call, you’ll have to use different Python code.\nSuppose you want to use the function inv(), which is in the linalg subpackage of the scipy package. You want to be able to use this function as follows:\nmy_inv([[1,2], [3,4]]) Which import statement will you need in order to run the above code without an error?\n\nfrom scipy.linalg import inv as my_inv\n\nmy_inv([[1,2], [3,4]])\n\narray([[-2. ,  1. ],\n       [ 1.5, -0.5]])"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#your-first-numpy-array",
    "href": "datacamp/introduction_python/introduction_python.html#your-first-numpy-array",
    "title": "Introduction to Python",
    "section": "Your First NumPy Array",
    "text": "Your First NumPy Array\nIn this chapter, we’re going to dive into the world of baseball. Along the way, you’ll get comfortable with the basics of numpy, a powerful package to do data science.\nA list baseball has already been defined in the Python script, representing the height of some baseball players in centimeters. Can you add some code here and there to create a numpy array from it?\n\n# Create list baseball\nbaseball = [180, 215, 210, 210, 188, 176, 209, 200]\n\n# Import the numpy package as np\nimport numpy as np\n\n# Create a NumPy array from baseball: np_baseball\nnp_baseball = np.array(baseball)\nnp_baseball\n\narray([180, 215, 210, 210, 188, 176, 209, 200])\n\n# Print out type of np_baseball\nprint(type(np_baseball))\n\n<class 'numpy.ndarray'>"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#baseball-players-height",
    "href": "datacamp/introduction_python/introduction_python.html#baseball-players-height",
    "title": "Introduction to Python",
    "section": "Baseball players’ height",
    "text": "Baseball players’ height\nYou are a huge baseball fan. You decide to call the MLB (Major League Baseball) and ask around for some more statistics on the height of the main players. They pass along data on more than a thousand players, which is stored as a regular Python list: height_in. The height is expressed in inches. Can you make a numpy array out of it and convert the units to meters?\nheight_in is already available and the numpy package is loaded, so you can start straight away (Source: stat.ucla.edu).\n\n# height is available as a regular list\n\n# Import numpy\nimport numpy as np\n\nheight_in = np.array([74, 74, 72, 72, 73, 69, 69, 71, 76, 71])\n# Create a numpy array from height_in: np_height_in\nnp_height_in=np.array(height_in)\n# Print out np_height_in\nprint(np_height_in)\n\n[74 74 72 72 73 69 69 71 76 71]\n\n# Convert np_height_in to m: np_height_m\nnp_height_m = np_height_in*0.0254\n\n# Print np_height_m\nprint(np_height_m)\n\n[1.8796 1.8796 1.8288 1.8288 1.8542 1.7526 1.7526 1.8034 1.9304 1.8034]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#baseball-players-bmi",
    "href": "datacamp/introduction_python/introduction_python.html#baseball-players-bmi",
    "title": "Introduction to Python",
    "section": "Baseball player’s BMI",
    "text": "Baseball player’s BMI\nThe MLB also offers to let you analyze their weight data. Again, both are available as regular Python lists: height_in and weight_lb. height_in is in inches and weight_lb is in pounds.\nIt’s now possible to calculate the BMI of each baseball player. Python code to convert height_in to a numpy array with the correct units is already available in the workspace. Follow the instructions step by step and finish the game! height_in and weight_lb are available as regular lists.\n\n# height_in and weight_lb are available as regular lists\n\n# Import numpy\nimport numpy as np\n\n# Create array from height_in with metric units: np_height_m\nnp_height_m = np.array(height_in) * 0.0254\nweight_lb = np.array([180, 215, 210, 210, 188, 176, 209, 200, 231, 180])\n# Create array from weight_lb with metric units: np_weight_kg\nnp_weight_kg = np.array(weight_lb) * 0.453592\n\n# Calculate the BMI: bmi\nbmi = np_weight_kg / np_height_m ** 2\n\n# Print out bmi\nprint(bmi)\n\n[23.11037639 27.60406069 28.48080465 28.48080465 24.80333518 25.99036864\n 30.86356276 27.89402921 28.11789135 25.10462629]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#lightweight-baseball-players",
    "href": "datacamp/introduction_python/introduction_python.html#lightweight-baseball-players",
    "title": "Introduction to Python",
    "section": "Lightweight baseball players",
    "text": "Lightweight baseball players\nTo subset both regular Python lists and numpy arrays, you can use square brackets:\nx = [4 , 9 , 6, 3, 1] x[1] import numpy as np y = np.array(x) y[1] For numpy specifically, you can also use boolean numpy arrays:\nhigh = y > 5 y[high] The code that calculates the BMI of all baseball players is already included. Follow the instructions and reveal interesting things from the data! height_in and weight_lb are available as regular lists.\n\n# height and weight are available as a regular lists\n\n# Import numpy\nimport numpy as np\n\n# Calculate the BMI: bmi\nnp_height_m = np.array(height_in) * 0.0254\nnp_weight_kg = np.array(weight_lb) * 0.453592\nbmi = np_weight_kg / np_height_m ** 2\n\n# Create the light array\n\nlight = bmi < 21\n# Print out light\nprint(light)\n\n[False False False False False False False False False False]\n\n# Print out BMIs of all baseball players whose BMI is below 21\nprint(bmi[light])\n\n[]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#numpy-side-effects",
    "href": "datacamp/introduction_python/introduction_python.html#numpy-side-effects",
    "title": "Introduction to Python",
    "section": "NumPy Side Effects",
    "text": "NumPy Side Effects\nAs Hugo explained before, numpy is great for doing vector arithmetic. If you compare its functionality with regular Python lists, however, some things have changed.\nFirst of all, numpy arrays cannot contain elements with different types. If you try to build such a list, some of the elements’ types are changed to end up with a homogeneous list. This is known as type coercion.\nSecond, the typical arithmetic operators, such as +, -, * and / have a different meaning for regular Python lists and numpy arrays.\nHave a look at this line of code:\nnp.array([True, 1, 2]) + np.array([3, 4, False]) Can you tell which code chunk builds the exact same Python object? The numpy package is already imported as np, so you can start experimenting in the IPython Shell straight away!\n\nprint(np.array([True, 1, 2]) + np.array([3, 4, False]))\n\n[4 5 2]\n\nprint(np.array([4, 3, 0]) + np.array([0, 2, 2]))\n\n[4 5 2]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#subsetting-numpy-arrays",
    "href": "datacamp/introduction_python/introduction_python.html#subsetting-numpy-arrays",
    "title": "Introduction to Python",
    "section": "Subsetting NumPy Arrays",
    "text": "Subsetting NumPy Arrays\nYou’ve seen it with your own eyes: Python lists and numpy arrays sometimes behave differently. Luckily, there are still certainties in this world. For example, subsetting (using the square bracket notation on lists or arrays) works exactly the same. To see this for yourself, try the following lines of code in the IPython Shell:\nx = [“a”, “b”, “c”] x[1]\nnp_x = np.array(x) np_x[1] The script in the editor already contains code that imports numpy as np, and stores both the height and weight of the MLB players as numpy arrays. height_in and weight_lb are available as regular lists.\n\n# height and weight are available as a regular lists\n\n# Import numpy\nimport numpy as np\n\n# Store weight and height lists as numpy arrays\nnp_weight_lb = np.array(weight_lb)\nnp_height_in = np.array(height_in)\n\n# Print out the weight at index 5\n\nprint(np_weight_lb[5])\n\n176\n\n# Print out sub-array of np_height_in: index 1 up to and including index 3\n\nprint(np_height_in[1:3])\n\n[74 72]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#your-first-2d-numpy-array",
    "href": "datacamp/introduction_python/introduction_python.html#your-first-2d-numpy-array",
    "title": "Introduction to Python",
    "section": "Your First 2D NumPy Array",
    "text": "Your First 2D NumPy Array\nBefore working on the actual MLB data, let’s try to create a 2D numpy array from a small list of lists.\nIn this exercise, baseball is a list of lists. The main list contains 4 elements. Each of these elements is a list containing the height and the weight of 4 baseball players, in this order. baseball is already coded for you in the script.\n\n# Create baseball, a list of lists\nbaseball = [[180, 78.4],\n            [215, 102.7],\n            [210, 98.5],\n            [188, 75.2]]\n\n# Import numpy\nimport numpy as np\n\n# Create a 2D numpy array from baseball: np_baseball\nnp_baseball = np.array(baseball)\n\n# Print out the type of np_baseball\nprint(type(np_baseball))\n\n<class 'numpy.ndarray'>\n\n# Print out the shape of np_baseball\nprint(np_baseball.shape)\n\n(4, 2)"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#baseball-data-in-2d-form",
    "href": "datacamp/introduction_python/introduction_python.html#baseball-data-in-2d-form",
    "title": "Introduction to Python",
    "section": "Baseball data in 2D form",
    "text": "Baseball data in 2D form\nYou have another look at the MLB data and realize that it makes more sense to restructure all this information in a 2D numpy array. This array should have 1015 rows, corresponding to the 1015 baseball players you have information on, and 2 columns (for height and weight).\nThe MLB was, again, very helpful and passed you the data in a different structure, a Python list of lists. In this list of lists, each sublist represents the height and weight of a single baseball player. The name of this embedded list is baseball.\nCan you store the data as a 2D array to unlock numpy’s extra functionality? baseball is available as a regular list of lists.\n\n# baseball is available as a regular list of lists\n\n# Import numpy package\nimport numpy as np\n\n# Create a 2D numpy array from baseball: np_baseball\nnp_baseball = np.array(baseball)\n\n# Print out the shape of np_baseball\nprint(np_baseball.shape)\n\n(4, 2)"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#subsetting-2d-numpy-arrays",
    "href": "datacamp/introduction_python/introduction_python.html#subsetting-2d-numpy-arrays",
    "title": "Introduction to Python",
    "section": "Subsetting 2D NumPy Arrays",
    "text": "Subsetting 2D NumPy Arrays\nIf your 2D numpy array has a regular structure, i.e. each row and column has a fixed number of values, complicated ways of subsetting become very easy. Have a look at the code below where the elements “a” and “c” are extracted from a list of lists."
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#d-arithmetic",
    "href": "datacamp/introduction_python/introduction_python.html#d-arithmetic",
    "title": "Introduction to Python",
    "section": "2D Arithmetic",
    "text": "2D Arithmetic\nRemember how you calculated the Body Mass Index for all baseball players? numpy was able to perform all calculations element-wise (i.e. element by element). For 2D numpy arrays this isn’t any different! You can combine matrices with single numbers, with vectors, and with other matrices.\nExecute the code below in the IPython shell and see if you understand:\nimport numpy as np np_mat = np.array([[1, 2], [3, 4], [5, 6]]) np_mat * 2 np_mat + np.array([10, 10]) np_mat + np_mat np_baseball is coded for you; it’s again a 2D numpy array with 3 columns representing height (in inches), weight (in pounds) and age (in years). baseball is available as a regular list of lists and updated is available as 2D numpy array.\n\n# baseball is available as a regular list of lists\n# updated is available as 2D numpy array\n\n# Import numpy package\nimport numpy as np\n\n# Create np_baseball (3 cols)\nnp_baseball = np.array(baseball)\n\n# Print out addition of np_baseball and updated\nprint( np_baseball + updated)\n\n# Create numpy array: conversion\nconversion = np.array([0.0254, 0.453592, 1])\n\n# Print out product of np_baseball and conversion\nprint(np_baseball * conversion)"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#average-versus-median",
    "href": "datacamp/introduction_python/introduction_python.html#average-versus-median",
    "title": "Introduction to Python",
    "section": "Average versus median",
    "text": "Average versus median\nYou now know how to use numpy functions to get a better feeling for your data. It basically comes down to importing numpy and then calling several simple functions on the numpy arrays:\nimport numpy as np x = [1, 4, 8, 10, 12] np.mean(x) np.median(x) The baseball data is available as a 2D numpy array with 3 columns (height, weight, age) and 1015 rows. The name of this numpy array is np_baseball. After restructuring the data, however, you notice that some height values are abnormally high. Follow the instructions and discover which summary statistic is best suited if you’re dealing with so-called outliers. np_baseball is available.\n\n# np_baseball is available\n\n# Import numpy\nimport numpy as np\n\n# Create np_height_in from np_baseball\nnp_height_in = np_baseball[:,0]\n\n# Print out the mean of np_height_in\n\nprint(np.mean(np_height_in))\n\n198.25\n\n# Print out the median of np_height_in\nprint(np.median(np_height_in))\n\n199.0"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#explore-the-baseball-data",
    "href": "datacamp/introduction_python/introduction_python.html#explore-the-baseball-data",
    "title": "Introduction to Python",
    "section": "Explore the baseball data",
    "text": "Explore the baseball data\nBecause the mean and median are so far apart, you decide to complain to the MLB. They find the error and send the corrected data over to you. It’s again available as a 2D NumPy array np_baseball, with three columns.\nThe Python script in the editor already includes code to print out informative messages with the different summary statistics. Can you finish the job? np_baseball is available.\n\n# np_baseball is available\n\n# Import numpy\nimport numpy as np\n\n# Print mean height (first column)\navg = np.mean(np_baseball[:,0])\nprint(\"Average: \" + str(avg))\n\nAverage: 198.25\n\n# Print median height. Replace 'None'\nmed = np.median(np_baseball[:,0])\nprint(\"Median: \" + str(med))\n\nMedian: 199.0\n\n# Print out the standard deviation on height. Replace 'None'\nstddev = np.std(np_baseball[:,0])\nprint(\"Standard Deviation: \" + str(stddev))\n\nStandard Deviation: 14.635146053251399\n\n# Print out correlation between first and second column. Replace 'None'\ncorr = np.corrcoef(np_baseball[:, 0], np_baseball[:, 1])\nprint(\"Correlation: \" + str(corr))\n\nCorrelation: [[1.         0.95865738]\n [0.95865738 1.        ]]"
  },
  {
    "objectID": "datacamp/introduction_python/introduction_python.html#blend-it-all-together",
    "href": "datacamp/introduction_python/introduction_python.html#blend-it-all-together",
    "title": "Introduction to Python",
    "section": "Blend it all together",
    "text": "Blend it all together\nIn the last few exercises you’ve learned everything there is to know about heights and weights of baseball players. Now it’s time to dive into another sport: soccer.\nYou’ve contacted FIFA for some data and they handed you two lists. The lists are the following:\npositions = [‘GK’, ‘M’, ‘A’, ‘D’, …] heights = [191, 184, 185, 180, …] Each element in the lists corresponds to a player. The first list, positions, contains strings representing each player’s position. The possible positions are: ‘GK’ (goalkeeper), ‘M’ (midfield), ‘A’ (attack) and ‘D’ (defense). The second list, heights, contains integers representing the height of the player in cm. The first player in the lists is a goalkeeper and is pretty tall (191 cm).\nYou’re fairly confident that the median height of goalkeepers is higher than that of other players on the soccer field. Some of your friends don’t believe you, so you are determined to show them using the data you received from FIFA and your newly acquired Python skills. heights and positions are available as lists\n\nnp_positions = np.array(['GK', 'M', 'A', 'D', 'M', 'D', 'M', 'M', 'M', 'A'])\nnp_heights = np.array([191, 184, 185, 180, 181, 187, 170, 179, 183, 186])\n\n# Heights of the goalkeepers: gk_heights\ngk_heights = np_heights[np_positions == 'GK']\n\n# Heights of the other players: other_heights\nother_heights = np_heights[np_positions != 'GK']\n\n# Print out the median height of goalkeepers. Replace 'None'\nprint(\"Median height of goalkeepers: \" + str(np.median(gk_heights)))\n\nMedian height of goalkeepers: 191.0\n\n# Print out the median height of other players. Replace 'None'\nprint(\"Median height of other players: \" + str(np.median(other_heights)))\n\nMedian height of other players: 183.0"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html",
    "href": "datacamp/intermediatePython/IntermediatePython.html",
    "title": "Intermediate Python",
    "section": "",
    "text": "With matplotlib, you can create a bunch of different plots in Python. The most basic plot is the line plot. A general recipe is given here.\nimport matplotlib.pyplot as plt plt.plot(x,y) plt.show() In the video, you already saw how much the world population has grown over the past years. Will it continue to do so? The world bank has estimates of the world population for the years 1950 up to 2100. The years are loaded in your workspace as a list called year, and the corresponding populations as a list called pop.\nThis course touches on a lot of concepts you may have forgotten, so if you ever need a quick refresher, download the Python For Data Science Cheat Sheet and keep it handy!\n\n# Print the last item from year and pop\n\nyear = [1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, \n        1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, \n        1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, \n        1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, \n        1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, \n        2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, \n        2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, \n        2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, \n        2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, \n        2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, \n        2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, \n        2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, \n        2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, \n        2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100]\n\npop = [2.53, 2.57, 2.62, 2.67, 2.71, 2.76, 2.81, 2.86, 2.92, 2.97, 3.03, \n      3.08, 3.14, 3.2, 3.26, 3.33, 3.4, 3.47, 3.54, 3.62, 3.69, 3.77,\n      3.84, 3.92, 4, 4.07, 4.15, 4.22, 4.3, 4.37, 4.45, 4.53, 4.61, \n      4.69, 4.78, 4.86, 4.95, 5.05, 5.14, 5.23, 5.32, 5.41, 5.49, \n      5.58, 5.66, 5.74, 5.82, 5.9, 5.98, 6.05, 6.13, 6.2, 6.28, 6.36,\n      6.44, 6.51, 6.59, 6.67, 6.75, 6.83, 6.92, 7, 7.08, 7.16, 7.24, \n      7.32, 7.4, 7.48, 7.56, 7.64, 7.72, 7.79, 7.87, 7.94, 8.01, 8.08, \n      8.15, 8.22, 8.29, 8.36, 8.42, 8.49, 8.56, 8.62, 8.68, 8.74, 8.8, \n      8.86, 8.92, 8.98, 9.04, 9.09, 9.15, 9.2, 9.26, 9.31, 9.36, 9.41, \n      9.46, 9.5, 9.55, 9.6, 9.64, 9.68, 9.73, 9.77, 9.81, 9.85, 9.88, 9.92, \n      9.96, 9.99, 10.03, 10.06, 10.09, 10.13, 10.16, 10.19, 10.22, 10.25, \n      10.28, 10.31, 10.33, 10.36, 10.38, 10.41, 10.43, 10.46, 10.48, 10.5, \n      10.52, 10.55, 10.57, 10.59, 10.61, 10.63, 10.65, 10.66, 10.68, 10.7, \n      10.72, 10.73, 10.75, 10.77, 10.78, 10.79, 10.81, 10.82, 10.83, 10.84, 10.85]\n\nprint(year[-1])\n\n2100\n\nprint(pop[-1])\n\n10.85\n\n# Import matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\n\n# Make a line plot: year on the x-axis, pop on the y-axis\n\nplt.plot(year, pop)\n# Display the plot with plt.show()\n\nplt.show()\n\n\n\n\n\n\n\nNow that you’ve built your first line plot, let’s start working on the data that professor Hans Rosling used to build his beautiful bubble chart. It was collected in 2007. Two lists are available for you:\nlife_exp which contains the life expectancy for each country and gdp_cap, which contains the GDP per capita (i.e. per person) for each country expressed in US Dollars. GDP stands for Gross Domestic Product. It basically represents the size of the economy of a country. Divide this by the population and you get the GDP per capita.\nmatplotlib.pyplot is already imported as plt, so you can get started straight away.\n\n# Print the last item of gdp_cap and life_exp\n\ngdp_cap = [974.5803384, 5937.029526, 6223.367465, 4797.231267, 12779.37964, \n            34435.36744, 36126.4927, 29796.04834, 1391.253792, 33692.60508, \n            1441.284873, 3822.137084, 7446.298803, 12569.85177, 9065.800825, \n            10680.79282, 1217.032994, 430.0706916, 1713.778686, 2042.09524, \n            36319.23501, 706.016537, 1704.063724, 13171.63885, 4959.114854, \n            7006.580419, 986.1478792, 277.5518587, 3632.557798, 9645.06142, \n            1544.750112, 14619.22272, 8948.102923, 22833.30851, 35278.41874, \n            2082.481567, 6025.374752, 6873.262326, 5581.180998, 5728.353514, \n            12154.08975, 641.3695236, 690.8055759, 33207.0844, 30470.0167, \n            13206.48452, 752.7497265, 32170.37442, 1327.60891, 27538.41188, \n            5186.050003, 942.6542111, 579.231743, 1201.637154, 3548.330846, \n            39724.97867, 18008.94444, 36180.78919, 2452.210407, 3540.651564, \n            11605.71449, 4471.061906, 40675.99635, 25523.2771, 28569.7197, \n            7320.880262, 31656.06806, 4519.461171, 1463.249282, 1593.06548, \n            23348.13973, 47306.98978, 10461.05868, 1569.331442, 414.5073415, \n            12057.49928, 1044.770126, 759.3499101, 12451.6558, 1042.581557, \n            1803.151496, 10956.99112, 11977.57496, 3095.772271, 9253.896111, \n            3820.17523, 823.6856205, 944, 4811.060429, 1091.359778, 36797.93332, \n            25185.00911, 2749.320965, 619.6768924, 2013.977305, 49357.19017, \n            22316.19287, 2605.94758, 9809.185636, 4172.838464, 7408.905561, \n            3190.481016, 15389.92468, 20509.64777, 19328.70901, 7670.122558, \n            10808.47561, 863.0884639, 1598.435089, 21654.83194, 1712.472136, \n            9786.534714, 862.5407561, 47143.17964, 18678.31435, 25768.25759, \n            926.1410683, 9269.657808, 28821.0637, 3970.095407, 2602.394995, \n            4513.480643, 33859.74835, 37506.41907, 4184.548089, 28718.27684, \n            1107.482182, 7458.396327, 882.9699438, 18008.50924, 7092.923025, \n            8458.276384, 1056.380121, 33203.26128, 42951.65309, 10611.46299, \n            11415.80569, 2441.576404, 3025.349798, 2280.769906, 1271.211593, \n            469.7092981]\n            \n            \n\nlife_exp = [43.828, 76.423, 72.301, 42.731, 75.32, 81.235, 79.829, 75.635, \n             64.062, 79.441, 56.728, 65.554, 74.852, 50.728, 72.39, 73.005, \n             52.295, 49.58, 59.723, 50.43, 80.653, 44.741, 50.651, 78.553, \n             72.961, 72.889, 65.152, 46.462, 55.322, 78.782, 48.328, 75.748, \n             78.273, 76.486, 78.332, 54.791, 72.235, 74.994, 71.338, 71.878, \n             51.579, 58.04, 52.947, 79.313, 80.657, 56.735, 59.448, 79.406, \n             60.022, 79.483, 70.259, 56.007, 46.388, 60.916, 70.198, 82.208, \n             73.338, 81.757, 64.698, 70.65, 70.964, 59.545, 78.885, 80.745, \n             80.546, 72.567, 82.603, 72.535, 54.11, 67.297, 78.623, 77.588, \n             71.993, 42.592, 45.678, 73.952, 59.443, 48.303, 74.241, 54.467, \n             64.164, 72.801, 76.195, 66.803, 74.543, 71.164, 42.082, 62.069, \n             52.906, 63.785, 79.762, 80.204, 72.899, 56.867, 46.859, 80.196, \n             75.64, 65.483, 75.537, 71.752, 71.421, 71.688, 75.563, 78.098, \n             78.746, 76.442, 72.476, 46.242, 65.528, 72.777, 63.062, 74.002, \n             42.568, 79.972, 74.663, 77.926, 48.159, 49.339, 80.941, 72.396, \n             58.556, 39.613, 80.884, 81.701, 74.143, 78.4, 52.517, 70.616, \n             58.42, 69.819, 73.923, 71.777, 51.542, 79.425, 78.242, 76.384, \n             73.747, 74.249, 73.422, 62.698, 42.384, 43.487]\n\n\nprint(gdp_cap[-1])\n\n469.7092981\n\nprint(life_exp[-1])\n\n43.487\n\nplt.plot(gdp_cap, life_exp)\n\n# Display the plot\n\nplt.show()\n\n\n\n\n\n\n\nWhen you have a time scale along the horizontal axis, the line plot is your friend. But in many other cases, when you’re trying to assess if there’s a correlation between two variables, for example, the scatter plot is the better choice. Below is an example of how to build a scatter plot.\nimport matplotlib.pyplot as plt plt.scatter(x,y) plt.show() Let’s continue with the gdp_cap versus life_exp plot, the GDP and life expectancy data for different countries in 2007. Maybe a scatter plot will be a better alternative?\n\n# Change the line plot below to a scatter plot\nplt.scatter(gdp_cap, life_exp)\n\n# Put the x-axis on a logarithmic scale\nplt.xscale('log')\n# Show plot.\nplt.show()\n\n\n\n\n\n\n\nIn the previous exercise, you saw that the higher GDP usually corresponds to a higher life expectancy. In other words, there is a positive correlation.\nDo you think there’s a relationship between population and life expectancy of a country? The list life_exp from the previous exercise is already available. In addition, now also pop is available, listing the corresponding populations for the countries in 2007. The populations are in millions of people.\n\n# Build Scatter plot\npop = pop[0:142]\nplt.scatter(pop[0:142], life_exp)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\nlife_exp, the list containing data on the life expectancy for different countries in 2007, is available in your Python shell.\nTo see how life expectancy in different countries is distributed, let’s create a histogram of life_exp.\nmatplotlib.pyplot is already available as plt.\n\n# Create histogram of life_exp data\nplt.hist(life_exp)\n\n# Display histogram\nplt.show()\n\n\n\n\n\n\n\nIn the previous exercise, you didn’t specify the number of bins. By default, Python sets the number of bins to 10 in that case. The number of bins is pretty important. Too few bins will oversimplify reality and won’t show you the details. Too many bins will overcomplicate reality and won’t show the bigger picture.\nTo control the number of bins to divide your data in, you can set the bins argument.\nThat’s exactly what you’ll do in this exercise. You’ll be making two plots here. The code in the script already includes plt.show() and plt.clf() calls; plt.show() displays a plot; plt.clf() cleans it up again so you can start afresh.\nAs before, life_exp is available and matplotlib.pyplot is imported as plt.\n\n# Build histogram with 5 bins\nplt.hist(life_exp, bins= 5)\n\n# Show and clean up plot\nplt.show()\n\n\n\nplt.clf()\n\n# Build histogram with 20 bins\n\nplt.hist(life_exp, bins= 20)\n\n# Show and clean up again\nplt.show()\n\n\n\n#plt.clf()\n\n\n\n\nIn the video, you saw population pyramids for the present day and for the future. Because we were using a histogram, it was very easy to make a comparison.\nLet’s do a similar comparison. life_exp contains life expectancy data for different countries in 2007. You also have access to a second list now, life_exp1950, containing similar data for 1950. Can you make a histogram for both datasets?\nYou’ll again be making two plots. The plt.show() and plt.clf() commands to render everything nicely are already included. Also matplotlib.pyplot is imported for you, as plt.\n\n# Histogram of life_exp, 15 bins\n\nplt.hist(life_exp, bins = 15)\n# Show and clear plot\nplt.show()\n\n\n\nplt.clf()\n\n# Histogram of life_exp1950, 15 bins\nlife_exp1950 = [28.8, 55.23, 43.08, 30.02, 62.48, 69.12, 66.8, 50.94, 37.48, \n                 68, 38.22, 40.41, 53.82, 47.62, 50.92, 59.6, 31.98, 39.03, 39.42, \n                 38.52, 68.75, 35.46, 38.09, 54.74, 44, 50.64, 40.72, 39.14, 42.11, \n                 57.21, 40.48, 61.21, 59.42, 66.87, 70.78, 34.81, 45.93, 48.36, \n                 41.89, 45.26, 34.48, 35.93, 34.08, 66.55, 67.41, 37, 30, 67.5, \n                 43.15, 65.86, 42.02, 33.61, 32.5, 37.58, 41.91, 60.96, 64.03, \n                 72.49, 37.37, 37.47, 44.87, 45.32, 66.91, 65.39, 65.94, 58.53, \n                 63.03, 43.16, 42.27, 50.06, 47.45, 55.56, 55.93, 42.14, 38.48, \n                 42.72, 36.68, 36.26, 48.46, 33.68, 40.54, 50.99, 50.79, 42.24, \n                 59.16, 42.87, 31.29, 36.32, 41.72, 36.16, 72.13, 69.39, 42.31, \n                 37.44, 36.32, 72.67, 37.58, 43.44, 55.19, 62.65, 43.9, 47.75, \n                 61.31, 59.82, 64.28, 52.72, 61.05, 40, 46.47, 39.88, 37.28, 58, \n                 30.33, 60.4, 64.36, 65.57, 32.98, 45.01, 64.94, 57.59, 38.64, \n                 41.41, 71.86, 69.62, 45.88, 58.5, 41.22, 50.85, 38.6, 59.1, 44.6, \n                 43.58, 39.98, 69.18, 68.44, 66.07, 55.09, 40.41, 43.16, 32.55, \n                 42.04, 48.45]\nplt.hist(life_exp1950, bins = 15)\n# Show and clear plot again\nplt.show()\n\n\n\n#plt.clf()\n\n\n\n\nIt’s time to customize your own plot. This is the fun part, you will see your plot come to life!\nYou’re going to work on the scatter plot with world development data: GDP per capita on the x-axis (logarithmic scale), life expectancy on the y-axis. The code for this plot is available in the script.\nAs a first step, let’s add axis labels and a title to the plot. You can do this with the xlabel(), ylabel() and title() functions, available in matplotlib.pyplot. This sub-package is already imported as plt.\n\n# Basic scatter plot, log scale\nplt.scatter(gdp_cap, life_exp)\nplt.xscale('log') \n\n# Strings\nxlab = 'GDP per Capita [in USD]'\nylab = 'Life Expectancy [in years]'\ntitle = 'World Development in 2007'\n\n# Add axis labels\nplt.xlabel(xlab)\nplt.ylabel(ylab)\n\n\n# Add title\nplt.title(title)\n\n# After customizing, display the plot\nplt.show()\n\n\n\n\n\n\n\nThe customizations you’ve coded up to now are available in the script, in a more concise form.\nIn the video, Hugo has demonstrated how you could control the y-ticks by specifying two arguments:\nplt.yticks([0,1,2], [“one”,“two”,“three”]) In this example, the ticks corresponding to the numbers 0, 1 and 2 will be replaced by one, two and three, respectively.\nLet’s do a similar thing for the x-axis of your world development chart, with the xticks() function. The tick values 1000, 10000 and 100000 should be replaced by 1k, 10k and 100k. To this end, two lists have already been created for you: tick_val and tick_lab.\n\n# Scatter plot\nplt.scatter(gdp_cap, life_exp)\n\n# Previous customizations\nplt.xscale('log') \nplt.xlabel('GDP per Capita [in USD]')\nplt.ylabel('Life Expectancy [in years]')\nplt.title('World Development in 2007')\n\n# Definition of tick_val and tick_lab\ntick_val = [1000, 10000, 100000]\ntick_lab = ['1k', '10k', '100k']\n\n# Adapt the ticks on the x-axis\nplt.xticks(tick_val, tick_lab)\n\n([<matplotlib.axis.XTick object at 0x7f21a695e150>, <matplotlib.axis.XTick object at 0x7f21a81f2810>, <matplotlib.axis.XTick object at 0x7f21abb9db90>], [Text(1000, 0, '1k'), Text(10000, 0, '10k'), Text(100000, 0, '100k')])\n\n# After customizing, display the plot\nplt.show()\n\n\n\n\n\n\n\nRight now, the scatter plot is just a cloud of blue dots, indistinguishable from each other. Let’s change this. Wouldn’t it be nice if the size of the dots corresponds to the population?\nTo accomplish this, there is a list pop loaded in your workspace. It contains population numbers for each country expressed in millions. You can see that this list is added to the scatter method, as the argument s, for size.\n\n # Import numpy as np\n\nimport numpy as np\n# Store pop as a numpy array: np_pop\nnp_pop = np.array(pop)\n\n# Double np_pop\n\nnp_pop = np_pop*2\n# Update: set s argument to np_pop\nplt.scatter(gdp_cap, life_exp,  s = np_pop)\n\n# Previous customizations\nplt.xscale('log') \nplt.xlabel('GDP per Capita [in USD]')\nplt.ylabel('Life Expectancy [in years]')\nplt.title('World Development in 2007')\nplt.xticks([1000, 10000, 100000],['1k', '10k', '100k'])\n\n([<matplotlib.axis.XTick object at 0x7f21a8180e90>, <matplotlib.axis.XTick object at 0x7f21abb42810>, <matplotlib.axis.XTick object at 0x7f21a6922810>], [Text(1000, 0, '1k'), Text(10000, 0, '10k'), Text(100000, 0, '100k')])\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\nThe code you’ve written up to now is available in the script.\nThe next step is making the plot more colorful! To do this, a list col has been created for you. It’s a list with a color for each corresponding country, depending on the continent the country is part of.\nHow did we make the list col you ask? The Gapminder data contains a list continent with the continent each country belongs to. A dictionary is constructed that maps continents onto colors:\ndict = { ‘Asia’:‘red’, ‘Europe’:‘green’, ‘Africa’:‘blue’, ‘Americas’:‘yellow’, ‘Oceania’:‘black’ } Nothing to worry about now; you will learn about dictionaries in the next chapter.\n\n# Specify c and alpha inside plt.scatter()\ncol =  [\"red\", \"green\", \"blue\", \"blue\", \"yellow\", \"black\", \"green\", \n        \"red\", \"red\", \"green\", \"blue\", \"yellow\", \"green\", \"blue\", \"yellow\", \n        \"green\", \"blue\", \"blue\", \"red\", \"blue\", \"yellow\", \"blue\", \"blue\", \n        \"yellow\", \"red\", \"yellow\", \"blue\", \"blue\", \"blue\", \"yellow\", \n        \"blue\", \"green\", \"yellow\", \"green\", \"green\", \"blue\", \"yellow\", \n        \"yellow\", \"blue\", \"yellow\", \"blue\", \"blue\", \"blue\", \"green\", \n        \"green\", \"blue\", \"blue\", \"green\", \"blue\", \"green\", \"yellow\", \n        \"blue\", \"blue\", \"yellow\", \"yellow\", \"red\", \"green\", \"green\", \n        \"red\", \"red\", \"red\", \"red\", \"green\", \"red\", \"green\", \"yellow\", \n        \"red\", \"red\", \"blue\", \"red\", \"red\", \"red\", \"red\", \"blue\", \"blue\", \n        \"blue\", \"blue\", \"blue\", \"red\", \"blue\", \"blue\", \"blue\", \"yellow\", \n        \"red\", \"green\", \"blue\", \"blue\", \"red\", \"blue\", \"red\", \"green\", \n        \"black\", \"yellow\", \"blue\", \"blue\", \"green\", \"red\", \"red\", \"yellow\", \n        \"yellow\", \"yellow\", \"red\", \"green\", \"green\", \"yellow\", \"blue\", \n        \"green\", \"blue\", \"blue\", \"red\", \"blue\", \"green\", \"blue\", \"red\", \n        \"green\", \"green\", \"blue\", \"blue\", \"green\", \"red\", \"blue\", \"blue\", \n        \"green\", \"green\", \"red\", \"red\", \"blue\", \"red\", \"blue\", \"yellow\", \n        \"blue\", \"green\", \"blue\", \"green\", \"yellow\", \"yellow\", \"yellow\", \n        \"red\", \"red\", \"red\", \"blue\", \"blue\"]\nplt.scatter(x = gdp_cap, y = life_exp, s = np.array(pop) * 2, alpha = 0.8, c = col)\n\n# Previous customizations\nplt.xscale('log') \nplt.xlabel('GDP per Capita [in USD]')\nplt.ylabel('Life Expectancy [in years]')\nplt.title('World Development in 2007')\nplt.xticks([1000,10000,100000], ['1k','10k','100k'])\n\n([<matplotlib.axis.XTick object at 0x7f21a69598d0>, <matplotlib.axis.XTick object at 0x7f21a6390f50>, <matplotlib.axis.XTick object at 0x7f21a6312810>], [Text(1000, 0, '1k'), Text(10000, 0, '10k'), Text(100000, 0, '100k')])\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nIf you have another look at the script, under # Additional Customizations, you’ll see that there are two plt.text() functions now. They add the words “India” and “China” in the plot.\n\n# Scatter plot\nplt.scatter(x = gdp_cap, y = life_exp, s = np.array(pop) * 2, c = col, alpha = 0.8)\n\n# Previous customizations\nplt.xscale('log') \nplt.xlabel('GDP per Capita [in USD]')\nplt.ylabel('Life Expectancy [in years]')\nplt.title('World Development in 2007')\nplt.xticks([1000,10000,100000], ['1k','10k','100k'])\n\n([<matplotlib.axis.XTick object at 0x7f21a6912590>, <matplotlib.axis.XTick object at 0x7f21a63bd510>, <matplotlib.axis.XTick object at 0x7f21a622db90>], [Text(1000, 0, '1k'), Text(10000, 0, '10k'), Text(100000, 0, '100k')])\n\n# Additional customizations\nplt.text(1550, 71, 'India')\nplt.text(5700, 80, 'China')\n\n# Add grid() call\n\nplt.grid(True)\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#motivation-for-dictionaries",
    "href": "datacamp/intermediatePython/IntermediatePython.html#motivation-for-dictionaries",
    "title": "Intermediate Python",
    "section": "Motivation for dictionaries",
    "text": "Motivation for dictionaries\nTo see why dictionaries are useful, have a look at the two lists defined in the script. countries contains the names of some European countries. capitals lists the corresponding names of their capital.\n\n# Definition of countries and capital\ncountries = ['spain', 'france', 'germany', 'norway']\ncapitals = ['madrid', 'paris', 'berlin', 'oslo']\n\n# Get index of 'germany': ind_ger\nind_ger = countries.index(\"germany\")\n\n# Use ind_ger to print out capital of Germany\nprint(capitals[ind_ger])\n\nberlin"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#access-dictionary",
    "href": "datacamp/intermediatePython/IntermediatePython.html#access-dictionary",
    "title": "Intermediate Python",
    "section": "Access dictionary",
    "text": "Access dictionary\nIf the keys of a dictionary are chosen wisely, accessing the values in a dictionary is easy and intuitive. For example, to get the capital for France from europe you can use:\neurope[‘france’] Here, ‘france’ is the key and ‘paris’ the value is returned.\n\n# Definition of dictionary\neurope = {'spain':'madrid', 'france':'paris', 'germany':'berlin', 'norway':'oslo' }\n\n# Print out the keys in europe\nprint(europe.keys())\n\ndict_keys(['spain', 'france', 'germany', 'norway'])\n\n# Print out value that belongs to key 'norway'\n\nprint(europe['norway'])\n\noslo"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#create-dictionary",
    "href": "datacamp/intermediatePython/IntermediatePython.html#create-dictionary",
    "title": "Intermediate Python",
    "section": "Create dictionary",
    "text": "Create dictionary\nThe countries and capitals lists are again available in the script. It’s your job to convert this data to a dictionary where the country names are the keys and the capitals are the corresponding values. As a refresher, here is a recipe for creating a dictionary:\n\nmy_dict = {\n   \"key1\":\"value1\",\n   \"key2\":\"value2\",\n}\n\nIn this recipe, both the keys and the values are strings. This will also be the case for this exercise.\n\n# Definition of countries and capital\ncountries = ['spain', 'france', 'germany', 'norway']\ncapitals = ['madrid', 'paris', 'berlin', 'oslo']\n\n# From string in countries and capitals, create dictionary europe\neurope = { 'spain':'madrid', 'france': 'paris','germany' : 'berlin', 'norway' : 'oslo'}\n\n# Print europe\nprint(europe)\n\n{'spain': 'madrid', 'france': 'paris', 'germany': 'berlin', 'norway': 'oslo'}"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#dictionary-manipulation-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#dictionary-manipulation-1",
    "title": "Intermediate Python",
    "section": "Dictionary Manipulation (1)",
    "text": "Dictionary Manipulation (1)\nIf you know how to access a dictionary, you can also assign a new value to it. To add a new key-value pair to europe you can use something like this:\n\n# Definition of dictionary\neurope = {'spain':'madrid', 'france':'paris', 'germany':'berlin', 'norway':'oslo' }\n\n# Add italy to europe\n\neurope['italy'] = 'rome'\n# Print out italy in europe\n\nprint('italy' in europe)\n\nTrue\n\n# Add poland to europe\n\neurope['poland'] = 'warsaw'\n# Print europe\nprint(europe)\n\n{'spain': 'madrid', 'france': 'paris', 'germany': 'berlin', 'norway': 'oslo', 'italy': 'rome', 'poland': 'warsaw'}"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#dictionary-manipulation-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#dictionary-manipulation-2",
    "title": "Intermediate Python",
    "section": "Dictionary Manipulation (2)",
    "text": "Dictionary Manipulation (2)\nSomebody thought it would be funny to mess with your accurately generated dictionary. An adapted version of the europe dictionary is available in the script.\nCan you clean up? Do not do this by adapting the definition of europe, but by adding Python commands to the script to update and remove key:value pairs.\n\n# Definition of dictionary\neurope = {'spain':'madrid', 'france':'paris', 'germany':'bonn',\n          'norway':'oslo', 'italy':'rome', 'poland':'warsaw',\n          'australia':'vienna' }\n\n# Update capital of germany\n\neurope['germany'] = 'berlin'\n# Remove australia\n\ndel(europe['australia'])\n# Print europe\nprint(europe)\n\n{'spain': 'madrid', 'france': 'paris', 'germany': 'berlin', 'norway': 'oslo', 'italy': 'rome', 'poland': 'warsaw'}"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#dictionariception",
    "href": "datacamp/intermediatePython/IntermediatePython.html#dictionariception",
    "title": "Intermediate Python",
    "section": "Dictionariception",
    "text": "Dictionariception\nRemember lists? They could contain anything, even other lists. Well, for dictionaries the same holds. Dictionaries can contain key:value pairs where the values are again dictionaries.\nAs an example, have a look at the script where another version of europe - the dictionary you’ve been working with all along - is coded. The keys are still the country names, but the values are dictionaries that contain more information than just the capital.\nIt’s perfectly possible to chain square brackets to select elements. To fetch the population for Spain from europe, for example, you need:\n\n# Dictionary of dictionaries\neurope = { 'spain': { 'capital':'madrid', 'population':46.77 },\n           'france': { 'capital':'paris', 'population':66.03 },\n           'germany': { 'capital':'berlin', 'population':80.62 },\n           'norway': { 'capital':'oslo', 'population':5.084 } }\n\n\n# Print out the capital of France\n\nprint(europe['spain']['capital'])\n\nmadrid\n\n# Create sub-dictionary data\ndata = {'capital' : 'rome', 'population' : 59.83}\n\n# Add data to europe under key 'italy'\neurope['italy'] = data\n\n# Print europe\n\nprint(europe)\n\n{'spain': {'capital': 'madrid', 'population': 46.77}, 'france': {'capital': 'paris', 'population': 66.03}, 'germany': {'capital': 'berlin', 'population': 80.62}, 'norway': {'capital': 'oslo', 'population': 5.084}, 'italy': {'capital': 'rome', 'population': 59.83}}"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#dictionary-to-dataframe-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#dictionary-to-dataframe-1",
    "title": "Intermediate Python",
    "section": "Dictionary to DataFrame (1)",
    "text": "Dictionary to DataFrame (1)\nPandas is an open source library, providing high-performance, easy-to-use data structures and data analysis tools for Python. Sounds promising!\nThe DataFrame is one of Pandas’ most important data structures. It’s basically a way to store tabular data where you can label the rows and the columns. One way to build a DataFrame is from a dictionary.\nIn the exercises that follow you will be working with vehicle data from different countries. Each observation corresponds to a country and the columns give information about the number of vehicles per capita, whether people drive left or right, and so on.\nThree lists are defined in the script:\nnames, containing the country names for which data is available. dr, a list with booleans that tells whether people drive left or right in the corresponding country. cpc, the number of motor vehicles per 1000 people in the corresponding country. Each dictionary key is a column label and each value is a list which contains the column elements.\n\n# Pre-defined lists\nnames = ['United States', 'Australia', 'Japan', 'India', 'Russia', 'Morocco', 'Egypt']\ndr =  [True, False, False, False, True, True, True]\ncpc = [809, 731, 588, 18, 200, 70, 45]\n\n# Import pandas as pd\n\nimport pandas as pd\n# Create dictionary my_dict with three key:value pairs: my_dict\n\nmy_dict = {'country' :names, 'drives_right' : dr, 'cars_per_cap' : cpc }\n# Build a DataFrame cars from my_dict: cars\ncars = pd.DataFrame(my_dict)\n\n\nfrom IPython.display import HTML\nHTML(cars.to_html())\n\n\n\n  \n    \n      \n      country\n      drives_right\n      cars_per_cap\n    \n  \n  \n    \n      0\n      United States\n      True\n      809\n    \n    \n      1\n      Australia\n      False\n      731\n    \n    \n      2\n      Japan\n      False\n      588\n    \n    \n      3\n      India\n      False\n      18\n    \n    \n      4\n      Russia\n      True\n      200\n    \n    \n      5\n      Morocco\n      True\n      70\n    \n    \n      6\n      Egypt\n      True\n      45"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#dictionary-to-dataframe-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#dictionary-to-dataframe-2",
    "title": "Intermediate Python",
    "section": "Dictionary to DataFrame (2)",
    "text": "Dictionary to DataFrame (2)\nThe Python code that solves the previous exercise is included in the script. Have you noticed that the row labels (i.e. the labels for the different observations) were automatically set to integers from 0 up to 6?\nTo solve this a list row_labels has been created. You can use it to specify the row labels of the cars DataFrame. You do this by setting the index attribute of cars, that you can access as cars.index.\n\n# Build cars DataFrame\nnames = ['United States', 'Australia', 'Japan', 'India', 'Russia', 'Morocco', 'Egypt']\ndr =  [True, False, False, False, True, True, True]\ncpc = [809, 731, 588, 18, 200, 70, 45]\ncars_dict = { 'country':names, 'drives_right':dr, 'cars_per_cap':cpc }\ncars = pd.DataFrame(cars_dict)\nprint(cars)\n\n         country  drives_right  cars_per_cap\n0  United States          True           809\n1      Australia         False           731\n2          Japan         False           588\n3          India         False            18\n4         Russia          True           200\n5        Morocco          True            70\n6          Egypt          True            45\n\n# Definition of row_labels\nrow_labels = ['US', 'AUS', 'JPN', 'IN', 'RU', 'MOR', 'EG']\n\n# Specify row labels of cars\ncars.index = row_labels\n\n# Print cars again\nHTML(cars.to_html())\n\n\n\n  \n    \n      \n      country\n      drives_right\n      cars_per_cap\n    \n  \n  \n    \n      US\n      United States\n      True\n      809\n    \n    \n      AUS\n      Australia\n      False\n      731\n    \n    \n      JPN\n      Japan\n      False\n      588\n    \n    \n      IN\n      India\n      False\n      18\n    \n    \n      RU\n      Russia\n      True\n      200\n    \n    \n      MOR\n      Morocco\n      True\n      70\n    \n    \n      EG\n      Egypt\n      True\n      45"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#csv-to-dataframe-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#csv-to-dataframe-1",
    "title": "Intermediate Python",
    "section": "CSV to DataFrame (1)",
    "text": "CSV to DataFrame (1)\nPutting data in a dictionary and then building a DataFrame works, but it’s not very efficient. What if you’re dealing with millions of observations? In those cases, the data is typically available as files with a regular structure. One of those file types is the CSV file, which is short for “comma-separated values”.\nTo import CSV data into Python as a Pandas DataFrame you can use read_csv().\nLet’s explore this function with the same cars data from the previous exercises. This time, however, the data is available in a CSV file, named cars.csv. It is available in your current working directory, so the path to the file is simply ‘cars.csv’.\n\n# Import the cars.csv data: cars\n\ncars = pd.read_csv('data/cars.csv')\n# Print out cars\nHTML(cars.to_html())\n\n\n\n  \n    \n      \n      Unnamed: 0\n      cars_per_cap\n      country\n      drives_right\n    \n  \n  \n    \n      0\n      US\n      809\n      United States\n      True\n    \n    \n      1\n      AUS\n      731\n      Australia\n      False\n    \n    \n      2\n      JAP\n      588\n      Japan\n      False\n    \n    \n      3\n      IN\n      18\n      India\n      False\n    \n    \n      4\n      RU\n      200\n      Russia\n      True\n    \n    \n      5\n      MOR\n      70\n      Morocco\n      True\n    \n    \n      6\n      EG\n      45\n      Egypt\n      True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#csv-to-dataframe-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#csv-to-dataframe-2",
    "title": "Intermediate Python",
    "section": "CSV to DataFrame (2)",
    "text": "CSV to DataFrame (2)\nYour read_csv() call to import the CSV data didn’t generate an error, but the output is not entirely what we wanted. The row labels were imported as another column without a name.\nRemember index_col, an argument of read_csv(), that you can use to specify which column in the CSV file should be used as a row label? Well, that’s exactly what you need here!\nPython code that solves the previous exercise is already included; can you make the appropriate changes to fix the data import?\n\n# Fix import by including index_col\ncars = pd.read_csv('data/cars.csv', index_col = 0)\n\n# Print out cars\nHTML(cars.to_html())\n\n\n\n  \n    \n      \n      cars_per_cap\n      country\n      drives_right\n    \n  \n  \n    \n      US\n      809\n      United States\n      True\n    \n    \n      AUS\n      731\n      Australia\n      False\n    \n    \n      JAP\n      588\n      Japan\n      False\n    \n    \n      IN\n      18\n      India\n      False\n    \n    \n      RU\n      200\n      Russia\n      True\n    \n    \n      MOR\n      70\n      Morocco\n      True\n    \n    \n      EG\n      45\n      Egypt\n      True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#square-brackets-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#square-brackets-1",
    "title": "Intermediate Python",
    "section": "Square Brackets (1)",
    "text": "Square Brackets (1)\nIn the video, you saw that you can index and select Pandas DataFrames in many different ways. The simplest, but not the most powerful way, is to use square brackets.\nIn the sample code, the same cars data is imported from a CSV files as a Pandas DataFrame. To select only the cars_per_cap column from cars, you can use:\ncars[‘cars_per_cap’] cars[[‘cars_per_cap’]] The single bracket version gives a Pandas Series, the double bracket version gives a Pandas DataFrame.\n\n# Print out country column as Pandas Series\n\nprint(cars['country'])\n\nUS     United States\nAUS        Australia\nJAP            Japan\nIN             India\nRU            Russia\nMOR          Morocco\nEG             Egypt\nName: country, dtype: object\n\n# Print out country column as Pandas DataFrame\nprint(cars[['country']])\n\n           country\nUS   United States\nAUS      Australia\nJAP          Japan\nIN           India\nRU          Russia\nMOR        Morocco\nEG           Egypt\n\n# Print out DataFrame with country and drives_right columns\nprint(cars[['country', 'drives_right']])\n\n           country  drives_right\nUS   United States          True\nAUS      Australia         False\nJAP          Japan         False\nIN           India         False\nRU          Russia          True\nMOR        Morocco          True\nEG           Egypt          True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#square-brackets-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#square-brackets-2",
    "title": "Intermediate Python",
    "section": "Square Brackets (2)",
    "text": "Square Brackets (2)\nSquare brackets can do more than just selecting columns. You can also use them to get rows, or observations, from a DataFrame. The following call selects the first five rows from the cars DataFrame:\ncars[0:5] The result is another DataFrame containing only the rows you specified.\nPay attention: You can only select rows using square brackets if you specify a slice, like 0:4. Also, you’re using the integer indexes of the rows here, not the row labels!\n\n# Print out first 3 observations\nprint(cars[0:3])\n\n     cars_per_cap        country  drives_right\nUS            809  United States          True\nAUS           731      Australia         False\nJAP           588          Japan         False\n\n# Print out fourth, fifth and sixth observation\nprint(cars[3:6])\n\n     cars_per_cap  country  drives_right\nIN             18    India         False\nRU            200   Russia          True\nMOR            70  Morocco          True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#loc-and-iloc-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#loc-and-iloc-1",
    "title": "Intermediate Python",
    "section": "loc and iloc (1)",
    "text": "loc and iloc (1)\nWith loc and iloc you can do practically any data selection operation on DataFrames you can think of. loc is label-based, which means that you have to specify rows and columns based on their row and column labels. iloc is integer index based, so you have to specify rows and columns by their integer index like you did in the previous exercise.\nTry out the following commands in the IPython Shell to experiment with loc and iloc to select observations. Each pair of commands here gives the same result.\ncars.loc[‘RU’] cars.iloc[4]\ncars.loc[[‘RU’]] cars.iloc[[4]]\ncars.loc[[‘RU’, ‘AUS’]] cars.iloc[[4, 1]] As before, code is included that imports the cars data as a Pandas DataFrame.\n\n# Print out observation for Japan\nprint(cars.loc['JAP'])\n\ncars_per_cap      588\ncountry         Japan\ndrives_right    False\nName: JAP, dtype: object\n\n# Print out observations for Australia and Egypt\nprint(cars.loc[['AUS', 'EG']])\n\n     cars_per_cap    country  drives_right\nAUS           731  Australia         False\nEG             45      Egypt          True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#loc-and-iloc-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#loc-and-iloc-2",
    "title": "Intermediate Python",
    "section": "loc and iloc (2)",
    "text": "loc and iloc (2)\nloc and iloc also allow you to select both rows and columns from a DataFrame. To experiment, try out the following commands in the IPython Shell. Again, paired commands produce the same result.\n\n# Print out drives_right value of Morocco\nprint(cars.loc[['MOR'], 'drives_right'])\n\nMOR    True\nName: drives_right, dtype: bool\n\n# Print sub-DataFrame\nprint(cars.loc[['RU','MOR'], ['country','drives_right']])\n\n     country  drives_right\nRU    Russia          True\nMOR  Morocco          True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#loc-and-iloc-3",
    "href": "datacamp/intermediatePython/IntermediatePython.html#loc-and-iloc-3",
    "title": "Intermediate Python",
    "section": "loc and iloc (3)",
    "text": "loc and iloc (3)\nIt’s also possible to select only columns with loc and iloc. In both cases, you simply put a slice going from beginning to end in front of the comma:\n\n# Print out drives_right column as Series\nprint(cars.loc[:, 'drives_right'])\n\nUS      True\nAUS    False\nJAP    False\nIN     False\nRU      True\nMOR     True\nEG      True\nName: drives_right, dtype: bool\n\n# Print out drives_right column as DataFrame\n\nprint(cars.loc[:, ['drives_right']])\n\n     drives_right\nUS           True\nAUS         False\nJAP         False\nIN          False\nRU           True\nMOR          True\nEG           True\n\n# Print out cars_per_cap and drives_right as DataFrame\n\nprint(cars.loc[:, ['cars_per_cap','drives_right']])\n\n     cars_per_cap  drives_right\nUS            809          True\nAUS           731         False\nJAP           588         False\nIN             18         False\nRU            200          True\nMOR            70          True\nEG             45          True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#equality",
    "href": "datacamp/intermediatePython/IntermediatePython.html#equality",
    "title": "Intermediate Python",
    "section": "Equality",
    "text": "Equality\nTo check if two Python values, or variables, are equal you can use ==. To check for inequality, you need !=. As a refresher, have a look at the following examples that all result in True. Feel free to try them out in the IPython Shell.\n2 == (1 + 1) “intermediate” != “python” True != False “Python” != “python” When you write these comparisons in a script, you will need to wrap a print() function around them to see the output.\n\n# Comparison of booleans\nprint(True == False)\n\nFalse\n\n# Comparison of integers\nprint(-5*15 != 75)\n\nTrue\n\n# Comparison of strings\nprint(\"pyscript\" == \"PyScript\")\n\nFalse\n\n# Compare a boolean with an integer\nprint(True == 1)\n\nTrue"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#greater-and-less-than",
    "href": "datacamp/intermediatePython/IntermediatePython.html#greater-and-less-than",
    "title": "Intermediate Python",
    "section": "Greater and less than",
    "text": "Greater and less than\nIn the video, Hugo also talked about the less than and greater than signs, < and > in Python. You can combine them with an equals sign: <= and >=. Pay attention: <= is valid syntax, but =< is not.\nAll Python expressions in the following code chunk evaluate to True:\n\n# Comparison of integers\nx = -3 * 6\nprint(x >= -10)\n\nFalse\n\n# Comparison of strings\ny=\"test\"\nprint(\"test\" <= y)\n\nTrue\n\n# Comparison of booleans\nprint(True > False)\n\nTrue"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#compare-arrays",
    "href": "datacamp/intermediatePython/IntermediatePython.html#compare-arrays",
    "title": "Intermediate Python",
    "section": "Compare arrays",
    "text": "Compare arrays\nOut of the box, you can also use comparison operators with NumPy arrays.\nRemember areas, the list of area measurements for different rooms in your house from Introduction to Python? This time there’s two NumPy arrays: my_house and your_house. They both contain the areas for the kitchen, living room, bedroom and bathroom in the same order, so you can compare them.\n\n# Create arrays\nmy_house = np.array([18.0, 20.0, 10.75, 9.50])\nyour_house = np.array([14.0, 24.0, 14.25, 9.0])\n\n# my_house greater than or equal to 18\nprint(my_house>=18)\n\n[ True  True False False]\n\n# my_house less than your_house\nprint(my_house < your_house)\n\n[False  True  True False]"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#and-or-not-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#and-or-not-1",
    "title": "Intermediate Python",
    "section": "and, or, not (1)",
    "text": "and, or, not (1)\nA boolean is either 1 or 0, True or False. With boolean operators such as and, or and not, you can combine these booleans to perform more advanced queries on your data.\nIn the sample code, two variables are defined: my_kitchen and your_kitchen, representing areas\n\n# Define variables\nmy_kitchen = 18.0\nyour_kitchen = 14.0\n\n# my_kitchen bigger than 10 and smaller than 18?\nprint(my_kitchen> 10 and my_kitchen < 18)\n\nFalse\n\n# my_kitchen smaller than 14 or bigger than 17?\n\nprint(my_kitchen> 17 or my_kitchen < 14)\n\nTrue\n\n# Double my_kitchen smaller than triple your_kitchen?\n\nprint(my_kitchen * 2 <  your_kitchen * 3)\n\nTrue\n\n\nand, or, not (2) To see if you completely understood the boolean operators, have a look at the following piece of Python code:\n\nx = 8\ny = 9\nnot(not(x < 3) and not(y > 14 or y > 10))\n\nFalse\n\n\nWhat will the result be if you execute these three commands in the IPython Shell?\nNB: Notice that not has a higher priority than and and or, it is executed first."
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#boolean-operators-with-numpy",
    "href": "datacamp/intermediatePython/IntermediatePython.html#boolean-operators-with-numpy",
    "title": "Intermediate Python",
    "section": "Boolean operators with NumPy",
    "text": "Boolean operators with NumPy\nBefore, the operational operators like < and >= worked with NumPy arrays out of the box. Unfortunately, this is not true for the boolean operators and, or, and not.\nTo use these operators with NumPy, you will need np.logical_and(), np.logical_or() and np.logical_not(). Here’s an example on the my_house and your_house arrays from before to give you an idea:\n\n# Create arrays\nimport numpy as np\nmy_house = np.array([18.0, 20.0, 10.75, 9.50])\nyour_house = np.array([14.0, 24.0, 14.25, 9.0])\n\n# my_house greater than 18.5 or smaller than 10\nprint(np.logical_or(your_house > 18.5, \n               your_house < 10))\n\n[False  True False  True]\n\n# Both my_house and your_house smaller than 11\nprint(np.logical_and(my_house < 11, \n               your_house < 11))\n\n[False False False  True]"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#warmup",
    "href": "datacamp/intermediatePython/IntermediatePython.html#warmup",
    "title": "Intermediate Python",
    "section": "Warmup",
    "text": "Warmup\nTo experiment with if and else a bit, have a look at this code sample:\n\narea = 10.0\nif(area < 9) :\n    print(\"small\")\nelif(area < 12) :\n    print(\"medium\")\nelse :\n    print(\"large\")\n\nmedium"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#if",
    "href": "datacamp/intermediatePython/IntermediatePython.html#if",
    "title": "Intermediate Python",
    "section": "if",
    "text": "if\nIt’s time to take a closer look around in your house.\nTwo variables are defined in the sample code: room, a string that tells you which room of the house we’re looking at, and area, the area of that room.\n\n# Define variables\nroom = \"kit\"\narea = 14.0\n\n# if statement for room\nif room == \"kit\" :\n    print(\"looking around in the kitchen.\")\n\nlooking around in the kitchen.\n\n\n# if statement for area\nif area > 15 :\n    print(\"big place!\")"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#add-else",
    "href": "datacamp/intermediatePython/IntermediatePython.html#add-else",
    "title": "Intermediate Python",
    "section": "Add else",
    "text": "Add else\nIn the script, the if construct for room has been extended with an else statement so that “looking around elsewhere.” is printed if the condition room == “kit” evaluates to False.\nCan you do a similar thing to add more functionality to the if construct for area?\n\n# Define variables\nroom = \"kit\"\narea = 14.0\n\n# if-else construct for room\nif room == \"kit\" :\n    print(\"looking around in the kitchen.\")\nelse :\n    print(\"looking around elsewhere.\")\n\nlooking around in the kitchen.\n\n# if-else construct for area\nif area > 15 :\n    print(\"big place!\")\nelse:\n    print(\"pretty small.\")\n\npretty small."
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#customize-further-elif",
    "href": "datacamp/intermediatePython/IntermediatePython.html#customize-further-elif",
    "title": "Intermediate Python",
    "section": "Customize further: elif",
    "text": "Customize further: elif\nIt’s also possible to have a look around in the bedroom. The sample code contains an elif part that checks if room equals “bed”. In that case, “looking around in the bedroom.” is printed out.\nIt’s up to you now! Make a similar addition to the second control structure to further customize the messages for different values of area.\n\n# Define variables\nroom = \"bed\"\narea = 14.0\n\n# if-elif-else construct for room\nif room == \"kit\" :\n    print(\"looking around in the kitchen.\")\nelif room == \"bed\":\n    print(\"looking around in the bedroom.\")\nelse :\n    print(\"looking around elsewhere.\")\n\nlooking around in the bedroom.\n\n# if-elif-else construct for area\nif area > 15 :\n    print(\"big place!\")\nelif area > 10:\n    print(\"medium size, nice!\")\nelse :\n    print(\"pretty small.\")\n\nmedium size, nice!"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#driving-right-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#driving-right-1",
    "title": "Intermediate Python",
    "section": "Driving right (1)",
    "text": "Driving right (1)\nRemember that cars dataset, containing the cars per 1000 people (cars_per_cap) and whether people drive right (drives_right) for different countries (country)? The code that imports this data in CSV format into Python as a DataFrame is included in the script.\nIn the video, you saw a step-by-step approach to filter observations from a DataFrame based on boolean arrays. Let’s start simple and try to find all observations in cars where drives_right is True. drives_right is a boolean column, so you’ll have to extract it as a Series and then use this boolean Series to select observations from cars.\n\n# Extract drives_right column as Series: dr\ndr = cars['drives_right']\n\n# Use dr to subset cars: sel\nsel= cars[dr]\n\n# Print sel\nprint(sel)\n\n     cars_per_cap        country  drives_right\nUS            809  United States          True\nRU            200         Russia          True\nMOR            70        Morocco          True\nEG             45          Egypt          True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#driving-right-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#driving-right-2",
    "title": "Intermediate Python",
    "section": "Driving right (2)",
    "text": "Driving right (2)\nThe code in the previous example worked fine, but you actually unnecessarily created a new variable dr. You can achieve the same result without this intermediate variable. Put the code that computes dr straight into the square brackets that select observations from cars.\n\n# Convert code to a one-liner\n#dr = cars['drives_right']\nsel = cars[cars['drives_right']]\n\n# Print sel\nprint(sel)\n\n     cars_per_cap        country  drives_right\nUS            809  United States          True\nRU            200         Russia          True\nMOR            70        Morocco          True\nEG             45          Egypt          True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#cars-per-capita-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#cars-per-capita-1",
    "title": "Intermediate Python",
    "section": "Cars per capita (1)",
    "text": "Cars per capita (1)\nLet’s stick to the cars data some more. This time you want to find out which countries have a high cars per capita figure. In other words, in which countries do many people have a car, or maybe multiple cars.\nSimilar to the previous example, you’ll want to build up a boolean Series, that you can then use to subset the cars DataFrame to select certain observations. If you want to do this in a one-liner, that’s perfectly fine!\n\n# Create car_maniac: observations that have a cars_per_cap over 500\n\ncar_maniac = cars[cars['cars_per_cap'] > 500]\n\n\n# Print car_maniac\nprint(car_maniac)\n\n     cars_per_cap        country  drives_right\nUS            809  United States          True\nAUS           731      Australia         False\nJAP           588          Japan         False"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#cars-per-capita-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#cars-per-capita-2",
    "title": "Intermediate Python",
    "section": "Cars per capita (2)",
    "text": "Cars per capita (2)\nRemember about np.logical_and(), np.logical_or() and np.logical_not(), the NumPy variants of the and, or and not operators? You can also use them on Pandas Series to do more advanced filtering operations.\nTake this example that selects the observations that have a cars_per_cap between 10 and 80. Try out these lines of code step by step to see what’s happening.\n\n# Create medium: observations with cars_per_cap between 100 and 500\ncpc = cars['cars_per_cap']\nbetween = np.logical_and(cpc > 100, cpc < 500)\nmedium = cars[between]\n\n# Print medium\n\nprint(medium)\n\n    cars_per_cap country  drives_right\nRU           200  Russia          True"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#while-warming-up",
    "href": "datacamp/intermediatePython/IntermediatePython.html#while-warming-up",
    "title": "Intermediate Python",
    "section": "while: warming up",
    "text": "while: warming up\nThe while loop is like a repeated if statement. The code is executed over and over again, as long as the condition is True. Have another look at its recipe.\nwhile condition : expression Can you tell how many printouts the following while loop will do?\n\nx = 1\nwhile x < 4 :\n    print(x)\n    x = x + 1\n\n1\n2\n3"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#basic-while-loop",
    "href": "datacamp/intermediatePython/IntermediatePython.html#basic-while-loop",
    "title": "Intermediate Python",
    "section": "Basic while loop",
    "text": "Basic while loop\nBelow you can find the example from the video where the error variable, initially equal to 50.0, is divided by 4 and printed out on every run:\nerror = 50.0\nwhile error > 1 :\n    error = error / 4\n    print(error)\n\nThis example will come in handy, because it’s time to build a while loop yourself! We’re going to code a while loop that implements a very basic control system for an inverted pendulum. If there’s an offset from standing perfectly straight, the while loop will incrementally fix this offset.\nNote that if your while loop takes too long to run, you might have made a mistake. In particular, remember to indent the contents of the loop using four spaces or auto-indentation!\n\n# Initialize offset\n\noffset = 8\n# Code the while loop\nwhile offset > 0:\n    print(\"correcting...\")\n    offset = offset - 1\n    print(offset)\n\ncorrecting...\n7\ncorrecting...\n6\ncorrecting...\n5\ncorrecting...\n4\ncorrecting...\n3\ncorrecting...\n2\ncorrecting...\n1\ncorrecting...\n0"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#add-conditionals",
    "href": "datacamp/intermediatePython/IntermediatePython.html#add-conditionals",
    "title": "Intermediate Python",
    "section": "Add conditionals",
    "text": "Add conditionals\nThe while loop that corrects the offset is a good start, but what if offset is negative? You can try to run the following code where offset is initialized to -6:\n# Initialize offset\noffset = -6\n\n# Code the while loop\n\nwhile offset != 0 :\n    print(\"correcting...\")\n    offset = offset - 1\n    print(offset)\n    \nbut your session will be disconnected. The while loop will never stop running, because offset will be further decreased on every run. offset != 0 will never become False and the while loop continues forever.\nFix things by putting an if-else statement inside the while loop. If your code is still taking too long to run, you probably made a mistake!\n\n# Initialize offset\noffset = -6\n\n# Code the while loop\nwhile offset != 0 :\n    print(\"correcting...\")\n    if offset > 0 :\n      offset = offset - 1\n\n    else : \n      offset = offset + 1    \n    print(offset)\n\ncorrecting...\n-5\ncorrecting...\n-4\ncorrecting...\n-3\ncorrecting...\n-2\ncorrecting...\n-1\ncorrecting...\n0"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#loop-over-a-list",
    "href": "datacamp/intermediatePython/IntermediatePython.html#loop-over-a-list",
    "title": "Intermediate Python",
    "section": "Loop over a list",
    "text": "Loop over a list\nHave another look at the for loop that Hugo showed in the video:\n\nfam = [1.73, 1.68, 1.71, 1.89]\nfor height in fam : \n    print(height)\n    \nAs usual, you simply have to indent the code with 4 spaces to tell Python which code should be executed in the for loop.\nThe areas variable, containing the area of different rooms in your house, is already defined.\n\n# areas list\nareas = [11.25, 18.0, 20.0, 10.75, 9.50]\n\n# Code the for loop\nfor area in areas:\n    print(area)\n\n11.25\n18.0\n20.0\n10.75\n9.5"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#indexes-and-values-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#indexes-and-values-1",
    "title": "Intermediate Python",
    "section": "Indexes and values (1)",
    "text": "Indexes and values (1)\nUsing a for loop to iterate over a list only gives you access to every list element in each run, one after the other. If you also want to access the index information, so where the list element you’re iterating over is located, you can use enumerate().\nAs an example, have a look at how the for loop from the video was converted:\n\nfam = [1.73, 1.68, 1.71, 1.89]\nfor index, height in enumerate(fam) :\n    print(\"person \" + str(index) + \": \" + str(height))\n\n\n# areas list\nareas = [11.25, 18.0, 20.0, 10.75, 9.50]\n\n# Change for loop to use enumerate() and update print()\nfor index, area in enumerate(areas):\n    print('room' + str(index)+ \":\" + str(area))\n\nroom0:11.25\nroom1:18.0\nroom2:20.0\nroom3:10.75\nroom4:9.5"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#indexes-and-values-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#indexes-and-values-2",
    "title": "Intermediate Python",
    "section": "Indexes and values (2)",
    "text": "Indexes and values (2)\nFor non-programmer folks, room 0: 11.25 is strange. Wouldn’t it be better if the count started at 1?\n\n# areas list\nareas = [11.25, 18.0, 20.0, 10.75, 9.50]\n\n# Code the for loop\nfor index, area in enumerate(areas) :\n    print(\"room \" + str(index+1) + \": \" + str(area))\n\nroom 1: 11.25\nroom 2: 18.0\nroom 3: 20.0\nroom 4: 10.75\nroom 5: 9.5"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#loop-over-list-of-lists",
    "href": "datacamp/intermediatePython/IntermediatePython.html#loop-over-list-of-lists",
    "title": "Intermediate Python",
    "section": "Loop over list of lists",
    "text": "Loop over list of lists\nRemember the house variable from the Intro to Python course? Have a look at its definition in the script. It’s basically a list of lists, where each sublist contains the name and area of a room in your house.\nIt’s up to you to build a for loop from scratch this time!\n# house list of lists\nhouse = [[\"hallway\", 11.25], \n         [\"kitchen\", 18.0], \n         [\"living room\", 20.0], \n         [\"bedroom\", 10.75], \n         [\"bathroom\", 9.50]]\n         \n# Build a for loop from scratch\nfor x in house:\n    print(\"- The \" + x[0] + \" is \" + str(x[1]) + \" sqm\")\n\nThe hallway is 11.25 sqm\nThe kitchen is 18.0 sqm\nThe living room is 20.0 sqm\nThe bedroom is 10.75 sqm\nThe bathroom is 9.5 sqm"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#loop-over-dictionary",
    "href": "datacamp/intermediatePython/IntermediatePython.html#loop-over-dictionary",
    "title": "Intermediate Python",
    "section": "Loop over dictionary",
    "text": "Loop over dictionary\nIn Python 3, you need the items() method to loop over a dictionary:\n\nworld = { \"afghanistan\":30.55, \n          \"albania\":2.77,\n          \"algeria\":39.21 }\n\nfor key, value in world.items() :\n    print(key + \" -- \" + str(value))\n    \nRemember the europe dictionary that contained the names of some European countries as key and their capitals as corresponding value? Go ahead and write a loop to iterate over it!\n# Definition of dictionary\neurope = {'spain':'madrid', 'france':'paris', 'germany':'berlin',\n          'norway':'oslo', 'italy':'rome', 'poland':'warsaw', 'austria':'vienna' }\n          \n# Iterate over europe\nfor k, v in europe.items():\n    print('- The ' + 'capital '+ 'of ' + str(k) + ' is '+ str(v))\n\nThe capital of spain is madrid\nThe capital of france is paris\nThe capital of germany is berlin\nThe capital of norway is oslo\nThe capital of italy is rome\nThe capital of poland is warsaw\nThe capital of austria is vienna"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#loop-over-numpy-array",
    "href": "datacamp/intermediatePython/IntermediatePython.html#loop-over-numpy-array",
    "title": "Intermediate Python",
    "section": "Loop over NumPy array",
    "text": "Loop over NumPy array\nIf you’re dealing with a 1D NumPy array, looping over all elements can be as simple as:\nfor x in my_array :\n    ...\nIf you’re dealing with a 2D NumPy array, it’s more complicated. A 2D array is built up of multiple 1D arrays. To explicitly iterate over all separate elements of a multi-dimensional array, you’ll need this syntax:\nfor x in np.nditer(my_array) :\n    ...\nTwo NumPy arrays that you might recognize from the intro course are available in your Python session: np_height, a NumPy array containing the heights of Major League Baseball players, and np_baseball, a 2D NumPy array that contains both the heights (first column) and weights (second column) of those players.\n\nimport numpy as np\nheight =[1.7, 1.6, 1.3, 1.4, 1.65]\nweight = [80, 75, 86, 72, 83]\nnp_height = np.array(height)\nnp_weight = np.array(weight)\nnp_baseball = np.array([weight, height])\n# For loop over np_height\nfor x in np_height:\n    print(str(x) + ' metres')\n\n1.7 metres\n1.6 metres\n1.3 metres\n1.4 metres\n1.65 metres\n\n# For loop over np_baseball\nfor x in np.nditer(np_baseball):\n    print(x)\n\n80.0\n75.0\n86.0\n72.0\n83.0\n1.7\n1.6\n1.3\n1.4\n1.65"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#loop-over-dataframe-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#loop-over-dataframe-1",
    "title": "Intermediate Python",
    "section": "Loop over DataFrame (1)",
    "text": "Loop over DataFrame (1)\nIterating over a Pandas DataFrame is typically done with the iterrows() method. Used in a for loop, every observation is iterated over and on every iteration the row label and actual row contents are available:\nfor lab, row in brics.iterrows() :\n    ...\nIn this and the following exercises you will be working on the cars DataFrame. It contains information on the cars per capita and whether people drive right or left for seven countries in the world.\n\n# Iterate over rows of cars\nfor lab, row in cars.iterrows():\n    print(lab)\n    print(row)\n\nUS\ncars_per_cap              809\ncountry         United States\ndrives_right             True\nName: US, dtype: object\nAUS\ncars_per_cap          731\ncountry         Australia\ndrives_right        False\nName: AUS, dtype: object\nJAP\ncars_per_cap      588\ncountry         Japan\ndrives_right    False\nName: JAP, dtype: object\nIN\ncars_per_cap       18\ncountry         India\ndrives_right    False\nName: IN, dtype: object\nRU\ncars_per_cap       200\ncountry         Russia\ndrives_right      True\nName: RU, dtype: object\nMOR\ncars_per_cap         70\ncountry         Morocco\ndrives_right       True\nName: MOR, dtype: object\nEG\ncars_per_cap       45\ncountry         Egypt\ndrives_right     True\nName: EG, dtype: object"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#loop-over-dataframe-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#loop-over-dataframe-2",
    "title": "Intermediate Python",
    "section": "Loop over DataFrame (2)",
    "text": "Loop over DataFrame (2)\nThe row data that’s generated by iterrows() on every run is a Pandas Series. This format is not very convenient to print out. Luckily, you can easily select variables from the Pandas Series using square brackets:\nfor lab, row in brics.iterrows() :\n    print(row['country'])\n\n\n# Adapt for loop\nfor lab, row in cars.iterrows() :\n    print(str(lab) + ': ' + str(row['cars_per_cap']))\n\nUS: 809\nAUS: 731\nJAP: 588\nIN: 18\nRU: 200\nMOR: 70\nEG: 45"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#add-column-1",
    "href": "datacamp/intermediatePython/IntermediatePython.html#add-column-1",
    "title": "Intermediate Python",
    "section": "Add column (1)",
    "text": "Add column (1)\nIn the video, Hugo showed you how to add the length of the country names of the brics DataFrame in a new column:\n\nfor lab, row in brics.iterrows() :\n    brics.loc[lab, \"name_length\"] = len(row[\"country\"])\n    \nYou can do similar things on the cars DataFrame.\n\n# Code for loop that adds COUNTRY column\nfor lab, row in cars.iterrows() :\n    cars.loc[lab, \"COUNTRY\"] = row[\"country\"].upper()\n\n\n# Print cars\nprint(cars)\n\n     cars_per_cap        country  drives_right        COUNTRY\nUS            809  United States          True  UNITED STATES\nAUS           731      Australia         False      AUSTRALIA\nJAP           588          Japan         False          JAPAN\nIN             18          India         False          INDIA\nRU            200         Russia          True         RUSSIA\nMOR            70        Morocco          True        MOROCCO\nEG             45          Egypt          True          EGYPT"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#add-column-2",
    "href": "datacamp/intermediatePython/IntermediatePython.html#add-column-2",
    "title": "Intermediate Python",
    "section": "Add column (2)",
    "text": "Add column (2)\nUsing iterrows() to iterate over every observation of a Pandas DataFrame is easy to understand, but not very efficient. On every iteration, you’re creating a new Pandas Series.\nIf you want to add a column to a DataFrame by calling a function on another column, the iterrows() method in combination with a for loop is not the preferred way to go. Instead, you’ll want to use apply().\nCompare the iterrows() version with the apply() version to get the same result in the brics DataFrame:\n\nfor lab, row in brics.iterrows() :\n    brics.loc[lab, \"name_length\"] = len(row[\"country\"])\n\nbrics[\"name_length\"] = brics[\"country\"].apply(len)\n\nWe can do a similar thing to call the upper() method on every name in the country column. However, upper() is a method, so we’ll need a slightly different approach:\n\n# Import cars data\nimport pandas as pd\ncars = pd.read_csv('data/cars.csv', index_col = 0)\n\n# Use .apply(str.upper)\n\ncars[\"COUNTRY\"] = cars[\"country\"].apply(str.upper)\nprint(cars)\n\n     cars_per_cap        country  drives_right        COUNTRY\nUS            809  United States          True  UNITED STATES\nAUS           731      Australia         False      AUSTRALIA\nJAP           588          Japan         False          JAPAN\nIN             18          India         False          INDIA\nRU            200         Russia          True         RUSSIA\nMOR            70        Morocco          True        MOROCCO\nEG             45          Egypt          True          EGYPT"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#random-float",
    "href": "datacamp/intermediatePython/IntermediatePython.html#random-float",
    "title": "Intermediate Python",
    "section": "Random float",
    "text": "Random float\nRandomness has many uses in science, art, statistics, cryptography, gaming, gambling, and other fields. You’re going to use randomness to simulate a game.\nAll the functionality you need is contained in the random package, a sub-package of numpy. In this exercise, you’ll be using two functions from this package:\n\nseed(): sets the random seed, so that your results are reproducible between simulations. As an argument, it takes an integer of your choosing. If you call the function, no output will be generated.\nrand(): if you don’t specify any arguments, it generates a random float between zero and one.\n\n\n# Set the seed\nnp.random.seed(123)\n\n# Generate and print random float\nprint(np.random.rand())\n\n0.6964691855978616"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#roll-the-dice",
    "href": "datacamp/intermediatePython/IntermediatePython.html#roll-the-dice",
    "title": "Intermediate Python",
    "section": "Roll the dice",
    "text": "Roll the dice\nIn the previous exercise, you used rand(), that generates a random float between 0 and 1.\nAs Hugo explained in the video you can just as well use randint(), also a function of the random package, to generate integers randomly. The following call generates the integer 4, 5, 6 or 7 randomly. 8 is not included.\n\nimport numpy as np\nnp.random.randint(4, 8)\n\nNumPy has already been imported as np and a seed has been set. Can you roll some dice?\n\nnp.random.seed(123)\n\n# Use randint() to simulate a dice\nprint(np.random.randint(1, 7))\n\n6\n\n# Use randint() again\nprint(np.random.randint(1, 7))\n\n3"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#determine-your-next-move",
    "href": "datacamp/intermediatePython/IntermediatePython.html#determine-your-next-move",
    "title": "Intermediate Python",
    "section": "Determine your next move",
    "text": "Determine your next move\nIn the Empire State Building bet, your next move depends on the number you get after throwing the dice. We can perfectly code this with an if-elif-else construct!\nThe sample code assumes that you’re currently at step 50. Can you fill in the missing pieces to finish the script? numpy is already imported as np and the seed has been set to 123, so you don’t have to worry about that anymore.\n\nnp.random.seed(100)\nstep = 50\n\n# Roll the dice\ndice = np.random.randint(1, 7)\n\n# Finish the control construct\nif dice <= 2 :\n    step = step - 1\nelif dice <= 5 :\n    step = step + 1\nelse :\n    step = step + np.random.randint(1,7)\n\n# Print out dice and step\nprint(dice)\n\n1\n\nprint(step)\n\n49"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#the-next-step",
    "href": "datacamp/intermediatePython/IntermediatePython.html#the-next-step",
    "title": "Intermediate Python",
    "section": "The next step",
    "text": "The next step\nBefore, you have already written Python code that determines the next step based on the previous step. Now it’s time to put this code inside a for loop so that we can simulate a random walk.\nnumpy has been imported as np.\n\n# Numpy is imported, seed is set\n\n# Initialize random_walk\nrandom_walk =[0,]\n\n# Complete the ___\nfor x in range(100) :\n    # Set step: last element in random_walk\n    step = random_walk[-1]\n\n    # Roll the dice\n    dice = np.random.randint(1,7)\n\n    # Determine next step\n    if dice <= 2:\n        step = step - 1\n    elif dice <= 5:\n        step = step + 1\n    else:\n        step = step + np.random.randint(1,7)\n\n    # append next_step to random_walk\n    random_walk.append(step)\n\n# Print random_walk\nprint(random_walk)\n\n[0, -1, 0, -1, 0, 1, 2, 5, 6, 7, 6, 5, 4, 5, 6, 7, 8, 7, 8, 7, 10, 11, 12, 13, 12, 18, 19, 20, 21, 22, 23, 24, 23, 22, 26, 25, 26, 25, 24, 25, 26, 30, 29, 28, 27, 32, 33, 32, 31, 32, 35, 34, 33, 36, 35, 40, 41, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 53, 52, 53, 54, 55, 58, 59, 60, 61, 66, 65, 66, 67, 66, 67, 68, 69, 74, 75, 76, 77, 79, 78, 77, 79, 82, 83, 84, 85, 86]"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#how-low-can-you-go",
    "href": "datacamp/intermediatePython/IntermediatePython.html#how-low-can-you-go",
    "title": "Intermediate Python",
    "section": "How low can you go?",
    "text": "How low can you go?\nThings are shaping up nicely! You already have code that calculates your location in the Empire State Building after 100 dice throws. However, there’s something we haven’t thought about - you can’t go below 0!\nA typical way to solve problems like this is by using max(). If you pass max() two arguments, the biggest one gets returned. For example, to make sure that a variable x never goes below 10 when you decrease it, you can use:\nx = max(10, x - 1)\n\n\n# Initialize random_walk\nrandom_walk = [0]\n\nfor x in range(100) :\n    step = random_walk[-1]\n    dice = np.random.randint(1,7)\n\n    if dice <= 2:\n        # Replace below: use max to make sure step can't go below 0\n        step = max(0, step - 1)\n    elif dice <= 5:\n        step = step + 1\n    else:\n        step = step + np.random.randint(1,7)\n\n    random_walk.append(step)\n\nprint(random_walk)\n\n[0, 0, 1, 2, 1, 0, 6, 5, 9, 8, 9, 10, 15, 16, 17, 18, 19, 18, 17, 16, 17, 20, 25, 26, 28, 27, 26, 25, 26, 27, 26, 30, 31, 32, 33, 38, 37, 38, 37, 38, 37, 36, 39, 40, 41, 42, 43, 42, 43, 42, 45, 46, 47, 48, 52, 53, 56, 55, 56, 58, 60, 61, 62, 63, 64, 69, 70, 69, 70, 69, 70, 73, 75, 74, 75, 80, 79, 80, 81, 82, 83, 84, 85, 84, 86, 87, 88, 89, 88, 87, 90, 89, 88, 87, 86, 89, 90, 91, 90, 95, 99]"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#visualize-the-walk",
    "href": "datacamp/intermediatePython/IntermediatePython.html#visualize-the-walk",
    "title": "Intermediate Python",
    "section": "Visualize the walk",
    "text": "Visualize the walk\nLet’s visualize this random walk! Remember how you could use matplotlib to build a line plot?\nimport matplotlib.pyplot as plt\nplt.plot(x, y)\nplt.show()\n\nThe first list you pass is mapped onto the x axis and the second list is mapped onto the y axis.\nIf you pass only one argument, Python will know what to do and will use the index of the list to map onto the x axis, and the values in the list onto the y axis.\n\n# Numpy is imported, seed is set\n\n# Initialization\nrandom_walk = [0]\n\nfor x in range(100) :\n    step = random_walk[-1]\n    dice = np.random.randint(1,7)\n\n    if dice <= 2:\n        step = max(0, step - 1)\n    elif dice <= 5:\n        step = step + 1\n    else:\n        step = step + np.random.randint(1,7)\n\n    random_walk.append(step)\n\n# Import matplotlib.pyplot as plt\n\nimport matplotlib.pyplot as plt\n\n# Plot random_walk\n\nplt.plot(random_walk)\n\n# Show the plot\n\nplt.show()"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#simulate-multiple-walks",
    "href": "datacamp/intermediatePython/IntermediatePython.html#simulate-multiple-walks",
    "title": "Intermediate Python",
    "section": "Simulate multiple walks",
    "text": "Simulate multiple walks\nA single random walk is one thing, but that doesn’t tell you if you have a good chance at winning the bet.\nTo get an idea about how big your chances are of reaching 60 steps, you can repeatedly simulate the random walk and collect the results. That’s exactly what you’ll do in this exercise.\nThe sample code already sets you off in the right direction. Another for loop is wrapped around the code you already wrote. It’s up to you to add some bits and pieces to make sure all of the results are recorded correctly.\nNote: Don’t change anything about the initialization of all_walks that is given. Setting any number inside the list will cause the exercise to crash!\n\n# Numpy is imported; seed is set\n\n# Initialize all_walks (don't change this line)\nall_walks = []\n\n# Simulate random walk 10 times\nfor i in range(10) :\n\n    # Code from before\n    random_walk = [0]\n    for x in range(100) :\n        step = random_walk[-1]\n        dice = np.random.randint(1,7)\n\n        if dice <= 2:\n            step = max(0, step - 1)\n        elif dice <= 5:\n            step = step + 1\n        else:\n            step = step + np.random.randint(1,7)\n        random_walk.append(step)\n\n    # Append random_walk to all_walks\n    all_walks.append(random_walk)\n\n# Print all_walks\nprint(all_walks)\n\n[[0, 0, 1, 2, 3, 2, 5, 4, 8, 7, 8, 12, 13, 14, 18, 17, 16, 18, 19, 20, 21, 22, 23, 24, 23, 25, 26, 27, 26, 25, 24, 25, 24, 25, 26, 27, 28, 29, 30, 31, 32, 31, 32, 33, 32, 33, 32, 35, 36, 37, 38, 37, 38, 43, 44, 45, 46, 45, 44, 45, 44, 45, 46, 47, 48, 47, 46, 47, 48, 49, 50, 51, 52, 54, 59, 58, 57, 63, 62, 63, 62, 61, 60, 61, 62, 63, 64, 65, 68, 69, 70, 71, 70, 72, 73, 74, 75, 76, 75, 76, 75], [0, 1, 0, 1, 4, 5, 4, 5, 4, 3, 2, 1, 2, 1, 0, 1, 2, 1, 2, 3, 4, 5, 6, 5, 4, 5, 6, 7, 6, 11, 10, 11, 13, 14, 18, 17, 16, 15, 14, 15, 14, 13, 12, 16, 21, 22, 21, 22, 21, 22, 21, 22, 23, 25, 26, 28, 29, 34, 35, 36, 37, 38, 39, 40, 41, 42, 46, 45, 44, 45, 47, 48, 49, 50, 56, 55, 54, 55, 56, 57, 56, 58, 59, 60, 61, 62, 63, 62, 63, 65, 67, 68, 72, 74, 79, 80, 79, 85, 86, 85, 86], [0, 1, 2, 1, 2, 6, 5, 6, 7, 11, 12, 11, 10, 9, 10, 11, 12, 11, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 20, 26, 27, 26, 27, 28, 29, 30, 31, 32, 31, 30, 31, 32, 31, 32, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 42, 43, 44, 45, 46, 47, 46, 47, 48, 49, 50, 49, 48, 49, 48, 49, 50, 51, 52, 51, 53, 54, 53, 52, 51, 52, 51, 50, 54, 53, 54, 55, 56, 55, 56, 57, 62, 61, 67, 68, 69, 70], [0, 1, 2, 3, 2, 1, 2, 3, 4, 5, 6, 7, 8, 7, 9, 8, 10, 11, 12, 11, 10, 11, 12, 11, 14, 15, 16, 17, 18, 19, 24, 25, 24, 23, 24, 25, 26, 27, 28, 29, 30, 29, 28, 27, 28, 27, 28, 27, 28, 29, 28, 27, 28, 27, 33, 32, 33, 32, 31, 32, 36, 37, 38, 37, 38, 39, 40, 41, 42, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 50, 51, 52, 53, 54, 55, 56, 55, 54, 53, 54, 53, 52, 51, 52, 53, 54, 56, 55, 56, 55], [0, 6, 7, 6, 7, 6, 7, 12, 11, 13, 14, 13, 14, 13, 14, 15, 14, 13, 14, 13, 14, 16, 15, 16, 15, 16, 17, 18, 17, 16, 17, 18, 19, 18, 17, 18, 19, 20, 21, 20, 21, 20, 19, 24, 27, 28, 27, 26, 27, 28, 29, 30, 29, 34, 35, 36, 37, 40, 39, 40, 42, 41, 43, 44, 45, 46, 45, 46, 45, 46, 45, 44, 45, 44, 45, 46, 45, 50, 51, 55, 60, 59, 60, 59, 60, 61, 67, 66, 65, 66, 65, 67, 68, 69, 74, 73, 78, 79, 80, 81, 80], [0, 1, 0, 1, 4, 5, 4, 5, 6, 5, 6, 7, 6, 7, 12, 13, 14, 13, 14, 13, 14, 15, 16, 15, 16, 17, 18, 19, 20, 19, 20, 25, 24, 25, 26, 31, 32, 31, 30, 29, 30, 31, 32, 33, 32, 33, 34, 33, 32, 36, 35, 36, 37, 36, 40, 39, 40, 42, 43, 44, 45, 46, 51, 52, 51, 52, 53, 52, 51, 52, 57, 58, 59, 60, 61, 67, 66, 67, 68, 67, 68, 67, 66, 65, 66, 67, 68, 71, 76, 75, 76, 77, 76, 77, 76, 77, 76, 77, 80, 79, 80], [0, 0, 0, 1, 2, 4, 5, 6, 7, 8, 7, 6, 5, 6, 7, 6, 11, 12, 14, 15, 16, 17, 16, 15, 14, 15, 16, 15, 16, 17, 22, 23, 24, 25, 26, 25, 24, 23, 22, 21, 22, 21, 22, 23, 24, 25, 26, 25, 26, 28, 29, 28, 29, 28, 29, 30, 31, 30, 29, 28, 33, 34, 40, 41, 42, 45, 46, 45, 44, 47, 46, 45, 46, 47, 48, 47, 46, 47, 50, 49, 50, 51, 52, 58, 57, 56, 57, 58, 59, 64, 68, 69, 70, 75, 76, 75, 74, 73, 72, 71, 72], [0, 1, 2, 3, 4, 5, 4, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 13, 14, 15, 14, 13, 14, 13, 12, 13, 14, 15, 16, 15, 14, 15, 14, 13, 14, 15, 14, 15, 14, 15, 16, 22, 25, 24, 23, 22, 23, 22, 21, 22, 25, 29, 30, 31, 36, 37, 36, 35, 36, 37, 36, 41, 40, 39, 38, 39, 40, 39, 40, 39, 40, 41, 42, 43, 46, 49, 54, 53, 52, 53, 54, 55, 56, 57, 58, 59, 58, 57, 56, 57, 58, 59, 58, 57, 58, 64, 65, 66, 67, 72, 73], [0, 1, 2, 1, 0, 0, 1, 0, 1, 2, 1, 2, 3, 6, 7, 8, 7, 10, 11, 10, 12, 13, 14, 13, 12, 13, 12, 13, 14, 13, 14, 18, 22, 23, 26, 27, 28, 31, 32, 31, 32, 33, 34, 35, 36, 35, 36, 35, 34, 37, 43, 42, 41, 42, 41, 42, 41, 40, 39, 40, 39, 40, 41, 40, 41, 40, 41, 42, 43, 44, 48, 54, 55, 56, 57, 58, 59, 60, 59, 60, 61, 60, 61, 60, 59, 60, 61, 62, 63, 62, 63, 64, 67, 66, 65, 64, 65, 66, 67, 68, 67], [0, 1, 0, 5, 6, 7, 8, 7, 8, 9, 8, 7, 8, 7, 6, 7, 8, 7, 8, 7, 8, 9, 14, 17, 16, 17, 18, 17, 18, 19, 20, 19, 20, 21, 20, 21, 20, 21, 26, 25, 24, 25, 26, 27, 26, 27, 28, 29, 30, 29, 30, 31, 33, 37, 36, 37, 38, 39, 40, 41, 40, 41, 40, 41, 40, 41, 42, 43, 44, 43, 44, 43, 42, 43, 42, 43, 42, 41, 46, 45, 44, 43, 42, 43, 44, 43, 44, 43, 44, 43, 42, 41, 40, 41, 42, 43, 44, 45, 44, 45, 46]]"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#visualize-all-walks",
    "href": "datacamp/intermediatePython/IntermediatePython.html#visualize-all-walks",
    "title": "Intermediate Python",
    "section": "Visualize all walks",
    "text": "Visualize all walks\nall_walks is a list of lists: every sub-list represents a single random walk. If you convert this list of lists to a NumPy array, you can start making interesting plots! matplotlib.pyplot is already imported as plt.\nThe nested for loop is already coded for you - don’t worry about it. For now, focus on the code that comes after this for loop.\n\n# numpy and matplotlib imported, seed set.\n\n# initialize and populate all_walks\nall_walks = []\nfor i in range(10) :\n    random_walk = [0]\n    for x in range(100) :\n        step = random_walk[-1]\n        dice = np.random.randint(1,7)\n        if dice <= 2:\n            step = max(0, step - 1)\n        elif dice <= 5:\n            step = step + 1\n        else:\n            step = step + np.random.randint(1,7)\n        random_walk.append(step)\n    all_walks.append(random_walk)\n\n# Convert all_walks to Numpy array: np_aw\nnp_aw = np.array(all_walks)\n\n# Plot np_aw and show\nplt.plot(np_aw)\nplt.show()\n\n\n\n# Clear the figure\nplt.clf()\n\n# Transpose np_aw: np_aw_t\n\nnp_aw_t = np.transpose(np_aw)\n# Plot np_aw_t and show\n\nplt.plot(np_aw_t)\nplt.show()"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#implement-clumsiness",
    "href": "datacamp/intermediatePython/IntermediatePython.html#implement-clumsiness",
    "title": "Intermediate Python",
    "section": "Implement clumsiness",
    "text": "Implement clumsiness\nWith this neatly written code of yours, changing the number of times the random walk should be simulated is super-easy. You simply update the range() function in the top-level for loop.\nThere’s still something we forgot! You’re a bit clumsy and you have a 0.5% chance of falling down. That calls for another random number generation. Basically, you can generate a random float between 0 and 1. If this value is less than or equal to 0.005, you should reset step to 0.\n\n# numpy and matplotlib imported, seed set\n\n# Simulate random walk 250 times\nall_walks = []\nfor i in range(250) :\n    random_walk = [0]\n    for x in range(100) :\n        step = random_walk[-1]\n        dice = np.random.randint(1,7)\n        if dice <= 2:\n            step = max(0, step - 1)\n        elif dice <= 5:\n            step = step + 1\n        else:\n            step = step + np.random.randint(1,7)\n\n        # Implement clumsiness\n        if np.random.rand() <= 0.001 :\n            step = 0\n\n        random_walk.append(step)\n    all_walks.append(random_walk)\n\n# Create and plot np_aw_t\nnp_aw_t = np.transpose(np.array(all_walks))\nplt.plot(np_aw_t)\nplt.show()"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#plot-the-distribution",
    "href": "datacamp/intermediatePython/IntermediatePython.html#plot-the-distribution",
    "title": "Intermediate Python",
    "section": "Plot the distribution",
    "text": "Plot the distribution\nAll these fancy visualizations have put us on a sidetrack. We still have to solve the million-dollar problem: What are the odds that you’ll reach 60 steps high on the Empire State Building?\nBasically, you want to know about the end points of all the random walks you’ve simulated. These end points have a certain distribution that you can visualize with a histogram.\nNote that if your code is taking too long to run, you might be plotting a histogram of the wrong data!\n\n# numpy and matplotlib imported, seed set\n\n# Simulate random walk 500 times\nall_walks = []\nfor i in range(500) :\n    random_walk = [0]\n    for x in range(100) :\n        step = random_walk[-1]\n        dice = np.random.randint(1,7)\n        if dice <= 2:\n            step = max(0, step - 1)\n        elif dice <= 5:\n            step = step + 1\n        else:\n            step = step + np.random.randint(1,7)\n        if np.random.rand() <= 0.001 :\n            step = 0\n        random_walk.append(step)\n    all_walks.append(random_walk)\n\n# Create and plot np_aw_t\nnp_aw_t = np.transpose(np.array(all_walks))\n\n# Select last row from np_aw_t: ends\nends = np_aw_t[100,:]\n\n# Plot histogram of ends, display plot\nplt.hist(ends)\nplt.show()"
  },
  {
    "objectID": "datacamp/intermediatePython/IntermediatePython.html#calculate-the-odds",
    "href": "datacamp/intermediatePython/IntermediatePython.html#calculate-the-odds",
    "title": "Intermediate Python",
    "section": "Calculate the odds",
    "text": "Calculate the odds\nThe histogram of the previous exercise was created from a NumPy array ends, that contains 500 integers. Each integer represents the end point of a random walk. To calculate the chance that this end point is greater than or equal to 60, you can count the number of integers in ends that are greater than or equal to 60 and divide that number by 500, the total number of simulations.\nWell then, what’s the estimated chance that you’ll reach at least 60 steps high if you play this Empire State Building game? The ends array is everything you need; it’s available in your Python session so you can make calculations in the IPython Shell.\n\nround(len(ends[ends>=60])/len(ends) * 100, 2)\n\n75.8"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html",
    "title": "Data Manipulation with pandas",
    "section": "",
    "text": "When you get a new DataFrame to work with, the first thing you need to do is explore it and see what it contains. There are several useful methods and attributes for this.\n\nhead() returns the first few rows (the “head” of the DataFrame).\ninfo() shows information on each of the columns, such as the data type and number of missing values.\nshape returns the number of rows and columns of the DataFrame.\ndescribe() calculates a few summary statistics for each column. homelessness is a DataFrame containing estimates of homelessness in each U.S. state in 2018. The individual column is the number of homeless individuals not part of a family with children. The family_members column is the number of homeless individuals part of a family with children. The state_pop column is the state’s total population.\n\n\nimport pandas as pd\n\nhomelessness = pd.read_csv('data/homelessness.csv', index_col = 0)\n\n\n# Print the head of the homelessness data\nprint(homelessness.head())\n\n               region       state  individuals  family_members  state_pop\n0  East South Central     Alabama       2570.0           864.0    4887681\n1             Pacific      Alaska       1434.0           582.0     735139\n2            Mountain     Arizona       7259.0          2606.0    7158024\n3  West South Central    Arkansas       2280.0           432.0    3009733\n4             Pacific  California     109008.0         20964.0   39461588\n\n# Print information about homelessness\nprint(homelessness.info())\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 51 entries, 0 to 50\nData columns (total 5 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   region          51 non-null     object \n 1   state           51 non-null     object \n 2   individuals     51 non-null     float64\n 3   family_members  51 non-null     float64\n 4   state_pop       51 non-null     int64  \ndtypes: float64(2), int64(1), object(2)\nmemory usage: 2.4+ KB\nNone\n\n# Print the shape of homelessness\nprint(homelessness.shape)\n\n(51, 5)\n\n# Print a description of homelessness\n\nprint(homelessness.describe())\n\n         individuals  family_members     state_pop\ncount      51.000000       51.000000  5.100000e+01\nmean     7225.784314     3504.882353  6.405637e+06\nstd     15991.025083     7805.411811  7.327258e+06\nmin       434.000000       75.000000  5.776010e+05\n25%      1446.500000      592.000000  1.777414e+06\n50%      3082.000000     1482.000000  4.461153e+06\n75%      6781.500000     3196.000000  7.340946e+06\nmax    109008.000000    52070.000000  3.946159e+07\n\n\n\n\n\nTo better understand DataFrame objects, it’s useful to know that they consist of three components, stored as attributes:\n\n.values: A two-dimensional NumPy array of values.\n.columns: An index of columns: the column names.\n.index: An index for the rows: either row numbers or row names. You can usually think of indexes as a list of strings or numbers, though the pandas Index data type allows for more sophisticated options. (These will be covered later in the course.)\n\n\n# Print the values of homelessness\n\nprint(homelessness.values)\n\n[['East South Central' 'Alabama' 2570.0 864.0 4887681]\n ['Pacific' 'Alaska' 1434.0 582.0 735139]\n ['Mountain' 'Arizona' 7259.0 2606.0 7158024]\n ['West South Central' 'Arkansas' 2280.0 432.0 3009733]\n ['Pacific' 'California' 109008.0 20964.0 39461588]\n ['Mountain' 'Colorado' 7607.0 3250.0 5691287]\n ['New England' 'Connecticut' 2280.0 1696.0 3571520]\n ['South Atlantic' 'Delaware' 708.0 374.0 965479]\n ['South Atlantic' 'District of Columbia' 3770.0 3134.0 701547]\n ['South Atlantic' 'Florida' 21443.0 9587.0 21244317]\n ['South Atlantic' 'Georgia' 6943.0 2556.0 10511131]\n ['Pacific' 'Hawaii' 4131.0 2399.0 1420593]\n ['Mountain' 'Idaho' 1297.0 715.0 1750536]\n ['East North Central' 'Illinois' 6752.0 3891.0 12723071]\n ['East North Central' 'Indiana' 3776.0 1482.0 6695497]\n ['West North Central' 'Iowa' 1711.0 1038.0 3148618]\n ['West North Central' 'Kansas' 1443.0 773.0 2911359]\n ['East South Central' 'Kentucky' 2735.0 953.0 4461153]\n ['West South Central' 'Louisiana' 2540.0 519.0 4659690]\n ['New England' 'Maine' 1450.0 1066.0 1339057]\n ['South Atlantic' 'Maryland' 4914.0 2230.0 6035802]\n ['New England' 'Massachusetts' 6811.0 13257.0 6882635]\n ['East North Central' 'Michigan' 5209.0 3142.0 9984072]\n ['West North Central' 'Minnesota' 3993.0 3250.0 5606249]\n ['East South Central' 'Mississippi' 1024.0 328.0 2981020]\n ['West North Central' 'Missouri' 3776.0 2107.0 6121623]\n ['Mountain' 'Montana' 983.0 422.0 1060665]\n ['West North Central' 'Nebraska' 1745.0 676.0 1925614]\n ['Mountain' 'Nevada' 7058.0 486.0 3027341]\n ['New England' 'New Hampshire' 835.0 615.0 1353465]\n ['Mid-Atlantic' 'New Jersey' 6048.0 3350.0 8886025]\n ['Mountain' 'New Mexico' 1949.0 602.0 2092741]\n ['Mid-Atlantic' 'New York' 39827.0 52070.0 19530351]\n ['South Atlantic' 'North Carolina' 6451.0 2817.0 10381615]\n ['West North Central' 'North Dakota' 467.0 75.0 758080]\n ['East North Central' 'Ohio' 6929.0 3320.0 11676341]\n ['West South Central' 'Oklahoma' 2823.0 1048.0 3940235]\n ['Pacific' 'Oregon' 11139.0 3337.0 4181886]\n ['Mid-Atlantic' 'Pennsylvania' 8163.0 5349.0 12800922]\n ['New England' 'Rhode Island' 747.0 354.0 1058287]\n ['South Atlantic' 'South Carolina' 3082.0 851.0 5084156]\n ['West North Central' 'South Dakota' 836.0 323.0 878698]\n ['East South Central' 'Tennessee' 6139.0 1744.0 6771631]\n ['West South Central' 'Texas' 19199.0 6111.0 28628666]\n ['Mountain' 'Utah' 1904.0 972.0 3153550]\n ['New England' 'Vermont' 780.0 511.0 624358]\n ['South Atlantic' 'Virginia' 3928.0 2047.0 8501286]\n ['Pacific' 'Washington' 16424.0 5880.0 7523869]\n ['South Atlantic' 'West Virginia' 1021.0 222.0 1804291]\n ['East North Central' 'Wisconsin' 2740.0 2167.0 5807406]\n ['Mountain' 'Wyoming' 434.0 205.0 577601]]\n\n# Print the column index of homelessness\n\nprint(homelessness.columns)\n\nIndex(['region', 'state', 'individuals', 'family_members', 'state_pop'], dtype='object')\n\n# Print the row index of homelessness\n\nprint(homelessness.index)\n\nInt64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n            34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n            50],\n           dtype='int64')\n\n\n\n\n\nFinding interesting bits of data in a DataFrame is often easier if you change the order of the rows. You can sort the rows by passing a column name to .sort_values().\nIn cases where rows have the same value (this is common if you sort on a categorical variable), you may wish to break the ties by sorting on another column. You can sort on multiple columns in this way by passing a list of column names.\n\n\n\none column df.sort_values(“breed”)\nmultiple columns df.sort_values([“breed”, “weight_kg”])\nBy combining .sort_values() with .head(), you can answer questions in the form, “What are the top cases where…?”.\n\n\n# Sort homelessness by individuals\nhomelessness_ind = homelessness.sort_values('individuals')\n\n# Print the top few rows\nprint(homelessness_ind.head())\n\n                region         state  individuals  family_members  state_pop\n50            Mountain       Wyoming        434.0           205.0     577601\n34  West North Central  North Dakota        467.0            75.0     758080\n7       South Atlantic      Delaware        708.0           374.0     965479\n39         New England  Rhode Island        747.0           354.0    1058287\n45         New England       Vermont        780.0           511.0     624358\n\n# Sort homelessness by descending family members\nhomelessness_fam = homelessness.sort_values('family_members', ascending = False)\n\n# Print the top few rows\n\nprint(homelessness_fam.head())\n\n                region          state  individuals  family_members  state_pop\n32        Mid-Atlantic       New York      39827.0         52070.0   19530351\n4              Pacific     California     109008.0         20964.0   39461588\n21         New England  Massachusetts       6811.0         13257.0    6882635\n9       South Atlantic        Florida      21443.0          9587.0   21244317\n43  West South Central          Texas      19199.0          6111.0   28628666\n\n# Sort homelessness by region, then descending family members\nhomelessness_reg_fam =  homelessness.sort_values(['region', 'family_members'], ascending = [True, False])\n\n# Print the top few rows\n\nprint(homelessness_reg_fam.head())\n\n                region      state  individuals  family_members  state_pop\n13  East North Central   Illinois       6752.0          3891.0   12723071\n35  East North Central       Ohio       6929.0          3320.0   11676341\n22  East North Central   Michigan       5209.0          3142.0    9984072\n49  East North Central  Wisconsin       2740.0          2167.0    5807406\n14  East North Central    Indiana       3776.0          1482.0    6695497\n\n\n\n\n\n\nWhen working with data, you may not need all of the variables in your dataset. Square brackets ([]) can be used to select only the columns that matter to you in an order that makes sense to you. To select only “col_a” of the DataFrame df, use\ndf[\"col_a\"]\n\nTo select “col_a” and “col_b” of df, use\ndf[[\"col_a\", \"col_b\"]]\n\n\n# Select the individuals column\nindividuals = homelessness['individuals']\n\n# Print the head of the result\nprint(individuals.head())\n\n0      2570.0\n1      1434.0\n2      7259.0\n3      2280.0\n4    109008.0\nName: individuals, dtype: float64\n\n# Select the state and family_members columns\nstate_fam = homelessness[['state', 'family_members']]\n\n# Print the head of the result\n\nprint(state_fam.head())\n\n        state  family_members\n0     Alabama           864.0\n1      Alaska           582.0\n2     Arizona          2606.0\n3    Arkansas           432.0\n4  California         20964.0\n\n# Select only the individuals and state columns, in that order\nind_state =  homelessness[['individuals', 'state']]\n\n# Print the head of the result\n\nprint(ind_state.head())\n\n   individuals       state\n0       2570.0     Alabama\n1       1434.0      Alaska\n2       7259.0     Arizona\n3       2280.0    Arkansas\n4     109008.0  California\n\n\n\n\n\nA large part of data science is about finding which bits of your dataset are interesting. One of the simplest techniques for this is to find a subset of rows that match some criteria. This is sometimes known as filtering rows or selecting rows.\nThere are many ways to subset a DataFrame, perhaps the most common is to use relational operators to return True or False for each row, then pass that inside square brackets.\ndogs[dogs[\"height_cm\"] > 60]\ndogs[dogs[\"color\"] == \"tan\"]\n\nYou can filter for multiple conditions at once by using the “bitwise and” operator, &.\ndogs[(dogs[\"height_cm\"] > 60) & (dogs[\"color\"] == \"tan\")]\n\n\n# Filter for rows where individuals is greater than 10000\nind_gt_10k = homelessness[homelessness['individuals'] > 10000]\n\n# See the result\nprint(ind_gt_10k)\n\n                region       state  individuals  family_members  state_pop\n4              Pacific  California     109008.0         20964.0   39461588\n9       South Atlantic     Florida      21443.0          9587.0   21244317\n32        Mid-Atlantic    New York      39827.0         52070.0   19530351\n37             Pacific      Oregon      11139.0          3337.0    4181886\n43  West South Central       Texas      19199.0          6111.0   28628666\n47             Pacific  Washington      16424.0          5880.0    7523869\n\n# Filter for rows where region is Mountain\nmountain_reg =  homelessness[homelessness['region'] == 'Mountain']\n\n# See the result\nprint(mountain_reg)\n\n      region       state  individuals  family_members  state_pop\n2   Mountain     Arizona       7259.0          2606.0    7158024\n5   Mountain    Colorado       7607.0          3250.0    5691287\n12  Mountain       Idaho       1297.0           715.0    1750536\n26  Mountain     Montana        983.0           422.0    1060665\n28  Mountain      Nevada       7058.0           486.0    3027341\n31  Mountain  New Mexico       1949.0           602.0    2092741\n44  Mountain        Utah       1904.0           972.0    3153550\n50  Mountain     Wyoming        434.0           205.0     577601\n\n# Filter for rows where family_members is less than 1000 \n# and region is Pacific\nfam_lt_1k_pac = homelessness[(homelessness['region'] == 'Pacific') & (homelessness['family_members']<1000 )]\n\n# See the result\nprint(fam_lt_1k_pac)\n\n    region   state  individuals  family_members  state_pop\n1  Pacific  Alaska       1434.0           582.0     735139\n\n\n\n\n\nSubsetting data based on a categorical variable often involves using the “or” operator (|) to select rows from multiple categories. This can get tedious when you want all states in one of three different regions, for example. Instead, use the .isin() method, which will allow you to tackle this problem by writing one condition instead of three separate ones.\ncolors = [\"brown\", \"black\", \"tan\"]\ncondition = dogs[\"color\"].isin(colors)\ndogs[condition]\n\n\n# Subset for rows in South Atlantic or Mid-Atlantic regions\nsouth_mid_atlantic =  homelessness[homelessness['region'].isin([\"South Atlantic\", \"Mid-Atlantic\"])]\n\n# See the result\nprint(south_mid_atlantic)\n\n            region                 state  ...  family_members  state_pop\n7   South Atlantic              Delaware  ...           374.0     965479\n8   South Atlantic  District of Columbia  ...          3134.0     701547\n9   South Atlantic               Florida  ...          9587.0   21244317\n10  South Atlantic               Georgia  ...          2556.0   10511131\n20  South Atlantic              Maryland  ...          2230.0    6035802\n30    Mid-Atlantic            New Jersey  ...          3350.0    8886025\n32    Mid-Atlantic              New York  ...         52070.0   19530351\n33  South Atlantic        North Carolina  ...          2817.0   10381615\n38    Mid-Atlantic          Pennsylvania  ...          5349.0   12800922\n40  South Atlantic        South Carolina  ...           851.0    5084156\n46  South Atlantic              Virginia  ...          2047.0    8501286\n48  South Atlantic         West Virginia  ...           222.0    1804291\n\n[12 rows x 5 columns]\n\n# The Mojave Desert states\ncanu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\n\n# Filter for rows in the Mojave Desert states\nmojave_homelessness = homelessness[homelessness['state'].isin(canu)]\n\n# See the result\nprint(mojave_homelessness)\n\n      region       state  individuals  family_members  state_pop\n2   Mountain     Arizona       7259.0          2606.0    7158024\n4    Pacific  California     109008.0         20964.0   39461588\n28  Mountain      Nevada       7058.0           486.0    3027341\n44  Mountain        Utah       1904.0           972.0    3153550\n\n\n\n\n\nYou aren’t stuck with just the data you are given. Instead, you can add new columns to a DataFrame. This has many names, such as transforming, mutating, and feature engineering.\nYou can create new columns from scratch, but it is also common to derive them from other columns, for example, by adding columns together or by changing their units.\nhomelessness is available and pandas is loaded as pd.\n\n# Add total col as sum of individuals and family_members\n\nhomelessness['total'] = homelessness['family_members'] + homelessness['individuals']\n\n# Add p_individuals col as proportion of total that are individuals\n\nhomelessness['p_individuals'] = homelessness['individuals']/homelessness['total'] \n\n\n# See the result\nprint(homelessness.head())\n\n               region       state  ...     total  p_individuals\n0  East South Central     Alabama  ...    3434.0       0.748398\n1             Pacific      Alaska  ...    2016.0       0.711310\n2            Mountain     Arizona  ...    9865.0       0.735834\n3  West South Central    Arkansas  ...    2712.0       0.840708\n4             Pacific  California  ...  129972.0       0.838704\n\n[5 rows x 7 columns]\n\n\n\n\n\nYou’ve seen the four most common types of data manipulation: sorting rows, subsetting columns, subsetting rows, and adding new columns. In a real-life data analysis, you can mix and match these four manipulations to answer a multitude of questions.\nIn this exercise, you’ll answer the question, “Which state has the highest number of homeless individuals per 10,000 people in the state?” Combine your new pandas skills to find out.\n\n# Create indiv_per_10k col as homeless individuals per 10k state pop\nhomelessness[\"indiv_per_10k\"] = 10000 * homelessness['individuals']/ homelessness['state_pop']\n\n# Subset rows for indiv_per_10k greater than 20\nhigh_homelessness = homelessness[homelessness[\"indiv_per_10k\"]  > 20]\n\n# Sort high_homelessness by descending indiv_per_10k\nhigh_homelessness_srt = high_homelessness.sort_values('indiv_per_10k', ascending = False)\n\n# From high_homelessness_srt, select the state and indiv_per_10k cols\nresult = high_homelessness_srt[['state', 'indiv_per_10k']]\n\n# See the result\nprint(result)\n\n                   state  indiv_per_10k\n8   District of Columbia      53.738381\n11                Hawaii      29.079406\n4             California      27.623825\n37                Oregon      26.636307\n28                Nevada      23.314189\n47            Washington      21.829195\n32              New York      20.392363"
  },
  {
    "objectID": "inProcess/Diabetes/predict_diabetes_tidymodels.html",
    "href": "inProcess/Diabetes/predict_diabetes_tidymodels.html",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(data.table)\nlibrary(gtsummary)\nlibrary(mTools)\nDiabetes data"
  },
  {
    "objectID": "inProcess/Diabetes/predict_diabetes_tidymodels.html#data-processing",
    "href": "inProcess/Diabetes/predict_diabetes_tidymodels.html#data-processing",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "Data processing",
    "text": "Data processing\n\ndiabetes_df_all[, diabetes_char := factor(diabetes, \n                                      levels = c(0, 1),\n                                      labels = c(\"Non diabetic\", \"Diabetic\"))]\n\nre_balance_class <- function(df, outcome_col = \"diabetes_char\", pos_class = \"Diabetic\", pos_class_perc = .4){\n    \n    pos_class_df = df[get(outcome_col) == pos_class]\n    neg_class = df[get(outcome_col) != pos_class]\n    pos_perc = nrow(pos_class_df)/nrow(df)\n    N = round(nrow(pos_class_df)/pos_class_perc)\n    Nneg = N - nrow(pos_class_df)\n    neg_class_df = neg_class[sample(1:.N, Nneg)]\n    rbind(pos_class_df,neg_class_df )\n    \n    \n}\n\ndiabetes_df = re_balance_class(df = diabetes_df_all)"
  },
  {
    "objectID": "inProcess/Diabetes/predict_diabetes_tidymodels.html#summary-stats",
    "href": "inProcess/Diabetes/predict_diabetes_tidymodels.html#summary-stats",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "Summary Stats",
    "text": "Summary Stats\n\nlibrary(ggiraph)\ndb_perc <- diabetes_df[, .(freq = .N),\n                       by = diabetes_char][\n                           ,perc := round(freq/sum(freq) * 100, 1)]\n\n\nggplot(db_perc, aes(diabetes_char, freq, fill = diabetes_char))+\n    geom_bar_interactive(width = 0.5, stat = \"identity\")+\n    geom_text(aes(label = paste0(freq, \"(\", perc, \"%)\")),\n              position = position_dodge(width = 0.5),\n              vjust = 0.05)+\n    scale_fill_brewer(name = \"\", type = \"qual\", palette = \"Dark2\")+\n    theme_minimal()+\n    theme(\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\ntab2 <- diabetes_df %>%\n    tbl_summary(\n        by = diabetes_char,\n        type = all_continuous() ~ \"continuous2\",\n        statistic = all_continuous() ~ c(\n            \"{mean} ({sd})\",\n            \"{median} ({p25}, {p75})\",\n            \"[{min}, {max}]\"\n        ),\n        missing = \"ifany\"\n    ) %>%\n    add_p(pvalue_fun = ~ style_pvalue(.x, digits = 2))\n\ntab_df = as.data.frame(tab2)\nnms <- names(tab_df)\nnms <- gsub(\"\\\\*\", \"\", nms)\nnames(tab_df) <- nms\ndata_table(tab_df)"
  },
  {
    "objectID": "inProcess/Diabetes/predict_diabetes_tidymodels.html#model-fitting",
    "href": "inProcess/Diabetes/predict_diabetes_tidymodels.html#model-fitting",
    "title": "Diabetes Prediction using Tidymodels",
    "section": "Model Fitting",
    "text": "Model Fitting\n\nset.seed(100)\ndiabetes_df[, diabetes:= as.factor(diabetes)]\ndiabetes_df_split <- initial_split(diabetes_df[,.SD, .SDcols = !\"diabetes_char\"], \n                                   strata = diabetes)\n\ndiabetes_df_train <- training(diabetes_df_split)\n\ndiabetes_df_test <- testing(diabetes_df_split)\n\n\n# Specify a logistic regression model\nlogistic_model <- logistic_reg() %>% \n  # Set the engine\n  set_engine('glm') %>% \n  # Set the mode\n  set_mode('classification')\n\n# Fit to training data\nlogistic_fit <- logistic_model %>% \n  fit(diabetes ~ .,\n      data = diabetes_df_train)\n\n# Print model fit object\nlogistic_fit %>% \n    DT_tidy_model()\n\n\n\n\n\n\n\nxgb_spec <- boost_tree(\n    trees = 2000,\n    tree_depth = tune(), \n    min_n = tune(),\n    loss_reduction = tune(),                     ## first three: model complexity\n    sample_size = tune(), \n    mtry = tune(),         ## randomness\n    learn_rate = tune()                          ## step size\n) %>%\n    set_engine(\"xgboost\") %>%\n    set_mode(\"classification\")\n\nxgb_spec\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 2000\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost"
  },
  {
    "objectID": "inProcess/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to-2022",
    "href": "inProcess/cbk_data/R/kenya_income_exp.html#income-expenditure-from-2000-to-2022",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "Income & Expenditure from 2000 to 2022",
    "text": "Income & Expenditure from 2000 to 2022\nDecided to have a quick look of the Kenyan government expenditure & income due to the current cash crunch\nThere are a number of factors that could be contributing to this trend. One factor is the growth of the economy. As the economy grows, the government needs to spend more money on things like infrastructure, education, and healthcare. Another factor is the growth of the population. As the population grows, the government needs to spend more money on things like social welfare programs and security."
  },
  {
    "objectID": "inProcess/cbk_data/R/kenya_income_exp.html#what-type-of-expenditure-is-rising",
    "href": "inProcess/cbk_data/R/kenya_income_exp.html#what-type-of-expenditure-is-rising",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "What type of expenditure is rising",
    "text": "What type of expenditure is rising"
  },
  {
    "objectID": "inProcess/cbk_data/R/kenya_income_exp.html#what-sub-type-of-reccurrent-expenditure-is-causing-the-rise",
    "href": "inProcess/cbk_data/R/kenya_income_exp.html#what-sub-type-of-reccurrent-expenditure-is-causing-the-rise",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "What Sub type of reccurrent expenditure is causing the rise",
    "text": "What Sub type of reccurrent expenditure is causing the rise"
  },
  {
    "objectID": "inProcess/cbk_data/R/kenya_income_exp.html#domestic-debt-composition",
    "href": "inProcess/cbk_data/R/kenya_income_exp.html#domestic-debt-composition",
    "title": "Kenya Government Income & Expenditure from 2001 to 2022",
    "section": "Domestic debt composition",
    "text": "Domestic debt composition"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#mean-and-median",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#mean-and-median",
    "title": "Data Manipulation with pandas",
    "section": "Mean and median",
    "text": "Mean and median\nSummary statistics are exactly what they sound like - they summarize many numbers in one statistic. For example, mean, median, minimum, maximum, and standard deviation are summary statistics. Calculating summary statistics allows you to get a better sense of your data, even if there’s a lot of it.\nsales is available and pandas is loaded as pd.\n\nsales = pd.read_csv(\"data/sales_subset.csv\", index_col = 0)\n\n# Print the head of the sales DataFrame\nprint(sales.head())\n\n   store type  department  ... temperature_c  fuel_price_usd_per_l  unemployment\n0      1    A           1  ...      5.727778              0.679451         8.106\n1      1    A           1  ...      8.055556              0.693452         8.106\n2      1    A           1  ...     16.816667              0.718284         7.808\n3      1    A           1  ...     22.527778              0.748928         7.808\n4      1    A           1  ...     27.050000              0.714586         7.808\n\n[5 rows x 9 columns]\n\n# Print the info about the sales DataFrame\nprint(sales.info())\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10774 entries, 0 to 10773\nData columns (total 9 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   store                 10774 non-null  int64  \n 1   type                  10774 non-null  object \n 2   department            10774 non-null  int64  \n 3   date                  10774 non-null  object \n 4   weekly_sales          10774 non-null  float64\n 5   is_holiday            10774 non-null  bool   \n 6   temperature_c         10774 non-null  float64\n 7   fuel_price_usd_per_l  10774 non-null  float64\n 8   unemployment          10774 non-null  float64\ndtypes: bool(1), float64(4), int64(2), object(2)\nmemory usage: 768.1+ KB\nNone\n\n# Print the mean of weekly_sales\nprint(sales['weekly_sales'].mean())\n\n23843.95014850566\n\n# Print the median of weekly_sales\nprint(sales['weekly_sales'].median())\n\n12049.064999999999"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#summarizing-dates",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#summarizing-dates",
    "title": "Data Manipulation with pandas",
    "section": "Summarizing dates",
    "text": "Summarizing dates\nSummary statistics can also be calculated on date columns that have values with the data type datetime64. Some summary statistics — like mean — don’t make a ton of sense on dates, but others are super helpful, for example, minimum and maximum, which allow you to see what time range your data covers.\nsales is available and pandas is loaded as pd.\n\n# Print the maximum of the date column\nsales[\"date\"] = pd.to_datetime(sales[\"date\"])\nprint(sales['date'].max())\n\n2012-10-26 00:00:00\n\n# Print the minimum of the date column\nprint(sales['date'].min())\n\n2010-02-05 00:00:00"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#efficient-summaries",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#efficient-summaries",
    "title": "Data Manipulation with pandas",
    "section": "Efficient summaries",
    "text": "Efficient summaries\nWhile pandas and NumPy have tons of functions, sometimes, you may need a different function to summarize your data.\nThe .agg() method allows you to apply your own custom functions to a DataFrame, as well as apply functions to more than one column of a DataFrame at once, making your aggregations super-efficient. For example,\n\ndf['column'].agg(function)\n\nIn the custom function for this exercise, “IQR” is short for inter-quartile range, which is the 75th percentile minus the 25th percentile. It’s an alternative to standard deviation that is helpful if your data contains outliers.\nsales is available and pandas is loaded as pd.\n\nimport numpy as np\n\n# A custom IQR function\ndef iqr(column):\n    return column.quantile(0.75) - column.quantile(0.25)\n\n# Update to print IQR of temperature_c, fuel_price_usd_per_l, & unemployment\nprint(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg(iqr))\n\ntemperature_c           16.583333\nfuel_price_usd_per_l     0.073176\nunemployment             0.565000\ndtype: float64\n\n# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\nprint(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr, np.median]))\n\n        temperature_c  fuel_price_usd_per_l  unemployment\niqr         16.583333              0.073176         0.565\nmedian      16.966667              0.743381         8.099"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#cumulative-statistics",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#cumulative-statistics",
    "title": "Data Manipulation with pandas",
    "section": "Cumulative statistics",
    "text": "Cumulative statistics\nCumulative statistics can also be helpful in tracking summary statistics over time. In this exercise, you’ll calculate the cumulative sum and cumulative max of a department’s weekly sales, which will allow you to identify what the total sales were so far as well as what the highest weekly sales were so far.\nA DataFrame called sales_1_1 has been created for you, which contains the sales data for department 1 of store 1. pandas is loaded as pd.\n\n# Sort sales_1_1 by date\nsales_1_1 = sales.sort_values(by = \"date\")\n\n# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col\nsales_1_1['cum_weekly_sales'] = sales_1_1['weekly_sales'].cumsum()\n\n# Get the cumulative max of weekly_sales, add as cum_max_sales col\n\nsales_1_1['cum_max_sales'] = sales_1_1['weekly_sales'].cummax()\n\n# See the columns you calculated\nprint(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]].head())\n\n           date  weekly_sales  cum_weekly_sales  cum_max_sales\n0    2010-02-05      24924.50          24924.50       24924.50\n6437 2010-02-05      38597.52          63522.02       38597.52\n1249 2010-02-05       3840.21          67362.23       38597.52\n6449 2010-02-05      17590.59          84952.82       38597.52\n6461 2010-02-05       4929.87          89882.69       38597.52"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#dropping-duplicates",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#dropping-duplicates",
    "title": "Data Manipulation with pandas",
    "section": "Dropping duplicates",
    "text": "Dropping duplicates\nRemoving duplicates is an essential skill to get accurate counts because often, you don’t want to count the same thing multiple times. In this exercise, you’ll create some new DataFrames using unique values from sales.\nsales is available and pandas is imported as pd.\n\n# Drop duplicate store/type combinations\nstore_types = sales.drop_duplicates(subset = ['store', 'type'])\nprint(store_types.head())\n\n      store type  department  ... temperature_c  fuel_price_usd_per_l  unemployment\n0         1    A           1  ...      5.727778              0.679451         8.106\n901       2    A           1  ...      4.550000              0.679451         8.324\n1798      4    A           1  ...      6.533333              0.686319         8.623\n2699      6    A           1  ...      4.683333              0.679451         7.259\n3593     10    B           1  ...     12.411111              0.782478         9.765\n\n[5 rows x 9 columns]\n\n# Drop duplicate store/department combinations\nstore_depts = sales.drop_duplicates(subset = ['store', 'department'])\nprint(store_depts.head())\n\n    store type  department  ... temperature_c  fuel_price_usd_per_l  unemployment\n0       1    A           1  ...      5.727778              0.679451         8.106\n12      1    A           2  ...      5.727778              0.679451         8.106\n24      1    A           3  ...      5.727778              0.679451         8.106\n36      1    A           4  ...      5.727778              0.679451         8.106\n48      1    A           5  ...      5.727778              0.679451         8.106\n\n[5 rows x 9 columns]\n\n# Subset the rows where is_holiday is True and drop duplicate dates\nholiday_dates = sales[sales['is_holiday']].drop_duplicates('date')\n\n# Print date col of holiday_dates\nprint(holiday_dates['date'])\n\n498    2010-09-10\n691    2011-11-25\n2315   2010-02-12\n6735   2012-09-07\n6810   2010-12-31\n6815   2012-02-10\n6820   2011-09-09\nName: date, dtype: datetime64[ns]"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#counting-categorical-variables",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#counting-categorical-variables",
    "title": "Data Manipulation with pandas",
    "section": "Counting categorical variables",
    "text": "Counting categorical variables\nCounting is a great way to get an overview of your data and to spot curiosities that you might not notice otherwise. In this exercise, you’ll count the number of each type of store and the number of each department number using the DataFrames you created in the previous exercise:\n# Drop duplicate store/type combinations\nstore_types = sales.drop_duplicates(subset=[\"store\", \"type\"])\n\n# Drop duplicate store/department combinations\nstore_depts = sales.drop_duplicates(subset=[\"store\", \"department\"])\nThe store_types and store_depts DataFrames you created in the last exercise are available, and pandas is imported as pd.\n\n# Count the number of stores of each type\nstore_counts = store_types.value_counts('type')\nprint(store_counts)\n\ntype\nA    11\nB     1\ndtype: int64\n\n# Get the proportion of stores of each type\nstore_props = store_types.value_counts('type', normalize = True)\nprint(store_props)\n\ntype\nA    0.916667\nB    0.083333\ndtype: float64\n\n# Count the number of each department number and sort\ndept_counts_sorted = store_depts['department'].value_counts( sort = True)\nprint(dept_counts_sorted)\n\n1     12\n55    12\n72    12\n71    12\n67    12\n      ..\n37    10\n48     8\n50     6\n39     4\n43     2\nName: department, Length: 80, dtype: int64\n\n# Get the proportion of departments of each number and sort\ndept_props_sorted = store_depts['department'].value_counts( sort = True, normalize= True)\nprint(dept_props_sorted)\n\n1     0.012917\n55    0.012917\n72    0.012917\n71    0.012917\n67    0.012917\n        ...   \n37    0.010764\n48    0.008611\n50    0.006459\n39    0.004306\n43    0.002153\nName: department, Length: 80, dtype: float64"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#what-percent-of-sales-occurred-at-each-store-type",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#what-percent-of-sales-occurred-at-each-store-type",
    "title": "Data Manipulation with pandas",
    "section": "What percent of sales occurred at each store type?",
    "text": "What percent of sales occurred at each store type?\nWhile .groupby() is useful, you can calculate grouped summary statistics without it.\nWalmart distinguishes three types of stores: “supercenters,” “discount stores,” and “neighborhood markets,” encoded in this dataset as type “A,” “B,” and “C.” In this exercise, you’ll calculate the total sales made at each store type, without using .groupby(). You can then use these numbers to see what proportion of Walmart’s total sales were made at each type.\nsales is available and pandas is imported as pd.\n\n# Calc total weekly sales\nsales_all = sales[\"weekly_sales\"].sum()\n\n# Subset for type A stores, calc total weekly sales\nsales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\n\n# Subset for type B stores, calc total weekly sales\nsales_B = sales[sales[\"type\"] == \"B\"][\"weekly_sales\"].sum()\n\n\n# Subset for type C stores, calc total weekly sales\nsales_C = sales[sales[\"type\"] == \"C\"][\"weekly_sales\"].sum()\n\n\n# Get proportion for each type\nsales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\nprint(sales_propn_by_type)\n\n[0.9097747 0.0902253 0.       ]"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#calculations-with-.groupby",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#calculations-with-.groupby",
    "title": "Data Manipulation with pandas",
    "section": "Calculations with .groupby()",
    "text": "Calculations with .groupby()\nThe .groupby() method makes life much easier. In this exercise, you’ll perform the same calculations as last time, except you’ll use the .groupby() method. You’ll also perform calculations on data grouped by two variables to see if sales differ by store type depending on if it’s a holiday week or not.\nsales is available and pandas is loaded as pd.\n\n# Group by type; calc total weekly sales\nsales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n\n# Get proportion for each type\nsales_propn_by_type = sales_by_type / sum(sales['weekly_sales'])\nprint(sales_propn_by_type)\n\ntype\nA    0.909775\nB    0.090225\nName: weekly_sales, dtype: float64\n\n# From previous step\nsales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n\n# Group by type and is_holiday; calc total weekly sales\nsales_by_type_is_holiday = sales.groupby([\"type\", 'is_holiday'])[\"weekly_sales\"].sum()\nprint(sales_by_type_is_holiday)\n\ntype  is_holiday\nA     False         2.336927e+08\n      True          2.360181e+04\nB     False         2.317678e+07\n      True          1.621410e+03\nName: weekly_sales, dtype: float64"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#multiple-grouped-summaries",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#multiple-grouped-summaries",
    "title": "Data Manipulation with pandas",
    "section": "Multiple grouped summaries",
    "text": "Multiple grouped summaries\nEarlier in this chapter, you saw that the .agg() method is useful to compute multiple statistics on multiple variables. It also works with grouped data. NumPy, which is imported as np, has many different summary statistics functions, including: np.min, np.max, np.mean, and np.median.\nsales is available and pandas is imported as pd.\n\n# Import numpy with the alias np\nimport numpy as np\n\n# For each store type, aggregate weekly_sales: get min, max, mean, and median\nsales_stats = sales.groupby([\"type\"])[\"weekly_sales\"].agg([np.min, np.max, np.mean, np.median])\n\n# Print sales_stats\nprint(sales_stats)\n\n        amin       amax          mean    median\ntype                                           \nA    -1098.0  293966.05  23674.667242  11943.92\nB     -798.0  232558.51  25696.678370  13336.08\n\n# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\nunemp_fuel_stats = sales.groupby([\"type\"])[\"unemployment\", \"fuel_price_usd_per_l\"].agg([np.min, np.max, np.mean, np.median])\n\n# Print unemp_fuel_stats\nprint(unemp_fuel_stats)\n\n     unemployment                   ... fuel_price_usd_per_l                    \n             amin   amax      mean  ...                 amax      mean    median\ntype                                ...                                         \nA           3.879  8.992  7.972611  ...             1.107410  0.744619  0.735455\nB           7.170  9.765  9.279323  ...             1.107674  0.805858  0.803348\n\n[2 rows x 8 columns]"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#pivoting-on-one-variable",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#pivoting-on-one-variable",
    "title": "Data Manipulation with pandas",
    "section": "Pivoting on one variable",
    "text": "Pivoting on one variable\nPivot tables are the standard way of aggregating data in spreadsheets.\nIn pandas, pivot tables are essentially another way of performing grouped calculations. That is, the .pivot_table() method is an alternative to .groupby().\nIn this exercise, you’ll perform calculations using .pivot_table() to replicate the calculations you performed in the last lesson using .groupby().\nsales is available and pandas is imported as pd.\n\n# Pivot for mean weekly_sales for each store type\nmean_sales_by_type = sales.pivot_table(values = 'weekly_sales', index = 'type')\n\n# Print mean_sales_by_type\nprint(mean_sales_by_type)\n\n      weekly_sales\ntype              \nA     23674.667242\nB     25696.678370\n\n# Pivot for mean and median weekly_sales for each store type\nmean_med_sales_by_type = sales.pivot_table(values = 'weekly_sales', index = 'type', aggfunc = [np.mean, np.median])\n\n# Print mean_med_sales_by_type\nprint(mean_med_sales_by_type)\n\n              mean       median\n      weekly_sales weekly_sales\ntype                           \nA     23674.667242     11943.92\nB     25696.678370     13336.08\n\n# Pivot for mean weekly_sales by store type and holiday \nmean_sales_by_type_holiday = sales.pivot_table(values= 'weekly_sales', index = 'type', columns = 'is_holiday')\n\n# Print mean_sales_by_type_holiday\nprint(mean_sales_by_type_holiday)\n\nis_holiday         False      True \ntype                               \nA           23768.583523  590.04525\nB           25751.980533  810.70500"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#fill-in-missing-values-and-sum-values-with-pivot-tables",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#fill-in-missing-values-and-sum-values-with-pivot-tables",
    "title": "Data Manipulation with pandas",
    "section": "Fill in missing values and sum values with pivot tables",
    "text": "Fill in missing values and sum values with pivot tables\nThe .pivot_table() method has several useful arguments, including fill_value and margins.\n\nfill_value replaces missing values with a real value (known as imputation). What to replace missing values with is a topic big enough to have its own course (Dealing with Missing Data in Python), but the simplest thing to do is to substitute a dummy value.\nmargins is a shortcut for when you pivoted by two variables, but also wanted to pivot by each of those variables separately: it gives the row and column totals of the pivot table contents. In this exercise, you’ll practice using these arguments to up your pivot table skills, which will help you crunch numbers more efficiently!\n\nsales is available and pandas is imported as pd.\n\n# Print mean weekly_sales by department and type; fill missing values with 0\nprint(sales.pivot_table(values = 'weekly_sales',\n                        index = 'department',\n                        columns = 'type', \n                        fill_value = 0))\n\ntype                    A              B\ndepartment                              \n1            30961.725379   44050.626667\n2            67600.158788  112958.526667\n3            17160.002955   30580.655000\n4            44285.399091   51219.654167\n5            34821.011364   63236.875000\n...                   ...            ...\n95          123933.787121   77082.102500\n96           21367.042857    9528.538333\n97           28471.266970    5828.873333\n98           12875.423182     217.428333\n99             379.123659       0.000000\n\n[80 rows x 2 columns]\n\n                        \n# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\nprint(sales.pivot_table(values=\"weekly_sales\",\n    index=\"department\",\n    columns=\"type\", fill_value = 0, \n    margins = True))\n\ntype                   A              B           All\ndepartment                                           \n1           30961.725379   44050.626667  32052.467153\n2           67600.158788  112958.526667  71380.022778\n3           17160.002955   30580.655000  18278.390625\n4           44285.399091   51219.654167  44863.253681\n5           34821.011364   63236.875000  37189.000000\n...                  ...            ...           ...\n96          21367.042857    9528.538333  20337.607681\n97          28471.266970    5828.873333  26584.400833\n98          12875.423182     217.428333  11820.590278\n99            379.123659       0.000000    379.123659\nAll         23674.667242   25696.678370  23843.950149\n\n[81 rows x 3 columns]"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#setting-and-removing-indexes",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#setting-and-removing-indexes",
    "title": "Data Manipulation with pandas",
    "section": "Setting and removing indexes",
    "text": "Setting and removing indexes\npandas allows you to designate columns as an index. This enables cleaner code when taking subsets (as well as providing more efficient lookup under some circumstances).\nIn this chapter, you’ll be exploring temperatures, a DataFrame of average temperatures in cities around the world. pandas is loaded as pd.\n\ntemperatures = pd.read_csv(\"data/temperatures.csv\", index_col = 0 )\n\n# Look at temperatures\nprint(temperatures)\n\n             date     city        country  avg_temp_c\n0      2000-01-01  Abidjan  Côte D'Ivoire      27.293\n1      2000-02-01  Abidjan  Côte D'Ivoire      27.685\n2      2000-03-01  Abidjan  Côte D'Ivoire      29.061\n3      2000-04-01  Abidjan  Côte D'Ivoire      28.162\n4      2000-05-01  Abidjan  Côte D'Ivoire      27.547\n...           ...      ...            ...         ...\n16495  2013-05-01     Xian          China      18.979\n16496  2013-06-01     Xian          China      23.522\n16497  2013-07-01     Xian          China      25.251\n16498  2013-08-01     Xian          China      24.528\n16499  2013-09-01     Xian          China         NaN\n\n[16500 rows x 4 columns]\n\n# Set the index of temperatures to city\ntemperatures_ind = temperatures.set_index('city')\n\n# Look at temperatures_ind\nprint(temperatures_ind)\n\n               date        country  avg_temp_c\ncity                                          \nAbidjan  2000-01-01  Côte D'Ivoire      27.293\nAbidjan  2000-02-01  Côte D'Ivoire      27.685\nAbidjan  2000-03-01  Côte D'Ivoire      29.061\nAbidjan  2000-04-01  Côte D'Ivoire      28.162\nAbidjan  2000-05-01  Côte D'Ivoire      27.547\n...             ...            ...         ...\nXian     2013-05-01          China      18.979\nXian     2013-06-01          China      23.522\nXian     2013-07-01          China      25.251\nXian     2013-08-01          China      24.528\nXian     2013-09-01          China         NaN\n\n[16500 rows x 3 columns]\n\n# Reset the temperatures_ind index, keeping its contents\nprint(temperatures_ind.reset_index(drop = False))\n\n          city        date        country  avg_temp_c\n0      Abidjan  2000-01-01  Côte D'Ivoire      27.293\n1      Abidjan  2000-02-01  Côte D'Ivoire      27.685\n2      Abidjan  2000-03-01  Côte D'Ivoire      29.061\n3      Abidjan  2000-04-01  Côte D'Ivoire      28.162\n4      Abidjan  2000-05-01  Côte D'Ivoire      27.547\n...        ...         ...            ...         ...\n16495     Xian  2013-05-01          China      18.979\n16496     Xian  2013-06-01          China      23.522\n16497     Xian  2013-07-01          China      25.251\n16498     Xian  2013-08-01          China      24.528\n16499     Xian  2013-09-01          China         NaN\n\n[16500 rows x 4 columns]\n\n# Reset the temperatures_ind index, dropping its contents\nprint(temperatures_ind.reset_index(drop = True))\n\n             date        country  avg_temp_c\n0      2000-01-01  Côte D'Ivoire      27.293\n1      2000-02-01  Côte D'Ivoire      27.685\n2      2000-03-01  Côte D'Ivoire      29.061\n3      2000-04-01  Côte D'Ivoire      28.162\n4      2000-05-01  Côte D'Ivoire      27.547\n...           ...            ...         ...\n16495  2013-05-01          China      18.979\n16496  2013-06-01          China      23.522\n16497  2013-07-01          China      25.251\n16498  2013-08-01          China      24.528\n16499  2013-09-01          China         NaN\n\n[16500 rows x 3 columns]"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#subsetting-with-.loc",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#subsetting-with-.loc",
    "title": "Data Manipulation with pandas",
    "section": "Subsetting with .loc[]",
    "text": "Subsetting with .loc[]\nThe killer feature for indexes is .loc[]: a subsetting method that accepts index values. When you pass it a single argument, it will take a subset of rows.\nThe code for subsetting using .loc[] can be easier to read than standard square bracket subsetting, which can make your code less burdensome to maintain.\npandas is loaded as pd. temperatures and temperatures_ind are available; the latter is indexed by city\n\n# Make a list of cities to subset on\ncities = [\"Moscow\",  \"Saint Petersburg\"]\n\n# Subset temperatures using square brackets\nprint(temperatures[temperatures['city'].isin(cities)])\n\n             date              city country  avg_temp_c\n10725  2000-01-01            Moscow  Russia      -7.313\n10726  2000-02-01            Moscow  Russia      -3.551\n10727  2000-03-01            Moscow  Russia      -1.661\n10728  2000-04-01            Moscow  Russia      10.096\n10729  2000-05-01            Moscow  Russia      10.357\n...           ...               ...     ...         ...\n13360  2013-05-01  Saint Petersburg  Russia      12.355\n13361  2013-06-01  Saint Petersburg  Russia      17.185\n13362  2013-07-01  Saint Petersburg  Russia      17.234\n13363  2013-08-01  Saint Petersburg  Russia      17.153\n13364  2013-09-01  Saint Petersburg  Russia         NaN\n\n[330 rows x 4 columns]\n\n# Subset temperatures_ind using .loc[]\nprint(temperatures_ind.loc[cities])\n\n                        date country  avg_temp_c\ncity                                            \nMoscow            2000-01-01  Russia      -7.313\nMoscow            2000-02-01  Russia      -3.551\nMoscow            2000-03-01  Russia      -1.661\nMoscow            2000-04-01  Russia      10.096\nMoscow            2000-05-01  Russia      10.357\n...                      ...     ...         ...\nSaint Petersburg  2013-05-01  Russia      12.355\nSaint Petersburg  2013-06-01  Russia      17.185\nSaint Petersburg  2013-07-01  Russia      17.234\nSaint Petersburg  2013-08-01  Russia      17.153\nSaint Petersburg  2013-09-01  Russia         NaN\n\n[330 rows x 3 columns]"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#setting-multi-level-indexes",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#setting-multi-level-indexes",
    "title": "Data Manipulation with pandas",
    "section": "Setting multi-level indexes",
    "text": "Setting multi-level indexes\nIndexes can also be made out of multiple columns, forming a multi-level index (sometimes called a hierarchical index). There is a trade-off to using these.\nThe benefit is that multi-level indexes make it more natural to reason about nested categorical variables. For example, in a clinical trial, you might have control and treatment groups. Then each test subject belongs to one or another group, and we can say that a test subject is nested inside the treatment group. Similarly, in the temperature dataset, the city is located in the country, so we can say a city is nested inside the country.\nThe main downside is that the code for manipulating indexes is different from the code for manipulating columns, so you have to learn two syntaxes and keep track of how your data is represented.\npandas is loaded as pd. temperatures is available.\n\n# Index temperatures by country & city\ntemperatures_ind = temperatures.set_index([\"country\", \"city\"])\n\n# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\nrows_to_keep = [('Brazil','Rio De Janeiro'), ('Pakistan', 'Lahore')]\n\n# Subset for rows to keep\nprint(temperatures_ind.loc[rows_to_keep])\n\n                               date  avg_temp_c\ncountry  city                                  \nBrazil   Rio De Janeiro  2000-01-01      25.974\n         Rio De Janeiro  2000-02-01      26.699\n         Rio De Janeiro  2000-03-01      26.270\n         Rio De Janeiro  2000-04-01      25.750\n         Rio De Janeiro  2000-05-01      24.356\n...                             ...         ...\nPakistan Lahore          2013-05-01      33.457\n         Lahore          2013-06-01      34.456\n         Lahore          2013-07-01      33.279\n         Lahore          2013-08-01      31.511\n         Lahore          2013-09-01         NaN\n\n[330 rows x 2 columns]"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#sorting-by-index-values",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#sorting-by-index-values",
    "title": "Data Manipulation with pandas",
    "section": "Sorting by index values",
    "text": "Sorting by index values\nPreviously, you changed the order of the rows in a DataFrame by calling .sort_values(). It’s also useful to be able to sort by elements in the index. For this, you need to use .sort_index().\npandas is loaded as pd. temperatures_ind has a multi-level index of country and city, and is available.\n\n# Sort temperatures_ind by index values\nprint(temperatures_ind.sort_index(level = [\"country\", \"city\"]))\n\n                          date  avg_temp_c\ncountry     city                          \nAfghanistan Kabul   2000-01-01       3.326\n            Kabul   2000-02-01       3.454\n            Kabul   2000-03-01       9.612\n            Kabul   2000-04-01      17.925\n            Kabul   2000-05-01      24.658\n...                        ...         ...\nZimbabwe    Harare  2013-05-01      18.298\n            Harare  2013-06-01      17.020\n            Harare  2013-07-01      16.299\n            Harare  2013-08-01      19.232\n            Harare  2013-09-01         NaN\n\n[16500 rows x 2 columns]\n\n# Sort temperatures_ind by index values at the city level\nprint(temperatures_ind.sort_index(level = [\"city\"]))\n\n                             date  avg_temp_c\ncountry       city                           \nCôte D'Ivoire Abidjan  2000-01-01      27.293\n              Abidjan  2000-02-01      27.685\n              Abidjan  2000-03-01      29.061\n              Abidjan  2000-04-01      28.162\n              Abidjan  2000-05-01      27.547\n...                           ...         ...\nChina         Xian     2013-05-01      18.979\n              Xian     2013-06-01      23.522\n              Xian     2013-07-01      25.251\n              Xian     2013-08-01      24.528\n              Xian     2013-09-01         NaN\n\n[16500 rows x 2 columns]\n\n# Sort temperatures_ind by country then descending city\nprint(temperatures_ind.sort_index(level = [\"country\", \"city\"], ascending = [True, False]))\n\n                          date  avg_temp_c\ncountry     city                          \nAfghanistan Kabul   2000-01-01       3.326\n            Kabul   2000-02-01       3.454\n            Kabul   2000-03-01       9.612\n            Kabul   2000-04-01      17.925\n            Kabul   2000-05-01      24.658\n...                        ...         ...\nZimbabwe    Harare  2013-05-01      18.298\n            Harare  2013-06-01      17.020\n            Harare  2013-07-01      16.299\n            Harare  2013-08-01      19.232\n            Harare  2013-09-01         NaN\n\n[16500 rows x 2 columns]"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#slicing-index-values",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#slicing-index-values",
    "title": "Data Manipulation with pandas",
    "section": "Slicing index values",
    "text": "Slicing index values\nSlicing lets you select consecutive elements of an object using first:last syntax. DataFrames can be sliced by index values or by row/column number; we’ll start with the first case. This involves slicing inside the .loc[] method.\nCompared to slicing lists, there are a few things to remember.\n\nYou can only slice an index if the index is sorted (using .sort_index()).\nTo slice at the outer level, first and last can be strings.\nTo slice at inner levels, first and last should be tuples.\nIf you pass a single slice to .loc[], it will slice the rows. pandas is loaded as pd. temperatures_ind has country and city in the index, and is available.\n\n\n# Sort the index of temperatures_ind\ntemperatures_srt = temperatures_ind.sort_index(level = [\"country\", \"city\"])\n\n# Subset rows from Pakistan to Russia\nprint(temperatures_srt.loc[\"Pakistan\" : \"Russia\"])\n\n                                 date  avg_temp_c\ncountry  city                                    \nPakistan Faisalabad        2000-01-01      12.792\n         Faisalabad        2000-02-01      14.339\n         Faisalabad        2000-03-01      20.309\n         Faisalabad        2000-04-01      29.072\n         Faisalabad        2000-05-01      34.845\n...                               ...         ...\nRussia   Saint Petersburg  2013-05-01      12.355\n         Saint Petersburg  2013-06-01      17.185\n         Saint Petersburg  2013-07-01      17.234\n         Saint Petersburg  2013-08-01      17.153\n         Saint Petersburg  2013-09-01         NaN\n\n[1155 rows x 2 columns]\n\n# Try to subset rows from Lahore to Moscow\nprint(temperatures_srt.loc[\"Lahore\" : \"Moscow\"])\n\n                          date  avg_temp_c\ncountry city                              \nMexico  Mexico      2000-01-01      12.694\n        Mexico      2000-02-01      14.677\n        Mexico      2000-03-01      17.376\n        Mexico      2000-04-01      18.294\n        Mexico      2000-05-01      18.562\n...                        ...         ...\nMorocco Casablanca  2013-05-01      19.217\n        Casablanca  2013-06-01      23.649\n        Casablanca  2013-07-01      27.488\n        Casablanca  2013-08-01      27.952\n        Casablanca  2013-09-01         NaN\n\n[330 rows x 2 columns]\n\n# Subset rows from Pakistan, Lahore to Russia, Moscow\nprint(temperatures_srt.loc[(\"Pakistan\" ,\"Lahore\") : (\"Russia\", \"Moscow\")])\n\n                       date  avg_temp_c\ncountry  city                          \nPakistan Lahore  2000-01-01      12.792\n         Lahore  2000-02-01      14.339\n         Lahore  2000-03-01      20.309\n         Lahore  2000-04-01      29.072\n         Lahore  2000-05-01      34.845\n...                     ...         ...\nRussia   Moscow  2013-05-01      16.152\n         Moscow  2013-06-01      18.718\n         Moscow  2013-07-01      18.136\n         Moscow  2013-08-01      17.485\n         Moscow  2013-09-01         NaN\n\n[660 rows x 2 columns]"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#slicing-in-both-directions",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#slicing-in-both-directions",
    "title": "Data Manipulation with pandas",
    "section": "Slicing in both directions",
    "text": "Slicing in both directions\nYou’ve seen slicing DataFrames by rows and by columns, but since DataFrames are two-dimensional objects, it is often natural to slice both dimensions at once. That is, by passing two arguments to .loc[], you can subset by rows and columns in one go.\npandas is loaded as pd. temperatures_srt is indexed by country and city, has a sorted index, and is available.\n\n# Subset rows from India, Hyderabad to Iraq, Baghdad\nprint(temperatures_srt.loc[(\"India\" ,\"Hyderabad\") : (\"Iraq\", \"Baghdad\")])\n\n                         date  avg_temp_c\ncountry city                             \nIndia   Hyderabad  2000-01-01      23.779\n        Hyderabad  2000-02-01      25.826\n        Hyderabad  2000-03-01      28.821\n        Hyderabad  2000-04-01      32.698\n        Hyderabad  2000-05-01      32.438\n...                       ...         ...\nIraq    Baghdad    2013-05-01      28.673\n        Baghdad    2013-06-01      33.803\n        Baghdad    2013-07-01      36.392\n        Baghdad    2013-08-01      35.463\n        Baghdad    2013-09-01         NaN\n\n[2145 rows x 2 columns]\n\n# Subset columns from date to avg_temp_c\nprint(temperatures_srt.loc[:, \"date\": \"avg_temp_c\"])\n\n                          date  avg_temp_c\ncountry     city                          \nAfghanistan Kabul   2000-01-01       3.326\n            Kabul   2000-02-01       3.454\n            Kabul   2000-03-01       9.612\n            Kabul   2000-04-01      17.925\n            Kabul   2000-05-01      24.658\n...                        ...         ...\nZimbabwe    Harare  2013-05-01      18.298\n            Harare  2013-06-01      17.020\n            Harare  2013-07-01      16.299\n            Harare  2013-08-01      19.232\n            Harare  2013-09-01         NaN\n\n[16500 rows x 2 columns]\n\n# Subset in both directions at once\nprint(temperatures_srt.loc[(\"India\" ,\"Hyderabad\") : (\"Iraq\", \"Baghdad\"), \"date\": \"avg_temp_c\"])\n\n                         date  avg_temp_c\ncountry city                             \nIndia   Hyderabad  2000-01-01      23.779\n        Hyderabad  2000-02-01      25.826\n        Hyderabad  2000-03-01      28.821\n        Hyderabad  2000-04-01      32.698\n        Hyderabad  2000-05-01      32.438\n...                       ...         ...\nIraq    Baghdad    2013-05-01      28.673\n        Baghdad    2013-06-01      33.803\n        Baghdad    2013-07-01      36.392\n        Baghdad    2013-08-01      35.463\n        Baghdad    2013-09-01         NaN\n\n[2145 rows x 2 columns]"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#slicing-time-series",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#slicing-time-series",
    "title": "Data Manipulation with pandas",
    "section": "Slicing time series",
    "text": "Slicing time series\nSlicing is particularly useful for time series since it’s a common thing to want to filter for data within a date range. Add the date column to the index, then use .loc[] to perform the subsetting. The important thing to remember is to keep your dates in ISO 8601 format, that is, “yyyy-mm-dd” for year-month-day, “yyyy-mm” for year-month, and “yyyy” for year.\nRecall from Chapter 1 that you can combine multiple Boolean conditions using logical operators, such as &. To do so in one line of code, you’ll need to add parentheses () around each condition.\n\n# Use Boolean conditions to subset temperatures for rows in 2010 and 2011\ntemperatures_bool = temperatures[(temperatures['date'] >= \"2010-01-01\") & (temperatures['date'] <= \"2011-12-31\")]\nprint(temperatures_bool)\n\n             date     city        country  avg_temp_c\n120    2010-01-01  Abidjan  Côte D'Ivoire      28.270\n121    2010-02-01  Abidjan  Côte D'Ivoire      29.262\n122    2010-03-01  Abidjan  Côte D'Ivoire      29.596\n123    2010-04-01  Abidjan  Côte D'Ivoire      29.068\n124    2010-05-01  Abidjan  Côte D'Ivoire      28.258\n...           ...      ...            ...         ...\n16474  2011-08-01     Xian          China      23.069\n16475  2011-09-01     Xian          China      16.775\n16476  2011-10-01     Xian          China      12.587\n16477  2011-11-01     Xian          China       7.543\n16478  2011-12-01     Xian          China      -0.490\n\n[2400 rows x 4 columns]\n\n# Set date as the index and sort the index\ntemperatures_ind = temperatures.set_index(\"date\").sort_index()\n\n# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011\nprint(temperatures_ind.loc[\"2010-01-01\": \"2011-12-31\"])\n\n                  city    country  avg_temp_c\ndate                                         \n2010-01-01  Faisalabad   Pakistan      11.810\n2010-01-01   Melbourne  Australia      20.016\n2010-01-01   Chongqing      China       7.921\n2010-01-01   São Paulo     Brazil      23.738\n2010-01-01   Guangzhou      China      14.136\n...                ...        ...         ...\n2011-12-01      Nagoya      Japan       6.476\n2011-12-01   Hyderabad      India      23.613\n2011-12-01        Cali   Colombia      21.559\n2011-12-01        Lima       Peru      18.293\n2011-12-01     Bangkok   Thailand      25.021\n\n[2400 rows x 3 columns]\n\n# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011\nprint(temperatures_ind.loc[\"2010-08-01\": \"2011-02-01\"])\n\n                city        country  avg_temp_c\ndate                                           \n2010-08-01  Calcutta          India      30.226\n2010-08-01      Pune          India      24.941\n2010-08-01     Izmir         Turkey      28.352\n2010-08-01   Tianjin          China      25.543\n2010-08-01    Manila    Philippines      27.101\n...              ...            ...         ...\n2011-02-01     Kabul    Afghanistan       3.914\n2011-02-01   Chicago  United States       0.276\n2011-02-01    Aleppo          Syria       8.246\n2011-02-01     Delhi          India      18.136\n2011-02-01   Rangoon          Burma      26.631\n\n[700 rows x 3 columns]"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#subsetting-by-rowcolumn-number",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#subsetting-by-rowcolumn-number",
    "title": "Data Manipulation with pandas",
    "section": "Subsetting by row/column number",
    "text": "Subsetting by row/column number\nThe most common ways to subset rows are the ways we’ve previously discussed: using a Boolean condition or by index labels. However, it is also occasionally useful to pass row numbers.\nThis is done using .iloc[], and like .loc[], it can take two arguments to let you subset by rows and columns.\npandas is loaded as pd. temperatures (without an index) is available.\n\n# Get 23rd row, 2nd column (index 22, 1)\nprint(temperatures.iloc[22, 1])\n\nAbidjan\n\n# Use slicing to get the first 5 rows\nprint(temperatures.iloc[0:5, ])\n\n         date     city        country  avg_temp_c\n0  2000-01-01  Abidjan  Côte D'Ivoire      27.293\n1  2000-02-01  Abidjan  Côte D'Ivoire      27.685\n2  2000-03-01  Abidjan  Côte D'Ivoire      29.061\n3  2000-04-01  Abidjan  Côte D'Ivoire      28.162\n4  2000-05-01  Abidjan  Côte D'Ivoire      27.547\n\n# Use slicing to get columns 3 to 4\nprint(temperatures.iloc[:, 2:4])\n\n             country  avg_temp_c\n0      Côte D'Ivoire      27.293\n1      Côte D'Ivoire      27.685\n2      Côte D'Ivoire      29.061\n3      Côte D'Ivoire      28.162\n4      Côte D'Ivoire      27.547\n...              ...         ...\n16495          China      18.979\n16496          China      23.522\n16497          China      25.251\n16498          China      24.528\n16499          China         NaN\n\n[16500 rows x 2 columns]\n\n# Use slicing in both directions at once\nprint(temperatures.iloc[0:5, 2:4])\n\n         country  avg_temp_c\n0  Côte D'Ivoire      27.293\n1  Côte D'Ivoire      27.685\n2  Côte D'Ivoire      29.061\n3  Côte D'Ivoire      28.162\n4  Côte D'Ivoire      27.547"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#pivot-temperature-by-city-and-year",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#pivot-temperature-by-city-and-year",
    "title": "Data Manipulation with pandas",
    "section": "Pivot temperature by city and year",
    "text": "Pivot temperature by city and year\nIt’s interesting to see how temperatures for each city change over time—looking at every month results in a big table, which can be tricky to reason about. Instead, let’s look at how temperatures change by year.\nYou can access the components of a date (year, month and day) using code of the form dataframe[“column”].dt.component. For example, the month component is dataframe[“column”].dt.month, and the year component is dataframe[“column”].dt.year.\nOnce you have the year column, you can create a pivot table with the data aggregated by city and year, which you’ll explore in the coming exercises.\n\n# Add a year column to temperatures\ntemperatures[\"date\"]= pd.to_datetime(temperatures[\"date\"])\n\n\ntemperatures[\"year\"] = temperatures[\"date\"].dt.year\n\n# Pivot avg_temp_c by country and city vs year\ntemp_by_country_city_vs_year = temperatures.pivot_table(values =\"avg_temp_c\" , index = ['country',\"city\"], columns = \"year\" )\n\n# See the result\nprint(temp_by_country_city_vs_year)\n\nyear                                 2000       2001  ...       2012       2013\ncountry       city                                    ...                      \nAfghanistan   Kabul             15.822667  15.847917  ...  14.510333  16.206125\nAngola        Luanda            24.410333  24.427083  ...  24.240083  24.553875\nAustralia     Melbourne         14.320083  14.180000  ...  14.268667  14.741500\n              Sydney            17.567417  17.854500  ...  17.474333  18.089750\nBangladesh    Dhaka             25.905250  25.931250  ...  26.283583  26.587000\n...                                   ...        ...  ...        ...        ...\nUnited States Chicago           11.089667  11.703083  ...  12.821250  11.586889\n              Los Angeles       16.643333  16.466250  ...  17.089583  18.120667\n              New York           9.969083  10.931000  ...  11.971500  12.163889\nVietnam       Ho Chi Minh City  27.588917  27.831750  ...  28.248750  28.455000\nZimbabwe      Harare            20.283667  20.861000  ...  20.523333  19.756500\n\n[100 rows x 14 columns]"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#subsetting-pivot-tables",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#subsetting-pivot-tables",
    "title": "Data Manipulation with pandas",
    "section": "Subsetting pivot tables",
    "text": "Subsetting pivot tables\nA pivot table is just a DataFrame with sorted indexes, so the techniques you have learned already can be used to subset them. In particular, the .loc[] + slicing combination is often helpful.\npandas is loaded as pd. temp_by_country_city_vs_year is available.\n\n# Subset for Egypt to India\n\ntemp_by_country_city_vs_year.loc[\"Egypt\" : \"India\"]\n\nyear                       2000       2001  ...       2012       2013\ncountry  city                               ...                      \nEgypt    Alexandria   20.744500  21.454583  ...  21.552583  21.438500\n         Cairo        21.486167  22.330833  ...  22.484250  22.907000\n         Gizeh        21.486167  22.330833  ...  22.484250  22.907000\nEthiopia Addis Abeba  18.241250  18.296417  ...  18.448583  19.539000\nFrance   Paris        11.739667  11.371250  ...  11.219917  11.011625\nGermany  Berlin       10.963667   9.690250  ...   9.964333  10.121500\nIndia    Ahmadabad    27.436000  27.198083  ...  27.027250  27.608625\n         Bangalore    25.337917  25.528167  ...  26.042333  26.610500\n         Bombay       27.203667  27.243667  ...  27.192500  26.713000\n         Calcutta     26.491333  26.515167  ...  26.935083  27.369250\n         Delhi        26.048333  25.862917  ...  25.889417  26.709250\n         Hyderabad    27.231833  27.555167  ...  28.018583  28.851250\n         Jaipur       26.430250  26.023000  ...  25.884500  26.844125\n         Kanpur       25.353917  25.326500  ...  25.445417  26.121250\n         Lakhnau      25.353917  25.326500  ...  25.445417  26.121250\n         Madras       28.811667  29.162917  ...  29.778417  30.411750\n         Nagpur       26.181417  26.321667  ...  26.327917  27.112375\n         New Delhi    26.048333  25.862917  ...  25.889417  26.709250\n         Pune         25.110917  25.337833  ...  25.296833  25.847625\n         Surat        27.029000  26.897250  ...  26.889250  27.437750\n\n[20 rows x 14 columns]\n\n# Subset for Egypt, Cairo to India, Delhi\ntemp_by_country_city_vs_year.loc[(\"Egypt\", \"Cairo\") : (\"India\", \"Delhi\")]\n\nyear                       2000       2001  ...       2012       2013\ncountry  city                               ...                      \nEgypt    Cairo        21.486167  22.330833  ...  22.484250  22.907000\n         Gizeh        21.486167  22.330833  ...  22.484250  22.907000\nEthiopia Addis Abeba  18.241250  18.296417  ...  18.448583  19.539000\nFrance   Paris        11.739667  11.371250  ...  11.219917  11.011625\nGermany  Berlin       10.963667   9.690250  ...   9.964333  10.121500\nIndia    Ahmadabad    27.436000  27.198083  ...  27.027250  27.608625\n         Bangalore    25.337917  25.528167  ...  26.042333  26.610500\n         Bombay       27.203667  27.243667  ...  27.192500  26.713000\n         Calcutta     26.491333  26.515167  ...  26.935083  27.369250\n         Delhi        26.048333  25.862917  ...  25.889417  26.709250\n\n[10 rows x 14 columns]\n\n# Subset for Egypt, Cairo to India, Delhi, and 2005 to 2010\n\ntemp_by_country_city_vs_year.loc[(\"Egypt\", \"Cairo\") : (\"India\", \"Delhi\"), 2005:2010]\n\nyear                       2005       2006  ...       2009       2010\ncountry  city                               ...                      \nEgypt    Cairo        22.006500  22.050000  ...  22.625000  23.718250\n         Gizeh        22.006500  22.050000  ...  22.625000  23.718250\nEthiopia Addis Abeba  18.312833  18.427083  ...  18.765333  18.298250\nFrance   Paris        11.552917  11.788500  ...  11.464083  10.409833\nGermany  Berlin        9.919083  10.545333  ...  10.062500   8.606833\nIndia    Ahmadabad    26.828083  27.282833  ...  28.095833  28.017833\n         Bangalore    25.476500  25.418250  ...  25.725750  25.705250\n         Bombay       27.035750  27.381500  ...  27.844500  27.765417\n         Calcutta     26.729167  26.986250  ...  27.153250  27.288833\n         Delhi        25.716083  26.365917  ...  26.554250  26.520250\n\n[10 rows x 6 columns]"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#calculating-on-a-pivot-table",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#calculating-on-a-pivot-table",
    "title": "Data Manipulation with pandas",
    "section": "Calculating on a pivot table",
    "text": "Calculating on a pivot table\nPivot tables are filled with summary statistics, but they are only a first step to finding something insightful. Often you’ll need to perform further calculations on them. A common thing to do is to find the rows or columns where the highest or lowest value occurs.\nRecall from Chapter 1 that you can easily subset a Series or DataFrame to find rows of interest using a logical condition inside of square brackets. For example: series[series > value].\npandas is loaded as pd and the DataFrame temp_by_country_city_vs_year is available.\n\n# Get the worldwide mean temp by year\nmean_temp_by_year = temp_by_country_city_vs_year.mean(axis = \"index\")\n\n# Filter for the year that had the highest mean temp\nprint(mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()])\n\nyear\n2013    20.312285\ndtype: float64\n\n# Get the mean temp by city\nmean_temp_by_city = temp_by_country_city_vs_year.mean(axis = \"columns\")\n\n# Filter for the city that had the lowest mean temp\nprint(mean_temp_by_city[mean_temp_by_city == mean_temp_by_city.min()])\n\ncountry  city  \nChina    Harbin    4.876551\ndtype: float64"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#which-avocado-size-is-most-popular",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#which-avocado-size-is-most-popular",
    "title": "Data Manipulation with pandas",
    "section": "Which avocado size is most popular?",
    "text": "Which avocado size is most popular?\nAvocados are increasingly popular and delicious in guacamole and on toast. The Hass Avocado Board keeps track of avocado supply and demand across the USA, including the sales of three different sizes of avocado. In this exercise, you’ll use a bar plot to figure out which size is the most popular.\nBar plots are great for revealing relationships between categorical (size) and numeric (number sold) variables, but you’ll often have to manipulate your data first in order to get the numbers you need for plotting.\npandas has been imported as pd, and avocados is available.\n\navocados =  pd.read_pickle(\"data/avoplotto.pkl\")\n\nprint(avocados.head(5))\n\n         date          type  year  avg_price   size     nb_sold\n0  2015-12-27  conventional  2015       0.95  small  9626901.09\n1  2015-12-20  conventional  2015       0.98  small  8710021.76\n2  2015-12-13  conventional  2015       0.93  small  9855053.66\n3  2015-12-06  conventional  2015       0.89  small  9405464.36\n4  2015-11-29  conventional  2015       0.99  small  8094803.56\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Look at the first few rows of data\nprint(avocados.head())\n\n         date          type  year  avg_price   size     nb_sold\n0  2015-12-27  conventional  2015       0.95  small  9626901.09\n1  2015-12-20  conventional  2015       0.98  small  8710021.76\n2  2015-12-13  conventional  2015       0.93  small  9855053.66\n3  2015-12-06  conventional  2015       0.89  small  9405464.36\n4  2015-11-29  conventional  2015       0.99  small  8094803.56\n\n# Get the total number of avocados sold of each size\nnb_sold_by_size = avocados.groupby([\"size\"])['nb_sold'].sum()\n\n# Create a bar plot of the number of avocados sold by size\n\nnb_sold_by_size.plot(kind = \"bar\")\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#changes-in-sales-over-time",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#changes-in-sales-over-time",
    "title": "Data Manipulation with pandas",
    "section": "Changes in sales over time",
    "text": "Changes in sales over time\nLine plots are designed to visualize the relationship between two numeric variables, where each data values is connected to the next one. They are especially useful for visualizing the change in a number over time since each time point is naturally connected to the next time point. In this exercise, you’ll visualize the change in avocado sales over three years.\npandas has been imported as pd, and avocados is available.\n\navocados[\"date\"]= pd.to_datetime(avocados[\"date\"])\n# Get the total number of avocados sold on each date\n\nnb_sold_by_date = nb_sold_by_size = avocados.groupby([\"date\"])['nb_sold'].sum()\n\n# Create a line plot of the number of avocados sold by date\nnb_sold_by_date.plot(kind = \"line\")\n\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#avocado-supply-and-demand",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#avocado-supply-and-demand",
    "title": "Data Manipulation with pandas",
    "section": "Avocado supply and demand",
    "text": "Avocado supply and demand\nScatter plots are ideal for visualizing relationships between numerical variables. In this exercise, you’ll compare the number of avocados sold to average price and see if they’re at all related. If they’re related, you may be able to use one number to predict the other.\nmatplotlib.pyplot has been imported as plt, pandas has been imported as pd, and avocados is available.\n\n# Scatter plot of avg_price vs. nb_sold with title\navocados.plot(y= \"avg_price\", x = \"nb_sold\", kind = \"scatter\", title = 'Number of avocados sold vs. average price')\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#price-of-conventional-vs.-organic-avocados",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#price-of-conventional-vs.-organic-avocados",
    "title": "Data Manipulation with pandas",
    "section": "Price of conventional vs. organic avocados",
    "text": "Price of conventional vs. organic avocados\nCreating multiple plots for different subsets of data allows you to compare groups. In this exercise, you’ll create multiple histograms to compare the prices of conventional and organic avocados.\nmatplotlib.pyplot has been imported as plt and pandas has been imported as pd.\n\n# Modify bins to 20\navocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(alpha=0.5, bins = 20)\n\n# Modify bins to 20\navocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(alpha=0.5, bins = 20)\n\n# Add a legend\nplt.legend([\"conventional\", \"organic\"])\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#finding-missing-values",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#finding-missing-values",
    "title": "Data Manipulation with pandas",
    "section": "Finding missing values",
    "text": "Finding missing values\nMissing values are everywhere, and you don’t want them interfering with your work. Some functions ignore missing data by default, but that’s not always the behavior you might want. Some functions can’t handle missing values at all, so these values need to be taken care of before you can use them. If you don’t know where your missing values are, or if they exist, you could make mistakes in your analysis. In this exercise, you’ll determine if there are missing values in the dataset, and if so, how many.\npandas has been imported as pd and avocados_2016, a subset of avocados that contains only sales from 2016, is available.\n\n# Create a new column 'year' by extracting the year from the 'date' column\navocados['year'] = avocados['date'].dt.year\n\navocadoes_wide = avocados.pivot(index=['date', 'type', 'year'], columns='size', values='nb_sold').reset_index()\n\n\navocados_2016 = avocadoes_wide[avocadoes_wide['year'] == 2016]\n# Check individual values for missing values\nprint(avocados_2016.isna())\n\nsize   date   type   year  extra_large  large  small\n104   False  False  False        False  False  False\n105   False  False  False        False  False  False\n106   False  False  False        False  False  False\n107   False  False  False        False  False  False\n108   False  False  False        False  False  False\n..      ...    ...    ...          ...    ...    ...\n203   False  False  False        False  False  False\n204   False  False  False        False  False  False\n205   False  False  False        False  False  False\n206   False  False  False        False  False  False\n207   False  False  False        False  False  False\n\n[104 rows x 6 columns]\n\n# Check each column for missing values\nprint(avocados_2016.isna().any())\n\nsize\ndate           False\ntype           False\nyear           False\nextra_large    False\nlarge          False\nsmall          False\ndtype: bool\n\n# Bar plot of missing values by variable\navocados_2016.isna().sum().plot(kind = \"bar\")\n\n# Show plot\nplt.show()"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#removing-missing-values",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#removing-missing-values",
    "title": "Data Manipulation with pandas",
    "section": "Removing missing values",
    "text": "Removing missing values\nNow that you know there are some missing values in your DataFrame, you have a few options to deal with them. One way is to remove them from the dataset completely. In this exercise, you’ll remove missing values by removing all rows that contain missing values.\npandas has been imported as pd and avocados_2016 is available.\n\n# Remove rows with missing values\navocados_complete = avocados_2016.dropna()\n\n# Check if any columns contain missing values\nprint(avocados_complete.isna().any())\n\nsize\ndate           False\ntype           False\nyear           False\nextra_large    False\nlarge          False\nsmall          False\ndtype: bool"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#replacing-missing-values",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#replacing-missing-values",
    "title": "Data Manipulation with pandas",
    "section": "Replacing missing values",
    "text": "Replacing missing values\nAnother way of handling missing values is to replace them all with the same value. For numerical variables, one option is to replace values with 0— you’ll do this here. However, when you replace missing values, you make assumptions about what a missing value means. In this case, you will assume that a missing number sold means that no sales for that avocado type were made that week.\nIn this exercise, you’ll see how replacing missing values can affect the distribution of a variable using histograms. You can plot histograms for multiple variables at a time as follows:\ndogs[[\"height_cm\", \"weight_kg\"]].hist()\n\n\n# From previous step\ncols_with_missing = [ 'extra_large', 'large', 'small']\navocados_2016[cols_with_missing].hist()\n\narray([[<Axes: title={'center': 'extra_large'}>,\n        <Axes: title={'center': 'large'}>],\n       [<Axes: title={'center': 'small'}>, <Axes: >]], dtype=object)\n\nplt.show()\n\n\n\n# Fill in missing values with 0\navocados_filled = avocados_2016.fillna(0)\n\n# Create histograms of the filled columns\navocados_filled[cols_with_missing].hist()\n\narray([[<Axes: title={'center': 'extra_large'}>,\n        <Axes: title={'center': 'large'}>],\n       [<Axes: title={'center': 'small'}>, <Axes: >]], dtype=object)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#list-of-dictionaries",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#list-of-dictionaries",
    "title": "Data Manipulation with pandas",
    "section": "List of dictionaries",
    "text": "List of dictionaries\nYou recently got some new avocado data from 2019 that you’d like to put in a DataFrame using the list of dictionaries method. Remember that with this method, you go through the data row by row.\n\n# Create a list of dictionaries with new data\navocados_list = [\n    {\"date\": \"2019-11-03\", \"small_sold\": 10376832, \"large_sold\": 7835071},\n     {\"date\": \"2019-11-10\", \"small_sold\": 10717154, \"large_sold\": 8561348}]\n\n# Convert list into DataFrame\navocados_2019 = pd.DataFrame(avocados_list)\n\n# Print the new DataFrame\n\nprint(avocados_2019)\n\n         date  small_sold  large_sold\n0  2019-11-03    10376832     7835071\n1  2019-11-10    10717154     8561348"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#dictionary-of-lists",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#dictionary-of-lists",
    "title": "Data Manipulation with pandas",
    "section": "Dictionary of lists",
    "text": "Dictionary of lists\nSome more data just came in! This time, you’ll use the dictionary of lists method, parsing the data column by column.\n\n# Create a dictionary of lists with new data\navocados_dict = {\n  \"date\": ['2019-11-17', '2019-12-01'],\n  \"small_sold\":  [10859987, 9291631],\n  \"large_sold\": [7674135, 6238096]\n}\n\n# Convert dictionary into DataFrame\navocados_2019 = pd.DataFrame(avocados_dict)\n\n# Print the new DataFrame\nprint(avocados_2019)\n\n         date  small_sold  large_sold\n0  2019-11-17    10859987     7674135\n1  2019-12-01     9291631     6238096"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#csv-to-dataframe",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#csv-to-dataframe",
    "title": "Data Manipulation with pandas",
    "section": "CSV to DataFrame",
    "text": "CSV to DataFrame\nYou work for an airline, and your manager has asked you to do a competitive analysis and see how often passengers flying on other airlines are involuntarily bumped from their flights. You got a CSV file (airline_bumping.csv) from the Department of Transportation containing data on passengers that were involuntarily denied boarding in 2016 and 2017, but it doesn’t have the exact numbers you want. In order to figure this out, you’ll need to get the CSV into a pandas DataFrame and do some manipulation!\npandas is imported for you as pd. “airline_bumping.csv” is in your working directory.\n\n# From previous steps\nairline_bumping = pd.read_csv(\"data/airline_bumping.csv\")\nprint(airline_bumping.head())\n\n             airline  year  nb_bumped  total_passengers\n0    DELTA AIR LINES  2017        679          99796155\n1     VIRGIN AMERICA  2017        165           6090029\n2    JETBLUE AIRWAYS  2017       1475          27255038\n3    UNITED AIRLINES  2017       2067          70030765\n4  HAWAIIAN AIRLINES  2017         92           8422734\n\nairline_totals = airline_bumping.groupby(\"airline\")[[\"nb_bumped\", \"total_passengers\"]].sum()\nairline_totals[\"bumps_per_10k\"] = airline_totals[\"nb_bumped\"] / airline_totals[\"total_passengers\"] * 10000\n\n# Print airline_totals\nprint(airline_totals)\n\n                     nb_bumped  total_passengers  bumps_per_10k\nairline                                                        \nALASKA AIRLINES           1392          36543121       0.380920\nAMERICAN AIRLINES        11115         197365225       0.563169\nDELTA AIR LINES           1591         197033215       0.080748\nEXPRESSJET AIRLINES       3326          27858678       1.193883\nFRONTIER AIRLINES         1228          22954995       0.534960\nHAWAIIAN AIRLINES          122          16577572       0.073593\nJETBLUE AIRWAYS           3615          53245866       0.678926\nSKYWEST AIRLINES          3094          47091737       0.657015\nSOUTHWEST AIRLINES       18585         228142036       0.814624\nSPIRIT AIRLINES           2920          32304571       0.903897\nUNITED AIRLINES           4941         134468897       0.367446\nVIRGIN AMERICA             242          12017967       0.201365"
  },
  {
    "objectID": "datacamp/DataManipulationPandas/DataManipulationPandas.html#dataframe-to-csv",
    "href": "datacamp/DataManipulationPandas/DataManipulationPandas.html#dataframe-to-csv",
    "title": "Data Manipulation with pandas",
    "section": "DataFrame to CSV",
    "text": "DataFrame to CSV\nYou’re almost there! To make things easier to read, you’ll need to sort the data and export it to CSV so that your colleagues can read it.\n\n# Create airline_totals_sorted\nairline_totals_sorted = airline_totals.sort_values(\"bumps_per_10k\", ascending = False)\n\n# Print airline_totals_sorted\n\nprint(airline_totals_sorted)\n\n                     nb_bumped  total_passengers  bumps_per_10k\nairline                                                        \nEXPRESSJET AIRLINES       3326          27858678       1.193883\nSPIRIT AIRLINES           2920          32304571       0.903897\nSOUTHWEST AIRLINES       18585         228142036       0.814624\nJETBLUE AIRWAYS           3615          53245866       0.678926\nSKYWEST AIRLINES          3094          47091737       0.657015\nAMERICAN AIRLINES        11115         197365225       0.563169\nFRONTIER AIRLINES         1228          22954995       0.534960\nALASKA AIRLINES           1392          36543121       0.380920\nUNITED AIRLINES           4941         134468897       0.367446\nVIRGIN AMERICA             242          12017967       0.201365\nDELTA AIR LINES           1591         197033215       0.080748\nHAWAIIAN AIRLINES          122          16577572       0.073593\n\n\n# Save as airline_totals_sorted.csv\n\nairline_totals_sorted.to_csv(\"data/airline_totals_sorted.csv\")"
  },
  {
    "objectID": "datacamp/Python_Data_Science_Toolbox_Part_1/python_data_science_toolbox_part_1.html",
    "href": "datacamp/Python_Data_Science_Toolbox_Part_1/python_data_science_toolbox_part_1.html",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "",
    "text": "In the video, you learned of another standard Python datatype, strings. Recall that these represent textual data. To assign the string ‘DataCamp’ to a variable company, you execute:\ncompany = 'DataCamp'\nYou’ve also learned to use the operations + and * with strings. Unlike with numeric types such as ints and floats, the + operator concatenates strings together, while the * concatenates multiple copies of a string together. In this exercise, you will use the + and * operations on strings to answer the question below. Execute the following code in the shell:\nobject1 = \"data\" + \"analysis\" + \"visualization\"\nobject2 = 1 * 3\nobject3 = \"1\" * 3\nWhat are the values in object1, object2, and object3, respectively?\n\nobject1 = \"data\" + \"analysis\" + \"visualization\"\nobject2 = 1 * 3\nobject3 = \"1\" * 3\nprint(\"object1:\", object1)\n\nobject1: dataanalysisvisualization\n\nprint(\"object2:\", object2)\n\nobject2: 3\n\nprint(\"object3:\", object3)\n\nobject3: 111\n\n\n\n\n\nIn the video, Hugo briefly examined the return behavior of the built-in functions print() and str(). Here, you will use both functions and examine their return values. A variable x has been preloaded for this exercise. Run the code below in the console. Pay close attention to the results to answer the question that follows.\n\nAssign str(x) to a variable y1: y1 = str(x)\nAssign print(x) to a variable y2: y2 = print(x)\nCheck the types of the variables x, y1, and y2. What are the types of x, y1, and y2?\n\n\nx = 4.89\ny1 = str(x)\ny2 = print(x)\n\n4.89\n\nprint(type(y1))\n\n<class 'str'>\n\nprint(type(y2))\n\n<class 'NoneType'>\n\n\n\n\n\nIn the last video, Hugo described the basics of how to define a function. You will now write your own function!\nDefine a function, shout(), which simply prints out a string with three exclamation marks ‘!!!’ at the end. The code for the square() function that we wrote earlier is found below. You can use it as a pattern to define shout().\ndef square():\n    new_value = 4 ** 2\n    return new_value\n    \nNote that the function body is indented 4 spaces already for you. Function bodies need to be indented by a consistent number of spaces and the choice of 4 is common.\nThis course touches on a lot of concepts you may have forgotten, so if you ever need a quick refresher, download the Python for Data Science Cheat Sheet and keep it handy!\n\n# Define the function shout\ndef shout():\n    \"\"\"Print a string with three exclamation marks\"\"\"\n    # Concatenate the strings: shout_word\n    shout_word = 'congratulations' +  '!!!'\n\n    # Print shout_word\n    print(shout_word)\n\nshout()\n\ncongratulations!!!\n\n\n\n\n\nCongratulations! You have successfully defined and called your own function! That’s pretty cool.\nIn the previous exercise, you defined and called the function shout(), which printed out a string concatenated with ‘!!!’. You will now update shout() by adding a parameter so that it can accept and process any string argument passed to it. Also note that shout(word), the part of the header that specifies the function name and parameter(s), is known as the signature of the function. You may encounter this term in the wild!\n\n# Define shout with the parameter, word\ndef shout(word):\n    \"\"\"Print a string with three exclamation marks\"\"\"\n    # Concatenate the strings: shout_word\n    shout_word = word + '!!!'\n\n    # Print shout_word\n    print(shout_word)\n\n# Call shout with the string 'congratulations'\nshout('congratulations')\n\ncongratulations!!!\n\n\n\n\n\nYou’re getting very good at this! Try your hand at another modification to the shout() function so that it now returns a single value instead of printing within the function. Recall that the return keyword lets you return values from functions. Parts of the function shout(), which you wrote earlier, are shown. Returning values is generally more desirable than printing them out because, as you saw earlier, a print() call assigned to a variable has type NoneType.\n\n# Define shout with the parameter, word\ndef shout(word):\n    \"\"\"Return a string with three exclamation marks\"\"\"\n    # Concatenate the strings: shout_word\n    shout_word = word + \"!!!\"\n\n    # Replace print with return\n    return(shout_word)\n\n# Pass 'congratulations' to shout: yell\n\nyell = shout(\"congratulations\")\n\n# Print yell\n\nprint(yell)\n\ncongratulations!!!\n\n\n\n\n\nHugo discussed the use of multiple parameters in defining functions in the last lecture. You are now going to use what you’ve learned to modify the shout() function further. Here, you will modify shout() to accept two arguments. Parts of the function shout(), which you wrote earlier, are shown.\n\n# Define shout with parameters word1 and word2\ndef shout(word1, word2):\n    \"\"\"Concatenate strings with three exclamation marks\"\"\"\n    # Concatenate word1 with '!!!': shout1\n    shout1 = word1 + \"!!!\"\n    \n    # Concatenate word2 with '!!!': shout2\n    shout2 = word2 + \"!!!\"\n    \n    # Concatenate shout1 with shout2: new_shout\n    \n    new_shout = shout1 + shout2\n\n    # Return new_shout\n    return new_shout\n\n# Pass 'congratulations' and 'you' to shout(): yell\n\nyell = shout(\"congratulations\", \"you\")\n\n# Print yell\nprint(yell)\n\ncongratulations!!!you!!!\n\n\n\n\n\nAlongside learning about functions, you’ve also learned about tuples! Here, you will practice what you’ve learned about tuples: how to construct, unpack, and access tuple elements. Recall how Hugo unpacked the tuple even_nums in the video:\na, b, c = even_nums\nA three-element tuple named nums has been preloaded for this exercise. Before completing the script, perform the following:\nPrint out the value of nums in the IPython shell. Note the elements in the tuple. In the IPython shell, try to change the first element of nums to the value 2 by doing an assignment: nums[0] = 2. What happens?\n\n\nnums =  (3, 4, 6)\n\n# Unpack nums into num1, num2, and num3\nnum1, num2, num3 = nums\n\n# Construct even_nums\n\neven_nums = (2, num2, num3)\n\n\n\n\nIn the previous exercise, you constructed tuples, assigned tuples to variables, and unpacked tuples. Here you will return multiple values from a function using tuples. Let’s now update our shout() function to return multiple values. Instead of returning just one string, we will return two strings with the string !!! concatenated to each.\nNote that the return statement return x, y has the same result as return (x, y): the former actually packs x and y into a tuple under the hood!\n\n# Define shout_all with parameters word1 and word2\ndef shout_all(word1, word2):\n    \n    # Concatenate word1 with '!!!': shout1\n    shout1 = word1 + \"!!!\"\n    \n    # Concatenate word2 with '!!!': shout2\n    shout2 = word2 + \"!!!\"\n    \n    # Construct a tuple with shout1 and shout2: shout_words\n    shout_words = (shout1, shout2)\n\n    # Return shout_words\n    return shout_words\n\n# Pass 'congratulations' and 'you' to shout_all(): yell1, yell2\n\nyell1, yell2 = shout_all(\"congratulations\", \"you\")\n# Print yell1 and yell2\nprint(yell1)\n\ncongratulations!!!\n\nprint(yell2)\n\nyou!!!\n\n\n\n\n\nYou’ve got your first taste of writing your own functions in the previous exercises. You’ve learned how to add parameters to your own function definitions, return a value or multiple values with tuples, and how to call the functions you’ve defined.\nIn this and the following exercise, you will bring together all these concepts and apply them to a simple data science problem. You will load a dataset and develop functionalities to extract simple insights from the data.\nFor this exercise, your goal is to recall how to load a dataset into a DataFrame. The dataset contains Twitter data and you will iterate over entries in a column to build a dictionary in which the keys are the names of languages and the values are the number of tweets in the given language. The file tweets.csv is available in your current directory.\nBe aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real Twitter data).\n\n# Import pandas\n\nimport pandas as pd \n# Import Twitter data as DataFrame: df\ndf = pd.read_csv(\"data/tweets.csv\")\n\n# Initialize an empty dictionary: langs_count\nlangs_count = {}\n\n# Extract column from DataFrame: col\ncol = df['lang']\n\n# Iterate over lang column in DataFrame\nfor entry in col:\n\n    # If the language is in langs_count, add 1 \n    if entry in langs_count.keys():\n        langs_count[entry] += 1\n    # Else add the language to langs_count, set the value to 1\n    else:\n        langs_count[entry] = 1\n\n# Print the populated dictionary\nprint(langs_count)\n\n{'en': 97, 'et': 1, 'und': 2}\n\n\n\n\n\nGreat job! You’ve now defined the functionality for iterating over entries in a column and building a dictionary with keys the names of languages and values the number of tweets in the given language.\nIn this exercise, you will define a function with the functionality you developed in the previous exercise, return the resulting dictionary from within the function, and call the function with the appropriate arguments.\nFor your convenience, the pandas package has been imported as pd and the ‘tweets.csv’ file has been imported into the tweets_df variable.\n\n# Define count_entries()\ndef count_entries(df, col_name):\n    \"\"\"Return a dictionary with counts of \n    occurrences as value for each key.\"\"\"\n\n    # Initialize an empty dictionary: langs_count\n    langs_count = {}\n    \n    # Extract column from DataFrame: col\n    col = df[col_name]\n    \n    # Iterate over lang column in DataFrame\n    for entry in col:\n\n        # If the language is in langs_count, add 1\n        if entry in langs_count.keys():\n            langs_count[entry] += 1\n        # Else add the language to langs_count, set the value to 1\n        else:\n            langs_count[entry] = 1\n\n    # Return the langs_count dictionary\n    return langs_count\n    \n\n# Call count_entries(): result\n\nresult = count_entries(df, \"lang\")\n\n# Print the result\nprint(result)\n\n{'en': 97, 'et': 1, 'und': 2}\n\n\n\n\n\nIn this exercise, you will practice what you’ve learned about scope in functions. The variable num has been predefined as 5, alongside the following function definitions:\n\nnum = 5\ndef func1():\n    num = 3\n    print(num)\n\ndef func2():\n    global num\n    double_num = num * 2\n    num = 6\n    print(double_num)\n    \nfunc1()\n\n3\n\nfunc2()\n\n10\n\nprint(num)\n\n6\n\n\n\nTry calling func1() and func2() in the shell, then answer the following questions:\n\n\n\n\nLet’s work more on your mastery of scope. In this exercise, you will use the keyword global within a function to alter the value of a variable defined in the global scope.\n\n# Create a string: team\nteam = \"teen titans\"\n\n# Define change_team()\ndef change_team():\n    \"\"\"Change the value of the global variable team.\"\"\"\n\n    # Use team in global scope\n    global team\n\n    # Change the value of team in global: team\n    team = \"justice league\"\n# Print team\nprint(team)\n\nteen titans\n\n# Call change_team()\nchange_team()\n\n# Print team\nprint(team)\n\njustice league\n\n\n\n\n\nHere you’re going to check out Python’s built-in scope, which is really just a built-in module called builtins. However, to query builtins, you’ll need to import builtins ‘because the name builtins is not itself built in…No, I’m serious!’ (Learning Python, 5th edition, Mark Lutz). After executing import builtins in the IPython Shell, execute dir(builtins) to print a list of all the names in the module builtins. Have a look and you’ll see a bunch of names that you’ll recognize! Which of the following names is NOT in the module builtins?\n\n# Define three_shouts\ndef three_shouts(word1, word2, word3):\n    \"\"\"Returns a tuple of strings\n    concatenated with '!!!'.\"\"\"\n\n    # Define inner\n    def inner(word):\n        \"\"\"Returns a string concatenated with '!!!'.\"\"\"\n        return word + '!!!'\n\n    # Return a tuple of strings\n    return (inner(word1), inner(word2), inner(word3))\n\n# Call three_shouts() and print\nprint(three_shouts('a', 'b', 'c'))\n\n('a!!!', 'b!!!', 'c!!!')"
  },
  {
    "objectID": "datacamp/introduction_to_shell/intoroduction_shell.html",
    "href": "datacamp/introduction_to_shell/intoroduction_shell.html",
    "title": "Introduction to Shell",
    "section": "",
    "text": "An operating system like Windows, Linux, or Mac OS is a special kind of program. It controls the computer’s processor, hard drive, and network connection, but its most important job is to run other programs.\nSince human beings aren’t digital, they need an interface to interact with the operating system. The most common one these days is a graphical file explorer, which translates clicks and double-clicks into commands to open files and run programs. Before computers had graphical displays, though, people typed instructions into a program called a command-line shell. Each time a command is entered, the shell runs some other programs, prints their output in human-readable form, and then displays a prompt to signal that it’s ready to accept the next command. (Its name comes from the notion that it’s the “outer shell” of the computer.)\nTyping commands instead of clicking and dragging may seem clumsy at first, but as you will see, once you start spelling out what you want the computer to do, you can combine old commands to create new ones and automate repetitive operations with just a few keystrokes.\n\nThey are both interfaces for issuing commands to the operating system.\n\n\n\n\nThe filesystem manages files and directories (or folders). Each is identified by an absolute path that shows how to reach it from the filesystem’s root directory: /home/repl is the directory repl in the directory home, while /home/repl/course.txt is a file course.txt in that directory, and / on its own is the root directory.\nTo find out where you are in the filesystem, run the command pwd (short for “print working directory”). This prints the absolute path of your current working directory, which is where the shell runs commands and looks for files by default.\nRun pwd. Where are you right now?\n\n\npwd\n\n/home/mburu/r_projects/m-mburu.github.io/datacamp/introduction_to_shell\n\n\n\n\n\npwd tells you where you are. To find out what’s there, type ls (which is short for “listing”) and press the enter key. On its own, ls lists the contents of your current directory (the one displayed by pwd). If you add the names of some files, ls will list them, and if you add the names of directories, it will list their contents. For example, ls /home/repl shows you what’s in your starting directory (usually called your home directory).\nUse ls with an appropriate argument to list the files in the directory /home/repl/seasonal (which holds information on dental surgeries by date, broken down by season). Which of these files is not in that directory?\n\nls seasonal\n\nautumn.csv\nspring.csv\nsummer.csv\nwinter.csv\nwinter.csv.bck\n\n\n\n\n\nAn absolute path is like a latitude and longitude: it has the same value no matter where you are. A relative path, on the other hand, specifies a location starting from where you are: it’s like saying “20 kilometers north”.\nAs examples:\nIf you are in the directory /home/repl, the relative path seasonal specifies the same directory as the absolute path /home/repl/seasonal. If you are in the directory /home/repl/seasonal, the relative path winter.csv specifies the same file as the absolute path /home/repl/seasonal/winter.csv. The shell decides if a path is absolute or relative by looking at its first character: If it begins with /, it is absolute. If it does not begin with /, it is relative\n\nls course.txt\nls seasonal/summer.csv\nls people\n\ncourse.txt\nseasonal/summer.csv\nagarwal.txt\n\n\n\n\n\nJust as you can move around in a file browser by double-clicking on folders, you can move around in the filesystem using the command cd (which stands for “change directory”).\nIf you type cd seasonal and then type pwd, the shell will tell you that you are now in /home/repl/seasonal. If you then run ls on its own, it shows you the contents of /home/repl/seasonal, because that’s where you are. If you want to get back to your home directory /home/repl, you can use the command cd /home/repl.\n\ncd seasonal\npwd\nls\n\n/home/mburu/r_projects/m-mburu.github.io/datacamp/introduction_to_shell/seasonal\nautumn.csv\nspring.csv\nsummer.csv\nwinter.csv\nwinter.csv.bck\n\n\n\n\n\nThe parent of a directory is the directory above it. For example, /home is the parent of /home/repl, and /home/repl is the parent of /home/repl/seasonal. You can always give the absolute path of your parent directory to commands like cd and ls. More often, though, you will take advantage of the fact that the special path .. (two dots with no spaces) means “the directory above the one I’m currently in”. If you are in /home/repl/seasonal, then cd .. moves you up to /home/repl. If you use cd .. once again, it puts you in /home. One more cd .. puts you in the root directory /, which is the very top of the filesystem. (Remember to put a space between cd and .. - it is a command and a path, not a single four-letter command.)\nA single dot on its own, ., always means “the current directory”, so ls on its own and ls . do the same thing, while cd . has no effect (because it moves you into the directory you’re currently in).\nOne final special path is ~ (the tilde character), which means “your home directory”, such as /home/repl. No matter where you are, ls ~ will always list the contents of your home directory, and cd ~ will always take you home.\n\npwd ~/../.\n\n/home/mburu/r_projects/m-mburu.github.io/datacamp/introduction_to_shell\n\n\n\n\n\nYou will often want to copy files, move them into other directories to organize them, or rename them. One command to do this is cp, which is short for “copy”. If original.txt is an existing file, then:\ncp original.txt duplicate.txt creates a copy of original.txt called duplicate.txt. If there already was a file called duplicate.txt, it is overwritten. If the last parameter to cp is an existing directory, then a command like:\ncp seasonal/autumn.csv seasonal/winter.csv backup copies all of the files into that directory.\n\ncp seasonal/summer.csv  backup/summer.bck\ncp seasonal/spring.csv seasonal/summer.csv backup\nls backup\n\nagarwal.txt\nautumn.csv\nspring.csv\nsummer.bck\nsummer.csv\n\n\n\n\n\nWhile cp copies a file, mv moves it from one directory to another, just as if you had dragged it in a graphical file browser. It handles its parameters the same way as cp, so the command:\nmv autumn.csv winter.csv .. moves the files autumn.csv and winter.csv from the current working directory up one level to its parent directory (because .. always refers to the directory above your current location).\n\nmv seasonal/spring.csv seasonal/summer.csv backup\nls backup\necho \"spring and summer moved to backup\"\nls seasonal\n# move them back\nmv backup/spring.csv backup/summer.csv seasonal\n\nagarwal.txt\nautumn.csv\nspring.csv\nsummer.bck\nsummer.csv\nspring and summer moved to backup\nautumn.csv\nwinter.csv\nwinter.csv.bck\n\n\n\n\n\nmv can also be used to rename files. If you run:\nmv course.txt old-course.txt then the file course.txt in the current working directory is “moved” to the file old-course.txt. This is different from the way file browsers work, but is often handy.\nOne warning: just like cp, mv will overwrite existing files. If, for example, you already have a file called old-course.txt, then the command shown above will replace it with whatever is in course.txt.\n\ncp seasonal/winter.csv backup/winter.csv \nmv seasonal/winter.csv seasonal/winter.csv.bck\nls seasonal\necho \"winter.csv moved from backup for further examples\"\nmv backup/winter.csv seasonal/winter.csv\n\nautumn.csv\nspring.csv\nsummer.csv\nwinter.csv.bck\nwinter.csv moved from backup for further examples\n\n\n\n\n\nWe can copy files and move them around; to delete them, we use rm, which stands for “remove”. As with cp and mv, you can give rm the names of as many files as you’d like, so:\nrm thesis.txt backup/thesis-2017-08.txt removes both thesis.txt and backup/thesis-2017-08.txt\nrm does exactly what its name says, and it does it right away: unlike graphical file browsers, the shell doesn’t have a trash can, so when you type the command above, your thesis is gone for good.\n\ncp seasonal/autumn.csv backup\nrm seasonal/autumn.csv\nls seasonal\ncp backup/autumn.csv seasonal\n\nspring.csv\nsummer.csv\nwinter.csv\nwinter.csv.bck\n\n\n\n\n\nmv treats directories the same way it treats files: if you are in your home directory and run mv seasonal by-season, for example, mv changes the name of the seasonal directory to by-season. However, rm works differently.\nIf you try to rm a directory, the shell prints an error message telling you it can’t do that, primarily to stop you from accidentally deleting an entire directory full of work. Instead, you can use a separate command called rmdir. For added safety, it only works when the directory is empty, so you must delete the files in a directory before you delete the directory. (Experienced users can use the -r option to rm to get the same effect; we will discuss command options in the next chapter.)\n\n\n#Without changing directories, delete the file agarwal.txt in the people directory.\ncp people/agarwal.txt backup/agarwal.txt\nrm people/agarwal.txt\nls people\nrmdir people\nmkdir people\ncp backup/agarwal.txt people/agarwal.txt \nmkdir yearly\nmkdir yearly/2017\nls\n\nmkdir: cannot create directory ‘yearly’: File exists\nmkdir: cannot create directory ‘yearly/2017’: File exists\nbackup\nbottom.csv\ncount-records.sh\ncourse.txt\ndate-range.sh\ndates.sh\nget-field.sh\nintoroduction_shell.rmarkdown\nintoroduction_shell.Rmd\nintroduction_to_shell.Rproj\nlast.csv\nnames.txt\nnum-records.out\npeople\nrange.out\nrange.sh\nresult.txt\nseasonal\nspring.csv\nsteps.txt\nsummer.csv\nteeth.out\nteeth.sh\ntemp.csv\nyearly\n\n\n\n\n\nYou will often create intermediate files when analyzing data. Rather than storing them in your home directory, you can put them in /tmp, which is where people and programs often keep files they only need briefly. (Note that /tmp is immediately below the root directory /, not below your home directory.) This wrap-up exercise will show you how to do that.\n\n\n\n\ncd /tmp \nls\nmkdir /tmp/scratch\n\ngdm3-config-err-b7pfou\nhsperfdata_shinyproxy\nquarto-session48e77b42\nRtmp0wKeLi\nRtmpvP9jW5\nscoped_dirsrgn3e\nsnap-private-tmp\nsystemd-private-c5ac1c15909343a4a214ea447616222f-bluetooth.service-JzWb6M\nsystemd-private-c5ac1c15909343a4a214ea447616222f-bolt.service-TlexIm\nsystemd-private-c5ac1c15909343a4a214ea447616222f-colord.service-qLFMQM\nsystemd-private-c5ac1c15909343a4a214ea447616222f-fwupd.service-AVogvg\nsystemd-private-c5ac1c15909343a4a214ea447616222f-ModemManager.service-aG2ZxZ\nsystemd-private-c5ac1c15909343a4a214ea447616222f-power-profiles-daemon.service-eH11QT\nsystemd-private-c5ac1c15909343a4a214ea447616222f-switcheroo-control.service-y36O1v\nsystemd-private-c5ac1c15909343a4a214ea447616222f-systemd-logind.service-ltwabB\nsystemd-private-c5ac1c15909343a4a214ea447616222f-systemd-oomd.service-wREtPS\nsystemd-private-c5ac1c15909343a4a214ea447616222f-systemd-resolved.service-z9o7s2\nsystemd-private-c5ac1c15909343a4a214ea447616222f-systemd-timesyncd.service-I1DToq\nsystemd-private-c5ac1c15909343a4a214ea447616222f-upower.service-ygra31\nundertow.8080.3698114627944858908\nundertow.9090.2177916117139773732\nundertow-docbase.8080.13094858791798125321\nundertow-docbase.9090.5730517480924681890"
  },
  {
    "objectID": "datacamp/introduction_to_shell/intoroduction_shell.html#manipulating-data",
    "href": "datacamp/introduction_to_shell/intoroduction_shell.html#manipulating-data",
    "title": "Introduction to Shell",
    "section": "Manipulating data",
    "text": "Manipulating data\n\nHow can I view a file’s contents?\nBefore you rename or delete files, you may want to have a look at their contents. The simplest way to do this is with cat, which just prints the contents of files onto the screen. (Its name is short for “concatenate”, meaning “to link things together”, since it will print all the files whose names you give it, one after the other.)\ncat agarwal.txt\nname: Agarwal, Jasmine\nposition: RCT2\nstart: 2017-04-01\nbenefits: full\n\ncat course.txt\n\nIntroduction to the Unix Shell for Data Science\n\nThe Unix command line has survived and thrived for almost fifty years\nbecause it lets people to do complex things with just a few\nkeystrokes. Sometimes called \"the duct tape of programming\", it helps\nusers combine existing programs in new ways, automate repetitive\ntasks, and run programs on clusters and clouds that may be halfway\naround the world. This lesson will introduce its key elements and show\nyou how to use them efficiently.\n\n\n\n\nHow can I view a file’s contents piece by piece?\nYou can use cat to print large files and then scroll through the output, but it is usually more convenient to page the output. The original command for doing this was called more, but it has been superseded by a more powerful command called less. (This kind of naming is what passes for humor in the Unix world.) When you less a file, one page is displayed at a time; you can press spacebar to page down or type q to quit.\nIf you give less the names of several files, you can type :n (colon and a lower-case ‘n’) to move to the next file, :p to go back to the previous one, or :q to quit.\nNote: If you view solutions to exercises that use less, you will see an extra command at the end that turns paging off so that we can test your solutions efficiently.\n\nless seasonal/spring.csv seasonal/summer.csv\n\nDate,Tooth\n2017-01-25,wisdom\n2017-02-19,canine\n2017-02-24,canine\n2017-02-28,wisdom\n2017-03-04,incisor\n2017-03-12,wisdom\n2017-03-14,incisor\n2017-03-21,molar\n2017-04-29,wisdom\n2017-05-08,canine\n2017-05-20,canine\n2017-05-21,canine\n2017-05-25,canine\n2017-06-04,molar\n2017-06-13,bicuspid\n2017-06-14,canine\n2017-07-10,incisor\n2017-07-16,bicuspid\n2017-07-23,bicuspid\n2017-08-13,bicuspid\n2017-08-13,incisor\n2017-08-13,wisdom\n2017-09-07,molarDate,Tooth\n2017-01-11,canine\n2017-01-18,wisdom\n2017-01-21,bicuspid\n2017-02-02,molar\n2017-02-27,wisdom\n2017-02-27,wisdom\n2017-03-07,bicuspid\n2017-03-15,wisdom\n2017-03-20,canine\n2017-03-23,molar\n2017-04-02,bicuspid\n2017-04-22,wisdom\n2017-05-07,canine\n2017-05-09,canine\n2017-05-11,incisor\n2017-05-14,incisor\n2017-05-19,canine\n2017-05-23,incisor\n2017-05-24,incisor\n2017-06-18,incisor\n2017-07-25,canine\n2017-08-02,canine\n2017-08-03,bicuspid\n2017-08-04,canine\n\n\n\n\nHow can I look at the start of a file?\nThe first thing most data scientists do when given a new dataset to analyze is figure out what fields it contains and what values those fields have. If the dataset has been exported from a database or spreadsheet, it will often be stored as comma-separated values (CSV). A quick way to figure out what it contains is to look at the first few rows.\nWe can do this in the shell using a command called head. As its name suggests, it prints the first few lines of a file (where “a few” means 10), so the command:\nhead seasonal/summer.csv\ndisplays:\nDate,Tooth\n2017-01-11,canine\n2017-01-18,wisdom\n2017-01-21,bicuspid\n2017-02-02,molar\n2017-02-27,wisdom\n2017-02-27,wisdom\n2017-03-07,bicuspid\n2017-03-15,wisdom\n2017-03-20,canine\nWhat does head do if there aren’t 10 lines in the file? (To find out, use it to look at the top of people/agarwal.txt.)\n\n\nhead -n 10 people/agarwal.txt\n\nname: Agarwal, Jasmine\nposition: RCT2\nstart: 2017-04-01\nbenefits: full\n\n\n\ndisplays as many lines as there are in the file\n\n\n\nHow can I type less?\nOne of the shell’s power tools is tab completion. If you start typing the name of a file and then press the tab key, the shell will do its best to auto-complete the path. For example, if you type sea and press tab, it will fill in the directory name seasonal/ (with a trailing slash). If you then type a and tab, it will complete the path as seasonal/autumn.csv.\nIf the path is ambiguous, such as seasonal/s, pressing tab a second time will display a list of possibilities. Typing another character or two to make your path more specific and then pressing tab will fill in the rest of the name.\n\necho \"head seasonal/autumn.csv\"\nhead seasonal/autumn.csv\necho \"head seasonal/autumn.csv\"\nhead seasonal/spring.csv\n\nhead seasonal/autumn.csv\nDate,   Tooth\n2017-01-05,canine\n2017-01-17,wisdom\n2017-01-18,canine\n2017-02-01,molar\n2017-02-22,bicuspid\n2017-03-10,canine\n2017-03-13,canine\n2017-04-30,incisor\n2017-05-02,canine\nhead seasonal/autumn.csv\nDate,Tooth\n2017-01-25,wisdom\n2017-02-19,canine\n2017-02-24,canine\n2017-02-28,wisdom\n2017-03-04,incisor\n2017-03-12,wisdom\n2017-03-14,incisor\n2017-03-21,molar\n2017-04-29,wisdom\n\n\n\n\nHow can I control what commands do?\nYou won’t always want to look at the first 10 lines of a file, so the shell lets you change head’s behavior by giving it a command-line flag (or just “flag” for short). If you run the command:\nhead -n 3 seasonal/summer.csv head will only display the first three lines of the file. If you run head -n 100, it will display the first 100 (assuming there are that many), and so on.\nA flag’s name usually indicates its purpose (for example, -n is meant to signal “number of lines”). Command flags don’t have to be a - followed by a single letter, but it’s a widely-used convention.\nNote: it’s considered good style to put all flags before any filenames, so in this course, we only accept answers that do that.\n\n\nhead -n 5 seasonal/winter.csv\n\nDate,Tooth\n2017-01-03,bicuspid\n2017-01-05,incisor\n2017-01-21,wisdom\n2017-02-05,molar\n\n\n\n\nHow can I list everything below a directory?\nIn order to see everything underneath a directory, no matter how deeply nested it is, you can give ls the flag -R (which means “recursive”). If you use ls -R in your home directory, you will see something like this:\n\n\nls -R -F\n\n.:\nbackup/\nbottom.csv\ncount-records.sh\ncourse.txt*\ndate-range.sh\ndates.sh\nget-field.sh\nintoroduction_shell.rmarkdown\nintoroduction_shell.Rmd*\nintroduction_to_shell.Rproj*\nlast.csv\nnames.txt\nnum-records.out\npeople/\nrange.out\nrange.sh\nresult.txt\nseasonal/\nspring.csv*\nsteps.txt\nsummer.csv*\nteeth.out\nteeth.sh\ntemp.csv\nyearly/\n\n./backup:\nagarwal.txt*\nautumn.csv*\nsummer.bck*\n\n./people:\nagarwal.txt*\n\n./seasonal:\nautumn.csv*\nspring.csv*\nsummer.csv*\nwinter.csv*\nwinter.csv.bck*\n\n./yearly:\n2017/\n\n./yearly/2017:\n\n\n\n\nHow can I get help for a command?\nTo find out what commands do, people used to use the man command (short for “manual”). For example, the command man head brings up this information:\n\nHEAD(1)               BSD General Commands Manual              HEAD(1)\n\nNAME\n     head -- display first lines of a file\n\nSYNOPSIS\n     head [-n count | -c bytes] [file ...]\n\nDESCRIPTION\n     This filter displays the first count lines or bytes of each of\n     the specified files, or of the standard input if no files are\n     specified.  If count is omitted it defaults to 10.\n\n     If more than a single file is specified, each file is preceded by\n     a header consisting of the string ``==> XXX <=='' where ``XXX''\n     is the name of the file.\n\nSEE ALSO\n     tail(1)\nman automatically invokes less, so you may need to press spacebar to page through the information and :q to quit.\nThe one-line description under NAME tells you briefly what the command does, and the summary under SYNOPSIS lists all the flags it understands. Anything that is optional is shown in square brackets […], either/or alternatives are separated by |, and things that can be repeated are shown by …, so head’s manual page is telling you that you can either give a line count with -n or a byte count with -c, and that you can give it any number of filenames.\nThe problem with the Unix manual is that you have to know what you’re looking for. If you don’t, you can search Stack Overflow, ask a question on DataCamp’s Slack channels, or look at the SEE ALSO sections of the commands you already know.\n\nman tail\ntail -n +7 seasonal/spring.csv\n\nTAIL(1)                                                                                                                User Commands                                                                                                                TAIL(1)\n\nNAME\n       tail - output the last part of files\n\nSYNOPSIS\n       tail [OPTION]... [FILE]...\n\nDESCRIPTION\n       Print the last 10 lines of each FILE to standard output.  With more than one FILE, precede each with a header giving the file name.\n\n       With no FILE, or when FILE is -, read standard input.\n\n       Mandatory arguments to long options are mandatory for short options too.\n\n       -c, --bytes=[+]NUM\n              output the last NUM bytes; or use -c +NUM to output starting with byte NUM of each file\n\n       -f, --follow[={name|descriptor}]\n              output appended data as the file grows;\n\n              an absent option argument means 'descriptor'\n\n       -F     same as --follow=name --retry\n\n       -n, --lines=[+]NUM\n              output the last NUM lines, instead of the last 10; or use -n +NUM to output starting with line NUM\n\n       --max-unchanged-stats=N\n              with --follow=name, reopen a FILE which has not\n\n              changed size after N (default 5) iterations to see if it has been unlinked or renamed (this is the usual case of rotated log files); with inotify, this option is rarely useful\n\n       --pid=PID\n              with -f, terminate after process ID, PID dies\n\n       -q, --quiet, --silent\n              never output headers giving file names\n\n       --retry\n              keep trying to open a file if it is inaccessible\n\n       -s, --sleep-interval=N\n              with -f, sleep for approximately N seconds (default 1.0) between iterations; with inotify and --pid=P, check process P at least once every N seconds\n\n       -v, --verbose\n              always output headers giving file names\n\n       -z, --zero-terminated\n              line delimiter is NUL, not newline\n\n       --help display this help and exit\n\n       --version\n              output version information and exit\n\n       NUM may have a multiplier suffix: b 512, kB 1000, K 1024, MB 1000*1000, M 1024*1024, GB 1000*1000*1000, G 1024*1024*1024, and so on for T, P, E, Z, Y.  Binary prefixes can be used, too: KiB=K, MiB=M, and so on.\n\n       With  --follow  (-f), tail defaults to following the file descriptor, which means that even if a tail'ed file is renamed, tail will continue to track its end.  This default behavior is not desirable when you really want to track the actual name\n       of the file, not the file descriptor (e.g., log rotation).  Use --follow=name in that case.  That causes tail to track the named file in a way that accommodates renaming, removal and creation.\n\nAUTHOR\n       Written by Paul Rubin, David MacKenzie, Ian Lance Taylor, and Jim Meyering.\n\nREPORTING BUGS\n       GNU coreutils online help: <https://www.gnu.org/software/coreutils/>\n       Report any translation bugs to <https://translationproject.org/team/>\n\nCOPYRIGHT\n       Copyright © 2020 Free Software Foundation, Inc.  License GPLv3+: GNU GPL version 3 or later <https://gnu.org/licenses/gpl.html>.\n       This is free software: you are free to change and redistribute it.  There is NO WARRANTY, to the extent permitted by law.\n\nSEE ALSO\n       head(1)\n\n       Full documentation <https://www.gnu.org/software/coreutils/tail>\n       or available locally via: info '(coreutils) tail invocation'\n\nGNU coreutils 8.32                                                                                                     February 2022                                                                                                                TAIL(1)\n2017-03-12,wisdom\n2017-03-14,incisor\n2017-03-21,molar\n2017-04-29,wisdom\n2017-05-08,canine\n2017-05-20,canine\n2017-05-21,canine\n2017-05-25,canine\n2017-06-04,molar\n2017-06-13,bicuspid\n2017-06-14,canine\n2017-07-10,incisor\n2017-07-16,bicuspid\n2017-07-23,bicuspid\n2017-08-13,bicuspid\n2017-08-13,incisor\n2017-08-13,wisdom\n2017-09-07,molar\n\n\n\n\nHow can I select columns from a file?\nhead and tail let you select rows from a text file. If you want to select columns, you can use the command cut. It has several options (use man cut to explore them), but the most common is something like:\ncut -f 2-5,8 -d , values.csv\nwhich means “select columns 2 through 5 and columns 8, using comma as the separator”. cut uses -f (meaning “fields”) to specify columns and -d (meaning “delimiter”) to specify the separator. You need to specify the latter because some files may use spaces, tabs, or colons to separate columns.\nWhat command will select the first column (containing dates) from the file spring.csv?\n\ncut -f 1 -d , seasonal/spring.csv\n\nDate\n2017-01-25\n2017-02-19\n2017-02-24\n2017-02-28\n2017-03-04\n2017-03-12\n2017-03-14\n2017-03-21\n2017-04-29\n2017-05-08\n2017-05-20\n2017-05-21\n2017-05-25\n2017-06-04\n2017-06-13\n2017-06-14\n2017-07-10\n2017-07-16\n2017-07-23\n2017-08-13\n2017-08-13\n2017-08-13\n2017-09-07\n\n\n\n\nWhat can’t cut do?\ncut is a simple-minded command. In particular, it doesn’t understand quoted strings. If, for example, your file is:\nName,Age\n\"Johel,Ranjit\",28\n\"Sharma,Rupinder\",26\nthen:\ncut -f 2 -d , everyone.csv\nwill produce:\nAge\nRanjit\"\nRupinder\"\nrather than everyone’s age, because it will think the comma between last and first names is a column separator.\nWhat is the output of cut -d : -f 2-4 on the line:\nfirst:second:third: (Note the trailing colon.)\n\ncut -d , -f 2-4 seasonal/summer.csv\n\nTooth\ncanine\nwisdom\nbicuspid\nmolar\nwisdom\nwisdom\nbicuspid\nwisdom\ncanine\nmolar\nbicuspid\nwisdom\ncanine\ncanine\nincisor\nincisor\ncanine\nincisor\nincisor\nincisor\ncanine\ncanine\nbicuspid\ncanine\n\n\n\n\nHow can I repeat commands?\nOne of the biggest advantages of using the shell is that it makes it easy for you to do things over again. If you run some commands, you can then press the up-arrow key to cycle back through them. You can also use the left and right arrow keys and the delete key to edit them. Pressing return will then run the modified command.\nEven better, history will print a list of commands you have run recently. Each one is preceded by a serial number to make it easy to re-run particular commands: just type !55 to re-run the 55th command in your history (if you have that many). You can also re-run a command by typing an exclamation mark followed by the command’s name, such as !head or !cut, which will re-run the most recent use of that command.\n\ncd seasonal\necho \"head  winter.csv\"\nhead  winter.csv\necho \"latest head comman\"\n#!head\n\nhead  winter.csv\nDate,Tooth\n2017-01-03,bicuspid\n2017-01-05,incisor\n2017-01-21,wisdom\n2017-02-05,molar\n2017-02-17,incisor\n2017-02-25,bicuspid\n2017-03-12,incisor\n2017-03-25,molar\n2017-03-26,incisor\nlatest head comman\n\n\n\n\nHow can I select lines containing specific values?\nhead and tail select rows, cut selects columns, and grep selects lines according to what they contain. In its simplest form, grep takes a piece of text followed by one or more filenames and prints all of the lines in those files that contain that text. For example, grep bicuspid seasonal/winter.csv prints lines from winter.csv that contain “bicuspid”.\ngrep can search for patterns as well; we will explore those in the next course. What’s more important right now is some of grep’s more common flags:\n\n-c: print a count of matching lines rather than the lines themselves\n-h: do not print the names of files when searching multiple files\n-i: ignore case (e.g., treat “Regression” and “regression” as matches)\n-l: print the names of files that contain matches, not the matches\n-n: print line numbers for matching lines\n-v: invert the match, i.e., only show lines that don’t match\n\n\necho \"grep molars seasonal/autumn.csv\"\n\ngrep molar seasonal/autumn.csv\n\necho \"grep -v -n molar seasonal/spring.csv\"\ngrep -v -n molar seasonal/spring.csv\n\necho \"grep -c incisor seasonal/autumn.csv  seasonal/winter.csv\"\ngrep -c incisor seasonal/autumn.csv  seasonal/winter.csv\n\ngrep molars seasonal/autumn.csv\n2017-02-01,molar\n2017-05-25,molar\ngrep -v -n molar seasonal/spring.csv\n1:Date,Tooth\n2:2017-01-25,wisdom\n3:2017-02-19,canine\n4:2017-02-24,canine\n5:2017-02-28,wisdom\n6:2017-03-04,incisor\n7:2017-03-12,wisdom\n8:2017-03-14,incisor\n10:2017-04-29,wisdom\n11:2017-05-08,canine\n12:2017-05-20,canine\n13:2017-05-21,canine\n14:2017-05-25,canine\n16:2017-06-13,bicuspid\n17:2017-06-14,canine\n18:2017-07-10,incisor\n19:2017-07-16,bicuspid\n20:2017-07-23,bicuspid\n21:2017-08-13,bicuspid\n22:2017-08-13,incisor\n23:2017-08-13,wisdom\ngrep -c incisor seasonal/autumn.csv  seasonal/winter.csv\nseasonal/autumn.csv:3\nseasonal/winter.csv:6\n\n\n\n\nWhy isn’t it always safe to treat data as text?\nThe SEE ALSO section of the manual page for cut refers to a command called paste that can be used to combine data files instead of cutting them up.\nRead the manual page for paste, and then run paste to combine the autumn and winter data files in a single table using a comma as a separator. What’s wrong with the output from a data analysis point of view?\n\nThe last few rows have the wrong number of columns.\n\n\npaste -d , seasonal/autumn.csv  seasonal/winter.csv\n\nDate,   Tooth\n,Date,Tooth\n2017-01-05,canine\n,2017-01-03,bicuspid\n2017-01-17,wisdom\n,2017-01-05,incisor\n2017-01-18,canine\n,2017-01-21,wisdom\n2017-02-01,molar\n,2017-02-05,molar\n2017-02-22,bicuspid\n,2017-02-17,incisor\n2017-03-10,canine\n,2017-02-25,bicuspid\n2017-03-13,canine\n,2017-03-12,incisor\n2017-04-30,incisor\n,2017-03-25,molar\n2017-05-02,canine\n,2017-03-26,incisor\n2017-05-10,canine\n,2017-04-04,canine\n2017-05-19,bicuspid\n,2017-04-18,canine\n2017-05-25,molar\n,2017-04-26,canine\n2017-06-22,wisdom\n,2017-04-26,molar\n2017-06-25,canine\n,2017-04-26,wisdom\n2017-07-10,incisor\n,2017-04-27,canine\n2017-07-10,wisdom\n,2017-05-08,molar\n2017-07-20,incisor\n,2017-05-13,bicuspid\n2017-07-21,bicuspid\n,2017-05-14,wisdom\n2017-08-09,canine\n,2017-06-17,canine\n2017-08-16,canine\n,2017-07-01,incisor\n,2017-07-17,canine\n,2017-08-10,incisor\n,2017-08-11,bicuspid\n,2017-08-11,wisdom\n,2017-08-13,canine"
  },
  {
    "objectID": "datacamp/introduction_to_shell/intoroduction_shell.html#combining-tools",
    "href": "datacamp/introduction_to_shell/intoroduction_shell.html#combining-tools",
    "title": "Introduction to Shell",
    "section": "Combining tools",
    "text": "Combining tools\n\nHow can I store a command’s output in a file?\nAll of the tools you have seen so far let you name input files. Most don’t have an option for naming an output file because they don’t need one. Instead, you can use redirection to save any command’s output anywhere you want. If you run this command:\nhead -n 5 seasonal/summer.csv\nit prints the first 5 lines of the summer data on the screen. If you run this command instead:\nhead -n 5 seasonal/summer.csv > top.csv\n\nnothing appears on the screen. Instead, head’s output is put in a new file called top.csv. You can take a look at that file’s contents using cat:\ncat top.csv\nThe greater-than sign > tells the shell to redirect head’s output to a file. It isn’t part of the head command; instead, it works with every shell command that produces output.\n\n#Combine tail with redirection to save the last 5 lines of seasonal/winter.csv in a file called last.csv\ntail -n 5 seasonal/winter.csv > last.csv\n\n\n\nHow can I use a command’s output as an input?\nSuppose you want to get lines from the middle of a file. More specifically, suppose you want to get lines 3-5 from one of our data files. You can start by using head to get the first 5 lines and redirect that to a file, and then use tail to select the last 3:\nhead -n 5 seasonal/winter.csv > top.csv\ntail -n 3 top.csv\nA quick check confirms that this is lines 3-5 of our original file, because it is the last 3 lines of the first 5.\n\ntail -n 2 seasonal/winter.csv  > bottom.csv\nhead -n 1 bottom.csv\n\n2017-08-11,wisdom\n\n\n\n\nWhat’s a better way to combine commands?\nUsing redirection to combine commands has two drawbacks:\nIt leaves a lot of intermediate files lying around (like top.csv). The commands to produce your final result are scattered across several lines of history. The shell provides another tool that solves both of these problems at once called a pipe. Once again, start by running head:\nhead -n 5 seasonal/summer.csv\nInstead of sending head’s output to a file, add a vertical bar and the tail command without a filename:\nhead -n 5 seasonal/summer.csv | tail -n 3\nThe pipe symbol tells the shell to use the output of the command on the left as the input to the command on the right. - Use cut to select all of the tooth names from column 2 of the comma delimited file seasonal/summer.csv, then pipe the result to grep, with an inverted match, to exclude the header line containing the word “Tooth”. cut and grep were covered in detail in Chapter 2, exercises 8 and 11 respectively.\n\n# \ncut -f 2 -d , seasonal/summer.csv  | grep -v Tooth\n\ncanine\nwisdom\nbicuspid\nmolar\nwisdom\nwisdom\nbicuspid\nwisdom\ncanine\nmolar\nbicuspid\nwisdom\ncanine\ncanine\nincisor\nincisor\ncanine\nincisor\nincisor\nincisor\ncanine\ncanine\nbicuspid\ncanine\n\n\n\n\nHow can I combine many commands?\nYou can chain any number of commands together. For example, this command:\ncut -d , -f 1 seasonal/spring.csv | grep -v Date | head -n 10\nwill:\n\nselect the first column from the spring data;\nremove the header line containing the word “Date”; and\nselect the first 10 lines of actual data.\n\n\n\ncut -d , -f 2 seasonal/summer.csv | grep -v Tooth | head -n 1\n\ncanine\n\n\n\n\nHow can I count the records in a file?\nThe command wc (short for “word count”) prints the number of characters, words, and lines in a file. You can make it print only one of these using -c, -w, or -l respectively.\n\ngrep 2017-07 seasonal/spring.csv | wc -l\n\n3\n\n\n\n\nHow can I specify many files at once?\nMost shell commands will work on multiple files if you give them multiple filenames. For example, you can get the first column from all of the seasonal data files at once like this:\ncut -d , -f 1 seasonal/winter.csv seasonal/spring.csv seasonal/summer.csv seasonal/autumn.csv\nBut typing the names of many files over and over is a bad idea: it wastes time, and sooner or later you will either leave a file out or repeat a file’s name. To make your life better, the shell allows you to use wildcards to specify a list of files with a single expression. The most common wildcard is *, which means “match zero or more characters”. Using it, we can shorten the cut command above to this:\ncut -d , -f 1 seasonal/*\nor:\ncut -d , -f 1 seasonal/*.csv\n\n\nhead -n 3 seasonal/s*\n\n==> seasonal/spring.csv <==\nDate,Tooth\n2017-01-25,wisdom\n2017-02-19,canine\n\n==> seasonal/summer.csv <==\nDate,Tooth\n2017-01-11,canine\n2017-01-18,wisdom\n\n\n\n\nWhat other wildcards can I use?\nThe shell has other wildcards as well, though they are less commonly used:\n\n? matches a single character, so 201?.txt will match 2017.txt or 2018.txt, but not 2017-01.txt.\n[…] matches any one of the characters inside the square brackets, so 201[78].txt matches 2017.txt or 2018.txt, but not 2016.txt.\n{…} matches any of the comma-separated patterns inside the curly brackets, so {.txt, .csv} matches any file whose name ends with .txt or .csv, but not files whose names end with .pdf.\n\nWhich expression would match singh.pdf and johel.txt but not sandhu.pdf or sandhu.txt?\n\n{singh.pdf, j*.txt}\n\n\n\nHow can I sort lines of text?\nAs its name suggests, sort puts data in order. By default it does this in ascending alphabetical order, but the flags -n and -r can be used to sort numerically and reverse the order of its output, while -b tells it to ignore leading blanks and -f tells it to fold case (i.e., be case-insensitive). Pipelines often use grep to get rid of unwanted records and then sort to put the remaining records in order.\n\n cut -d , -f 2 seasonal/winter.csv | grep -v Tooth | sort -r\n \n\nwisdom\nwisdom\nwisdom\nwisdom\nmolar\nmolar\nmolar\nmolar\nincisor\nincisor\nincisor\nincisor\nincisor\nincisor\ncanine\ncanine\ncanine\ncanine\ncanine\ncanine\ncanine\nbicuspid\nbicuspid\nbicuspid\nbicuspid\n\n\n\n\nHow can I remove duplicate lines?\nAnother command that is often used with sort is uniq, whose job is to remove duplicated lines. More specifically, it removes adjacent duplicated lines. If a file contains:\n2017-07-03\n2017-07-03\n2017-08-03\n2017-08-03\nthen uniq will produce:\n2017-07-03\n2017-08-03\nbut if it contains:\n2017-07-03\n2017-08-03\n2017-07-03\n2017-08-03\nthen uniq will print all four lines. The reason is that uniq is built to work with very large files. In order to remove non-adjacent lines from a file, it would have to keep the whole file in memory (or at least, all the unique lines seen so far). By only removing adjacent duplicates, it only has to keep the most recent unique line in memory.\n\n\ncut -d , -f 2 seasonal/winter.csv | grep -v Tooth | sort | uniq -c\n\n      4 bicuspid\n      1 canine\n      6 canine\n      6 incisor\n      4 molar\n      4 wisdom\n\n\nHow can I save the output of a pipe? The shell lets us redirect the output of a sequence of piped commands:\ncut -d , -f 2 seasonal/*.csv | grep -v Tooth > teeth-only.txt\nHowever, > must appear at the end of the pipeline: if we try to use it in the middle, like this:\ncut -d , -f 2 seasonal/*.csv > teeth-only.txt | grep -v Tooth\nthen all of the output from cut is written to teeth-only.txt, so there is nothing left for grep and it waits forever for some input.\nWhat happens if we put redirection at the front of a pipeline as in:\n> result.txt head -n 3 seasonal/winter.csv\n\nThe command’s output is redirected to the file as usual.\n\n\n> result.txt head -n 3 seasonal/winter.csv\n\n\n\nHow can I stop a running program?\nThe commands and scripts that you have run so far have all executed quickly, but some tasks will take minutes, hours, or even days to complete. You may also mistakenly put redirection in the middle of a pipeline, causing it to hang up. If you decide that you don’t want a program to keep running, you can type Ctrl + C to end it. This is often written ^C in Unix documentation; note that the ‘c’ can be lower-case.\n\nuse control + c\n\n\n\nWrapping up\nTo wrap up, you will build a pipeline to find out how many records are in the shortest of the seasonal data files.\n\nwc -l seasonal/*.csv\nwc -l seasonal/*.csv | grep -v total\nwc -l seasonal/*.csv | grep -v total | sort -n | head -n 1\n\n  21 seasonal/autumn.csv\n  23 seasonal/spring.csv\n  24 seasonal/summer.csv\n  25 seasonal/winter.csv\n  93 total\n  21 seasonal/autumn.csv\n  23 seasonal/spring.csv\n  24 seasonal/summer.csv\n  25 seasonal/winter.csv\n  21 seasonal/autumn.csv"
  },
  {
    "objectID": "datacamp/introduction_to_shell/intoroduction_shell.html#batch-processing",
    "href": "datacamp/introduction_to_shell/intoroduction_shell.html#batch-processing",
    "title": "Introduction to Shell",
    "section": "Batch processing",
    "text": "Batch processing\n\nHow does the shell store information?\nLike other programs, the shell stores information in variables. Some of these, called environment variables, are available all the time. Environment variables’ names are conventionally written in upper case, and a few of the more commonly-used ones are shown below.\nVariable    Purpose Value\nHOME    User's home directory   /home/repl\nPWD Present working directory   Same as pwd command\nSHELL   Which shell program is being used   /bin/bash\nUSER    User's ID   repl\nTo get a complete list (which is quite long), you can type set in the shell.\nUse set and grep with a pipe to display the value of HISTFILESIZE, which determines how many old commands are stored in your command history. What is its value?\n\nset | grep HISTFILESIZE\n\nBASH_EXECUTION_STRING='set | grep HISTFILESIZE'\n\n\n\n\nHow can I print a variable’s value?\nA simpler way to find a variable’s value is to use a command called echo, which prints its arguments. Typing\necho hello DataCamp!\nprints\nhello DataCamp! If you try to use it to print a variable’s value like this:\necho USER\nit will print the variable’s name, USER.\nTo get the variable’s value, you must put a dollar sign $ in front of it. Typing\necho $USER\nprints\nrepl This is true everywhere: to get the value of a variable called X, you must write $X. (This is so that the shell can tell whether you mean “a file named X” or “the value of a variable named X”.)\n\nThe variable OSTYPE holds the name of the kind of operating system you are using. Display its value using echo.\n\n\necho $OSTYPE\n\nlinux-gnu\n\n\n\n\nHow else does the shell store information?\nThe other kind of variable is called a shell variable, which is like a local variable in a programming language.\nTo create a shell variable, you simply assign a value to a name:\ntraining=seasonal/summer.csv\nwithout any spaces before or after the = sign. Once you have done this, you can check the variable’s value with:\necho $training\nseasonal/summer.csv\n\ntesting=seasonal/winter.csv\nhead -n 1 $testing\n\nDate,Tooth\n\n\n\n\nHow can I repeat a command many times?\nShell variables are also used in loops, which repeat commands many times. If we run this command:\nfor filetype in gif jpg png; do echo $filetype; done it produces:\ngif\njpg\npng\nNotice these things about the loop:\n\nThe structure is for …variable… in …list… ; do …body… ; done\nThe list of things the loop is to process (in our case, the words gif, jpg, and png).\nThe variable that keeps track of which thing the loop is currently processing (in our case, filetype).\nThe body of the loop that does the processing (in our case, echo $filetype).\n\nNotice that the body uses $filetype to get the variable’s value instead of just filetype, just like it does with any other shell variable. Also notice where the semi-colons go: the first one comes between the list and the keyword do, and the second comes between the body and the keyword done.\nModify the loop so that it prints:\ndocx odt pdf\n\nfor filetype in docx odt pdf; do echo $filetype; done\n\ndocx\nodt\npdf\n\n\n\n\nHow can I repeat a command once for each file?\nYou can always type in the names of the files you want to process when writing the loop, but it’s usually better to use wildcards. Try running this loop in the console:\nfor filename in seasonal/*.csv; do echo $filename; done It prints:\nseasonal/autumn.csv\nseasonal/spring.csv\nseasonal/summer.csv\nseasonal/winter.csv\nbecause the shell expands seasonal/*.csv to be a list of four filenames before it runs the loop.\n\nModify the wildcard expression to people/* so that the loop prints the names of the files in the people directory regardless of what suffix they do or don’t have. Please use filename as the name of your loop variable.\n\n\nfor filename in people/*; do echo $filename; done\n\npeople/agarwal.txt\n\n\n\n\nHow can I record the names of a set of files?\nPeople often set a variable using a wildcard expression to record a list of filenames. For example, if you define datasets like this:\n\ndatasets=seasonal/*.csv\n\n\n\n#you can display the files' names later using:\n\n\nfor filename in $datasets; do echo $filename; done\n\nseasonal/autumn.csv\nseasonal/spring.csv\nseasonal/summer.csv\nseasonal/winter.csv\n\n\nThis saves typing and makes errors less likely.\nIf you run these two commands in your home directory, how many lines of output will they print?\n\nfiles=seasonal/*.csv\nfor f in $files; do echo $f; done\n\nseasonal/autumn.csv\nseasonal/spring.csv\nseasonal/summer.csv\nseasonal/winter.csv\n\n\n\nFour: the names of all four seasonal data files.\n\n\n\nA variable’s name versus its value\nA common mistake is to forget to use $ before the name of a variable. When you do this, the shell uses the name you have typed rather than the value of that variable.\nA more common mistake for experienced users is to mis-type the variable’s name. For example, if you define datasets like this:\ndatasets=seasonal/*.csv and then type:\necho $datsets the shell doesn’t print anything, because datsets (without the second “a”) isn’t defined.\nIf you were to run these two commands in your home directory, what output would be printed?\nfiles=seasonal/*.csv for f in files; do echo $f; done (Read the first part of the loop carefully before answering.)\n\nOne line: the word “files”. The variable f gets the value “files” for just one iteration of the loop.\n\n\n\nHow can I run many commands in a single loop?\nPrinting filenames is useful for debugging, but the real purpose of loops is to do things with multiple files. This loop prints the second line of each data file:\nfor file in seasonal/*.csv; do head -n 2 $file | tail -n 1; done It has the same structure as the other loops you have already seen: all that’s different is that its body is a pipeline of two commands instead of a single command.\nWrite a loop that prints the last entry from July 2017 (2017-07) in every seasonal file. It should produce a similar output to:\ngrep 2017-07 seasonal/winter.csv | tail -n 1 but for each seasonal file separately. Please use file as the name of the loop variable, and remember to loop through the list of files seasonal/*.csv (instead of ‘seasonal/winter.csv’ as in the example).\n\nfor file in seasonal/*.csv; do grep 2017-07 $file | tail -n 1; done\n\n2017-07-21,bicuspid\n2017-07-23,bicuspid\n2017-07-25,canine\n2017-07-17,canine\n\n\n\n\nWhy shouldn’t I use spaces in filenames?\nIt’s easy and sensible to give files multi-word names like July 2017.csv when you are using a graphical file explorer. However, this causes problems when you are working in the shell. For example, suppose you wanted to rename July 2017.csv to be 2017 July data.csv. You cannot type:\nmv July 2017.csv 2017 July data.csv\nbecause it looks to the shell as though you are trying to move four files called July, 2017.csv, 2017, and July (again) into a directory called data.csv. Instead, you have to quote the files’ names so that the shell treats each one as a single parameter:\nmv 'July 2017.csv' '2017 July data.csv'\nIf you have two files called current.csv and last year.csv (with a space in its name) and you type:\nrm current.csv last year.csv\nwhat will happen:\n\nwill remove current.csv but not last year.csv throws an error\nYou can use single quotes, ’, or double quotes, “, around the file names.\n\n\n\nHow can I do many things in a single loop?\nThe loops you have seen so far all have a single command or pipeline in their body, but a loop can contain any number of commands. To tell the shell where one ends and the next begins, you must separate them with semi-colons:\nfor f in seasonal/*.csv; do echo $f; head -n 2 $f | tail -n 1; done\n\nseasonal/autumn.csv 2017-01-05,canine seasonal/spring.csv 2017-01-25,wisdom seasonal/summer.csv 2017-01-11,canine seasonal/winter.csv 2017-01-03,bicuspid\nSuppose you forget the semi-colon between the echo and head commands in the previous loop, so that you ask the shell to run:\nfor f in seasonal/*.csv; do echo $f head -n 2 $f | tail -n 1; done\n\nWhat will the shell do?\nPrint one line for each of the four files.\nshoul be\n\n\nfor f in seasonal/*.csv; do echo $f head -n 2 $f | tail -n 1 $f; done\n\n2017-08-16,canine\n2017-09-07,molar2017-08-04,canine2017-08-13,canine"
  },
  {
    "objectID": "datacamp/introduction_to_shell/intoroduction_shell.html#creating-new-tools",
    "href": "datacamp/introduction_to_shell/intoroduction_shell.html#creating-new-tools",
    "title": "Introduction to Shell",
    "section": "Creating new tools",
    "text": "Creating new tools\n\nHow can I edit a file?\nUnix has a bewildering variety of text editors. For this course, we will use a simple one called Nano. If you type nano filename, it will open filename for editing (or create it if it doesn’t already exist). You can move around with the arrow keys, delete characters using backspace, and do other operations with control-key combinations:\n\nCtrl + K: delete a line.\nCtrl + U: un-delete a line.\nCtrl + O: save the file (‘O’ stands for ‘output’). You will also need to press Enter to confirm the filename!\nCtrl + X: exit the editor.\n\n\n#nano names.txt\n\n\n\nHow can I record what I just did?\nWhen you are doing a complex analysis, you will often want to keep a record of the commands you used. You can do this with the tools you have already seen:\n\nRun history.\nPipe its output to tail -n 10 (or however many recent steps you want to save).\nRedirect that to a file called something like figure-5.history. This is better than writing things down in a lab notebook because it is guaranteed not to miss any steps. It also illustrates the central idea of the shell: simple tools that produce and consume lines of text can be combined in a wide variety of ways to solve a broad range of problems\n\n\n#Copy the files seasonal/spring.csv and seasonal/summer.csv to your home directory.\ncp seasonal/spring.csv seasonal/summer.csv .\n# Use grep with the -h flag (to stop it from printing filenames) and -v Tooth (to select lines that don't match the header line) to select the data records from spring.csv and summer.csv in that order and redirect the output to temp.csv.\ngrep -h -v Tooth spring.csv summer.csv > temp.csv\n# Pipe history into tail -n 3 and redirect the output to steps.txt to save the last three commands in a file. (You need to save three instead of just two because the history command itself will be in the list.)\nhistory | tail -n 3 > steps.txt\n\n\n\nHow can I save commands to re-run later?\nYou have been using the shell interactively so far. But since the commands you type in are just text, you can store them in files for the shell to run over and over again. To start exploring this powerful capability, put the following command in a file called headers.sh:\nhead -n 1 seasonal/*.csv\nThis command selects the first row from each of the CSV files in the seasonal directory. Once you have created this file, you can run it by typing:\nbash headers.sh\n\nThis tells the shell (which is just a program called bash) to run the commands contained in the file headers.sh, which produces the same output as running the commands directly.\n\n#nano dates.sh\nbash dates.sh\n\nDate\n2017-01-05\n2017-01-17\n2017-01-18\n2017-02-01\n2017-02-22\n2017-03-10\n2017-03-13\n2017-04-30\n2017-05-02\n2017-05-10\n2017-05-19\n2017-05-25\n2017-06-22\n2017-06-25\n2017-07-10\n2017-07-10\n2017-07-20\n2017-07-21\n2017-08-09\n2017-08-16\nDate\n2017-01-25\n2017-02-19\n2017-02-24\n2017-02-28\n2017-03-04\n2017-03-12\n2017-03-14\n2017-03-21\n2017-04-29\n2017-05-08\n2017-05-20\n2017-05-21\n2017-05-25\n2017-06-04\n2017-06-13\n2017-06-14\n2017-07-10\n2017-07-16\n2017-07-23\n2017-08-13\n2017-08-13\n2017-08-13\n2017-09-07\nDate\n2017-01-11\n2017-01-18\n2017-01-21\n2017-02-02\n2017-02-27\n2017-02-27\n2017-03-07\n2017-03-15\n2017-03-20\n2017-03-23\n2017-04-02\n2017-04-22\n2017-05-07\n2017-05-09\n2017-05-11\n2017-05-14\n2017-05-19\n2017-05-23\n2017-05-24\n2017-06-18\n2017-07-25\n2017-08-02\n2017-08-03\n2017-08-04\nDate\n2017-01-03\n2017-01-05\n2017-01-21\n2017-02-05\n2017-02-17\n2017-02-25\n2017-03-12\n2017-03-25\n2017-03-26\n2017-04-04\n2017-04-18\n2017-04-26\n2017-04-26\n2017-04-26\n2017-04-27\n2017-05-08\n2017-05-13\n2017-05-14\n2017-06-17\n2017-07-01\n2017-07-17\n2017-08-10\n2017-08-11\n2017-08-11\n2017-08-13\n\n\n\n\nHow can I re-use pipes?\nA file full of shell commands is called a *shell script, or sometimes just a “script” for short. Scripts don’t have to have names ending in .sh, but this lesson will use that convention to help you keep track of which files are scripts.\nScripts can also contain pipes. For example, if all-dates.sh contains this line:\ncut -d , -f 1 seasonal/*.csv | grep -v Date | sort | uniq\nthen:\nbash all-dates.sh > dates.out\n\nwill extract the unique dates from the seasonal data files and save them in dates.out.\n\n#cut -d , -f 2 seasonal/*.csv | grep -v Tooth | sort | uniq -c\nbash teeth.sh > teeth.out\ncat teeth.out\n\n     15 bicuspid\n      2 canine\n     29 canine\n     18 incisor\n      1 molar\n     10 molar\n     17 wisdom\n\n\n\n\nHow can I pass filenames to scripts?\nA script that processes specific files is useful as a record of what you did, but one that allows you to process any files you want is more useful. To support this, you can use the special expression $@ (dollar sign immediately followed by at-sign) to mean “all of the command-line parameters given to the script”.\nFor example, if unique-lines.sh contains sort $@ | uniq, when you run:\nbash unique-lines.sh seasonal/summer.csv\nthe shell replaces $@ with seasonal/summer.csv and processes one file. If you run this:\nbash unique-lines.sh seasonal/summer.csv seasonal/autumn.csv\n\nit processes two data files, and so on.\nAs a reminder, to save what you have written in Nano, type Ctrl + O to write the file out, then Enter to confirm the filename, then Ctrl + X to exit the editor.\n\n#nano count-records.sh\n#tail -q -n +2 $@ | wc -l\nbash count-records.sh seasonal/*.csv > num-records.out\ncat num-records.out\n\n89\n\n\n\n\nHow can I process a single argument?\nAs well as $@, the shell lets you use $1, $2, and so on to refer to specific command-line parameters. You can use this to write commands that feel simpler or more natural than the shell’s. For example, you can create a script called column.sh that selects a single column from a CSV file when the user provides the filename as the first parameter and the column as the second:\ncut -d , -f $2 $1\n\nand then run it using:\nbash column.sh seasonal/autumn.csv 1\nNotice how the script uses the two parameters in reverse order.\nThe script get-field.sh is supposed to take a filename, the number of the row to select, the number of the column to select, and print just that field from a CSV file. For example:\nbash get-field.sh seasonal/summer.csv 4 2\nshould select the second field from line 4 of seasonal/summer.csv. Which of the following commands should be put in get-field.sh to do that?\n\n#head -n $2 $1 | tail -n 1 | cut -d , -f $3\nbash get-field.sh seasonal/summer.csv 4 2\n\nbicuspid\n\n\n\n\nHow can one shell script do many things?\nOur shells scripts so far have had a single command or pipe, but a script can contain many lines of commands. For example, you can create one that tells you how many records are in the shortest and longest of your data files, i.e., the range of your datasets’ lengths.\nNote that in Nano, “copy and paste” is achieved by navigating to the line you want to copy, pressing CTRL + K to cut the line, then CTRL + U twice to paste two copies of it.\nAs a reminder, to save what you have written in Nano, type Ctrl + O to write the file out, then Enter to confirm the filename, then Ctrl + X to exit the editor.\n\n#nano range.sh\n#wc -l $@ | grep -v total | sort -n | head -n 1\n#sort -n -r\ncat range.sh\n#Run the script on the files in the seasonal directory using seasonal/*.csv to match all of the files and redirect the output using > to a file called range.out in your home directory.\nbash range.sh seasonal/*.csv > range.out\n\nwc -l $@ | grep -v total | sort -n | head -n 1\nwc -l $@ | grep -v total | sort -n -r | head -n 1\n\n\n\n\nHow can I write loops in a shell script?\nShell scripts can also contain loops. You can write them using semi-colons, or split them across lines without semi-colons to make them more readable:\n#Print the first and last data records of each file.\n\n\nfor filename in $@\ndo\n    head -n 2 $filename | tail -n 1\n    tail -n 1 $filename\ndone\n(You don’t have to indent the commands inside the loop, but doing so makes things clearer.)\nThe first line of this script is a comment to tell readers what the script does. Comments start with the # character and run to the end of the line. Your future self will thank you for adding brief explanations like the one shown here to every script you write.\nAs a reminder, to save what you have written in Nano, type Ctrl + O to write the file out, then Enter to confirm the filename, then Ctrl + X to exit the editor.\n\n# Fill in the placeholders in the script date-range.sh with $filename (twice), head, and tail so that it prints the first and last date from one or more files.\n# for filename in $@\n# do\n#     cut -d , -f 1 $filename | grep -v Date | sort | head -n 1\n#     cut -d , -f 1 $filename | grep -v Date | sort | tail -n 1\n# done\n\nbash date-range.sh  seasonal/*.csv\n\n2017-01-05\n2017-08-16\n2017-01-25\n2017-09-07\n2017-01-11\n2017-08-04\n2017-01-03\n2017-08-13\n\n\n\n\nWhat happens when I don’t provide filenames?\nA common mistake in shell scripts (and interactive commands) is to put filenames in the wrong place. If you type:\ntail -n 3\nthen since tail hasn’t been given any filenames, it waits to read input from your keyboard. This means that if you type:\nhead -n 5 | tail -n 3 somefile.txt\nthen tail goes ahead and prints the last three lines of somefile.txt, but head waits forever for keyboard input, since it wasn’t given a filename and there isn’t anything ahead of it in the pipeline.\nSuppose you do accidentally type:\nhead -n 5 | tail -n 3 somefile.txt\n\nWhat should you do next?\n\nPress Ctrl + C to stop the running command."
  },
  {
    "objectID": "datacamp/working_with_the_OpenAI_API/working_with_the_OpenAI_API.html",
    "href": "datacamp/working_with_the_OpenAI_API/working_with_the_OpenAI_API.html",
    "title": "Introduction to the OpenAI API",
    "section": "",
    "text": "Software applications, web browser experiences, and even whole products are being built on top of the OpenAI API. In this exercise, you’ll be able to explore an application built on top of the OpenAI API: DataCamp’s own version of ChatGPT!\nThe text you type into the interface will be sent as a request to the OpenAI API and the response will be delivered and unpacked directly back to you.\nUsing the ChatGPT interface, answer the following question: In what year was OpenAI founded?\n\n2015\n\n\n\n\nThroughout the course, you’ll write Python code to interact with the OpenAI API. As a first step, you’ll need to create your own API key. API keys used in this course’s exercises will not be stored in any way.\nTo create a key, you’ll first need to create an OpenAI account by visiting their signup page. Next, navigate to the API keys page to create your secret key.\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\nTrue\n\n# Access the variables as you normally would with os.getenv()\n\napi_key = os.getenv('OPEN_API_KEY2')\nThe button to create a new secret key.\nOpenAI sometimes provides free credits for the API, but this can differ depending on geography. You may also need to add debit/credit card details. You’ll need less than $1 credit to complete this course.\nWarning: if you send many requests or use lots of tokens in a short period, you may see an openai.error.RateLimitError. If you see this error, please wait a minute for your quota to reset and you should be able to begin sending more requests. Please see OpenAI’s rate limit error support article for more information.\n# Import the OpenAI client\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\n\n# Create a request to the Completions endpoint\nresponse = client.completions.create(\n  # Specify the correct model\n  model=\"gpt-3.5-turbo-instruct\",\n  prompt=\"Who developed ChatGPT?\"\n)\n\nprint(response)\nCompletion(id=‘cmpl-8lAB6YHgnJYEyn8iTMZdPLLvUUJEZ’, choices=[CompletionChoice(finish_reason=‘stop’, index=0, logprobs=None, text=‘was developed by the company OpenAI.’)], created=1706251232, model=‘gpt-3.5-turbo-instruct’, object=‘text_completion’, system_fingerprint=None, usage=CompletionUsage(completion_tokens=12, prompt_tokens=6, total_tokens=18))\n\n\n\nOne of the key skills required to work with APIs is manipulating the response to extract the desired information. In this exercise, you’ll push your Python dictionary and list manipulation skills to the max to extract information from the API response.\nYou’ve been provided with response, which is a response from the OpenAI API when provided with the prompt, What is the goal of OpenAI?\nThis response object has been printed for you so you can see and understand its structure. If you’re struggling to picture the structure, view the dictionary form of the response with .model_dump().\n#Extract the model used from response using attributes.\nprint(response.model)\ngpt-3.5-turbo-instruct\n#Extract the total tokens used from response using attributes.\nprint(response.usage.total_tokens)\n18\n#Extract the text answer to the prompt from response.\nprint(response.choices[0].text)\nChatGPT was developed by the company OpenAI.\n\n\n\nAn Online Scientific Journal called Terra Scientia wants to use AI to make their scientific papers more accessible to a wider audience. To do this, they want to develop a feature where users can double-click on words they don’t understand, and an AI model will explain what it means in the context of the article.\nTo accomplish this, the developers at Terra Scientia want to build the feature on top of the OpenAI API.\nWhich OpenAI API endpoint(s) could they use to build this feature?\n\nCompletions\nChat\n\n\n\n\nYou’ve learned that you can set up organizations to manage API usage and billing. Users can be part of multiple organizations and attribute API requests to a specific organization. It’s best practice to structure organizations such that each business unit or product feature has a separate organization, depending on the number of features the business has built on the OpenAI API.\nWhat are the benefits of having separate organizations for each business unit or product feature?\n\nReducing risk of hitting rate limits\nImproved management of usage and billing\nRemoves single failure point"
  },
  {
    "objectID": "datacamp/working_with_the_OpenAI_API/working_with_the_OpenAI_API.html#openais-text-and-chat-capabilities",
    "href": "datacamp/working_with_the_OpenAI_API/working_with_the_OpenAI_API.html#openais-text-and-chat-capabilities",
    "title": "Introduction to the OpenAI API",
    "section": "OpenAI’s Text and Chat Capabilities",
    "text": "OpenAI’s Text and Chat Capabilities\n\nFind and replace\nText completion models can be used for much more than answering questions. In this exercise, you’ll explore the model’s ability to transform a text prompt.\nFind-and-replace tools have been around for decades, but they are often limited to identifying and replacing exact words or phrases. You’ve been provided with a block of text discussing cars, and you’ll use a completion model to update the text to discuss planes instead, updating the text appropriately.\nWarning: if you send many requests or use lots of tokens in a short period, you may hit your rate limit and see an openai.error.RateLimitError. If you see this error, please wait a minute for your quota to reset and you should be able to begin sending more requests. Please see OpenAI’s rate limit error support article for more informatio\nprompt=\"\"\"Replace car with plane and adjust phrase:\nA car is a vehicle that is typically powered by an internal combustion engine or an electric motor. It has four wheels, and is designed to carry passengers and/or cargo on roads or highways. Cars have become a ubiquitous part of modern society, and are used for a wide variety of purposes, such as commuting, travel, and transportation of goods. Cars are often associated with freedom, independence, and mobility.\"\"\"\n\n# Create a request to the Completions endpoint\nresponse = client.completions.create(\n  model=\"gpt-3.5-turbo-instruct\",\n  prompt=prompt,\n  max_tokens = 100\n\n)\n\n# Extract and print the response text\nprint(response.choices[0].text)\nA plane is an aircraft that is typically powered by jet engines or propellers. It has wings, and is designed to carry passengers and/or cargo through the air. Planes have become a vital part of modern society, and are used for a wide range of purposes, such as air travel, transportation of goods, and military operations. Planes are often associated with speed, efficiency, and global connectivity.\n\n\nText summarization\nOne really common use case for using OpenAI’s models is summarizing text. This has a ton of applications in business settings, including summarizing reports into concise one-pagers or a handful of bullet points, or extracting the next steps and timelines for different stakeholders.\nIn this exercise, you’ll summarize a passage of text on financial investment into two concise bullet points using a text completion model.\nprompt=\"\"\"Summarize the following text into two concise bullet points:\nInvestment refers to the act of committing money or capital to an enterprise with the expectation of obtaining an added income or profit in return. There are a variety of investment options available, including stocks, bonds, mutual funds, real estate, precious metals, and currencies. Making an investment decision requires careful analysis, assessment of risk, and evaluation of potential rewards. Good investments have the ability to produce high returns over the long term while minimizing risk. Diversification of investment portfolios reduces risk exposure. Investment can be a valuable tool for building wealth, generating income, and achieving financial security. It is important to be diligent and informed when investing to avoid losses.\"\"\"\n\n# Create a request to the Completions endpoint\nresponse = client.completions.create(\n  model=\"gpt-3.5-turbo-instruct\",\n  prompt=prompt,\n  max_tokens = 400,\n  temperature = 0\n)\n\nprint(response.choices[0].text)\n\nInvestment involves committing money or capital to an enterprise in order to obtain added income or profit.\nCareful analysis, risk assessment, and diversification are important for making successful investments that can build wealth and achieve financial security.\n\n\n\nContent generation\nAI is playing a much greater role in content generation, from creating marketing content such as blog post titles to creating outreach email templates for sales teams.\nIn this exercise, you’ll harness AI through the Completions endpoint to generate a catchy slogan for a new restaurant. Feel free to test out different prompts, such as varying the type of cuisine (e.g., Italian, Chinese) or the type of restaurant (e.g., fine-dining, fast-food), to see how the response changes.\n# Create a request to the Completions endpoint\nprompt = \"\"\"create a catchy slogan for a new restaurant:\n  The restaurant deals mainly in italian cuisine\"\"\"\n  \nresponse = client.completions.create(\n    model=\"gpt-3.5-turbo-instruct\",\n    prompt=prompt,\n    max_tokens = 100\n    )\n    \nprint(response.choices[0].text)\n“Indulge in Italian flair at our restaurant’s savory affair!”\n\n\nClassifying text sentiment\nAs well as answering questions, transforming text, and generating new text, Completions models can also be used for classification tasks, such as categorization and sentiment classification. This sort of task requires not only knowledge of the words but also a deeper understanding of their meaning.\nIn this exercise, you’ll explore using Completions models for sentiment classification using reviews from an online shoe store called Toe-Tally Comfortable:\n\nUnbelievably good!\nShoes fell apart on the second use.\nThe shoes look nice, but they aren’t very comfortable.\nCan’t wait to show them off!\n\n# Create a request to the Completions endpoint\nprompt = \"\"\"classify the sentiment of the following statements as either negative, positive, or neutral list in bullet points:\nUnbelievably good!\nShoes fell apart on the second use.\nThe shoes look nice, but they aren't very comfortable.\nCan't wait to show them off! \"\"\"\nresponse = client.completions.create(\n  model=\"gpt-3.5-turbo-instruct\",\n  prompt=prompt,\n  max_tokens=100\n)\n\nprint(response.choices[0].text)\n-Positive -Negative -Neutral -Positive\n\n\nCategorizing companies\nIn this exercise, you’ll use a Completions model to categorize different companies. At first, you won’t specify the categories to see how the model categorizes them. Then, you’ll specify the categories in the prompt to ensure they are categorized in a desirable and predictable way.\n# Create a request to the Completions endpoint\nprompt = \"\"\"Categorize the following companies  into, Tech, Energy, Luxury Goods, or Investment list in bullet points: \n  Apple, \n  Microsoft,\n  Saudi Aramco,\n  Alphabet,\n  Amazon, \n  Berkshire Hathaway, \n  NVIDIA,\n  Meta,\n  Tesla, \n  LVMH \"\"\"\n  \n  \nresponse = client.completions.create(\n    model=\"gpt-3.5-turbo-instruct\",\n    prompt=prompt,\n    max_tokens=100,\n    temperature=0.5\n)\n    \nprint(response.choices[0].text)\n\nTech: Apple, Microsoft, Alphabet, Amazon, NVIDIA, Meta\nEnergy: Saudi Aramco\nLuxury Goods: LVMH\nInvestment: Berkshire Hathaway\n\n\n\nThe Chat Completions endpoint\nThe models available via the Chat Completions endpoint can not only perform similar single-turn tasks as models from the Completions endpoint, but can also be used to have multi-turn conversations.\nTo enable multi-turn conversations, the endpoint supports three different roles:\nSystem: controls assistant’s behavior User: instruct the assistant Assistant: response to user instruction In this exercise, you’ll make your first request to the Chat Completions endpoint to answer the following question:\nWhat is the difference between a for loop and a while loop?\n# Create a request to the Chat Completions endpoint\nresponse = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  max_tokens=150,\n  messages=[\n    {\"role\": \"system\",\n     \"content\": \"You are a helpful data science tutor. Provide code examples for both while and for loops using this syntax ie code between ` code ` \"},\n    {\"role\": \"user\",\n    \"content\": \"What is the difference between a for loop and a while loop?\"}\n  ]\n)\n\n# Extract and print the assistant's text response\nprint(response.choices[0].message.content)\nA for loop and a while loop are both control structures used to execute a specific piece of code repeatedly. However, they differ in their syntax and the conditions under which they execute.\nA for loop is used when you know the number of times you want to iterate through a block of code. It consists of three parts: initialization, condition, and iteration.\nOn the other hand, a while loop is used when you want to repeat a block of code until a certain condition is met. It only requires a condition to evaluate. The loop will continue executing as long as the condition is True.\nHere are examples of both for and while loops:\nFor loop example:\nfor i in range(5):\n    print(i)\n\n\nCode explanation\nOne of the most popular use cases for using OpenAI models is for explaining complex content, such as technical jargon and code. This is a task that data practitioners, software engineers, and many others must tackle in their day-to-day as they review and utilize code written by others.\nIn this exercise, you’ll use the OpenAI API to explain a block of Python code to understand what it is doing.\ninstruction = \"\"\"Explain what this Python code does in one sentence:\nimport numpy as np\n\nheights_dict = {\"Mark\": 1.76, \"Steve\": 1.88, \"Adnan\": 1.73}\nheights = heights_dict.values()\nprint(np.mean(heights))\n\"\"\"\n\n# Create a request to the Chat Completions endpoint\nresponse = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  max_tokens=100,\n  messages=[\n    {\"role\": \"system\",\n     \"content\": \"You are a helpful data science tutor.\"},\n    {\"role\": \"user\",\n    \"content\": instruction}\n  ]\n)\n\nprint(response.choices[0].message.content)\nThis code calculates and prints the mean height from the values in the heights_dict dictionary using the numpy library.\n\n\nIn-context learning\nFor more complex use cases, the models lack the understanding or context of the problem to provide a suitable response from a prompt. In these cases, you need to provide examples to the model for it to learn from, so-called in-context learning.\nIn this exercise, you’ll improve on a Python programming tutor built on the OpenAI API by providing an example that the model can learn from.\nHere is an example of a user and assistant message you can use, but feel free to try out your own:\n\nUser → Explain what the min() function does.\nAssistant → The min() function returns the smallest item from an iterable.\n\nresponse = client.chat.completions.create(\n   model=\"gpt-3.5-turbo\",\n   # Add a user and assistant message for in-context learning\n   messages=[\n     {\"role\": \"system\", \"content\": \"You are a helpful Python programming tutor.\"},\n     {\"role\": \"user\", \"content\": \"Explain what sum function in python does\"},\n     {\"role\": \"assistant\", \n     \"content\": \"\"\"the sum() function is a built-in function used to calculate the sum of all the elements in an iterable, like a list, tuple, or  \n     set. The basic syntax of the sum() function is as follows:\n     `sum(iterable, start)`\n     example:\n     `numbers = [1, 2, 3, 4, 5]\n      result = sum(numbers)  # This will add 1 + 2 + 3 + 4 + 5\n      print(result)  # Output will be 15`\n     \"\"\"},\n     {\"role\": \"user\", \"content\": \"Explain what the type() function does.\"}\n   ]\n)\n\nprint(response.choices[0].message.content)\nThe type() function is a built-in function in Python that is used to determine the type of an object. It takes an object as its parameter and returns the type of that object.\nThe basic syntax of the type() function is as follows: type(object)\nHere, “object” can be any value or variable in Python. For example, it can be a string, integer, list, dictionary, function, class, etc.\nExamples:\nnumber = 10\nprint(type(number))  # Output: <class 'int'>\n\nname = \"John\"\nprint(type(name))  # Output: <class 'str'>\n\nmy_list = [1, 2, 3]\nprint(type(my_list))  # Output: <class 'list'>\n\nmy_dictionary = {\"apple\": 1, \"banana\": 2}\nprint(type(my_dictionary))  # Output: <class 'dict'>\nThe type() function is often used to perform type checking or type validation in Python code.\n\n\nCreating an AI chatbot\nAn online learning platform called Easy as Pi that specializes in teaching math skills has contracted you to help develop an AI tutor. You immediately see that you can build this feature on top of the OpenAI API, and start to design a simple proof-of-concept (POC) for the major stakeholders at the company. This POC will demonstrate the core functionality required to build the final feature and the power of the OpenAI’s GPT models.\nExample system and user messages have been provided for you, but feel free to play around with these to change the model’s behavior or design a completely different chatbot!\nmessages = [{\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"}]\nuser_msgs = [\"Explain what pi is.\", \"Summarize this in two bullet points.\"]\n\nfor q in user_msgs:\n    print(\"User: \", q)\n    \n    # Create a dictionary for the user message from q and append to messages\n    user_dict = {\"role\": \"user\", \"content\": q}\n    messages.append(user_dict)\n    \n    # Create the API request\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages = messages,\n        max_tokens=100)\n\n    \n    # Convert the assistant's message to a dict and append to messages\n    assistant_dict = {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n    messages.append(assistant_dict)\n\n    print(\"Assistant: \", response.choices[0].message.content, \"\\n\")\nUser: Explain what pi is. Assistant: Pi (π) is a mathematical constant that represents the ratio of the circumference of a circle to its diameter. It is an irrational number, which means it cannot be expressed as a finite decimal or fraction. The value of pi is approximately 3.14159, but it goes on infinitely without repeating.\nPi is a fundamental constant in mathematics and is used in a wide range of mathematical calculations, especially those involving circles, spheres, and trigonometry. It is commonly represented by the Greek letter ”\nUser: Summarize this in two bullet points. Assistant: - Pi (π) is a mathematical constant representing the ratio of a circle’s circumference to its diameter. - It is an irrational number approximately equal to 3.14159, with infinite decimal places."
  },
  {
    "objectID": "datacamp/working_with_the_OpenAI_API/working_with_the_OpenAI_API.html#going-beyond-text-completions",
    "href": "datacamp/working_with_the_OpenAI_API/working_with_the_OpenAI_API.html#going-beyond-text-completions",
    "title": "Introduction to the OpenAI API",
    "section": "Going Beyond Text Completions",
    "text": "Going Beyond Text Completions\n\nWhy use text moderation models?\nText moderation is a vital component of most social media platforms, internet chatrooms, and many other user-facing systems. It serves the purpose of preventing the distribution and promotion of inappropriate content, such as hate speech.\nIn this exercise, you’ll compare OpenAI’s text moderation model to traditional methods of moderation: manual moderation and keyword pattern matching.\nOpenAI’s Moderation model\n\nDesigned to moderate the prompts and responses to and from OpenAI models\nOutputs confidence of text violation\nA model that uses all of the words to inform its decision\nEvaluates content based on specific violation categories\n\nKeyword Pattern Matching\n\nDon’t understand context\n\nManual Moderation\n\nExpensive\nRequires 24/7 support\nInconsistent classification\n\n\n\nRequesting moderation\nAside from text and chat completion models, OpenAI provides models with other capabilities, including text moderation. OpenAI’s text moderation model is designed for evaluating prompts and responses to determine if they violate OpenAI’s usage policies, including inciting hate speech and promoting violence.\nIn this exercise, you’ll test out OpenAI’s moderation functionality on a sentence that may have been flagged as containing violent content using traditional word detection algorithms.\n#Ceate a request to the Moderation endpoint\n\nresponse = client.moderations.create(  \n  model=\"text-moderation-latest\",\n  input= \"My favorite book is How to Kill a Mockingbird.\")\n\n# Print the category scores\nprint(response.results[0].category_scores)\nCategoryScores(harassment=1.396209336235188e-05, harassment_threatening=8.344479283550754e-06, hate=4.862324567511678e-05, hate_threatening=1.5291941224404582e-07, self_harm=1.391318733112712e-06, self_harm_instructions=5.645471219395404e-07, self_harm_intent=4.05220532684325e-07, sexual=6.411371941794641e-06, sexual_minors=1.5648997759853955e-06, violence=0.0019001936307176948, violence_graphic=3.9556569390697405e-05, self-harm=1.391318733112712e-06, sexual/minors=1.5648997759853955e-06, hate/threatening=1.5291941224404582e-07, violence/graphic=3.9556569390697405e-05, self-harm/intent=4.05220532684325e-07, self-harm/instructions=5.645471219395404e-07, harassment/threatening=8.344479283550754e-06)\n\n\nExamining moderation category scores\nThe same request you created in the last exercise to the Moderation endpoint has been run again, sending the sentence “My favorite book is How to Kill a Mockingbird.” to the model. The response from the API has been printed for you, and is available as response.\nWhat is the correct interpretation of the category_scores here?\n\nThe model believes that there are no violations, as all categories are close to 0\n\n\n\nCreating a podcast transcript\nThe OpenAI API Audio endpoint provides access to the Whisper model, which can be used for speech-to-text transcription and translation. In this exercise, you’ll create a transcript from a DataFramed podcast episode with OpenAI Developer, Logan Kilpatrick.\nIf you’d like to hear more from Logan, check out the full ChatGPT and the OpenAI Developer Ecosystem podcast episode.\n# Open the openai-audio.mp3 file\naudio_file = open(\"openai-audio.mp3\", \"rb\")\n\n# Create a transcript from the audio file\nresponse = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n\n# Extract and print the transcript text\nprint(response.text)\nHi there, Logan, thank you for joining us on the show today. Thanks for having me. I’m super excited about this. Brilliant. We’re going to dive right in, and I think ChatGPT is maybe the most famous AI product that you have at OpenAI, but I’d just like to get an overview of what all the other AIs that are available are. So I think two and a half years ago, OpenAI released the API that we still have available today, which is essentially our giving people access to these models. And for a lot of people, giving people access to the model that powers ChatGPT, which is our consumer-facing first-party application, which essentially just, in very simple terms, puts a nice UI on top of what was already available through our API for the last two and a half years. So it’s sort of democratizing the access to this technology through our API. If you want to just play around with it, as an end user, we have ChatGPT available to the world as well.\n\n\nTranscribing a non-English language\nThe Whisper model can not only transcribe English language, but also performs well on speech in many other languages.\nIn this exercise, you’ll create a transcript from audio.m4a, which contains speech in Portuguese.\n# Open the audio.m4a file\naudio_file= open(\"audio.m4a\", \"rb\")\n\n# Create a transcript from the audio file\nresponse = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n\nprint(response.text)\nOlá, o meu nome é Eduardo, sou CTO no Datacamp. Espero que esteja a gostar deste curso que o James e eu criamos para você. Esta API permite enviar um áudio e trazer para inglês. O áudio original está em português.\n\n\nTranslating Portuguese\nWhisper can not only transcribe audio into its native language but also supports translation capabilities for creating English transcriptions.\nIn this exercise, you’ll return to the Portuguese audio, but this time, you’ll translate it into English!\n# Create a translation from the audio file\nresponse = client.audio.translations.create(model=\"whisper-1\", file=audio_file)\n\n# Extract and print the translated text\nprint(response.text)\nHello, my name is Eduardo, I am a CTO at Datacamp. I hope you are enjoying this course that James and I have created for you. This API allows you to send an audio and bring it to English. The original audio is in Portuguese.\n\n\nTranslating with prompts\nThe quality of Whisper’s translation can vary depending on the language spoken, the audio quality, and the model’s awareness of the subject matter. If you have any extra context about what is being spoken about, you can send it along with the audio to the model to give it a helping hand.\nYou’ve been provided with with an audio file, audio.wav; you’re not sure what language is spoken in it, but you do know it relates to a recent World Bank report. Because you don’t know how well the model will perform on this unknown language, you opt to send the model this extra context to steer it in the right direction.\n# Open the audio.wav file\naudio_file = open(\"audio.wav\", \"rb\")\n\n# Write an appropriate prompt to help the model\nprompt = \"The audio relates to a recent world bank report \"\n\n# Create a translation from the audio file\nresponse =  client.audio.translations.create(model=\"whisper-1\", file=audio_file, prompt = prompt)\n\nprint(response.text)\nThe World Bank said in its latest economic outlook report that the global economy is in a dangerous state. As interest rates rise, consumer spending and corporate investment will slow down, economic activities will be impacted, and the vulnerability of low-income countries will be exposed. Global economic growth will be significantly slowed down, and the stability of the financial system will be threatened.\n\n\nIdentifying audio language\nYou’ve learned that you’re not only limited to creating a single request, and that you can actually feed the output of one model as an input to another! This is called chaining, and it opens to the doors to more complex, multi-modal use cases.\nIn this exercise, you’ll practice model chaining to identify the language used in an audio file. You’ll do this by bringing together OpenAI’s audio transcription functionality and its text models with only a few lines of code.\naudio_file = open(\"arne-german-automotive-forecast.wav\", \"rb\")\n\n# Create a transcription request using audio_file\naudio_response = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n\n# Create a request to the API to identify the language spoken\nchat_response = client.chat.completions.create(  model=\"gpt-3.5-turbo\",  \n  messages=[\n    {\"role\": \"user\", \"content\": \"Identify the language in \" + audio_response.text }\n    ])\n\nprint(chat_response.choices[0].message.content)\nThe language used is German.\n\n\nCreating meeting summaries\nTime for business! One time-consuming task that many find themselves doing day-to-day is taking meeting notes to summarize attendees, discussion points, next steps, etc.\nIn this exercise, you’ll use AI to augment this task to not only save a substantial amount of time, but also to empower attendees to focus on the discussion rather than administrative tasks. You’ve been provided with a recording from DataCamp’s Q2 Roadmap webinar, which summarizes what DataCamp will be releasing during that quarter. You’ll chain the Whisper model with a text or chat model to discover which courses will be launched in Q2.\naudio_file = open(\"datacamp-q2-roadmap-short.mp3\", \"rb\")\n\n# Create a transcription request using audio_file\naudio_response =  client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n\n# Create a request to the API to summarize the transcript into bullet points\nchat_response = client.chat.completions.create(  model=\"gpt-3.5-turbo\",  \n  messages=[\n    {\"role\": \"user\", \"content\": \"Summarise text given, list in bullet points with a bit of explanation for each bullet point:\"  + audio_response.text }\n    ])\n\nprint(chat_response.choices[0].message.content)\n\nTechnical courses include working with the OpenAI API and Python programming against GPT and Whisper.\nUnderstanding Artificial Intelligence is aimed at a less technical audience and provides a broad background on the topic.\nArtificial Intelligence Ethics focuses on the potential risks and harm of improper AI implementation and is important for businesses and organizations.\nData literacy courses are also available, with a specific course on forming analytical questions to bridge the communication gap between technical and non-technical individuals.\nCommunication is identified as a key aspect of better data science and is highlighted as an important skill to develop."
  }
]