[
  {
    "objectID": "myblog/loan_prediction/clustering_loans.html#reading-data",
    "href": "myblog/loan_prediction/clustering_loans.html#reading-data",
    "title": "clustering",
    "section": "Reading data",
    "text": "Reading data"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html",
    "href": "myblog/datacamp/regression_r/regression.html",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "",
    "text": "We use the lm() function to fit linear models to data. In this case, we want to understand how the price of MarioKart games sold at auction varies as a function of not only the number of wheels included in the package, but also whether the item is new or used. Obviously, it is expected that you might have to pay a premium to buy these new. But how much is that premium? Can we estimate its value after controlling for the number of wheels?\nWe will fit a parallel slopes model using lm(). In addition to the data argument, lm() needs to know which variables you want to include in your regression model, and how you want to include them. It accomplishes this using a formula argument. A simple linear regression formula looks like y ~ x, where y is the name of the response variable, and x is the name of the explanatory variable. Here, we will simply extend this formula to include multiple explanatory variables. A parallel slopes model has the form y ~ x + z, where z is a categorical explanatory variable, and x is a numerical explanatory variable.\nThe output from lm() is a model object, which when printed, will show the fitted coefficients.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(openintro)\nlibrary(broom)\nlibrary(pander)\ndata( mariokart, package = \"openintro\")\nmario_kart <- mariokart\n\nmario_kart <- mario_kart %>% mutate(total_pr := ifelse(total_pr > 100, NA, total_pr))\n# Explore the data\nglimpse(mario_kart)\n\nRows: 143\nColumns: 12\n$ id          <dbl> 150377422259, 260483376854, 320432342985, 280405224677, 17…\n$ duration    <int> 3, 7, 3, 3, 1, 3, 1, 1, 3, 7, 1, 1, 1, 1, 7, 7, 3, 3, 1, 7…\n$ n_bids      <int> 20, 13, 16, 18, 20, 19, 13, 15, 29, 8, 15, 15, 13, 16, 6, …\n$ cond        <fct> new, used, new, new, new, new, used, new, used, used, new,…\n$ start_pr    <dbl> 0.99, 0.99, 0.99, 0.99, 0.01, 0.99, 0.01, 1.00, 0.99, 19.9…\n$ ship_pr     <dbl> 4.00, 3.99, 3.50, 0.00, 0.00, 4.00, 0.00, 2.99, 4.00, 4.00…\n$ total_pr    <dbl> 51.55, 37.04, 45.50, 44.00, 71.00, 45.00, 37.02, 53.99, 47…\n$ ship_sp     <fct> standard, firstClass, firstClass, standard, media, standar…\n$ seller_rate <int> 1580, 365, 998, 7, 820, 270144, 7284, 4858, 27, 201, 4858,…\n$ stock_photo <fct> yes, yes, no, yes, yes, yes, yes, yes, yes, no, yes, yes, …\n$ wheels      <int> 1, 1, 1, 1, 2, 0, 0, 2, 1, 1, 2, 2, 2, 2, 1, 0, 1, 1, 2, 2…\n$ title       <fct> \"~~ Wii MARIO KART &amp; WHEEL ~ NINTENDO Wii ~ BRAND NEW …\n\n# fit parallel slopes\n\nmod_mario <- lm(total_pr ~ wheels + cond, data = mario_kart)"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#reasoning-about-two-intercepts",
    "href": "myblog/datacamp/regression_r/regression.html#reasoning-about-two-intercepts",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Reasoning about two intercepts",
    "text": "Reasoning about two intercepts\nThe mario_kart data contains several other variables. The totalPr, startPr, and shipPr variables are numeric, while the cond and stockPhoto variables are categorical.\nWhich formula will result in a parallel slopes model?\n\ntotalPr ~ shipPr + stockPhoto"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#using-geom_line-and-augment",
    "href": "myblog/datacamp/regression_r/regression.html#using-geom_line-and-augment",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Using geom_line() and augment()",
    "text": "Using geom_line() and augment()\nParallel slopes models are so-named because we can visualize these models in the data space as not one line, but two parallel lines. To do this, we’ll draw two things:\na scatterplot showing the data, with color separating the points into groups a line for each value of the categorical variable Our plotting strategy is to compute the fitted values, plot these, and connect the points to form a line. The augment() function from the broom package provides an easy way to add the fitted values to our data frame, and the geom_line() function can then use that data frame to plot the points and connect them.\nNote that this approach has the added benefit of automatically coloring the lines appropriately to match the data.\nYou already know how to use ggplot() and geom_point() to make the scatterplot. The only twist is that now you’ll pass your augment()-ed model as the data argument in your ggplot() call. When you add your geom_line(), instead of letting the y aesthetic inherit its values from the ggplot() call, you can set it to the .fitted column of the augment()-ed model. This has the advantage of automatically coloring the lines for you.\n\n# Augment the model\naugmented_mod <- augment(mod_mario)\nglimpse(augmented_mod)\n\nRows: 141\nColumns: 10\n$ .rownames  <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"1…\n$ total_pr   <dbl> 51.55, 37.04, 45.50, 44.00, 71.00, 45.00, 37.02, 53.99, 47.…\n$ wheels     <int> 1, 1, 1, 1, 2, 0, 0, 2, 1, 1, 2, 2, 2, 2, 1, 0, 1, 1, 2, 0,…\n$ cond       <fct> new, used, new, new, new, new, used, new, used, used, new, …\n$ .fitted    <dbl> 49.60260, 44.01777, 49.60260, 49.60260, 56.83544, 42.36976,…\n$ .resid     <dbl> 1.9473995, -6.9777674, -4.1026005, -5.6026005, 14.1645592, …\n$ .hat       <dbl> 0.02103158, 0.01250410, 0.02103158, 0.02103158, 0.01915635,…\n$ .sigma     <dbl> 4.902339, 4.868399, 4.892414, 4.881308, 4.750591, 4.899816,…\n$ .cooksd    <dbl> 1.161354e-03, 8.712334e-03, 5.154337e-03, 9.612441e-03, 5.5…\n$ .std.resid <dbl> 0.40270893, -1.43671086, -0.84838977, -1.15857953, 2.926332…\n\n# scatterplot, with color\ndata_space <- ggplot(augmented_mod, aes(x = wheels, y = total_pr , color = cond )) + \n  geom_point()\n  \n# single call to geom_line()\ndata_space + \n  geom_line(aes(y = .fitted))"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#intercept-interpretation",
    "href": "myblog/datacamp/regression_r/regression.html#intercept-interpretation",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Intercept interpretation",
    "text": "Intercept interpretation\nRecall that the cond variable is either new or used. Here are the fitted coefficients from your model:\nCall: lm(formula = totalPr ~ wheels + cond, data = mario_kart)\nCoefficients: (Intercept) wheels condused\n42.370 7.233 -5.585\nChoose the correct interpretation of the coefficient on condused:\n\nThe expected price of a used MarioKart is $5.58 less than that of a new one with the same number of wheels.\nFor each additional wheel, the expected price of a MarioKart increases by $7.23 regardless of whether it is new or used.\n\nSyntax from math The babies data set contains observations about the birthweight and other characteristics of children born in the San Francisco Bay area from 1960–1967.\nWe would like to build a model for birthweight as a function of the mother’s age and whether this child was her first (parity == 0). Use the mathematical specification below to code the model in R.\n\\[birthweight = \\beta_0 + \\beta_1 * age  + \\beta_2 * parity + \\epsilon\\]\n\ndata( babies, package = \"openintro\")\n\nmod <- lm(bwt~ age+parity, data = babies)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n118.3\n2.788\n42.43\n3.957e-243\n\n\nage\n0.06315\n0.09577\n0.6594\n0.5097\n\n\nparity\n-1.652\n1.271\n-1.3\n0.1937"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#syntax-from-plot",
    "href": "myblog/datacamp/regression_r/regression.html#syntax-from-plot",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Syntax from plot",
    "text": "Syntax from plot\nThis time, we’d like to build a model for birthweight as a function of the length of gestation and the mother’s smoking status. Use the plot to inform your model specification.\n\nggplot(babies, aes(gestation, bwt, color = factor(smoke)))+\n    geom_point()\n\n\n\nmod <- lm(bwt~ gestation + smoke, data = babies)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.9317\n8.152\n-0.1143\n0.909\n\n\ngestation\n0.4429\n0.02902\n15.26\n3.156e-48\n\n\nsmoke\n-8.088\n0.9527\n-8.49\n5.963e-17\n\n\n\n\n\nR-squared vs. adjusted R-squared Two common measures of how well a model fits to data are \\[R^2\\] (the coefficient of determination) and the adjusted \\[R^2\\] . The former measures the percentage of the variability in the response variable that is explained by the model. To compute this, we define\n\\[R^2 = 1 - \\frac{sse}{sst} \\] where SSE and SST are the sum of the squared residuals, and the total sum of the squares, respectively. One issue with this measure is that the can only decrease as new variable are added to the model, while the SST depends only on the response variable and therefore is not affected by changes to the model. This means that you can increase \\[R^2\\] by adding any additional variable to your model—even random noise.\nThe adjusted \\[R^2\\] includes a term that penalizes a model for each additional explanatory variable (where is the number of explanatory variables). We can see both measures in the output of the summary() function on our model object.\n\n# R^2 and adjusted R^2\nsummary(mod_mario)\n\n\nCall:\nlm(formula = total_pr ~ wheels + cond, data = mario_kart)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.0078  -3.0754  -0.8254   2.9822  14.1646 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  42.3698     1.0651  39.780  < 2e-16 ***\nwheels        7.2328     0.5419  13.347  < 2e-16 ***\ncondused     -5.5848     0.9245  -6.041 1.35e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.887 on 138 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7165,    Adjusted R-squared:  0.7124 \nF-statistic: 174.4 on 2 and 138 DF,  p-value: < 2.2e-16\n\n# add random noise\nmario_kart_noisy <- mario_kart %>% \nmutate(noise = rnorm(n = nrow(mario_kart)))\n  \n# compute new model\nmod2_mario2 <- lm(total_pr ~ wheels + cond+noise, data = mario_kart_noisy)\n\n# new R^2 and adjusted R^2\nsummary(mod2_mario2)\n\n\nCall:\nlm(formula = total_pr ~ wheels + cond + noise, data = mario_kart_noisy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.1221  -3.0761  -0.8246   2.8795  14.3721 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  42.4152     1.0787  39.321  < 2e-16 ***\nwheels        7.2197     0.5454  13.239  < 2e-16 ***\ncondused     -5.6279     0.9380  -6.000 1.67e-08 ***\nnoise         0.1404     0.4546   0.309    0.758    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.904 on 137 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7167,    Adjusted R-squared:  0.7105 \nF-statistic: 115.5 on 3 and 137 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#prediction",
    "href": "myblog/datacamp/regression_r/regression.html#prediction",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Prediction",
    "text": "Prediction\nOnce we have fit a regression model, we can use it to make predictions for unseen observations or retrieve the fitted values. Here, we explore two methods for doing the latter.\nA traditional way to return the fitted values (i.e. the y ’s) is to run the predict() function on the model object. This will return a vector of the fitted values. Note that predict() will take an optional newdata argument that will allow you to make predictions for observations that are not in the original data.\nA newer alternative is the augment() function from the broom package, which returns a data.frame with the response varible (), the relevant explanatory variables (the ’s), the fitted value ( ) and some information about the residuals (). augment() will also take a newdata argument that allows you to make predictions.\n\n# return a vector\nlibrary(knitr)\n\npredict(mod_mario)\n\n       1        2        3        4        5        6        7        8 \n49.60260 44.01777 49.60260 49.60260 56.83544 42.36976 36.78493 56.83544 \n       9       10       11       12       13       14       15       16 \n44.01777 44.01777 56.83544 56.83544 56.83544 56.83544 44.01777 36.78493 \n      17       18       19       21       22       23       24       25 \n49.60260 49.60260 56.83544 36.78493 56.83544 56.83544 56.83544 44.01777 \n      26       27       28       29       30       31       32       33 \n56.83544 36.78493 36.78493 36.78493 49.60260 36.78493 36.78493 44.01777 \n      34       35       36       37       38       39       40       41 \n51.25061 44.01777 44.01777 36.78493 44.01777 56.83544 56.83544 49.60260 \n      42       43       44       45       46       47       48       49 \n44.01777 51.25061 56.83544 56.83544 44.01777 56.83544 36.78493 36.78493 \n      50       51       52       53       54       55       56       57 \n44.01777 56.83544 36.78493 44.01777 42.36976 36.78493 36.78493 44.01777 \n      58       59       60       61       62       63       64       66 \n44.01777 36.78493 36.78493 56.83544 36.78493 56.83544 36.78493 51.25061 \n      67       68       69       70       71       72       73       74 \n56.83544 44.01777 58.48345 51.25061 49.60260 44.01777 49.60260 56.83544 \n      75       76       77       78       79       80       81       82 \n56.83544 51.25061 44.01777 36.78493 36.78493 36.78493 44.01777 56.83544 \n      83       84       85       86       87       88       89       90 \n44.01777 65.71629 44.01777 56.83544 36.78493 49.60260 49.60260 36.78493 \n      91       92       93       94       95       96       97       98 \n44.01777 36.78493 51.25061 44.01777 36.78493 51.25061 42.36976 56.83544 \n      99      100      101      102      103      104      105      106 \n51.25061 44.01777 51.25061 56.83544 56.83544 56.83544 36.78493 49.60260 \n     107      108      109      110      111      112      113      114 \n51.25061 44.01777 56.83544 49.60260 36.78493 44.01777 51.25061 56.83544 \n     115      116      117      118      119      120      121      122 \n64.06828 44.01777 49.60260 44.01777 49.60260 51.25061 42.36976 44.01777 \n     123      124      125      126      127      128      129      130 \n56.83544 44.01777 49.60260 44.01777 51.25061 56.83544 56.83544 49.60260 \n     131      132      133      134      135      136      137      138 \n56.83544 36.78493 44.01777 44.01777 36.78493 56.83544 36.78493 44.01777 \n     139      140      141      142      143 \n36.78493 51.25061 49.60260 36.78493 56.83544 \n\n# return a data frame\n\naugment(mod_mario)%>% head() %>% kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.rownames\ntotal_pr\nwheels\ncond\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n1\n51.55\n1\nnew\n49.60260\n1.947399\n0.0210316\n4.902340\n0.0011614\n0.4027089\n\n\n2\n37.04\n1\nused\n44.01777\n-6.977767\n0.0125041\n4.868399\n0.0087123\n-1.4367109\n\n\n3\n45.50\n1\nnew\n49.60260\n-4.102601\n0.0210316\n4.892414\n0.0051543\n-0.8483898\n\n\n4\n44.00\n1\nnew\n49.60260\n-5.602601\n0.0210316\n4.881308\n0.0096124\n-1.1585795\n\n\n5\n71.00\n2\nnew\n56.83544\n14.164559\n0.0191563\n4.750591\n0.0557493\n2.9263328\n\n\n6\n45.00\n0\nnew\n42.36976\n2.630240\n0.0474932\n4.899816\n0.0050537\n0.5514192"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#thought-experiments",
    "href": "myblog/datacamp/regression_r/regression.html#thought-experiments",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Thought experiments",
    "text": "Thought experiments\nSuppose that after going apple picking you have 12 apples left over. You decide to conduct an experiment to investigate how quickly they will rot under certain conditions. You place six apples in a cool spot in your basement, and leave the other six on the window sill in the kitchen. Every week, you estimate the percentage of the surface area of the apple that is rotten or moldy.\nConsider the following models:\n\\[rot = \\beta_0 + \\beta_1 *t + \\beta_2 * temp + \\epsilon \\]\nand\n\\[rot = \\beta_0 + \\beta_1 *t + \\beta_2 * temp + \\beta_2 * temp *t +  \\epsilon \\]\n\nThe rate at which apples rot will vary based on the temperature."
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#fitting-a-model-with-interaction",
    "href": "myblog/datacamp/regression_r/regression.html#fitting-a-model-with-interaction",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Fitting a model with interaction",
    "text": "Fitting a model with interaction\nIncluding an interaction term in a model is easy—we just have to tell lm() that we want to include that new variable. An expression of the form\nlm(y ~ x + z + x:z, data = mydata)\nwill do the trick. The use of the colon (:) here means that the interaction between and will be a third term in the model.\n\n# include interaction\n\nmod <- lm(total_pr ~cond + duration + cond:duration, data = mario_kart)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n58.27\n1.366\n42.64\n5.832e-81\n\n\ncondused\n-17.12\n2.178\n-7.86\n1.014e-12\n\n\nduration\n-1.966\n0.4488\n-4.38\n2.342e-05\n\n\ncondused:duration\n2.325\n0.5484\n4.239\n4.102e-05"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#visualizing-interaction-models",
    "href": "myblog/datacamp/regression_r/regression.html#visualizing-interaction-models",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Visualizing interaction models",
    "text": "Visualizing interaction models\nInteraction allows the slope of the regression line in each group to vary. In this case, this means that the relationship between the final price and the length of the auction is moderated by the condition of each item.\nInteraction models are easy to visualize in the data space with ggplot2 because they have the same coefficients as if the models were fit independently to each group defined by the level of the categorical variable. In this case, new and used MarioKarts each get their own regression line. To see this, we can set an aesthetic (e.g. color) to the categorical variable, and then add a geom_smooth() layer to overlay the regression line for each color.\n\n# interaction plot\nggplot(mario_kart, aes(duration, total_pr, color = cond)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = 0)"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#consequences-of-simpsons-paradox",
    "href": "myblog/datacamp/regression_r/regression.html#consequences-of-simpsons-paradox",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Consequences of Simpson’s paradox",
    "text": "Consequences of Simpson’s paradox\nIn the simple linear regression model for average SAT score, (total) as a function of average teacher salary (salary), the fitted coefficient was -5.02 points per thousand dollars. This suggests that for every additional thousand dollars of salary for teachers in a particular state, the expected SAT score for a student from that state is about 5 points lower.\nIn the model that includes the percentage of students taking the SAT, the coefficient on salary becomes 1.84 points per thousand dollars. Choose the correct interpretation of this slope coefficient.\n\nFor every additional thousand dollars of salary for teachers in a particular state, the expected SAT score for a student from that state is about 2 points higher, after controlling for the percentage of students taking the SAT."
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#simpsons-paradox-in-action",
    "href": "myblog/datacamp/regression_r/regression.html#simpsons-paradox-in-action",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Simpson’s paradox in action",
    "text": "Simpson’s paradox in action\nA mild version of Simpson’s paradox can be observed in the MarioKart auction data. Consider the relationship between the final auction price and the length of the auction. It seems reasonable to assume that longer auctions would result in higher prices, since—other things being equal—a longer auction gives more bidders more time to see the auction and bid on the item.\nHowever, a simple linear regression model reveals the opposite: longer auctions are associated with lower final prices. The problem is that all other things are not equal. In this case, the new MarioKarts—which people pay a premium for—were mostly sold in one-day auctions, while a plurality of the used MarioKarts were sold in the standard seven-day auctions.\nOur simple linear regression model is misleading, in that it suggests a negative relationship between final auction price and duration. However, for the used MarioKarts, the relationship is positive.\n\nslr <- ggplot(mario_kart, aes(y = total_pr, x = duration)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n# model with one slope\nmod <- lm(total_pr ~ duration, data = mario_kart)\n\n# plot with two slopes\nslr + aes(color = cond)"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#fitting-a-mlr-model",
    "href": "myblog/datacamp/regression_r/regression.html#fitting-a-mlr-model",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Fitting a MLR model",
    "text": "Fitting a MLR model\nIn terms of the R code, fitting a multiple linear regression model is easy: simply add variables to the model formula you specify in the lm() command.\nIn a parallel slopes model, we had two explanatory variables: one was numeric and one was categorical. Here, we will allow both explanatory variables to be numeric.\n\n# Fit the model using duration and startPr\n\nmod <- lm(total_pr~ start_pr + duration, data = mario_kart)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n51.03\n1.179\n43.28\n3.666e-82\n\n\nstart_pr\n0.233\n0.04364\n5.339\n3.756e-07\n\n\nduration\n-1.508\n0.2555\n-5.902\n2.645e-08"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#tiling-the-plane",
    "href": "myblog/datacamp/regression_r/regression.html#tiling-the-plane",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Tiling the plane",
    "text": "Tiling the plane\nOne method for visualizing a multiple linear regression model is to create a heatmap of the fitted values in the plane defined by the two explanatory variables. This heatmap will illustrate how the model output changes over different combinations of the explanatory variables.\nThis is a multistep process:\nFirst, create a grid of the possible pairs of values of the explanatory variables. The grid should be over the actual range of the data present in each variable. We’ve done this for you and stored the result as a data frame called grid. Use augment() with the newdata argument to find the ’s corresponding to the values in grid. Add these to the data_space plot by using the fill aesthetic and geom_tile().\n\n# add predictions to grid\nprice_hats <- augment(mod, newdata = grid)\n\n# tile the plane\ndata_space + \n  geom_tile(data = price_hats, aes(fill = .fitted), alpha = 0.5)"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#models-in-3d",
    "href": "myblog/datacamp/regression_r/regression.html#models-in-3d",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Models in 3D",
    "text": "Models in 3D\nAn alternative way to visualize a multiple regression model with two numeric explanatory variables is as a plane in three dimensions. This is possible in R using the plotly package.\nWe have created three objects that you will need:\nx: a vector of unique values of duration y: a vector of unique values of startPr plane: a matrix of the fitted values across all combinations of x and y Much like ggplot(), the plot_ly() function will allow you to create a plot object with variables mapped to x, y, and z aesthetics. The add_markers() function is similar to geom_point() in that it allows you to add points to your 3D plot.\nNote that plot_ly uses the pipe (%>%) operator to chain commands together.\n\n# draw the 3D scatterplot\np <- plot_ly(data = mario_kart, z = ~totalPr, x = ~duration, y = ~startPr, opacity = 0.6) %>%\n  add_markers() \n  \n# draw the plane\np %>%\n  add_surface(x = ~x, y = ~y, z = ~plane, showscale = FALSE)"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html",
    "href": "myblog/datacamp/tidymodels/tidymodels.html",
    "title": "Modeling with tidymodels in R",
    "section": "",
    "text": "The rsample package is designed to create training and test datasets. Creating a test dataset is important for estimating how a trained model will likely perform on new data. It also guards against overfitting, where a model memorizes patterns that exist only in the training data and performs poorly on new data.\nIn this exercise, you will create training and test datasets from the home_sales data. This data contains information on homes sold in the Seattle, Washington area between 2015 and 2016.\nThe outcome variable in this data is selling_price.\nThe tidymodels package will be pre-loaded in every exercise in the course. The home_sales tibble has also been loaded for you.\n\n\n\nTidy model packages\n\n\n\nhome_sales <- readRDS(\"home_sales.rds\")\n\n# Create a data split object\nhome_split <- initial_split(home_sales, \n                            prop = 0.7, \n                            strata = selling_price)\n\n# Create the training data\nhome_training <- home_split %>%\n  training()\n\n# Create the test data\nhome_test <- home_split %>% \n  testing()\n\n# Check number of rows in each dataset\nnrow(home_training)\n\n[1] 1042\n\nnrow(home_test)\n\n[1] 450\n\n\nDistribution of outcome variable values Stratifying by the outcome variable when generating training and test datasets ensures that the outcome variable values have a similar range in both datasets.\nSince the original data is split at random, stratification avoids placing all the expensive homes in home_sales into the test dataset, for example. In this case, your model would most likely perform poorly because it was trained on less expensive homes.\nIn this exercise, you will calculate summary statistics for the selling_price variable in the training and test datasets. The home_training and home_test tibbles have been loaded from the previous exercise.\n\n# Distribution of selling_price in training data\nlibrary(knitr)\nsummary_func <- function(df){\n      df %>% summarize(min_sell_price = min(selling_price),\n            max_sell_price = max(selling_price),\n            mean_sell_price = mean(selling_price),\n            sd_sell_price = sd(selling_price)) %>%\n    kable()\n\n}\nhome_training %>% \n    summary_func()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n479438.2\n80925.92\n\n\n\n\nhome_test %>% \n  summary_func()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n478265.5\n81185.76"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#fitting-a-linear-regression-model",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#fitting-a-linear-regression-model",
    "title": "Modeling with tidymodels in R",
    "section": "Fitting a linear regression model",
    "text": "Fitting a linear regression model\nThe parsnip package provides a unified syntax for the model fitting process in R.\nWith parsnip, it is easy to define models using the various packages, or engines, that exist in the R ecosystem.\nIn this exercise, you will define a parsnip linear regression object and train your model to predict selling_price using home_age and sqft_living as predictor variables from the home_sales data.\nThe home_training and home_test tibbles that you created in the previous lesson have been loaded into this session.\n\n# Specify a linear regression model, linear_model\nlinear_model <- linear_reg() %>% \n  # Set the model engine\n  set_engine('lm') %>% \n  # Set the model mode\n  set_mode('regression')\n\n# Train the model with the training data\nlm_fit <- linear_model %>% \n  fit(selling_price~ home_age + sqft_living,\n      data = home_training)\n\n# Print lm_fit to view model information\ntidy(lm_fit) %>%\n    kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n285035.0613\n7691.199132\n37.059899\n0\n\n\nhome_age\n-1231.5792\n178.152470\n-6.913063\n0\n\n\nsqft_living\n103.9248\n2.771416\n37.498823\n0"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#predicting-home-selling-prices",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#predicting-home-selling-prices",
    "title": "Modeling with tidymodels in R",
    "section": "Predicting home selling prices",
    "text": "Predicting home selling prices\nAfter fitting a model using the training data, the next step is to use it to make predictions on the test dataset. The test dataset acts as a new source of data for the model and will allow you to evaluate how well it performs.\nBefore you can evaluate model performance, you must add your predictions to the test dataset.\nIn this exercise, you will use your trained model, lm_fit, to predict selling_price in the home_test dataset.\nYour trained model, lm_fit, as well as the test dataset, home_test have been loaded into your session.\n\n# Predict selling_price\nhome_predictions <- predict(lm_fit,\n                        new_data = home_test)\n\n# View predicted selling prices\n#home_predictions\n\n# Combine test data with predictions\nhome_test_results <- home_test %>% \n  select(selling_price, home_age, sqft_living) %>% \n  bind_cols(home_predictions)\n\n# View results\nhome_test_results %>% \n    head()\n\n# A tibble: 6 × 4\n  selling_price home_age sqft_living   .pred\n          <dbl>    <dbl>       <dbl>   <dbl>\n1        635000        4        3350 628257.\n2        380000       24        2130 476837.\n3        495000       21        1650 430648.\n4        355000       19        1430 410248.\n5        464950       19        2190 489230.\n6        475000        0        2300 524062."
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#model-performance-metrics",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#model-performance-metrics",
    "title": "Modeling with tidymodels in R",
    "section": "Model performance metrics",
    "text": "Model performance metrics\nEvaluating model results is an important step in the modeling process. Model evaluation should be done on the test dataset in order to see how well a model will generalize to new datasets.\nIn the previous exercise, you trained a linear regression model to predict selling_price using home_age and sqft_living as predictor variables. You then created the home_test_results tibble using your trained model on the home_test data.\nIn this exercise, you will calculate the RMSE and R squared metrics using your results in home_test_results.\nThe home_test_results tibble has been loaded into your session.\n\n# Print home_test_results\n#home_test_results\n\n# Caculate the RMSE metric\nhome_test_results %>% \n  rmse(truth = selling_price, estimate =.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      46214.\n\n# Calculate the R squared metric\nhome_test_results %>% \n  rsq(truth = selling_price, estimate =.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.677"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#r-squared-plot",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#r-squared-plot",
    "title": "Modeling with tidymodels in R",
    "section": "R squared plot",
    "text": "R squared plot\nIn the previous exercise, you got an R squared value of 0.651. The R squared metric ranges from 0 to 1, 0 being the worst and 1 the best.\nCalculating the R squared value is only the first step in studying your model’s predictions.\nMaking an R squared plot is extremely important because it will uncover potential problems with your model, such as non-linear patterns or regions where your model is either over or under-predicting the outcome variable.\nIn this exercise, you will create an R squared plot of your model’s performance.\nThe home_test_results tibble has been loaded into your session.\n\n# Create an R squared plot of model performance\nggplot(home_test_results, aes(x = selling_price, y = .pred)) +\n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred()  +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#complete-model-fitting-process-with-last_fit",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#complete-model-fitting-process-with-last_fit",
    "title": "Modeling with tidymodels in R",
    "section": "Complete model fitting process with last_fit()",
    "text": "Complete model fitting process with last_fit()\nIn this exercise, you will train and evaluate the performance of a linear regression model that predicts selling_price using all the predictors available in the home_sales tibble.\nThis exercise will give you a chance to perform the entire model fitting process with tidymodels, from defining your model object to evaluating its performance on the test data.\nEarlier in the chapter, you created an rsample object called home_split by passing the home_sales tibble into initial_split(). The home_split object contains the instructions for randomly splitting home_sales into training and test sets.\nThe home_sales tibble, and home_split object have been loaded into this session.\n\n# Define a linear regression model\nlinear_model <- linear_reg() %>% \n  set_engine('lm') %>% \n  set_mode('regression')\n\n# Train linear_model with last_fit()\nlinear_fit <- linear_model %>% \n  last_fit(selling_price ~ ., split = home_split)\n\n# Collect predictions and view results\npredictions_df <- linear_fit %>% collect_predictions()\npredictions_df %>% head()\n\n# A tibble: 6 × 5\n  id                 .pred  .row selling_price .config             \n  <chr>              <dbl> <int>         <dbl> <chr>               \n1 train/test split 696151.     4        635000 Preprocessor1_Model1\n2 train/test split 408483.     5        380000 Preprocessor1_Model1\n3 train/test split 445489.     6        495000 Preprocessor1_Model1\n4 train/test split 399760.     7        355000 Preprocessor1_Model1\n5 train/test split 474256.     8        464950 Preprocessor1_Model1\n6 train/test split 456075.    16        475000 Preprocessor1_Model1\n\n# Make an R squared plot using predictions_df\nggplot(predictions_df, aes(x = selling_price, y = .pred)) + \n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred() +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#data-resampling",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#data-resampling",
    "title": "Modeling with tidymodels in R",
    "section": "Data resampling",
    "text": "Data resampling\nThe first step in a machine learning project is to create training and test datasets for model fitting and evaluation. The test dataset provides an estimate of how your model will perform on new data and helps to guard against overfitting.\nYou will be working with the telecom_df dataset which contains information on customers of a telecommunications company. The outcome variable is canceled_service and it records whether a customer canceled their contract with the company. The predictor variables contain information about customers’ cell phone and internet usage as well as their contract type and monthly charges.\nThe telecom_df tibble has been loaded into your session.\n\ntelecom_df <- readRDS(\"telecom_df.rds\")\n# Create data split object\ntelecom_split <- initial_split(telecom_df, prop = 0.75,\n                     strata = canceled_service)\n\n# Create the training data\ntelecom_training <- telecom_split %>% \n  training()\n\n# Create the test data\ntelecom_test <- telecom_split %>% \n  testing()\n\n# Check the number of rows\nnrow(telecom_training)\n\n[1] 731\n\nnrow(telecom_test)\n\n[1] 244"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#fitting-a-logistic-regression-model",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#fitting-a-logistic-regression-model",
    "title": "Modeling with tidymodels in R",
    "section": "Fitting a logistic regression model",
    "text": "Fitting a logistic regression model\nIn addition to regression models, the parsnip package also provides a general interface to classification models in R.\nIn this exercise, you will define a parsnip logistic regression object and train your model to predict canceled_service using avg_call_mins, avg_intl_mins, and monthly_charges as predictor variables from the telecom_df data.\nThe telecom_training and telecom_test tibbles that you created in the previous lesson have been loaded into this session.\n\n# Specify a logistic regression model\nlogistic_model <- logistic_reg() %>% \n  # Set the engine\n  set_engine('glm') %>% \n  # Set the mode\n  set_mode('classification')\n\n# Fit to training data\nlogistic_fit <- logistic_model %>% \n  fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n      data = telecom_training)\n\n# Print model fit object\nlogistic_fit %>% tidy()\n\n# A tibble: 4 × 5\n  term            estimate std.error statistic  p.value\n  <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)      2.46      0.590       4.17  3.02e- 5\n2 avg_call_mins   -0.0107    0.00129    -8.29  1.10e-16\n3 avg_intl_mins    0.0209    0.00307     6.82  9.07e-12\n4 monthly_charges -0.00144   0.00484    -0.298 7.65e- 1"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#combining-test-dataset-results",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#combining-test-dataset-results",
    "title": "Modeling with tidymodels in R",
    "section": "Combining test dataset results",
    "text": "Combining test dataset results\nEvaluating your model’s performance on the test dataset gives insights into how well your model predicts on new data sources. These insights will help you communicate your model’s value in solving problems or improving decision making.\nBefore you can calculate classification metrics such as sensitivity or specificity, you must create a results tibble with the required columns for yardstick metric functions.\nIn this exercise, you will use your trained model to predict the outcome variable in the telecom_test dataset and combine it with the true outcome values in the canceled_service column.\nYour trained model, logistic_fit, and test dataset, telecom_test, have been loaded from the previous exercise.\n\n# Predict outcome categories\nclass_preds <- predict(logistic_fit, new_data = telecom_test,\n                       type = 'class')\n\n# Obtain estimated probabilities for each outcome value\nprob_preds <- predict(logistic_fit, new_data = telecom_test, \n                      type = 'prob')\n\n# Combine test set results\ntelecom_results <- telecom_test %>% \n  select(canceled_service) %>% \n  bind_cols(class_preds, prob_preds)\n\n# View results tibble\ntelecom_results %>%\n    head()\n\n# A tibble: 6 × 4\n  canceled_service .pred_class .pred_yes .pred_no\n  <fct>            <fct>           <dbl>    <dbl>\n1 yes              no             0.364     0.636\n2 no               no             0.0166    0.983\n3 no               no             0.252     0.748\n4 yes              no             0.448     0.552\n5 no               no             0.209     0.791\n6 no               yes            0.585     0.415"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#evaluating-performance-with-yardstick",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#evaluating-performance-with-yardstick",
    "title": "Modeling with tidymodels in R",
    "section": "Evaluating performance with yardstick",
    "text": "Evaluating performance with yardstick\nIn the previous exercise, you calculated classification metrics from a sample confusion matrix. The yardstick package was designed to automate this process.\nFor classification models, yardstick functions require a tibble of model results as the first argument. This should include the actual outcome values, predicted outcome values, and estimated probabilities for each value of the outcome variable.\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate performance metrics.\nThe telecom_results tibble has been loaded into your session.\n\n# Calculate the confusion matrix\nconf_mat(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n          Truth\nPrediction yes  no\n       yes  25  11\n       no   57 151\n\n# Calculate the accuracy\naccuracy(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.721\n\n# Calculate the sensitivity\n\nsens(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.305\n\n# Calculate the specificity\n\nspec(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 spec    binary         0.932"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#creating-custom-metric-sets",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#creating-custom-metric-sets",
    "title": "Modeling with tidymodels in R",
    "section": "Creating custom metric sets",
    "text": "Creating custom metric sets\nThe yardstick package also provides the ability to create custom sets of model metrics. In cases where the cost of obtaining false negative errors is different from the cost of false positive errors, it may be important to examine a specific set of performance metrics.\nInstead of calculating accuracy, sensitivity, and specificity separately, you can create your own metric function that calculates all three at the same time.\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate a custom set of performance metrics. You will also use a confusion matrix to calculate all available binary classification metrics in tidymodelsall at once.\nThe telecom_results tibble has been loaded into your session.\n\n# Create a custom metric function\ntelecom_metrics <- metric_set(accuracy, sens, spec)\n\n# Calculate metrics using model results tibble\ntelecom_metrics(telecom_results, \n                truth = canceled_service,\n                estimate = .pred_class)\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.721\n2 sens     binary         0.305\n3 spec     binary         0.932\n\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class) %>% \n  # Pass to the summary() function\n  summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary         0.721\n 2 kap                  binary         0.275\n 3 sens                 binary         0.305\n 4 spec                 binary         0.932\n 5 ppv                  binary         0.694\n 6 npv                  binary         0.726\n 7 mcc                  binary         0.316\n 8 j_index              binary         0.237\n 9 bal_accuracy         binary         0.618\n10 detection_prevalence binary         0.148\n11 precision            binary         0.694\n12 recall               binary         0.305\n13 f_meas               binary         0.424"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#plotting-the-confusion-matrix",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#plotting-the-confusion-matrix",
    "title": "Modeling with tidymodels in R",
    "section": "Plotting the confusion matrix",
    "text": "Plotting the confusion matrix\nCalculating performance metrics with the yardstick package provides insight into how well a classification model is performing on the test dataset. Most yardstick functions return a single number that summarizes classification performance.\nMany times, it is helpful to create visualizations of the confusion matrix to more easily communicate your results.\nIn this exercise, you will make a heat map and mosaic plot of the confusion matrix from your logistic regression model on the telecom_df dataset.\nYour model results tibble, telecom_results, has been loaded into your session.\n\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a heat map\n  autoplot(type = \"heatmap\")\n\n\n\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a mosaic plot\n  autoplot(type = \"mosaic\")"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#roc-curves-and-area-under-the-roc-curve",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#roc-curves-and-area-under-the-roc-curve",
    "title": "Modeling with tidymodels in R",
    "section": "ROC curves and area under the ROC curve",
    "text": "ROC curves and area under the ROC curve\nROC curves are used to visualize the performance of a classification model across a range of probability thresholds. An ROC curve with the majority of points near the upper left corner of the plot indicates that a classification model is able to correctly predict both the positive and negative outcomes correctly across a wide range of probability thresholds.\nThe area under this curve provides a letter grade summary of model performance.\nIn this exercise, you will create an ROC curve from your logistic regression model results and calculate the area under the ROC curve with yardstick.\nYour model results tibble, telecom_results has been loaded into your session.\n\n# Calculate metrics across thresholds\nthreshold_df <- telecom_results %>% \n  roc_curve(truth = canceled_service, .pred_yes)\n\n# View results\nthreshold_df %>%\n  head()\n\n# A tibble: 6 × 3\n  .threshold specificity sensitivity\n       <dbl>       <dbl>       <dbl>\n1  -Inf          0             1    \n2     0.0109     0             1    \n3     0.0166     0             0.988\n4     0.0210     0.00617       0.988\n5     0.0371     0.0123        0.988\n6     0.0443     0.0185        0.988\n\n# Plot ROC curve\nthreshold_df %>% \n  autoplot()\n\n\n\n# Calculate ROC AUC\nroc_auc(telecom_results, truth = canceled_service, .pred_yes)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.751\n\n\nStreamlining the modeling process The last_fit() function is designed to streamline the modeling workflow in tidymodels. Instead of training your model on the training data and building a results tibble using the test data, last_fit() accomplishes this with one function.\nIn this exercise, you will train the same logistic regression model as you fit in the previous exercises, except with the last_fit() function.\nYour data split object, telecom_split, and model specification, logistic_model, have been loaded into your session.\n\n# Train model with last_fit()\ntelecom_last_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n           split = telecom_split)\n\n# View test set metrics\ntelecom_last_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.721 Preprocessor1_Model1\n2 roc_auc  binary         0.751 Preprocessor1_Model1"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#collecting-predictions-and-creating-custom-metrics",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#collecting-predictions-and-creating-custom-metrics",
    "title": "Modeling with tidymodels in R",
    "section": "Collecting predictions and creating custom metrics",
    "text": "Collecting predictions and creating custom metrics\nUsing the last_fit() modeling workflow also saves time in collecting model predictions. Instead of manually creating a tibble of model results, there are helper functions that extract this information automatically.\nIn this exercise, you will use your trained model, telecom_last_fit, to create a tibble of model results on the test dataset as well as calculate custom performance metrics.\nYou trained model, telecom_last_fit, has been loaded into this session.\n\n# Collect predictions\nlast_fit_results <- telecom_last_fit %>% \n  collect_predictions()\n\n# View results\nlast_fit_results %>%\n  head()\n\n# A tibble: 6 × 7\n  id               .pred_yes .pred_no  .row .pred_class canceled_service .config\n  <chr>                <dbl>    <dbl> <int> <fct>       <fct>            <chr>  \n1 train/test split    0.364     0.636     2 no          yes              Prepro…\n2 train/test split    0.0166    0.983    22 no          no               Prepro…\n3 train/test split    0.252     0.748    23 no          no               Prepro…\n4 train/test split    0.448     0.552    30 no          yes              Prepro…\n5 train/test split    0.209     0.791    33 no          no               Prepro…\n6 train/test split    0.585     0.415    43 yes         no               Prepro…\n\n# Custom metrics function\nlast_fit_metrics <- metric_set(accuracy, sens,\n                               spec, roc_auc)\n\n# Calculate metrics\nlast_fit_metrics(last_fit_results,\n                 truth = canceled_service,\n                 estimate = .pred_class,\n                 .pred_yes)\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.721\n2 sens     binary         0.305\n3 spec     binary         0.932\n4 roc_auc  binary         0.751"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#complete-modeling-workflow",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#complete-modeling-workflow",
    "title": "Modeling with tidymodels in R",
    "section": "Complete modeling workflow",
    "text": "Complete modeling workflow\nIn this exercise, you will use the last_fit() function to train a logistic regression model and evaluate its performance on the test data by assessing the ROC curve and the area under the ROC curve.\nSimilar to previous exercises, you will predict canceled_service in the telecom_df data, but with an additional predictor variable to see if you can improve model performance.\nThe telecom_df tibble, telecom_split, and logistic_model objects from the previous exercises have been loaded into your workspace. The telecom_split object contains the instructions for randomly splitting the telecom_df tibble into training and test sets. The logistic_model object is a parsnip specification of a logistic regression model.\n\n# Train a logistic regression model\nlogistic_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins + avg_intl_mins + monthly_charges + months_with_company, \n           split = telecom_split)\n\n# Collect metrics\nlogistic_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.811 Preprocessor1_Model1\n2 roc_auc  binary         0.865 Preprocessor1_Model1\n\n# Collect model predictions\nlogistic_fit %>% \n  collect_predictions() %>% \n  # Plot ROC curve\n  roc_curve(truth = canceled_service, .pred_yes) %>% \n  autoplot()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Projects",
    "section": "",
    "text": "Data Science projects I have been working on 😊\nWork in progress"
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html",
    "title": "Writing Efficient R Code",
    "section": "",
    "text": "One of the relatively easy optimizations available is to use an up-to-date version of R. In general, R is very conservative, so upgrading doesn’t break existing code. However, a new version will often provide free speed boosts for key functions.\nThe version command returns a list that contains (among other things) the major and minor version of R currently being used.\n\n# Print the R version details using version\nversion\n\n               _                           \nplatform       x86_64-pc-linux-gnu         \narch           x86_64                      \nos             linux-gnu                   \nsystem         x86_64, linux-gnu           \nstatus                                     \nmajor          4                           \nminor          1.2                         \nyear           2021                        \nmonth          11                          \nday            01                          \nsvn rev        81115                       \nlanguage       R                           \nversion.string R version 4.1.2 (2021-11-01)\nnickname       Bird Hippie                 \n\n# Assign the variable major to the major component\nmajor <- version$major\n\n# Assign the variable minor to the minor component\nminor <- version$minor"
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#comparing-read-times-of-csv-and-rds-files",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#comparing-read-times-of-csv-and-rds-files",
    "title": "Writing Efficient R Code",
    "section": "Comparing read times of CSV and RDS files",
    "text": "Comparing read times of CSV and RDS files\nOne of the most common tasks we perform is reading in data from CSV files. However, for large CSV files this can be slow. One neat trick is to read in the data and save as an R binary file (rds) using saveRDS(). To read in the rds file, we use readRDS().\nNote: Since rds is R’s native format for storing single objects, you have not introduced any third-party dependencies that may change in the future.\nTo benchmark the two approaches, you can use system.time(). This function returns the time taken to evaluate any R expression. For example, to time how long it takes to calculate the square root of the numbers from one to ten million, you would write the following:\n\n# How long does it take to read movies from CSV?\nsystem.time(read.csv(\"movies.csv\"))\n\n   user  system elapsed \n  0.131   0.003   0.135 \n\n# How long does it take to read movies from RDS?\nsystem.time(readRDS(\"movies.rds\"))\n\n   user  system elapsed \n   0.03    0.00    0.03"
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#elapsed-time",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#elapsed-time",
    "title": "Writing Efficient R Code",
    "section": "Elapsed time",
    "text": "Elapsed time\nUsing system.time() is convenient, but it does have its drawbacks when comparing multiple function calls. The microbenchmark package solves this problem with the microbenchmark() function.\n\n# Load the microbenchmark package\nlibrary(microbenchmark)\n\n# Compare the two functions\ncompare <- microbenchmark(read.csv(\"movies.csv\"), \n                          readRDS(\"movies.rds\"), \n                          times = 100)\n\n# Print compare\ncompare\n\nUnit: milliseconds\n                   expr       min        lq     mean   median        uq\n read.csv(\"movies.csv\") 124.06143 132.66351 147.2988 139.0496 152.97855\n  readRDS(\"movies.rds\")  30.09878  31.21481  33.3113  32.3566  34.34812\n       max neval cld\n 240.20944   100   b\n  60.82679   100  a \n\n\nMy hardware For many problems your time is the expensive part. If having a faster computer makes you more productive, it can be cost effective to buy one. However, before you splash out on new toys for yourself, your boss/partner may want to see some numbers to justify the expense. Measuring the performance of your computer is called benchmarking, and you can do that with the benchmarkme package.\n\n# Load the benchmarkme package\nlibrary(benchmarkme)\n\n# Assign the variable ram to the amount of RAM on this machine\nram <- get_ram()\nram\n\n16.5 GB\n\n# Assign the variable cpu to the cpu specs\ncpu <- get_cpu()\ncpu\n\n$vendor_id\n[1] \"GenuineIntel\"\n\n$model_name\n[1] \"11th Gen Intel(R) Core(TM) i7-11370H @ 3.30GHz\"\n\n$no_of_cores\n[1] 8"
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#benchmark-datacamps-machine",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#benchmark-datacamps-machine",
    "title": "Writing Efficient R Code",
    "section": "Benchmark DataCamp’s machine",
    "text": "Benchmark DataCamp’s machine\nThe benchmarkme package allows you to run a set of standardized benchmarks and compare your results to other users. One set of benchmarks tests is reading and writing speeds.\nThe function call\nres = benchmark_io(runs = 1, size = 5) records the length of time it takes to read and write a 5MB file.\n\n# Run the io benchmark\nres <- benchmark_io(runs = 1, size = 50)\n\nPreparing read/write io\n\n\n# IO benchmarks (2 tests) for size 50 MB:\n\n\n     Writing a csv with 6250000 values: 3.97 (sec).\n\n\n     Reading a csv with 6250000 values: 1.43 (sec).\n\n# Plot the results\nplot(res)\n\nYou are ranked 1 out of 119 machines.\n\n\nPress return to get next plot \n\n\nYou are ranked 2 out of 119 machines."
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#benchmark-r-operations",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#benchmark-r-operations",
    "title": "Writing Efficient R Code",
    "section": "Benchmark r operations",
    "text": "Benchmark r operations\n\n# Run each benchmark 3 times\nres <- benchmark_std(runs = 10)\n\n# Programming benchmarks (5 tests):\n\n\n    3,500,000 Fibonacci numbers calculation (vector calc): 0.114 (sec).\n\n\n    Grand common divisors of 1,000,000 pairs (recursion): 0.358 (sec).\n\n\n    Creation of a 3,500 x 3,500 Hilbert matrix (matrix calc): 0.169 (sec).\n\n\n    Creation of a 3,000 x 3,000 Toeplitz matrix (loops): 0.782 (sec).\n\n\n    Escoufier's method on a 60 x 60 matrix (mixed): 0.554 (sec).\n\n\n# Matrix calculation benchmarks (5 tests):\n\n\n    Creation, transp., deformation of a 5,000 x 5,000 matrix: 0.278 (sec).\n\n\n    2,500 x 2,500 normal distributed random matrix^1,000: 0.146 (sec).\n\n\n    Sorting of 7,000,000 random values: 0.524 (sec).\n\n\n    2,500 x 2,500 cross-product matrix (b = a' * a): 0.967 (sec).\n\n\n    Linear regr. over a 5,000 x 500 matrix (c = a \\ b'): 0.0893 (sec).\n\n\n# Matrix function benchmarks (5 tests):\n\n\n    Cholesky decomposition of a 3,000 x 3,000 matrix: 0.633 (sec).\n\n\n    Determinant of a 2,500 x 2,500 random matrix: 0.693 (sec).\n\n\n    Eigenvalues of a 640 x 640 random matrix: 0.287 (sec).\n\n\n    FFT over 2,500,000 random values: 0.171 (sec).\n\n\n    Inverse of a 1,600 x 1,600 random matrix: 0.573 (sec).\n\nplot(res)\n\nYou are ranked 1 out of 749 machines.\n\n\nPress return to get next plot \n\n\nYou are ranked 2 out of 747 machines.\n\n\n\n\n\nPress return to get next plot \n\n\nYou are ranked 47 out of 747 machines."
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#timings---growing-a-vector",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#timings---growing-a-vector",
    "title": "Writing Efficient R Code",
    "section": "Timings - growing a vector",
    "text": "Timings - growing a vector\nGrowing a vector is one of the deadly sins in R; you should always avoid it.\nThe growing() function defined below generates n random standard normal numbers, but grows the size of the vector each time an element is added!\nNote: Standard normal numbers are numbers drawn from a normal distribution with mean 0 and standard deviation 1.\nn <- 30000 # Slow code growing <- function(n) { x <- NULL for(i in 1:n) x <- c(x, rnorm(1)) x }\n\ngrowing <- function(n) {\n    x = NULL\n    for(i in 1:n) \n        x = c(x, rnorm(1))\n    x\n}\n\n# Use <- with system.time() to store the result as res_grow\nsystem.time(res_grow <- growing(30000))\n\n   user  system elapsed \n  0.646   0.000   0.647 \n\n\nTimings - pre-allocation In the previous exercise, growing the vector took around 2 seconds. How long does it take when we pre-allocate the vector? The pre_allocate() function is defined below.\n\nn <- 30000\n# Fast code\npre_allocate <- function(n) {\n    x <- numeric(n) # Pre-allocate\n    for(i in 1:n) \n        x[i] <- rnorm(1)\n    x\n}\n\n\n# Use <- with system.time() to store the result as res_allocate\nn <- 30000\nsystem.time(res_allocate <- pre_allocate(n))\n\n   user  system elapsed \n  0.036   0.000   0.035"
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#vectorized-code-multiplication",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#vectorized-code-multiplication",
    "title": "Writing Efficient R Code",
    "section": "Vectorized code: multiplication",
    "text": "Vectorized code: multiplication\nThe following piece of code is written like traditional C or Fortran code. Instead of using the vectorized version of multiplication, it uses a for loop.\nYour job is to make this code more “R-like” by vectorizing it.\n\nx <- rnorm(10)\nx2 <- numeric(length(x))\nfor(i in 1:10)\n    x2[i] <- x[i] * x[i]\n# Store your answer as x2_imp\nx2_imp <- x*x"
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#vectorized-code-calculating-a-log-sum",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#vectorized-code-calculating-a-log-sum",
    "title": "Writing Efficient R Code",
    "section": "Vectorized code: calculating a log-sum",
    "text": "Vectorized code: calculating a log-sum\nA common operation in statistics is to calculate the sum of log probabilities. The following code calculates the log-sum (the sum of the logs)."
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#data-frames-and-matrices---column-selection",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#data-frames-and-matrices---column-selection",
    "title": "Writing Efficient R Code",
    "section": "Data frames and matrices - column selection",
    "text": "Data frames and matrices - column selection\nAll values in a matrix must have the same data type, which has efficiency implications when selecting rows and columns.\nSuppose we have two objects, mat (a matrix) and df (a data frame).\n\n# Which is faster, mat[, 1] or df[, 1]? \nmicrobenchmark(mat[, 1], df[, 1])"
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#row-timings",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#row-timings",
    "title": "Writing Efficient R Code",
    "section": "Row timings",
    "text": "Row timings\nSimilar to the previous example, the objects mat and df are a matrix and a data frame, respectively. Using microbenchmark(), how long does it take to select the first row from each of these objects?\nTo make the comparison fair, just use mat[1, ] and df[1, ].\n\nInteresting! Accessing a row of a data frame is much slower than accessing that of a matrix, more so than when accessing a column from each data type. This is because the values of a column of a data frame must be the same data type, whereas that of a row doesn’t have to be. Do you see the pattern here?\n\n\n# Which is faster, mat[, 1] or df[, 1]? \nmicrobenchmark(mat[1, ], df[1, ])"
  },
  {
    "objectID": "myblog/datacamp/efficient_R_code/efficient_R_code.html#profvis-in-action",
    "href": "myblog/datacamp/efficient_R_code/efficient_R_code.html#profvis-in-action",
    "title": "Writing Efficient R Code",
    "section": "Profvis in action",
    "text": "Profvis in action\nExamine the code on the right that performs a standard data analysis. It loads and selects data, plots the data of interest, and adds in a regression line.\n\n# Load the data set\ndata(movies, package = \"ggplot2movies\") \n\n# Load the profvis package\nlibrary(profvis)\n\n# Profile the following code with the profvis function\nprofvis({\n  # Load and select data\n  comedies <- movies[movies$Comedy == 1, ]\n\n  # Plot data of interest\n  plot(comedies$year, comedies$rating)\n\n  # Loess regression line\n  model <- loess(rating ~ year, data = comedies)\n  j <- order(comedies$year)\n  \n  # Add fitted line to the plot\n  lines(comedies$year[j], model$fitted[j], col = \"red\")\n})    ## Remember the closing brackets!"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html",
    "href": "myblog/breast_cancer_prediction/cancer_data.html",
    "title": "Cancer Data",
    "section": "",
    "text": "In this tutorial I’m going to predict whether a breast cancer tumor is benign or malignant. Using Wiscosin breast cancer data set available on Kaggle. The 30 predictors are divided into three parts first is Mean ( variables 3-13), Standard Error(13-23) and Worst(23-32) and each contain 10 parameter (radius, texture,area, perimeter, smoothness,compactness,concavity,concave points,symmetry and fractal dimension of the tumor). When predicting cancer breast tumor types there two types of cost;\n\nThe cost of telling someone who has malignant tumor that they have benign these are the false negatives in this case someone might not seek medical help which is can cause death.\nTelling someone that they have malignant type of tumor but they don’t which is usually false positives. In this case you subject someone to unnecessary stress\n\nSo it’s highly desirable that our model has good accuracy $ f_1 score$ and high recall.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(xgboost)\nlibrary(knitr)\nlibrary(broom)\nlibrary(caret)\nlibrary(e1071)\nlibrary(kableExtra)\nlibrary(ggthemes)\nlibrary(DT)\nlibrary(glmnet)\n\noptions(scipen = 1, digits = 4)\ncancer <- fread(\"data.csv\")\n\ncancer[, V33 := NULL]\n\n\nhead(cancer)  %>%\n  datatable(options = list(scrollX = TRUE))"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#percentage-of-women-with-malignant-tumor",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#percentage-of-women-with-malignant-tumor",
    "title": "Cancer Data",
    "section": "Percentage of women with malignant tumor",
    "text": "Percentage of women with malignant tumor\nThe percentage of women with malignant tumor is 37.26%(212 out 569) while the rest 62.74%(357) had benign tumors.\n\ncancer[, .(freq = .N),\n       by = diagnosis] %>% \n    .[, perc := round(100 * freq/sum(freq), 2)] %>%\n  \nggplot(aes(x=diagnosis, y=perc, fill = diagnosis)) + \n    geom_bar(stat = \"identity\", width  = 0.5)+ theme_hc() +\n    geom_text(aes(x=diagnosis, y=perc, label = paste(perc, \"%\")),\n              position =  position_dodge(width = 0.5),\n              vjust = 0.05, hjust = 0.5, size = 5)+\n    scale_fill_hc(name = \"\")+\n    labs(x = \"Cancer Type\",\n         y = \"Percentage\",\n         title = \"Percentage of women with benign or malignant breast bancer\")+\n    theme(legend.position = \"none\",\n          axis.title = element_text(size =12))"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#boxplots",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#boxplots",
    "title": "Cancer Data",
    "section": "Boxplots",
    "text": "Boxplots\nFrom the boxplots we can identify variables where we expect there is a significance difference between the two groups of cancer tumors. When using a boxplot if two distributions do not averlap or more than 75% of two boxplot do not overlap then we expect that there is a significance difference in the mean/median between the two groups. Some of the variables where the distribution of two cancer tumors are significantly different are radius_mean, texture_mean etc. The visible differences between malignant tumors and benign tumors can be seen in means of all cells and worst means where worst means is the average of all the worst cells. The distribution of malignant tumors have higher scores than the benign tumors in this cases.\n\ncancerm <- melt(cancer[, -1, with = F], id.vars = \"diagnosis\")\n\nggplot(cancerm, aes(x = diagnosis, y = value))+\n    geom_boxplot() + facet_wrap(~variable, scales = \"free_y\")"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#features-scaling",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#features-scaling",
    "title": "Cancer Data",
    "section": "Features Scaling",
    "text": "Features Scaling\nWe find that some variables are highly correlated. We can use principle component analysis for dimension reduction. Since variables are correlated it’s evident that we can use a smaller set of features to build our models.\n\ncancer[, id := NULL]\npredictors <- names(cancer)[3:31]\ncancer[, (predictors) := lapply(.SD, function(x) scale(x)), .SDcols = predictors ]\ncancer[, diagnosis := as.factor(diagnosis)]"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#correlation-matrix",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#correlation-matrix",
    "title": "Cancer Data",
    "section": "Correlation matrix",
    "text": "Correlation matrix\n\ncor(cancer[, -(1:2), with = F]) %>%\n  datatable(options = list(scrollX = TRUE), style = \"bootstrap4\")"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#principle-component-analysis",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#principle-component-analysis",
    "title": "Cancer Data",
    "section": "Principle Component Analysis",
    "text": "Principle Component Analysis\nUsing the elbow rule we can use the first 5 principle components. Using 15 principle components we will have achieved al most 100% of the variance from the original data set.\n\npca <- prcomp(cancer[, predictors, with = F], scale. = F)"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#variance-explained",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#variance-explained",
    "title": "Cancer Data",
    "section": "Variance Explained",
    "text": "Variance Explained\nSince PCA forms new characteristics the variance explained plot shows the amount of variation of the original features captured by each principle component. The new features are simply linear combinations of the old features.\n\nstdpca <- pca$sdev\n\nvarpca <- stdpca^2\n\nprop_var <- varpca/sum(varpca)\nprop_var * 100\n\n [1] 43.706363 18.472237  9.716239  6.816736  5.676223  4.161723  2.292352\n [8]  1.643434  1.363238  1.191515  1.011032  0.897368  0.832105  0.539193\n[15]  0.323823  0.269517  0.198317  0.178851  0.153573  0.107095  0.102579\n[22]  0.093821  0.082603  0.058725  0.053331  0.027514  0.022985  0.005110\n[29]  0.002394\n\nsum(prop_var[1:15])\n\n[1] 0.9864"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#scree-plot",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#scree-plot",
    "title": "Cancer Data",
    "section": "Scree plot",
    "text": "Scree plot\nScree plot shows the variance explained by each principle component which reduces as the number of principle components increase.\n\nplot(prop_var, xlab = \"Principal Component\",\n     ylab = \"Proportion of Variance Explained\",\n     type = \"b\", xlim = c(0, 30))"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#cumulative-variance-explained",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#cumulative-variance-explained",
    "title": "Cancer Data",
    "section": "Cumulative Variance Explained",
    "text": "Cumulative Variance Explained\nThe cumulative of variance plot helps to choose the number of features based on the amount of variation from original data set you want captured. In this case, I wanted to use number of principle components that capture almost 100% of the variation. After trying with different number of principle components I found out that the accuracy of the models did not increase after the 15th principle components.\n\ncum_var <- cumsum(prop_var)\nplot(cum_var, xlab = \"Principal Component\",\n     ylab = \"Cumulative Proportion of Variance Explained\",\n     type = \"b\", xlim = c(0, 30))"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#construct-new-data-set",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#construct-new-data-set",
    "title": "Cancer Data",
    "section": "Construct new data set",
    "text": "Construct new data set\nWe use the first 15 principle components as our new predictors, then we randomly split data into training and test set in 7:3 ratio.\n\nset.seed(100)\ntrain_sample <- sample(1:nrow(cancer), round(0.7*nrow(cancer)))\n\npcadat <- data.table( label = cancer$diagnosis, pca$x[,1:15]) \npcadat[, label := factor(label, levels = c(\"M\", \"B\"))]\ntrain <- pcadat[train_sample,]\ntest <- pcadat[-train_sample,]"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#logistic-regression",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#logistic-regression",
    "title": "Cancer Data",
    "section": "Logistic regression",
    "text": "Logistic regression\nThis is one of generalized linear models which deals with binary data. There is a generalization of this model which is called multinomial regression where you can fit multi class data. The equation for logistic regression model is:\n\\[log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1*X_1 + ... \\beta_n * X_n\\] and using mle the cost function can be derived as: \\[J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m} y^i log(h_\\theta(x^i)) + (1-y^i) log(1 - h_\\theta(x^i))\\] Given that \\[y = 0\\] \\[y = 1\\] . Finding \\[\\beta\\] s we minimizing the cost function.\n\nfit_glm <- glm(label ~., data = train, family = binomial)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#regularization-in-logistic-regression",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#regularization-in-logistic-regression",
    "title": "Cancer Data",
    "section": "Regularization in logistic regression",
    "text": "Regularization in logistic regression\nThe warning “glm.fit: fitted probabilities numerically 0 or 1 occurred” shows that there is a perfect separation/over fitting. In this case you can load glmnet library and fit a regularized logistic regression. These can be achieved by adding a regularization term to the cost function.The L1 regularization(Lasso) adds a penalty equal to the sum of the absolute values of the coefficients.\n\\[J(\\theta) = -\\frac{1}{m}\\sum_{i=0}^{m} y^i log(h_\\theta(x^i)) + (1-y^i) log(1 - h_\\theta(x^i)) + \\frac {\\lambda}{2m}\\sum_{j=1}^{n} |\\theta^i|\\]\n\ntrainx <- train[,-1]\n\ny_train <- factor(train$label, levels = c(\"B\", \"M\"), labels = 0:1)\n#y <- as.numeric(as.character(y))\n\ny_test <- factor(test$label, levels = c(\"B\", \"M\"), labels = 0:1) %>% as.character() %>% as.numeric()\n#ytest <- as.numeric(as.character(ytest))\n\ntestx <- data.matrix(test[, -1]) \n\nTo find the optimal values \\(\\lambda\\) we use cross validation. We choose \\(\\lambda\\) which gives the highest cross validation accuracy.\n\ncv_fold <- createFolds(train$label, k = 10)\n\nmyControl <- trainControl(\n  method = \"cv\", \n  number = 10,\n  summaryFunction = twoClassSummary,\n  savePredictions = \"all\",\n  classProbs = TRUE,\n  verboseIter = FALSE,\n  index = cv_fold,\n  allowParallel = TRUE\n  \n)\n\ntuneGrid <-  expand.grid(\n    alpha = 0:1,\n    lambda = seq(0.001, 1, length.out = 10))\n    \nglmnet_model <- train(\n  label ~.,\n  data = train,\n  method = \"glmnet\",\n  metric = \"ROC\",\n  trControl = myControl,\n  tuneGrid = tuneGrid\n)\n\ns\n\nplot(glmnet_model) \n\n\n\n#lamda_min <- cv_glm$lambda.min\n\n\nresample_glmnet <- thresholder(glmnet_model, \n                              threshold = seq(.2, 1, by = 0.05), \n                              final = TRUE, \n                              statistics = \"all\")\n\nggplot(resample_glmnet , aes(x = prob_threshold, y = F1)) + \n  geom_point() + \n  geom_point(aes(y = Sensitivity), col = \"blue\")\n\n\n\n\n\nlibrary(caTools)\n\npred_glm <- predict(glmnet_model, test, type = \"prob\")\n\ncolAUC(pred_glm , test$label, plotROC = TRUE)\n\n\n\n\n             M      B\nM vs. B 0.9683 0.9683\n\npred_glm1 <- ifelse(pred_glm[, \"M\"] > 0.4, \"M\", \"B\")\n#pred_glm1 <- predict(glmnet_model, test, type = \"raw\")\n\n\npred_glm1 <- factor(pred_glm1, levels = levels(test$label))\n\n\nconfusionMatrix(pred_glm1, test$label,positive = \"M\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   M   B\n         M  53   7\n         B   7 104\n                                        \n               Accuracy : 0.918         \n                 95% CI : (0.866, 0.955)\n    No Information Rate : 0.649         \n    P-Value [Acc > NIR] : <2e-16        \n                                        \n                  Kappa : 0.82          \n                                        \n Mcnemar's Test P-Value : 1             \n                                        \n            Sensitivity : 0.883         \n            Specificity : 0.937         \n         Pos Pred Value : 0.883         \n         Neg Pred Value : 0.937         \n             Prevalence : 0.351         \n         Detection Rate : 0.310         \n   Detection Prevalence : 0.351         \n      Balanced Accuracy : 0.910         \n                                        \n       'Positive' Class : M"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#svm",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#svm",
    "title": "Cancer Data",
    "section": "SVM",
    "text": "SVM\nSupport Vector Machines is a type of supervised learning algorithm that is used for classification and regression. Most of the times however, it’s used for classification.\nTo understand how SVM works consider the following example of linearly separable data. It’s clear that we can separate the two classes using a straight line(decision boundary). Which is normally referred to a separating hyperplane.\n\n\n\n\n\nThe question is, since there exists many lines that can separate the red and the black classes which is the best one. This introduces us to the maximal margin classification, In short SVM finds the hyperplane/line that gives the biggest margin/gap between the two classes. In this case SVM will choose the solid line as the hyperplane while the margins are the dotted lines. The circled points that lie directly on the margin, or on the wrong side of the margin for their class, are known as support vectors. This shows that SVM uses this points to come up with a the decision boundary, the other points are not used. In this case since it’s a two dimensional space the equation of the separating line will be \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2\\]. Then when equations evaluates to more than 0 then 1 is predicted \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2 > 0, y = 1\\] and when it evaluates to less than zero then predicted class is -1 \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2 < 0, \\; y = -1\\] This becomes maximisation problem \\[width \\; of \\;the \\; margin = M \\] \\[\\sum_{j=1}^{n}\\beta_j = 1\\]\n\\[y_i(\\beta_0 + \\beta_1X_1 + \\beta_2X_2) >= M\\]\n\n\n\n\n\nThis is a best case scenario but in most cases the classes are noisy. Consider the plot below no matter which line you choose some points are bound to be on the wrong side of the desicion boundary. Thus maximal margin classification would not work.\n\n\n\n\n\nSVM then introduces what is called a soft margin. In naive explanation you can think of this as a margin that allows some points to be on the wrong side. By introducing an error term we allow for some slack. Thus in a two case the maximisation becomes \\[y_i(\\beta_0 + \\beta_1X_1 + \\beta_2X_2) >= M(1- \\epsilon)\\]\n\\[\\sum_{i=0}^{n} \\epsilon_i <= C\\] C is a tuning parameter which determines the width of the margin while \\[\\epsilon_i  \\;'s\\] are slack variables. that allow individual observations to fall on the wrong side of the margin. In some cases the decision boundary maybe non linear. In case your are dealing with logistic regression you will be forced to introduce polynomial terms which might result in a very large feature space. SVM then introduces what are called kernels"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#tuning-svm",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#tuning-svm",
    "title": "Cancer Data",
    "section": "Tuning SVM",
    "text": "Tuning SVM\n\nsvm_tune <-  expand.grid(\n    C =c(1 ,5 ,  10, 100, 150),\n    sigma = seq(0, .01, length.out = 5))\n    \nsvm_model <- train(\n  label ~.,\n  data = train,\n   metric=\"ROC\",\n  method = \"svmRadial\",\n  trControl = myControl,\n  tuneGrid = svm_tune,\n  verbose = FALSE\n)\n\n\nresample_svm <- thresholder(svm_model, \n                              threshold = seq(.0, 1, by = 0.05), \n                              final = TRUE, \n                              statistics = \"all\")\n\nggplot(resample_svm , aes(x = prob_threshold, y = F1, col = \"F1\")) + \n  geom_point() + \n  geom_point(aes(y = Sensitivity,  col = \"Sensitivity\"))+\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1))\n\n\n\n#mean(pred_svm == ytest)\n\n\npred_svm <-predict(svm_model, newdata = test, type = \"prob\")\n\npred_svm <- ifelse(pred_svm[, \"M\"] > 0.40, \"M\", \"B\")\n\npred_svm <- factor(pred_svm, levels = levels(test$label))\n\nconfusionMatrix(test$label, pred_svm, positive = \"M\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   M   B\n         M  58   2\n         B   2 109\n                                        \n               Accuracy : 0.977         \n                 95% CI : (0.941, 0.994)\n    No Information Rate : 0.649         \n    P-Value [Acc > NIR] : <2e-16        \n                                        \n                  Kappa : 0.949         \n                                        \n Mcnemar's Test P-Value : 1             \n                                        \n            Sensitivity : 0.967         \n            Specificity : 0.982         \n         Pos Pred Value : 0.967         \n         Neg Pred Value : 0.982         \n             Prevalence : 0.351         \n         Detection Rate : 0.339         \n   Detection Prevalence : 0.351         \n      Balanced Accuracy : 0.974         \n                                        \n       'Positive' Class : M"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#xgboost",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#xgboost",
    "title": "Cancer Data",
    "section": "Xgboost",
    "text": "Xgboost\nXGBoost is a type of an ensemble learner. Ensemble learning is where multiple machine learning algorithms are used at the same time for prediction. A good example will be Random Forests. In random Forest multiple decision trees are used together for prediction. There are two main types of ensemble learners, bagging and boosting. Random forest use the bagging approach. Trees are built from random subsets(rows and columns) of training set and then the final prediction is the weighted sum of all decision trees functions. Boosting methods are similar but in boosting samples are selected sequentially. For instance the first sample is selected and a decision tree is fitted, The model then picks the examples that were hard to learn and using this examples and a few others selected randomly from the training set the second model is fitted, Using the first model and the second model prediction is made, the model is evaluated and hard examples are picked and together with another randomly selected new examples from training set another model is trained. This is the process for boosting algorithms which continues for a specified number of n.\nIn gradient boosting the first model is fitted to the original training set. Let say your fitting a simple regression model for ease of explanation. Then your first model will be $ y = f(x) + $. When you find that the error is too large one of the things you might try to do is add more features, use another algorithm, tune your algorithm, look for more training data etc. But what if the error is not white noise and it has some relationship with output \\(y\\) . Then we can fit a second model. $ = f_1(x) + _1$. then this process can continue lets say until n times. Then the final model will be\n$ n = f*{n}(x) + _{n-1}$.\nThen the final step is to add this models together with some weighting criteria $ weights = ’s$ which gives us the final function used for prediction.\n\\(y = \\alpha * f(x) + \\alpha_1 * f_1(x) + \\alpha_2 * f_2(x)...+ \\alpha_n * f_n + \\epsilon\\)\n\n# \"subsample\" is the fraction of the training samples (randomly selected) that will be used to train each tree.\n# \"colsample_by_tree\" is the fraction of features (randomly selected) that will be used to train each tree.\n# \"colsample_bylevel\" is the fraction of features (randomly selected) that will be used in each node to train each tree.\n#eta learning rate\n\n\n\nxgb_ctrl <- trainControl(method = \"cv\",\n                        number = 5,\n                        summaryFunction = twoClassSummary,\n                        classProbs = TRUE,\n                        allowParallel=T,\n                        index = cv_fold,\n                        verboseIter = FALSE,\n                        savePredictions = TRUE,\n                        search = \"grid\")\n\nxgb_grid <- expand.grid(nrounds = c(10, 50, 100),\n                        eta = seq(0.06, .2, length.out = 3),\n                        max_depth = c(50, 80),\n                        gamma = c(0,.01, 0.1),\n                        colsample_bytree = c(0.6, 0.7,0.8),\n                        min_child_weight = 1,\n                        subsample =  .7\n                        \n    )\n\nxgb_model <-train(label~.,\n                 data=train,\n                 method=\"xgbTree\",\n                 trControl= xgb_ctrl,\n                 tuneGrid=xgb_grid,\n                 verbosity=0,\n                 metric=\"ROC\",\n                 nthread =4\n                     \n    )\n\nIncreasing cut of increases the precision. A greater fraction of those who will be predicted that they have cancer will turn out that they have, but the algorithm is likely to have lower recall. If we want to avoid too many cases of people cancer being predicted that they do not have cancer. It will be very bad to tell someone that they do not have cancer but they have. If we lower the probability let say to 0.3 then we want to make sure that even if there is a 30% chance you have cancer then you should be flagged.\n\nresample_xgb <- thresholder(xgb_model, \n                              threshold = seq(.0, 1, by = 0.01), \n                              final = TRUE, \n                              statistics = \"all\")\n\nggplot(resample_xgb , aes(x = prob_threshold, y = F1, col = \"F1\")) + \n  geom_point() + \n  geom_point(aes(y = Sensitivity, col = \"Sensitivity\"))+\n  scale_x_continuous(breaks = seq(0, 1, by =.1))\n\n\n\n\n\npred_xgb <-predict(xgb_model, newdata = test, type = \"prob\")\npred_xgb1 <- ifelse(pred_xgb[, \"M\"] > 0.4, \"M\", \"B\")\npred_xgb1 <- factor(pred_xgb1, levels = levels(test$label))\n\nconfusionMatrix(pred_xgb1,test$label,  positive = \"M\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   M   B\n         M  59   2\n         B   1 109\n                                       \n               Accuracy : 0.982        \n                 95% CI : (0.95, 0.996)\n    No Information Rate : 0.649        \n    P-Value [Acc > NIR] : <2e-16       \n                                       \n                  Kappa : 0.962        \n                                       \n Mcnemar's Test P-Value : 1            \n                                       \n            Sensitivity : 0.983        \n            Specificity : 0.982        \n         Pos Pred Value : 0.967        \n         Neg Pred Value : 0.991        \n             Prevalence : 0.351        \n         Detection Rate : 0.345        \n   Detection Prevalence : 0.357        \n      Balanced Accuracy : 0.983        \n                                       \n       'Positive' Class : M"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#learning-curves",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#learning-curves",
    "title": "Cancer Data",
    "section": "Learning Curves",
    "text": "Learning Curves\n\nsets <- seq(from = 50, to = nrow(train), by = 50)\nsets[length(sets)] <-nrow(train) \ntrain.err <- c()\ntest.err<- c()\ntune_grid <- expand.grid( nrounds = 50, max_depth = 50, eta = 0.06, gamma = 0.01, \n                         colsample_bytree = 0.6, min_child_weight = 1, subsample = 0.7)\nfor (i in 1:length(sets)) {\n    \n    traini = train[1:sets[i],]\n    fit_svm <- train(label ~., data = traini, metric=\"Accuracy\", method = \"svmRadial\",\n                 trControl = trainControl(method = \"none\", summaryFunction = twoClassSummary,\n                                          classProbs = TRUE),\n                 tuneGrid = expand_grid( sigma = 0.0075, C = 5),\n                 )\n    \n    # fit_svm <-train(label~.,\n    #              data=traini,\n    #              method=\"xgbTree\",\n    #              trControl= xgb_ctrl,\n    #              tuneGrid= tune_grid ,\n    #              verbose=T,\n    #              metric=\"ROC\",\n    #              nthread =3\n    #                  \n    # )\n    pred_train = predict(fit_svm, newdata = traini, type = \"prob\")\n    pred_train = ifelse(pred_train[[\"M\"]] > 0.4, \"M\", \"B\")\n    train.err[i] =1 -  mean(pred_train == traini$label)\n    pred_test = predict(fit_svm, newdata = test, type = 'prob')\n    pred_test = ifelse(pred_test[, \"M\"] > 0.4, \"M\", \"B\")\n    test.err[i] = 1 - mean(test$label == pred_test)\n    \n    cat(i,\" \")\n    \n}\n\n1  2  3  4  5  6  7  \n\ntrain.err\n\n[1] 0.00000 0.03000 0.03333 0.01500 0.02000 0.02000 0.02261\n\nmatplot(sets, cbind(test.err, train.err), pch = 19, col = c(\"red\", \"blue\"),\n        type = \"b\", ylab = \"Error\", xlab = \"Train sample size\", main = \"SVM Learning Curves\")\nlegend(\"topright\", legend = c(\"Test\", \"Train\"), pch = 19, col = c(\"red\", \"blue\"))"
  },
  {
    "objectID": "myblog/breast_cancer_prediction/cancer_data.html#error-analysis",
    "href": "myblog/breast_cancer_prediction/cancer_data.html#error-analysis",
    "title": "Cancer Data",
    "section": "Error Analysis",
    "text": "Error Analysis\nLook at the examples that the algorithm misclassified to see if there is a trend. Generally you are trying to find out the weak points of your algorithm. Checking why your algorithm is making those errors. For instance, from the boxplots below the malignant tumors that were misclassified had lower radius mean compared to mislassified benign tumors. This contrary to what we saw in the first boxplots graph.\n\ndf <- data.frame(cancer[-train_sample,], pred_svm) %>%\n    setDT()\n\n\ntest_mis_svm <- df[(diagnosis == \"M\" & pred_svm == 0) |( diagnosis == \"B\" & pred_svm == \"M\")]\n\n\n# test_mis_svm_m <- melt(test_mis_svm, \n#                 id.vars = c(\"diagnosis\", \"pred_svm\"))\n# \n# ggplot(test_mis_svm_m , aes(x = pred_svm, y = value))+\n#     geom_boxplot() + facet_wrap(~variable, scales = \"free_y\")"
  },
  {
    "objectID": "myblog/kenya_population/household_assets_2019census.html",
    "href": "myblog/kenya_population/household_assets_2019census.html",
    "title": "Kenya Household Assets",
    "section": "",
    "text": "Kenya selected households assets 2019 census\nI figured that it will be important for me to do this to show why it is impossible for online learning to be adopted in Kenya.\nI used 2019 census data set which can be found here and the counties shape file can be found here\n\n#Packages used\n\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sf)\nlibrary(DT)\nlibrary(tmap)\nlibrary(ggthemes)\nlibrary(ggiraph)\n\n\n\nRead data set\n\nhousehold_assets <- fread(\"percentage-distribution-of-conventional-households-by-ownership-of-selected-household-assets-201.csv\")\n\n#\nkenya_shapefile <- st_read(\"County\") %>% setDT()\n\nReading layer `County' from data source \n  `/home/mburu/personal_projects/github_blog/myblog/kenya_population/County' \n  using driver `ESRI Shapefile'\nSimple feature collection with 47 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 33.91182 ymin: -4.702271 xmax: 41.90626 ymax: 5.430648\nGeodetic CRS:  WGS 84\n\nkenya_shapefile[, county := tolower(COUNTY)]\n\n# rename column\nsetnames(household_assets, \n         c(\"County / Sub-County\", \"Conventional Households\"  ), \n         c(\"county\", \"households\"))\n\n\n\nSome minor cleaning\n\n# remove commas \nhousehold_assets[, households := as.numeric(gsub(\",|AR K    \", \"\", households))]\n\nhousehold_assets[,  county := tolower(county)]\n\nsub_county <- household_assets %>% \n    group_by(county) %>%\n    filter(households == max(households)) %>% setDT()\n\n\nsub_county_melt <- melt(sub_county, \n                        id.vars = c(\"county\", \"households\"))\n\n\n\n% of households with various assets 2019 census\n\nThis is overall data set for the whole country computer devices is ownership about 8.8% this just means that about 91% of the students can’t access online learning. This is is just a naive estimation the number could be higher.\n\n\nkenya_dat <- sub_county_melt[county == \"kenya\"]\n\np <- ggplot(kenya_dat, aes(variable, value, tooltip = paste(variable, \" : \", value))) +\n    geom_bar_interactive(stat = \"identity\", width = 0.5, fill =\"slategray2\"  ) +\n    geom_text_interactive(aes(variable, value, label = paste0(value, \"%\")),\n              position = position_dodge(width = 0.5),\n              vjust = 0.001, size = 3)+\n    labs(x = \"Household assets\", y = \"%\",\n         title = \"% of households with various assets 2019 census\",\n         caption = \"twitter\\n@mmburu_w\")+\n    theme_fivethirtyeight()+\n  theme(\n    axis.text = element_text(size = 7, angle = 45, vjust = 1, hjust =1),\n    plot.title = element_text_interactive(size =11)\n  )\np1 <- girafe(ggobj = p, width_svg = 7, height_svg = 4.5, \n  options = list(\n    opts_sizing(rescale = T) )\n  )\n\np1\n\n\n\n\n\np\n\n\n\n\n\n\nMerge shapefile with asset data sets\n\nsub_county_melt[county == \"elgeyo/marakwet\", county := \"keiyo-marakwet\"]\nsub_county_melt[county == \"tharaka-nithi\", county := \"tharaka\"]\nsub_county_melt[county ==  \"taita/taveta\", county := \"taita taveta\"]\nsub_county_melt[county ==  \"nairobi city\", county := \"nairobi\"]\n\ncounty_shapes <- merge(kenya_shapefile, sub_county_melt, by = \"county\") \n\nsetnames(county_shapes, \"value\", \"Percentage\")\n\n\n\nComputer devices data\n\ncomputer <- county_shapes[variable  %in% c(\"Desk Top\\nComputer/\\nLaptop/ Tablet\")]\n\n# this converts to sf object\ncomputer <- st_set_geometry(computer, \"geometry\")\n\n\n\nPercentage of households with computer devices per county\n\nThat is if a household owns a tablet, laptop or a desktop\n\n\n#ttm()\ntm_shape(computer)+\n    tm_borders(col = \"grey\")+\n    tm_fill(col = \"Percentage\")+\n    tm_layout(title = \"% of households with laptop/tablet/desk top \\n 2019 census\",\n              title.size = 1, title.position = c(0.3, 0.95))\n\n\n\n#ttm()\n\n\n\n% of households that can access internet\n\nThis looks like is internet access through mobile phones\n\n\ninternet <- county_shapes[variable == \"Internet\"]\n\ninternet <- st_set_geometry(internet, \"geometry\")\n\n#ttm()\ntm_shape(internet)+\n    tm_borders(col = \"grey\")+\n    tm_fill(col = \"Percentage\")+\n    tm_layout(title = \"% of households with internet acess\",\n              title.size = 1, title.position = c(0.3, 0.95))"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "myblog/datacamp/network-analysis-in-r/network_analysis_r.html",
    "href": "myblog/datacamp/network-analysis-in-r/network_analysis_r.html",
    "title": "Network analysis in r",
    "section": "",
    "text": "Here you will learn how to create an igraph ‘object’ from data stored in an edgelist. The data are friendships in a group of students. You will also learn how to make a basic visualization of the network.\nEach row of the friends dataframe represents an edge in the network.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(igraph)\nlibrary(knitr)\nfriends <- fread(\"friends.csv\")\n\n\n# Inspect the first few rows of the dataframe 'friends'\nhead(friends) %>% kable\n\n# Convert friends dataframe to a matrix\nfriends.mat <- as.matrix(friends)\n\n# Convert friends matrix to an igraph object\ng <- graph.edgelist(friends.mat, directed = FALSE)\n\n\n# Make a very basic plot of the network\nplot(g)"
  },
  {
    "objectID": "myblog/datacamp/network-analysis-in-r/network_analysis_r.html#neighbors",
    "href": "myblog/datacamp/network-analysis-in-r/network_analysis_r.html#neighbors",
    "title": "Network analysis in r",
    "section": "Neighbors",
    "text": "Neighbors\nOften in network analysis it is important to explore the patterning of connections that exist between vertices. One way is to identify neighboring vertices of each vertex. You can then determine which neighboring vertices are shared even by unconnected vertices indicating how two vertices may have an indirect relationship through others. In this exercise you will learn how to identify neighbors and shared neighbors between pairs of vertices.\n\n# Identify all neighbors of vertex 12 regardless of direction\nneighbors(g, '12', mode = c('all'))\n\n# Identify other vertices that direct edges towards vertex 12\nneighbors(g, '12', mode = c('in'))\n\n# Identify any vertices that receive an edge from vertex 42 and direct an edge to vertex 124\nn1 <-neighbors(g, '42', mode = c('out'))\nn2 <- neighbors(g, '124', mode = c('in'))\nintersection(n1, n2)"
  },
  {
    "objectID": "myblog/datacamp/network-analysis-in-r/network_analysis_r.html#finding-longest-path-between-two-vertices",
    "href": "myblog/datacamp/network-analysis-in-r/network_analysis_r.html#finding-longest-path-between-two-vertices",
    "title": "Network analysis in r",
    "section": "Finding longest path between two vertices",
    "text": "Finding longest path between two vertices\n\nWhat is the longest possible path in a network referred to as?\nDiameter"
  },
  {
    "objectID": "myblog/datacamp/network-analysis-in-r/network_analysis_r.html#network-density-and-average-path-length",
    "href": "myblog/datacamp/network-analysis-in-r/network_analysis_r.html#network-density-and-average-path-length",
    "title": "Network analysis in r",
    "section": "Network density and average path length",
    "text": "Network density and average path length\nThe first graph level metric you will explore is the density of a graph. This is essentially the proportion of all potential edges between vertices that actually exist in the network graph. It is an indicator of how well connected the vertices of the graph are.\nAnother measure of how interconnected a network is average path length. This is calculated by determining the mean of the lengths of the shortest paths between all pairs of vertices in the network. The longest path length between any pair of vertices is called the diameter of the network graph. You will calculate the diameter and average path length of the original graph g.\n\n# Get density of a graph\ngd <- edge_density(g)\n\n# Get the diameter of the graph g\ndiameter(g, directed = FALSE)\n\n# Get the average path length of the graph g\ng.apl <- mean_distance(g, directed = FALSE)\ng.apl\n\n\nIf a graph has 7 vertices there are 21 possible edges in the network. If 14 edges exist, what is the density of the network?\n0.67"
  },
  {
    "objectID": "myblog/datacamp/network-analysis-in-r/network_analysis_r.html#randomization-quiz",
    "href": "myblog/datacamp/network-analysis-in-r/network_analysis_r.html#randomization-quiz",
    "title": "Network analysis in r",
    "section": "Randomization quiz",
    "text": "Randomization quiz\n\nRandomization tests enable you to identify:\nWhether features of your original network are particularly unusual."
  },
  {
    "objectID": "myblog/datacamp/network-analysis-in-r/network_analysis_r.html#what-does-assortativity-measure",
    "href": "myblog/datacamp/network-analysis-in-r/network_analysis_r.html#what-does-assortativity-measure",
    "title": "Network analysis in r",
    "section": "What does assortativity measure",
    "text": "What does assortativity measure\n\nHow likely vertices are to connect to others that share some attribute in common."
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html",
    "title": "ML with tree based models in r",
    "section": "",
    "text": "Let’s get started and build our first classification tree. A classification tree is a decision tree that performs a classification (vs regression) task. You will train a decision tree model to understand which loan applications are at higher risk of default using a subset of the German Credit Dataset. The response variable, called “default”, indicates whether the loan went into a default or not, which means this is a binary classification problem (there are just two classes). You will use the rpart package to fit the decision tree and the rpart.plot package to visualize the tree.\n\n# Look at the data\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(rpart)\nlibrary(rpart.plot)\ncredit <- fread(\"credit.csv\", stringsAsFactors = TRUE) \ncredit[, default := factor(default)]\ncreditsub <- credit[sample(1:nrow(credit), 522),]\nstr(creditsub)\n\nClasses 'data.table' and 'data.frame':  522 obs. of  17 variables:\n $ checking_balance    : Factor w/ 4 levels \"1 - 200 DM\",\"< 0 DM\",..: 4 3 2 4 1 4 2 4 1 1 ...\n $ months_loan_duration: int  12 6 12 12 18 10 24 15 36 14 ...\n $ credit_history      : Factor w/ 5 levels \"critical\",\"good\",..: 1 2 1 2 1 2 2 2 2 2 ...\n $ purpose             : Factor w/ 6 levels \"business\",\"car\",..: 5 5 2 5 5 2 5 4 2 1 ...\n $ amount              : int  2331 2116 1409 1736 1795 1287 2384 4623 6948 1410 ...\n $ savings_balance     : Factor w/ 5 levels \"100 - 500 DM\",..: 5 3 3 3 3 5 3 1 3 2 ...\n $ employment_duration : Factor w/ 5 levels \"1 - 4 years\",..: 4 1 4 2 4 4 4 1 1 4 ...\n $ percent_of_income   : int  1 2 4 3 3 4 4 3 2 1 ...\n $ years_at_residence  : int  4 2 3 4 4 2 4 2 2 2 ...\n $ age                 : int  49 41 54 31 48 45 64 40 35 35 ...\n $ other_credit        : Factor w/ 3 levels \"bank\",\"none\",..: 2 2 2 2 1 2 1 2 2 2 ...\n $ housing             : Factor w/ 3 levels \"other\",\"own\",..: 2 2 2 2 3 2 3 2 3 2 ...\n $ existing_loans_count: int  1 1 1 1 2 1 1 1 1 1 ...\n $ job                 : Factor w/ 4 levels \"management\",\"skilled\",..: 2 2 2 4 4 4 4 1 1 2 ...\n $ dependents          : int  1 1 1 1 1 1 1 1 1 1 ...\n $ phone               : Factor w/ 2 levels \"no\",\"yes\": 2 2 1 1 2 1 1 2 2 2 ...\n $ default             : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 2 1 1 ...\n - attr(*, \".internal.selfref\")=<externalptr> \n\n# Create the model\ncredit_model <- rpart(formula = default ~ ., \n                      data = creditsub, \n                      method = \"class\")\n\n# Display the results\nrpart.plot(x = credit_model, yesno = 2, type = 0, extra = 0)"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#traintest-split",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#traintest-split",
    "title": "ML with tree based models in r",
    "section": "Train/test split",
    "text": "Train/test split\nFor this exercise, you’ll randomly split the German Credit Dataset into two pieces: a training set (80%) called credit_train and a test set (20%) that we will call credit_test. We’ll use these two sets throughout the chapter.\n\n# Total number of rows in the credit data frame\nn <- nrow(credit)\n\n# Number of rows for the training set (80% of the dataset)\nn_train <- round(.8 * n) \n\n# Create a vector of indices which is an 80% random sample\nset.seed(123)\ntrain_indices <- sample(1:n, n_train)\n\n# Subset the credit data frame to training indices only\ncredit_train <- credit[train_indices, ]  \n  \n# Exclude the training indices to create the test set\ncredit_test <- credit[-train_indices, ]  \n\n# Train the model (to predict 'default')\ncredit_model <- rpart(formula = default ~., \n                      data = credit_train, \n                      method = \"class\")\n\n# Look at the model output                      \nprint(credit_model)\n\nn= 800 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 800 230 no (0.7125000 0.2875000)  \n    2) checking_balance=> 200 DM,unknown 365  48 no (0.8684932 0.1315068) *\n    3) checking_balance=1 - 200 DM,< 0 DM 435 182 no (0.5816092 0.4183908)  \n      6) months_loan_duration< 22.5 259  85 no (0.6718147 0.3281853)  \n       12) credit_history=critical,good,poor 235  68 no (0.7106383 0.2893617)  \n         24) months_loan_duration< 11.5 70  11 no (0.8428571 0.1571429) *\n         25) months_loan_duration>=11.5 165  57 no (0.6545455 0.3454545)  \n           50) amount>=1282 112  30 no (0.7321429 0.2678571) *\n           51) amount< 1282 53  26 yes (0.4905660 0.5094340)  \n            102) purpose=business,education,furniture/appliances 34  12 no (0.6470588 0.3529412) *\n            103) purpose=car,renovations 19   4 yes (0.2105263 0.7894737) *\n       13) credit_history=perfect,very good 24   7 yes (0.2916667 0.7083333) *\n      7) months_loan_duration>=22.5 176  79 yes (0.4488636 0.5511364)  \n       14) savings_balance=> 1000 DM,unknown 29   7 no (0.7586207 0.2413793) *\n       15) savings_balance=100 - 500 DM,500 - 1000 DM,< 100 DM 147  57 yes (0.3877551 0.6122449)  \n         30) months_loan_duration< 47.5 119  54 yes (0.4537815 0.5462185)  \n           60) amount>=2313.5 93  45 no (0.5161290 0.4838710)  \n            120) amount< 3026 19   5 no (0.7368421 0.2631579) *\n            121) amount>=3026 74  34 yes (0.4594595 0.5405405)  \n              242) percent_of_income< 2.5 38  15 no (0.6052632 0.3947368)  \n                484) purpose=business,car,education 23   6 no (0.7391304 0.2608696) *\n                485) purpose=car0,furniture/appliances,renovations 15   6 yes (0.4000000 0.6000000) *\n              243) percent_of_income>=2.5 36  11 yes (0.3055556 0.6944444) *\n           61) amount< 2313.5 26   6 yes (0.2307692 0.7692308) *\n         31) months_loan_duration>=47.5 28   3 yes (0.1071429 0.8928571) *"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#compute-confusion-matrix",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#compute-confusion-matrix",
    "title": "ML with tree based models in r",
    "section": "Compute confusion matrix",
    "text": "Compute confusion matrix\nAs discussed in the previous video, there are a number of different metrics by which you can measure the performance of a classification model. In this exercise, we will evaluate the performance of the model using test set classification error. A confusion matrix is a convenient way to examine the per-class error rates for all classes at once. The confusionMatrix() function from the caret package prints both the confusion matrix and a number of other useful classification metrics such as “Accuracy” (fraction of correctly classified instances). The caret package has been loaded for you.\n\nlibrary(caret)\n# Generate predicted classes using the model object\nclass_prediction <- predict(object = credit_model,  \n                        newdata = credit_test,   \n                        type = \"class\")  \n                            \n# Calculate the confusion matrix for the test set\n\nclass_prediction <- factor(class_prediction, levels = levels(credit_test$default) )\nconfusionMatrix(data = class_prediction,       \n                reference = credit_test$default, positive = \"yes\")  \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  117  44\n       yes  13  26\n                                          \n               Accuracy : 0.715           \n                 95% CI : (0.6471, 0.7764)\n    No Information Rate : 0.65            \n    P-Value [Acc > NIR] : 0.03046         \n                                          \n                  Kappa : 0.3023          \n                                          \n Mcnemar's Test P-Value : 7.08e-05        \n                                          \n            Sensitivity : 0.3714          \n            Specificity : 0.9000          \n         Pos Pred Value : 0.6667          \n         Neg Pred Value : 0.7267          \n             Prevalence : 0.3500          \n         Detection Rate : 0.1300          \n   Detection Prevalence : 0.1950          \n      Balanced Accuracy : 0.6357          \n                                          \n       'Positive' Class : yes"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#compare-models-with-a-different-splitting-criterion",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#compare-models-with-a-different-splitting-criterion",
    "title": "ML with tree based models in r",
    "section": "Compare models with a different splitting criterion",
    "text": "Compare models with a different splitting criterion\nTrain two models that use a different splitting criterion and use the validation set to choose a “best” model from this group. To do this you’ll use the parms argument of the rpart() function. This argument takes a named list that contains values of different parameters you can use to change how the model is trained. Set the parameter split to control the splitting criterion.\n\n# Train a gini-based model\ncredit_model1 <- rpart(formula = default ~ ., \n                       data = credit_train, \n                       method = \"class\",\n                       parms = list(split = \"gini\"))\n\n# Train an information-based model\ncredit_model2 <- rpart(formula = default ~ ., \n                       data = credit_train, \n                       method = \"class\",\n                       parms = list(split = \"information\"))\n\n# Generate predictions on the validation set using the gini model\npred1 <- predict(object = credit_model1, \n             newdata = credit_test,\n             type = \"class\")    \n\n# Generate predictions on the validation set using the information model\npred2 <- predict(object = credit_model2, \n             newdata = credit_test,\n             type = \"class\") \n\ndt_preds <- predict(object = credit_model2, \n             newdata = credit_test,\n             type = \"prob\") \n\n# Compare classification error\nlibrary(Metrics)\nce(actual = credit_test$default, \n   predicted = pred1)\n\n[1] 0.285\n\nce(actual = credit_test$default, \n   predicted = pred2)  \n\n[1] 0.285"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#the-response-is-final_grade-numeric-from-0-to-20-output-target.",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#the-response-is-final_grade-numeric-from-0-to-20-output-target.",
    "title": "ML with tree based models in r",
    "section": "The response is final_grade (numeric: from 0 to 20, output target).",
    "text": "The response is final_grade (numeric: from 0 to 20, output target).\nAfter initial exploration, split the data into training, validation, and test sets. In this chapter, we will introduce the idea of a validation set, which can be used to select a “best” model from a set of competing models. In Chapter 1, we demonstrated a simple way to split the data into two pieces using the sample() function. In this exercise, we will take a slightly different approach to splitting the data that allows us to split the data into more than two parts (here, we want three: train, validation, test). We still use the sample() function, but instead of sampling the indices themselves, we will assign each row to either the training, validation or test sets according to a probability distribution. The dataset grade is already in your workspace.\n\ngrade <- read.csv(\"grade.csv\")\n# Look at the data\nstr(grade)\n\n'data.frame':   395 obs. of  8 variables:\n $ final_grade: num  3 3 5 7.5 5 7.5 5.5 3 9.5 7.5 ...\n $ age        : int  18 17 15 15 16 16 16 17 15 15 ...\n $ address    : chr  \"U\" \"U\" \"U\" \"U\" ...\n $ studytime  : int  2 2 2 3 2 2 2 2 2 2 ...\n $ schoolsup  : chr  \"yes\" \"no\" \"yes\" \"no\" ...\n $ famsup     : chr  \"no\" \"yes\" \"no\" \"yes\" ...\n $ paid       : chr  \"no\" \"no\" \"yes\" \"yes\" ...\n $ absences   : int  6 4 10 2 4 10 0 6 0 0 ...\n\n# Set seed and create assignment\nset.seed(1)\nassignment <- sample(1:3, size = nrow(grade), prob = c(.7, .15, .15), replace = TRUE)\n\n# Create a train, validation and tests from the original data frame \ngrade_train <- grade[assignment == 1, ]    # subset grade to training indices only\ngrade_valid <- grade[assignment == 2, ]  # subset grade to validation indices only\ngrade_test <- grade[assignment == 3, ]   # subset grade to test indices only"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-regression-tree-model",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-regression-tree-model",
    "title": "ML with tree based models in r",
    "section": "Train a regression tree model",
    "text": "Train a regression tree model\nIn this exercise, we will use the grade_train dataset to fit a regression tree using rpart() and visualize it using rpart.plot(). A regression tree plot looks identical to a classification tree plot, with the exception that there will be numeric values in the leaf nodes instead of predicted classes. This is very similar to what we did previously in Chapter 1. When fitting a classification tree, we use method = “class”, however, when fitting a regression tree, we need to set method = “anova”. By default, the rpart() function will make an intelligent guess as to what the method value should be based on the data type of your response column, but it’s recommened that you explictly set the method for reproducibility reasons (since the auto-guesser may change in the future). The grade_train training set is loaded into the workspace.\n\n# Train the model\ngrade_model <- rpart(formula = final_grade ~ ., \n                     data = grade_train, \n                     method = \"anova\")\n\n# Look at the model output                      \nprint(grade_model)\n\nn= 282 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 282 1519.49700 5.271277  \n   2) absences< 0.5 82  884.18600 4.323171  \n     4) paid=no 50  565.50500 3.430000  \n       8) famsup=yes 22  226.36360 2.272727 *\n       9) famsup=no 28  286.52680 4.339286 *\n     5) paid=yes 32  216.46880 5.718750  \n      10) age>=17.5 10   82.90000 4.100000 *\n      11) age< 17.5 22   95.45455 6.454545 *\n   3) absences>=0.5 200  531.38000 5.660000  \n     6) absences>=13.5 42  111.61900 4.904762 *\n     7) absences< 13.5 158  389.43670 5.860759  \n      14) schoolsup=yes 23   50.21739 4.847826 *\n      15) schoolsup=no 135  311.60000 6.033333  \n        30) studytime< 3.5 127  276.30710 5.940945 *\n        31) studytime>=3.5 8   17.00000 7.500000 *\n\n# Plot the tree model\nrpart.plot(x = grade_model, yesno = 2, type = 0, extra = 0)"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-a-regression-tree-model",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-a-regression-tree-model",
    "title": "ML with tree based models in r",
    "section": "Evaluate a regression tree model",
    "text": "Evaluate a regression tree model\nPredict the final grade for all students in the test set. The grade is on a 0-20 scale. Evaluate the model based on test set RMSE (Root Mean Squared Error). RMSE tells us approximately how far away our predictions are from the true values.\n\n# Generate predictions on a test set\npred <- predict(object = grade_model,   # model object \n                newdata = grade_test)  # test dataset\n\n# Compute the RMSE\nrmse(actual = grade_test$final_grade, \n     predicted = pred)\n\n[1] 2.278249"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#tuning-the-model",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#tuning-the-model",
    "title": "ML with tree based models in r",
    "section": "Tuning the model",
    "text": "Tuning the model\nTune (or “trim”) the model using the prune() function by finding the best “CP” value (CP stands for “Complexity Parameter”).\n\n# Plot the \"CP Table\"\nplotcp(grade_model)\n\n\n\n# Print the \"CP Table\"\nprint(grade_model$cptable)\n\n          CP nsplit rel error    xerror       xstd\n1 0.06839852      0 1.0000000 1.0066743 0.09169976\n2 0.06726713      1 0.9316015 1.0185398 0.08663026\n3 0.03462630      2 0.8643344 0.8923588 0.07351895\n4 0.02508343      3 0.8297080 0.9046335 0.08045100\n5 0.01995676      4 0.8046246 0.8920489 0.08153881\n6 0.01817661      5 0.7846679 0.9042142 0.08283114\n7 0.01203879      6 0.7664912 0.8833557 0.07945742\n8 0.01000000      7 0.7544525 0.8987112 0.08200148\n\n# Retrieve optimal cp value based on cross-validated error\nopt_index <- which.min(grade_model$cptable[, \"xerror\"])\ncp_opt <- grade_model$cptable[opt_index, \"CP\"]\n\n# Prune the model (to optimized cp value)\ngrade_model_opt <- prune(tree = grade_model, \n                         cp = cp_opt)\n                          \n# Plot the optimized model\nrpart.plot(x = grade_model_opt, yesno = 2, type = 0, extra = 0)"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#generate-a-grid-of-hyperparameter-values",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#generate-a-grid-of-hyperparameter-values",
    "title": "ML with tree based models in r",
    "section": "Generate a grid of hyperparameter values",
    "text": "Generate a grid of hyperparameter values\nUse expand.grid() to generate a grid of maxdepth and minsplit values.\n\n# Establish a list of possible values for minsplit and maxdepth\nminsplit <- seq(1, 4, 1)\nmaxdepth <- seq(1, 6, 1)\n\n# Create a data frame containing all combinations \nhyper_grid <- expand.grid(minsplit = minsplit, maxdepth = maxdepth)\n\n# Check out the grid\nhead(hyper_grid)\n\n  minsplit maxdepth\n1        1        1\n2        2        1\n3        3        1\n4        4        1\n5        1        2\n6        2        2\n\n# Print the number of grid combinations\nnrow(hyper_grid)\n\n[1] 24"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#generate-a-grid-of-models",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#generate-a-grid-of-models",
    "title": "ML with tree based models in r",
    "section": "Generate a grid of models",
    "text": "Generate a grid of models\nIn this exercise, we will write a simple loop to train a “grid” of models and store the models in a list called grade_models. R users who are familiar with the apply functions in R could think about how this loop could be easily converted into a function applied to a list as an extra-credit thought experiment.\n\n# Number of potential models in the grid\nnum_models <- nrow(hyper_grid)\n\n# Create an empty list to store models\ngrade_models <- list()\n\n# Write a loop over the rows of hyper_grid to train the grid of models\nfor (i in 1:num_models) {\n\n    # Get minsplit, maxdepth values at row i\n    minsplit <- hyper_grid$minsplit[i]\n    maxdepth <- hyper_grid$maxdepth[i]\n\n    # Train a model and store in the list\n    grade_models[[i]] <- rpart(formula = final_grade ~ ., \n                               data = grade_train, \n                               method = \"anova\",\n                               minsplit = minsplit,\n                               maxdepth = maxdepth)\n}\n\nEvaluate the grid Earlier in the chapter we split the dataset into three parts: training, validation and test.\nA dataset that is not used in training is sometimes referred to as a “holdout” set. A holdout set is used to estimate model performance and although both validation and test sets are considered to be holdout data, there is a key difference:\nJust like a test set, a validation set is used to evaluate the performance of a model. The difference is that a validation set is specifically used to compare the performance of a group of models with the goal of choosing a “best model” from the group. All the models in a group are evaluated on the same validation set and the model with the best performance is considered to be the winner. Once you have the best model, a final estimate of performance is computed on the test set. A test set should only ever be used to estimate model performance and should not be used in model selection. Typically if you use a test set more than once, you are probably doing something wrong.\n\n# Number of potential models in the grid\nnum_models <- length(grade_models)\n\n# Create an empty vector to store RMSE values\nrmse_values <- c()\n\n# Write a loop over the models to compute validation RMSE\nfor (i in 1:num_models) {\n\n    # Retrieve the i^th model from the list\n    model <- grade_models[[i]]\n    \n    # Generate predictions on grade_valid \n    pred <- predict(object = model,\n                    newdata = grade_valid)\n    \n    # Compute validation RMSE and add to the \n    rmse_values[i] <- rmse(actual = grade_valid$final_grade, \n                           predicted = pred)\n}\n\n# Identify the model with smallest validation set RMSE\nbest_model <- grade_models[[which.min(rmse_values)]]\n\n# Print the model paramters of the best model\nbest_model$control\n\n$minsplit\n[1] 2\n\n$minbucket\n[1] 1\n\n$cp\n[1] 0.01\n\n$maxcompete\n[1] 4\n\n$maxsurrogate\n[1] 5\n\n$usesurrogate\n[1] 2\n\n$surrogatestyle\n[1] 0\n\n$maxdepth\n[1] 1\n\n$xval\n[1] 10\n\n# Compute test set RMSE on best_model\npred <- predict(object = best_model,\n                newdata = grade_test)\nrmse(actual = grade_test$final_grade, \n     predicted = pred)\n\n[1] 2.124109"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-bagged-tree-model",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-bagged-tree-model",
    "title": "ML with tree based models in r",
    "section": "Train a bagged tree model",
    "text": "Train a bagged tree model\nLet’s start by training a bagged tree model. You’ll be using the bagging() function from the ipred package. The number of bagged trees can be specified using the nbagg parameter, but here we will use the default (25). If we want to estimate the model’s accuracy using the “out-of-bag” (OOB) samples, we can set the the coob parameter to TRUE. The OOB samples are the training obsevations that were not selected into the bootstrapped sample (used in training). Since these observations were not used in training, we can use them instead to evaluate the accuracy of the model (done automatically inside the bagging() function).\n\nlibrary(ipred)\n# Bagging is a randomized model, so let's set a seed (123) for reproducibility\nset.seed(123)\n\n# Train a bagged model\ncredit_model <- bagging(formula = default ~ ., \n                        data = credit_train,\n                        coob = TRUE)\n\n# Print the model\nprint(credit_model)\n\n\nBagging classification trees with 25 bootstrap replications \n\nCall: bagging.data.frame(formula = default ~ ., data = credit_train, \n    coob = TRUE)\n\nOut-of-bag estimate of misclassification error:  0.2537"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#prediction-and-confusion-matrix",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#prediction-and-confusion-matrix",
    "title": "ML with tree based models in r",
    "section": "Prediction and confusion matrix",
    "text": "Prediction and confusion matrix\nAs you saw in the video, a confusion matrix is a very useful tool for examining all possible outcomes of your predictions (true positive, true negative, false positive, false negative). In this exercise, you will predict those who will default using bagged trees. You will also create the confusion matrix using the confusionMatrix() function from the caret package. It’s always good to take a look at the output using the print() function.\n\n# Generate predicted classes using the model object\nclass_prediction <- predict(object = credit_model,    \n                            newdata = credit_test,  \n                            type = \"class\")  # return classification labels\n\n# Print the predicted classes\nprint(class_prediction)\n\n  [1] no  no  no  no  yes no  no  no  no  no  no  no  no  yes no  no  no  no \n [19] no  no  yes no  no  no  no  no  yes no  no  no  no  no  no  no  no  no \n [37] yes yes no  yes no  yes no  no  no  no  no  no  no  yes no  yes no  yes\n [55] yes no  yes no  yes no  no  yes no  no  yes yes no  yes no  no  no  yes\n [73] yes no  no  no  no  no  no  yes no  no  no  no  yes no  no  yes no  no \n [91] no  no  no  yes yes no  no  no  no  no  no  yes no  no  yes no  no  no \n[109] no  no  no  no  no  no  no  no  no  no  no  no  yes no  yes no  no  yes\n[127] yes no  yes no  no  no  no  no  yes no  yes yes no  no  no  no  yes no \n[145] no  no  yes no  no  no  no  yes no  no  no  no  no  no  no  yes no  no \n[163] yes no  yes no  no  no  no  no  no  no  no  no  no  no  no  no  no  no \n[181] no  no  yes yes yes no  yes no  no  no  no  no  yes no  no  no  yes no \n[199] no  yes\nLevels: no yes\n\n# Calculate the confusion matrix for the test set\nconfusionMatrix(data =  class_prediction,     \n                reference =  credit_test$default, positive = \"yes\")  \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  119  33\n       yes  11  37\n                                          \n               Accuracy : 0.78            \n                 95% CI : (0.7161, 0.8354)\n    No Information Rate : 0.65            \n    P-Value [Acc > NIR] : 4.557e-05       \n                                          \n                  Kappa : 0.4787          \n                                          \n Mcnemar's Test P-Value : 0.001546        \n                                          \n            Sensitivity : 0.5286          \n            Specificity : 0.9154          \n         Pos Pred Value : 0.7708          \n         Neg Pred Value : 0.7829          \n             Prevalence : 0.3500          \n         Detection Rate : 0.1850          \n   Detection Prevalence : 0.2400          \n      Balanced Accuracy : 0.7220          \n                                          \n       'Positive' Class : yes"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#predict-on-a-test-set-and-compute-auc",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#predict-on-a-test-set-and-compute-auc",
    "title": "ML with tree based models in r",
    "section": "Predict on a test set and compute AUC",
    "text": "Predict on a test set and compute AUC\nIn binary classification problems, we can predict numeric values instead of class labels. In fact, class labels are created only after you use the model to predict a raw, numeric, predicted value for a test point. The predicted label is generated by applying a threshold to the predicted value, such that all tests points with predicted value greater than that threshold get a predicted label of “1” and, points below that threshold get a predicted label of “0”. In this exercise, generate predicted values (rather than class labels) on the test set and evaluate performance based on AUC (Area Under the ROC Curve). The AUC is a common metric for evaluating the discriminatory ability of a binary classification model.\n\n# Generate predictions on the test set\npred <- predict(object = credit_model,\n                newdata = credit_test,\n                type = \"prob\")\n\n# `pred` is a matrix\nclass(pred)\n\n[1] \"matrix\" \"array\" \n\n# Look at the pred format\nhead(pred)\n\n       no  yes\n[1,] 0.92 0.08\n[2,] 0.92 0.08\n[3,] 1.00 0.00\n[4,] 1.00 0.00\n[5,] 0.16 0.84\n[6,] 0.84 0.16\n\n# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)\ncredit_ipred_model_test_auc <- auc(actual = ifelse(credit_test$default == \"yes\", 1, 0), \n    predicted = pred[,\"yes\"])  \n\ncredit_ipred_model_test_auc\n\n[1] 0.8084066"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#cross-validate-a-bagged-tree-model-in-caret",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#cross-validate-a-bagged-tree-model-in-caret",
    "title": "ML with tree based models in r",
    "section": "Cross-validate a bagged tree model in caret",
    "text": "Cross-validate a bagged tree model in caret\nUse caret::train() with the “treebag” method to train a model and evaluate the model using cross-validated AUC. The caret package allows the user to easily cross-validate any model across any relevant performance metric. In this case, we will use 5-fold cross validation and evaluate cross-validated AUC (Area Under the ROC Curve).\n\n# Specify the training configuration\nctrl_bag <- trainControl(method = \"cv\",     # Cross-validation\n                     number = 5,      # 5 folds\n                     classProbs = TRUE,                  # For AUC\n                     summaryFunction = twoClassSummary)  # For AUC\n\n# Cross validate the credit model using \"treebag\" method; \n# Track AUC (Area under the ROC curve)\nset.seed(1)  # for reproducibility\ncredit_caret_model <- train(default ~ .,\n                            data = credit_train, \n                            method = \"treebag\",\n                            metric = \"ROC\",\n                            trControl = ctrl_bag)\n\n# Look at the model object\nprint(credit_caret_model)\n\nBagged CART \n\n800 samples\n 16 predictor\n  2 classes: 'no', 'yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 640, 640, 640, 640, 640 \nResampling results:\n\n  ROC       Sens       Spec     \n  0.744508  0.8736842  0.4173913\n\n# Inspect the contents of the model list \nnames(credit_caret_model)\n\n [1] \"method\"       \"modelInfo\"    \"modelType\"    \"results\"      \"pred\"        \n [6] \"bestTune\"     \"call\"         \"dots\"         \"metric\"       \"control\"     \n[11] \"finalModel\"   \"preProcess\"   \"trainingData\" \"ptype\"        \"resample\"    \n[16] \"resampledCM\"  \"perfNames\"    \"maximize\"     \"yLimits\"      \"times\"       \n[21] \"levels\"       \"terms\"        \"coefnames\"    \"contrasts\"    \"xlevels\"     \n\n# Print the CV AUC\ncredit_caret_model$results[,\"ROC\"]\n\n[1] 0.744508"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#generate-predictions-from-the-caret-model",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#generate-predictions-from-the-caret-model",
    "title": "ML with tree based models in r",
    "section": "Generate predictions from the caret model",
    "text": "Generate predictions from the caret model\nGenerate predictions on a test set for the caret model.\n\n# Generate predictions on the test set\nbag_preds <- predict(object = credit_caret_model, \n                newdata = credit_test,\n                type = \"prob\")\n\n# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)\ncredit_caret_model_test_auc <- auc(actual = ifelse(credit_test$default == \"yes\", 1, 0), \n                    predicted = pred[,\"yes\"])\n\ncredit_caret_model_test_auc\n\n[1] 0.8084066"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#compare-test-set-performance-to-cv-performance",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#compare-test-set-performance-to-cv-performance",
    "title": "ML with tree based models in r",
    "section": "Compare test set performance to CV performance",
    "text": "Compare test set performance to CV performance\nIn this excercise, you will print test set AUC estimates that you computed in previous exercises. These two methods use the same code underneath, so the estimates should be very similar.\nThe credit_ipred_model_test_auc object stores the test set AUC from the model trained using the ipred::bagging() function. The credit_caret_model_test_auc object stores the test set AUC from the model trained using the caret::train() function with method = “treebag”. Lastly, we will print the 5-fold cross-validated estimate of AUC that is stored within the credit_caret_model object. This number will be a more accurate estimate of the true model performance since we have averaged the performance over five models instead of just one.\nOn small datasets like this one, the difference between test set model performance estimates and cross-validated model performance estimates will tend to be more pronounced. When using small data, it’s recommended to use cross-validated estimates of performance because they are more stable.\n\n# Print ipred::bagging test set AUC estimate\nprint(credit_ipred_model_test_auc)\n\n[1] 0.8084066\n\n# Print caret \"treebag\" test set AUC estimate\nprint(credit_caret_model_test_auc)\n\n[1] 0.8084066\n\n# Compare to caret 5-fold cross-validated AUC\ncredit_caret_model$results[, \"ROC\"]\n\n[1] 0.744508"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-random-forest-model",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-random-forest-model",
    "title": "ML with tree based models in r",
    "section": "Train a Random Forest model",
    "text": "Train a Random Forest model\nHere you will use the randomForest() function from the randomForest package to train a Random Forest classifier to predict loan default.\n\nlibrary(randomForest)\n# Train a Random Forest\nset.seed(1)  # for reproducibility\ncredit_model <- randomForest(formula = default ~ ., \n                             data = credit_train)\n                             \n# Print the model output                             \nprint(credit_model)\n\n\nCall:\n randomForest(formula = default ~ ., data = credit_train) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n        OOB estimate of  error rate: 24.12%\nConfusion matrix:\n     no yes class.error\nno  521  49  0.08596491\nyes 144  86  0.62608696"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-out-of-bag-error",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-out-of-bag-error",
    "title": "ML with tree based models in r",
    "section": "Evaluate out-of-bag error",
    "text": "Evaluate out-of-bag error\nHere you will plot the OOB error as a function of the number of trees trained, and extract the final OOB error of the Random Forest model from the trained model object.\n\n# Grab OOB error matrix & take a look\nerr <- credit_model$err.rate\nhead(err)\n\n           OOB        no       yes\n[1,] 0.3310105 0.2400000 0.5402299\n[2,] 0.3519313 0.2283951 0.6338028\n[3,] 0.3164129 0.1912833 0.6067416\n[4,] 0.3130564 0.1886792 0.6142132\n[5,] 0.3039890 0.1776062 0.6172249\n[6,] 0.2957560 0.1713222 0.6036866\n\n# Look at final OOB error rate (last row in err matrix)\noob_err <- err[nrow(err), \"OOB\"]\nprint(oob_err)\n\n    OOB \n0.24125 \n\n# Plot the model trained in the previous exercise\nplot(credit_model)\n\n# Add a legend since it doesn't have one by default\nlegend(x = \"right\", \n       legend = colnames(err),\n       fill = 1:ncol(err))"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-model-performance-on-a-test-set",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-model-performance-on-a-test-set",
    "title": "ML with tree based models in r",
    "section": "Evaluate model performance on a test set",
    "text": "Evaluate model performance on a test set\nUse the caret::confusionMatrix() function to compute test set accuracy and generate a confusion matrix. Compare the test set accuracy to the OOB accuracy.\n\n# Generate predicted classes using the model object\nclass_prediction <- predict(object = credit_model,   # model object \n                            newdata = credit_test,  # test dataset\n                            type = \"class\") # return classification labels\n                            \n# Calculate the confusion matrix for the test set\ncm <- confusionMatrix(data = class_prediction,       # predicted classes\n                      reference = credit_test$default, positive = \"yes\")  # actual classes\nprint(cm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  no yes\n       no  121  39\n       yes   9  31\n                                          \n               Accuracy : 0.76            \n                 95% CI : (0.6947, 0.8174)\n    No Information Rate : 0.65            \n    P-Value [Acc > NIR] : 0.0005292       \n                                          \n                  Kappa : 0.4146          \n                                          \n Mcnemar's Test P-Value : 2.842e-05       \n                                          \n            Sensitivity : 0.4429          \n            Specificity : 0.9308          \n         Pos Pred Value : 0.7750          \n         Neg Pred Value : 0.7562          \n             Prevalence : 0.3500          \n         Detection Rate : 0.1550          \n   Detection Prevalence : 0.2000          \n      Balanced Accuracy : 0.6868          \n                                          \n       'Positive' Class : yes             \n                                          \n\n# Compare test set accuracy to OOB accuracy\npaste0(\"Test Accuracy: \", cm$overall[1])\n\n[1] \"Test Accuracy: 0.76\"\n\npaste0(\"OOB Accuracy: \", 1 - oob_err)\n\n[1] \"OOB Accuracy: 0.75875\""
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#advantage-of-oob-error",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#advantage-of-oob-error",
    "title": "ML with tree based models in r",
    "section": "Advantage of OOB error",
    "text": "Advantage of OOB error\nWhat is the main advantage of using OOB error instead of validation or test error? - If you evaluate your model using OOB error, then you don’t need to create a separate test set"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-test-set-auc",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-test-set-auc",
    "title": "ML with tree based models in r",
    "section": "Evaluate test set AUC",
    "text": "Evaluate test set AUC\nIn Chapter 3, we learned about the AUC metric for evaluating binary classification models. In this exercise, you will compute test set AUC for the Random Forest model.\n\n# Generate predictions on the test set\npred <- predict(object = credit_model,\n            newdata = credit_test,\n            type = \"prob\")\n\n# `pred` is a matrix\nclass(pred)\n\n[1] \"matrix\" \"array\"  \"votes\" \n\n# Look at the pred format\nhead(pred)\n\n     no   yes\n1 0.910 0.090\n2 0.892 0.108\n3 0.992 0.008\n4 0.952 0.048\n5 0.224 0.776\n6 0.846 0.154\n\n# Compute the AUC (`actual` must be a binary 1/0 numeric vector)\nauc(actual = ifelse(credit_test$default == \"yes\", 1, 0), \n    predicted = pred[,\"yes\"])                    \n\n[1] 0.8175824"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#tuning-a-random-forest-via-mtry",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#tuning-a-random-forest-via-mtry",
    "title": "ML with tree based models in r",
    "section": "Tuning a Random Forest via mtry",
    "text": "Tuning a Random Forest via mtry\nIn this exercise, you will use the randomForest::tuneRF() to tune mtry (by training several models). This function is a specific utility to tune the mtry parameter based on OOB error, which is helpful when you want a quick & easy way to tune your model. A more generic way of tuning Random Forest parameters will be presented in the following exercise.\n\n# Execute the tuning process\nset.seed(1)              \nres <- tuneRF(x = subset(credit_train, select = -default),\n              y = credit_train$default,\n              ntreeTry = 500)\n\nmtry = 4  OOB error = 24.12% \nSearching left ...\nmtry = 2    OOB error = 24.5% \n-0.01554404 0.05 \nSearching right ...\nmtry = 8    OOB error = 23.87% \n0.01036269 0.05 \n\n\n\n\n# Look at results\nprint(res)\n\n      mtry OOBError\n2.OOB    2  0.24500\n4.OOB    4  0.24125\n8.OOB    8  0.23875\n\n# Find the mtry value that minimizes OOB Error\nmtry_opt <- res[,\"mtry\"][which.min(res[,\"OOBError\"])]\nprint(mtry_opt)\n\n8.OOB \n    8 \n\n# If you just want to return the best RF model (rather than results)\n# you can set `doBest = TRUE` in `tuneRF()` to return the best RF model\n# instead of a set performance matrix."
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#tuning-a-random-forest-via-tree-depth",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#tuning-a-random-forest-via-tree-depth",
    "title": "ML with tree based models in r",
    "section": "Tuning a Random Forest via tree depth",
    "text": "Tuning a Random Forest via tree depth\nIn Chapter 2, we created a manual grid of hyperparameters using the expand.grid() function and wrote code that trained and evaluated the models of the grid in a loop. In this exercise, you will create a grid of mtry, nodesize and sampsize values. In this example, we will identify the “best model” based on OOB error. The best model is defined as the model from our grid which minimizes OOB error. Keep in mind that there are other ways to select a best model from a grid, such as choosing the best model based on validation AUC. However, for this exercise, we will use the built-in OOB error calculations instead of using a separate validation set.\n\n# Establish a list of possible values for mtry, nodesize and sampsize\nmtry <- seq(4, ncol(credit_train) * 0.8, 2)\nnodesize <- seq(3, 8, 2)\nsampsize <- nrow(credit_train) * c(0.7, 0.8)\n\n# Create a data frame containing all combinations \nhyper_grid <- expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize)\n\n# Create an empty vector to store OOB error values\noob_err <- c()\n\n# Write a loop over the rows of hyper_grid to train the grid of models\nfor (i in 1:nrow(hyper_grid)) {\n\n    # Train a Random Forest model\n    model <- randomForest(formula = default ~ ., \n                          data = credit_train,\n                          mtry = hyper_grid$mtry[i],\n                          nodesize = hyper_grid$nodesize[i],\n                          sampsize = hyper_grid$sampsize[i])\n                          \n    # Store OOB error for the model                      \n    oob_err[i] <- model$err.rate[nrow(model$err.rate), \"OOB\"]\n}\n\n# Identify optimal set of hyperparmeters based on OOB error\nopt_i <- which.min(oob_err)\nprint(hyper_grid[opt_i,])\n\n   mtry nodesize sampsize\n17    6        3      640"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#bagged-trees-vs.-boosted-trees",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#bagged-trees-vs.-boosted-trees",
    "title": "ML with tree based models in r",
    "section": "Bagged trees vs. boosted trees",
    "text": "Bagged trees vs. boosted trees\nWhat is the main difference between bagged trees and boosted trees?\n\nBoosted trees improve the model fit by considering past fits and bagged trees do not"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-gbm-model",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#train-a-gbm-model",
    "title": "ML with tree based models in r",
    "section": "Train a GBM model",
    "text": "Train a GBM model\nHere you will use the gbm() function to train a GBM classifier to predict loan default. You will train a 10,000-tree GBM on the credit_train dataset, which is pre-loaded into your workspace. Using such a large number of trees (10,000) is probably not optimal for a GBM model, but we will build more trees than we need and then select the optimal number of trees based on early performance-based stopping. The best GBM model will likely contain fewer trees than we started with. For binary classification, gbm() requires the response to be encoded as 0/1 (numeric), so we will have to convert from a “no/yes” factor to a 0/1 numeric response column.\nAlso, the the gbm() function requires the user to specify a distribution argument. For a binary classification problem, you should set distribution = “bernoulli”. The Bernoulli distribution models a binary response.\n\nlibrary(gbm)\n# Convert \"yes\" to 1, \"no\" to 0\ncredit_train$default <- ifelse(as.character(credit_train$default) == \"yes\", 1, 0) \n\n# Train a 10000-tree GBM model\nset.seed(1)\ncredit_model <- gbm(formula = default ~ ., \n                    distribution = \"bernoulli\", \n                    data = credit_train,\n                    n.trees =  10000)\n                    \n# Print the model object                    \nprint(credit_model)\n\ngbm(formula = default ~ ., distribution = \"bernoulli\", data = credit_train, \n    n.trees = 10000)\nA gradient boosted model with bernoulli loss function.\n10000 iterations were performed.\nThere were 16 predictors of which 16 had non-zero influence.\n\n# summary() prints variable importance\nsummary(credit_model)\n\n\n\n\n                                      var    rel.inf\namount                             amount 22.0897595\nage                                   age 17.9626175\ncredit_history             credit_history 10.6369658\npurpose                           purpose 10.2584546\nemployment_duration   employment_duration  8.8596192\nchecking_balance         checking_balance  6.4650840\nmonths_loan_duration months_loan_duration  5.8863990\nsavings_balance           savings_balance  3.7722735\njob                                   job  2.9418015\nother_credit                 other_credit  2.8613862\nhousing                           housing  2.5237773\nyears_at_residence     years_at_residence  2.3409228\npercent_of_income       percent_of_income  1.7687143\nphone                               phone  0.6373101\nexisting_loans_count existing_loans_count  0.5870700\ndependents                     dependents  0.4078447"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#prediction-using-a-gbm-model",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#prediction-using-a-gbm-model",
    "title": "ML with tree based models in r",
    "section": "Prediction using a GBM model",
    "text": "Prediction using a GBM model\nThe gbm package uses a predict() function to generate predictions from a model, similar to many other machine learning packages in R. When you see a function like predict() that works on many different types of input (a GBM model, a RF model, a GLM model, etc), that indicates that predict() is an “alias” for a GBM-specific version of that function. The GBM specific version of that function is predict.gbm(), but for convenience sake, we can just use predict() (either works).\nOne thing that’s particular to the predict.gbm() however, is that you need to specify the number of trees used in the prediction. There is no default, so you have to specify this manually. For now, we can use the same number of trees that we specified when training the model, which is 10,000 (though this may not be the optimal number to use).\nAnother argument that you can specify is type, which is only relevant to Bernoulli and Poisson distributed outcomes. When using Bernoulli loss, the returned value is on the log odds scale by default and for Poisson, it’s on the log scale. If instead you specify type = “response”, then gbm converts the predicted values back to the same scale as the outcome. This will convert the predicted values into probabilities for Bernoulli and expected counts for Poisson.\n\n# Since we converted the training response col, let's also convert the test response col\ncredit_test$default <- ifelse(credit_test$default == \"yes\", 1, 0) \n\n# Generate predictions on the test set\npreds1 <- predict(object = credit_model, \n                  newdata = credit_test,\n                  n.trees = 10000 )\n\n# Generate predictions on the test set (scale to response)\npreds2 <- predict(object = credit_model, \n                  newdata = credit_test,\n                  n.trees = 10000,\n                  type = \"response\")\n\n# Compare the range of the two sets of predictions\nrange(preds1)\n\n[1] -6.004812  4.646991\n\nrange(preds2)\n\n[1] 0.002460783 0.990500685"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-test-set-auc-1",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#evaluate-test-set-auc-1",
    "title": "ML with tree based models in r",
    "section": "Evaluate test set AUC",
    "text": "Evaluate test set AUC\nCompute test set AUC of the GBM model for the two sets of predictions. We will notice that they are the same value. That’s because AUC is a rank-based metric, so changing the actual values does not change the value of the AUC.\nHowever, if we were to use a scale-aware metric like RMSE to evaluate performance, we would want to make sure we converted the predictions back to the original scale of the response.\n\n# Generate the test set AUCs using the two sets of preditions & compare\nauc(actual = credit_test$default, predicted = preds1)  #default\n\n[1] 0.7142857\n\nauc(actual = credit_test$default, predicted = preds2)  #rescaled\n\n[1] 0.7142857"
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#early-stopping-in-gbms",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#early-stopping-in-gbms",
    "title": "ML with tree based models in r",
    "section": "Early stopping in GBMs",
    "text": "Early stopping in GBMs\nUse the gbm.perf() function to estimate the optimal number of boosting iterations (aka n.trees) for a GBM model object using both OOB and CV error. When you set out to train a large number of trees in a GBM (such as 10,000) and you use a validation method to determine an earlier (smaller) number of trees, then that’s called “early stopping”. The term “early stopping” is not unique to GBMs, but can describe auto-tuning the number of iterations in an iterative learning algorithm.\n\n# Optimal ntree estimate based on OOB\nntree_opt_oob <- gbm.perf(object = credit_model, \n                          method = \"OOB\", \n                          oobag.curve = TRUE)\n\n\n\n\n\n\n# Train a CV GBM model\nset.seed(1)\ncredit_model_cv <- gbm(formula = default ~ ., \n                       distribution = \"bernoulli\", \n                       data = credit_train,\n                       n.trees = 10000,\n                       cv.folds = 5)\n\n# Optimal ntree estimate based on CV\nntree_opt_cv <- gbm.perf(object = credit_model_cv , \n                         method = \"cv\")\n\n\n\n# Compare the estimates                         \nprint(paste0(\"Optimal n.trees (OOB Estimate): \", ntree_opt_oob))                         \n\n[1] \"Optimal n.trees (OOB Estimate): 76\"\n\nprint(paste0(\"Optimal n.trees (CV Estimate): \", ntree_opt_cv))\n\n[1] \"Optimal n.trees (CV Estimate): 127\""
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#oob-vs-cv-based-early-stopping",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#oob-vs-cv-based-early-stopping",
    "title": "ML with tree based models in r",
    "section": "OOB vs CV-based early stopping",
    "text": "OOB vs CV-based early stopping\nIn the previous exercise, we used OOB error and cross-validated error to estimate the optimal number of trees in the GBM. These are two different ways to estimate the optimal number of trees, so in this exercise we will compare the performance of the models on a test set. We can use the same model object to make both of these estimates since the predict.gbm() function allows you to use any subset of the total number of trees (in our case, the total number is 10,000).\n\n# Generate predictions on the test set using ntree_opt_oob number of trees\npreds1 <- predict(object = credit_model, \n                  newdata = credit_test,\n                  n.trees = ntree_opt_oob)\n                  \n# Generate predictions on the test set using ntree_opt_cv number of trees\ngbm_preds <- predict(object = credit_model, \n                  newdata = credit_test,\n                  n.trees = ntree_opt_cv)   \n\n# Generate the test set AUCs using the two sets of preditions & compare\nauc1 <- auc(actual = credit_test$default, predicted = preds1)  #OOB\nauc2 <- auc(actual = credit_test$default, predicted =gbm_preds)  #CV \n\n# Compare AUC \nprint(paste0(\"Test set AUC (OOB): \", auc1))                         \n\n[1] \"Test set AUC (OOB): 0.802527472527472\"\n\nprint(paste0(\"Test set AUC (CV): \", auc2))\n\n[1] \"Test set AUC (CV): 0.792527472527473\""
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#compare-all-models-based-on-auc",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#compare-all-models-based-on-auc",
    "title": "ML with tree based models in r",
    "section": "Compare all models based on AUC",
    "text": "Compare all models based on AUC\nIn this final exercise, we will perform a model comparison across all types of models that we’ve learned about so far: Decision Trees, Bagged Trees, Random Forest and Gradient Boosting Machine (GBM). The models were all trained on the same training set, credit_train, and predictions were made for the credit_test dataset.\nWe have pre-loaded four sets of test set predictions, generated using the models we trained in previous chapters (one for each model type). The numbers stored in the prediction vectors are the raw predicted values themselves – not the predicted class labels. Using the raw predicted values, we can calculate test set AUC for each model and compare the results.\n\n#credit_train$default <-  factor(credit_train$default, levels  = c(0, 1))\n#credit_test$default <-  factor(credit_test$default, levels = c(0, 1))\n\nmyFolds <- createFolds(credit_train$default, k = 5)\n# tuneGridRf <- data.frame(\n#   .mtry = c(2, 3, 7),\n#   .splitrule = \"variance\",\n#   .min.node.size = 5\n# )\n\nmyControl <- trainControl(\n  method = \"cv\",\n  number = 5,\n  summaryFunction = twoClassSummary,\n  classProbs = TRUE, # IMPORTANT!\n  verboseIter = FALSE,\n  index = myFolds\n)\n\ny =  factor(credit_train$default,levels = c(1,0),  labels = c(\"yes\", \"no\"))\nx <- credit_train %>% select(-default)\nmodel_rf <- train(x =  x ,\n                  y= y,\n                  method = \"ranger\",\n                  classification = TRUE,\n                  metric = \"ROC\",\n                  trControl = myControl)\n \nrf_preds <- predict(model_rf, newdata = credit_test, type = \"prob\")\n\nrf_preds <- rf_preds[, \"yes\"]\n\nmodel_bag <- train(x = x, \n                   y = y,\n                   method = \"treebag\",\n                   metric = \"ROC\",\n                   trControl = myControl)\n\n\nbag_preds <- predict(model_bag, newdata = credit_test, type = \"prob\")\n\nbag_preds  <-bag_preds[, \"yes\"]\n\n\nhyperparams_gbm <- expand.grid(n.trees = seq(100,500, by = 50), \n                           interaction.depth = 1:7, \n                           shrinkage = seq(0.1, 0.9, by = .1), \n                           n.minobsinnode = seq(10, 30, 10))\nmodel_gbm <- train(x = x,\n                   y = y,\n                   method = \"gbm\",\n                   #tuneGrid = hyperparams_gbm,\n                   metric = \"ROC\",\n                   trControl = myControl)\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2650            -nan     0.1000    0.0039\n     2        1.2427            -nan     0.1000    0.0095\n     3        1.2226            -nan     0.1000    0.0072\n     4        1.2078            -nan     0.1000    0.0015\n     5        1.1963            -nan     0.1000    0.0018\n     6        1.1816            -nan     0.1000    0.0039\n     7        1.1712            -nan     0.1000    0.0007\n     8        1.1601            -nan     0.1000    0.0007\n     9        1.1506            -nan     0.1000   -0.0010\n    10        1.1402            -nan     0.1000    0.0044\n    20        1.0561            -nan     0.1000    0.0007\n    40        0.9714            -nan     0.1000   -0.0006\n    60        0.9192            -nan     0.1000   -0.0048\n    80        0.8788            -nan     0.1000   -0.0041\n   100        0.8515            -nan     0.1000   -0.0014\n   120        0.8191            -nan     0.1000   -0.0056\n   140        0.7927            -nan     0.1000   -0.0041\n   150        0.7834            -nan     0.1000   -0.0037\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2528            -nan     0.1000    0.0081\n     2        1.2246            -nan     0.1000    0.0064\n     3        1.1934            -nan     0.1000    0.0106\n     4        1.1729            -nan     0.1000    0.0022\n     5        1.1475            -nan     0.1000    0.0070\n     6        1.1305            -nan     0.1000    0.0000\n     7        1.1180            -nan     0.1000    0.0005\n     8        1.0926            -nan     0.1000    0.0005\n     9        1.0726            -nan     0.1000    0.0045\n    10        1.0576            -nan     0.1000   -0.0013\n    20        0.9674            -nan     0.1000   -0.0046\n    40        0.8488            -nan     0.1000   -0.0153\n    60        0.7727            -nan     0.1000   -0.0039\n    80        0.6981            -nan     0.1000   -0.0045\n   100        0.6502            -nan     0.1000   -0.0015\n   120        0.6014            -nan     0.1000   -0.0036\n   140        0.5684            -nan     0.1000   -0.0033\n   150        0.5464            -nan     0.1000   -0.0009\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2438            -nan     0.1000    0.0155\n     2        1.2147            -nan     0.1000    0.0054\n     3        1.1900            -nan     0.1000    0.0060\n     4        1.1626            -nan     0.1000    0.0052\n     5        1.1412            -nan     0.1000    0.0017\n     6        1.1199            -nan     0.1000    0.0026\n     7        1.0992            -nan     0.1000    0.0009\n     8        1.0813            -nan     0.1000   -0.0046\n     9        1.0672            -nan     0.1000   -0.0023\n    10        1.0506            -nan     0.1000   -0.0080\n    20        0.9000            -nan     0.1000   -0.0032\n    40        0.7530            -nan     0.1000   -0.0073\n    60        0.6553            -nan     0.1000   -0.0054\n    80        0.5664            -nan     0.1000   -0.0073\n   100        0.5062            -nan     0.1000   -0.0059\n   120        0.4603            -nan     0.1000   -0.0057\n   140        0.4111            -nan     0.1000   -0.0043\n   150        0.3856            -nan     0.1000   -0.0035\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0493            -nan     0.1000    0.0012\n     2        1.0312            -nan     0.1000    0.0065\n     3        1.0190            -nan     0.1000   -0.0028\n     4        1.0054            -nan     0.1000    0.0000\n     5        0.9983            -nan     0.1000   -0.0054\n     6        0.9814            -nan     0.1000   -0.0030\n     7        0.9722            -nan     0.1000    0.0000\n     8        0.9649            -nan     0.1000    0.0024\n     9        0.9579            -nan     0.1000    0.0004\n    10        0.9509            -nan     0.1000    0.0011\n    20        0.9017            -nan     0.1000   -0.0067\n    40        0.8232            -nan     0.1000   -0.0027\n    60        0.7655            -nan     0.1000   -0.0019\n    80        0.7302            -nan     0.1000   -0.0026\n   100        0.6907            -nan     0.1000   -0.0018\n   120        0.6691            -nan     0.1000   -0.0074\n   140        0.6566            -nan     0.1000   -0.0030\n   150        0.6459            -nan     0.1000   -0.0042\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0321            -nan     0.1000    0.0031\n     2        1.0123            -nan     0.1000   -0.0019\n     3        0.9904            -nan     0.1000    0.0021\n     4        0.9813            -nan     0.1000   -0.0088\n     5        0.9736            -nan     0.1000   -0.0005\n     6        0.9543            -nan     0.1000    0.0044\n     7        0.9453            -nan     0.1000   -0.0038\n     8        0.9327            -nan     0.1000   -0.0033\n     9        0.9221            -nan     0.1000   -0.0018\n    10        0.9059            -nan     0.1000    0.0043\n    20        0.8193            -nan     0.1000   -0.0042\n    40        0.6906            -nan     0.1000   -0.0047\n    60        0.6227            -nan     0.1000   -0.0049\n    80        0.5567            -nan     0.1000   -0.0028\n   100        0.5066            -nan     0.1000   -0.0055\n   120        0.4600            -nan     0.1000   -0.0059\n   140        0.4173            -nan     0.1000   -0.0066\n   150        0.4008            -nan     0.1000   -0.0024\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.0386            -nan     0.1000    0.0032\n     2        1.0199            -nan     0.1000    0.0012\n     3        1.0082            -nan     0.1000   -0.0025\n     4        0.9934            -nan     0.1000   -0.0032\n     5        0.9699            -nan     0.1000   -0.0042\n     6        0.9500            -nan     0.1000   -0.0011\n     7        0.9247            -nan     0.1000    0.0039\n     8        0.9062            -nan     0.1000   -0.0055\n     9        0.8931            -nan     0.1000   -0.0046\n    10        0.8790            -nan     0.1000   -0.0041\n    20        0.7603            -nan     0.1000   -0.0044\n    40        0.6094            -nan     0.1000   -0.0032\n    60        0.5139            -nan     0.1000   -0.0026\n    80        0.4287            -nan     0.1000   -0.0012\n   100        0.3686            -nan     0.1000   -0.0040\n   120        0.3319            -nan     0.1000   -0.0055\n   140        0.2831            -nan     0.1000   -0.0024\n   150        0.2612            -nan     0.1000   -0.0024\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1727            -nan     0.1000    0.0048\n     2        1.1566            -nan     0.1000    0.0042\n     3        1.1399            -nan     0.1000    0.0027\n     4        1.1174            -nan     0.1000    0.0048\n     5        1.0990            -nan     0.1000    0.0023\n     6        1.0846            -nan     0.1000    0.0023\n     7        1.0754            -nan     0.1000    0.0018\n     8        1.0645            -nan     0.1000    0.0035\n     9        1.0550            -nan     0.1000   -0.0008\n    10        1.0466            -nan     0.1000   -0.0008\n    20        0.9777            -nan     0.1000    0.0019\n    40        0.8611            -nan     0.1000   -0.0011\n    60        0.7978            -nan     0.1000   -0.0013\n    80        0.7429            -nan     0.1000   -0.0040\n   100        0.7077            -nan     0.1000   -0.0046\n   120        0.6889            -nan     0.1000   -0.0027\n   140        0.6654            -nan     0.1000   -0.0016\n   150        0.6559            -nan     0.1000   -0.0011\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1591            -nan     0.1000    0.0058\n     2        1.1253            -nan     0.1000    0.0114\n     3        1.0902            -nan     0.1000    0.0116\n     4        1.0662            -nan     0.1000    0.0025\n     5        1.0593            -nan     0.1000   -0.0112\n     6        1.0380            -nan     0.1000    0.0034\n     7        1.0167            -nan     0.1000    0.0040\n     8        0.9910            -nan     0.1000    0.0052\n     9        0.9677            -nan     0.1000    0.0079\n    10        0.9546            -nan     0.1000   -0.0062\n    20        0.8419            -nan     0.1000   -0.0025\n    40        0.6865            -nan     0.1000   -0.0037\n    60        0.6017            -nan     0.1000   -0.0013\n    80        0.5465            -nan     0.1000   -0.0042\n   100        0.4891            -nan     0.1000   -0.0037\n   120        0.4468            -nan     0.1000   -0.0030\n   140        0.4141            -nan     0.1000   -0.0066\n   150        0.3985            -nan     0.1000   -0.0030\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1740            -nan     0.1000   -0.0038\n     2        1.1288            -nan     0.1000    0.0121\n     3        1.0920            -nan     0.1000    0.0046\n     4        1.0637            -nan     0.1000    0.0088\n     5        1.0287            -nan     0.1000    0.0107\n     6        1.0025            -nan     0.1000   -0.0025\n     7        0.9746            -nan     0.1000    0.0011\n     8        0.9501            -nan     0.1000    0.0005\n     9        0.9284            -nan     0.1000   -0.0074\n    10        0.9116            -nan     0.1000   -0.0016\n    20        0.7753            -nan     0.1000   -0.0005\n    40        0.6060            -nan     0.1000   -0.0030\n    60        0.4993            -nan     0.1000    0.0012\n    80        0.4251            -nan     0.1000   -0.0042\n   100        0.3696            -nan     0.1000   -0.0020\n   120        0.3210            -nan     0.1000   -0.0020\n   140        0.2723            -nan     0.1000   -0.0012\n   150        0.2511            -nan     0.1000   -0.0026\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.2101            -nan     0.1000   -0.0015\n     2        1.1790            -nan     0.1000    0.0183\n     3        1.1480            -nan     0.1000    0.0163\n     4        1.1148            -nan     0.1000    0.0101\n     5        1.0826            -nan     0.1000    0.0125\n     6        1.0723            -nan     0.1000    0.0024\n     7        1.0499            -nan     0.1000    0.0121\n     8        1.0395            -nan     0.1000   -0.0027\n     9        1.0187            -nan     0.1000    0.0074\n    10        1.0063            -nan     0.1000    0.0043\n    20        0.9174            -nan     0.1000   -0.0016\n    40        0.8156            -nan     0.1000   -0.0010\n    60        0.7405            -nan     0.1000   -0.0031\n    80        0.7003            -nan     0.1000   -0.0016\n   100        0.6655            -nan     0.1000   -0.0023\n   120        0.6434            -nan     0.1000   -0.0027\n   140        0.6147            -nan     0.1000   -0.0048\n   150        0.6067            -nan     0.1000   -0.0012\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1551            -nan     0.1000    0.0184\n     2        1.1113            -nan     0.1000    0.0181\n     3        1.0712            -nan     0.1000    0.0178\n     4        1.0338            -nan     0.1000    0.0092\n     5        1.0134            -nan     0.1000    0.0058\n     6        0.9854            -nan     0.1000    0.0107\n     7        0.9625            -nan     0.1000    0.0084\n     8        0.9432            -nan     0.1000    0.0041\n     9        0.9335            -nan     0.1000   -0.0023\n    10        0.9221            -nan     0.1000   -0.0025\n    20        0.8041            -nan     0.1000   -0.0017\n    40        0.6691            -nan     0.1000   -0.0066\n    60        0.5943            -nan     0.1000   -0.0049\n    80        0.5265            -nan     0.1000   -0.0049\n   100        0.4683            -nan     0.1000   -0.0041\n   120        0.4215            -nan     0.1000   -0.0018\n   140        0.3862            -nan     0.1000   -0.0031\n   150        0.3698            -nan     0.1000   -0.0018\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1719            -nan     0.1000    0.0194\n     2        1.1111            -nan     0.1000    0.0264\n     3        1.0633            -nan     0.1000    0.0174\n     4        1.0323            -nan     0.1000    0.0050\n     5        1.0085            -nan     0.1000    0.0025\n     6        0.9997            -nan     0.1000   -0.0087\n     7        0.9728            -nan     0.1000    0.0075\n     8        0.9311            -nan     0.1000    0.0128\n     9        0.9227            -nan     0.1000   -0.0048\n    10        0.9037            -nan     0.1000    0.0010\n    20        0.7400            -nan     0.1000    0.0013\n    40        0.5865            -nan     0.1000   -0.0055\n    60        0.4703            -nan     0.1000   -0.0030\n    80        0.3800            -nan     0.1000   -0.0036\n   100        0.3205            -nan     0.1000   -0.0015\n   120        0.2803            -nan     0.1000   -0.0023\n   140        0.2401            -nan     0.1000   -0.0015\n   150        0.2183            -nan     0.1000   -0.0013\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1683            -nan     0.1000    0.0149\n     2        1.1484            -nan     0.1000    0.0054\n     3        1.1290            -nan     0.1000    0.0081\n     4        1.1046            -nan     0.1000    0.0100\n     5        1.0993            -nan     0.1000   -0.0032\n     6        1.0877            -nan     0.1000    0.0007\n     7        1.0761            -nan     0.1000    0.0006\n     8        1.0694            -nan     0.1000    0.0002\n     9        1.0624            -nan     0.1000   -0.0023\n    10        1.0458            -nan     0.1000   -0.0009\n    20        0.9643            -nan     0.1000   -0.0015\n    40        0.8710            -nan     0.1000   -0.0019\n    60        0.8263            -nan     0.1000   -0.0067\n    80        0.7926            -nan     0.1000   -0.0050\n   100        0.7772            -nan     0.1000   -0.0060\n   120        0.7532            -nan     0.1000   -0.0031\n   140        0.7267            -nan     0.1000   -0.0032\n   150        0.7225            -nan     0.1000   -0.0028\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1726            -nan     0.1000    0.0081\n     2        1.1329            -nan     0.1000    0.0210\n     3        1.0909            -nan     0.1000    0.0075\n     4        1.0686            -nan     0.1000    0.0064\n     5        1.0454            -nan     0.1000    0.0060\n     6        1.0313            -nan     0.1000   -0.0003\n     7        1.0107            -nan     0.1000    0.0060\n     8        0.9922            -nan     0.1000    0.0039\n     9        0.9778            -nan     0.1000   -0.0013\n    10        0.9617            -nan     0.1000    0.0008\n    20        0.8664            -nan     0.1000   -0.0030\n    40        0.7598            -nan     0.1000   -0.0031\n    60        0.6822            -nan     0.1000   -0.0028\n    80        0.6271            -nan     0.1000    0.0005\n   100        0.5784            -nan     0.1000   -0.0030\n   120        0.5264            -nan     0.1000   -0.0046\n   140        0.4798            -nan     0.1000   -0.0042\n   150        0.4659            -nan     0.1000   -0.0067\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1689            -nan     0.1000    0.0036\n     2        1.1327            -nan     0.1000    0.0121\n     3        1.0929            -nan     0.1000    0.0143\n     4        1.0462            -nan     0.1000    0.0100\n     5        1.0181            -nan     0.1000    0.0078\n     6        0.9997            -nan     0.1000    0.0011\n     7        0.9750            -nan     0.1000    0.0056\n     8        0.9586            -nan     0.1000   -0.0003\n     9        0.9303            -nan     0.1000    0.0071\n    10        0.9179            -nan     0.1000   -0.0073\n    20        0.8060            -nan     0.1000   -0.0029\n    40        0.6746            -nan     0.1000   -0.0055\n    60        0.5787            -nan     0.1000   -0.0073\n    80        0.4995            -nan     0.1000   -0.0056\n   100        0.4327            -nan     0.1000   -0.0024\n   120        0.3789            -nan     0.1000   -0.0030\n   140        0.3350            -nan     0.1000   -0.0035\n   150        0.3100            -nan     0.1000   -0.0036\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        1.1819            -nan     0.1000    0.0092\n     2        1.1663            -nan     0.1000    0.0076\n     3        1.1517            -nan     0.1000    0.0061\n     4        1.1427            -nan     0.1000    0.0039\n     5        1.1339            -nan     0.1000    0.0044\n     6        1.1280            -nan     0.1000    0.0022\n     7        1.1230            -nan     0.1000    0.0019\n     8        1.1151            -nan     0.1000    0.0023\n     9        1.1083            -nan     0.1000    0.0030\n    10        1.0985            -nan     0.1000    0.0035\n    20        1.0537            -nan     0.1000    0.0010\n    40        1.0014            -nan     0.1000   -0.0008\n    50        0.9848            -nan     0.1000   -0.0002\n\ngbm_preds <- predict(model_gbm, newdata = credit_test, type = \"prob\")\n\ngbm_preds  <- gbm_preds[, \"yes\"]\n\nmodel_dt <- train(x = x,\n                   y = y,\n                   method = \"rpart\",\n                   metric = \"ROC\",\n                   trControl = myControl)\n\n\ndt_preds <- predict(model_dt, newdata = credit_test, type = \"prob\")\ndt_preds  <- dt_preds[, \"yes\"]\n\n\n# Generate the test set AUCs using the two sets of predictions & compare\nactual <- credit_test$default\ndt_auc <- auc(actual = actual, predicted = dt_preds)\nbag_auc <- auc(actual = actual, predicted = bag_preds)\nrf_auc <- auc(actual = actual, predicted = rf_preds)\ngbm_auc <- auc(actual = actual, predicted = gbm_preds)\n# \n# # Print results\nsprintf(\"Decision Tree Test AUC: %.3f\", dt_auc)\n\n[1] \"Decision Tree Test AUC: 0.770\"\n\nsprintf(\"Bagged Trees Test AUC: %.3f\", bag_auc)\n\n[1] \"Bagged Trees Test AUC: 0.803\"\n\nsprintf(\"Random Forest Test AUC: %.3f\", rf_auc)\n\n[1] \"Random Forest Test AUC: 0.814\"\n\nsprintf(\"GBM Test AUC: %.3f\", gbm_auc)\n\n[1] \"GBM Test AUC: 0.804\""
  },
  {
    "objectID": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#plot-compare-roc-curves",
    "href": "myblog/datacamp/ml_tree_methods_r/ml_tree_methods.html#plot-compare-roc-curves",
    "title": "ML with tree based models in r",
    "section": "Plot & compare ROC curves",
    "text": "Plot & compare ROC curves\nWe conclude this course by plotting the ROC curves for all the models (one from each chapter) on the same graph. The ROCR package provides the prediction() and performance() functions which generate the data required for plotting the ROC curve, given a set of predictions and actual (true) values.\nThe more “up and to the left” the ROC curve of a model is, the better the model. The AUC performance metric is literally the “Area Under the ROC Curve”, so the greater the area under this curve, the higher the AUC, and the better-performing the model is.\n\nlibrary(ROCR)\n# List of predictions\npreds_list <- list(dt_preds, bag_preds, rf_preds, gbm_preds)\n\n# List of actual values (same for all)\nm <- length(preds_list)\n\nactuals_list <- rep(list(credit_test$default), m)\n\n# Plot the ROC curves\npred <- prediction(preds_list, actuals_list)\nrocs <- performance(pred, \"tpr\", \"fpr\")\nplot(rocs, col = as.list(1:m), main = \"Test Set ROC Curves\")\nlegend(x = \"bottomright\", \n       legend = c(\"Decision Tree\", \"Bagged Trees\", \"Random Forest\", \"GBM\"),\n       fill = 1:m)"
  }
]