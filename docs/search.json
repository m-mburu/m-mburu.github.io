[
  {
    "objectID": "myblog/loan_prediction/clustering_loans.html#reading-data",
    "href": "myblog/loan_prediction/clustering_loans.html#reading-data",
    "title": "clustering",
    "section": "Reading data",
    "text": "Reading data"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html",
    "href": "myblog/datacamp/regression_r/regression.html",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "",
    "text": "We use the lm() function to fit linear models to data. In this case, we want to understand how the price of MarioKart games sold at auction varies as a function of not only the number of wheels included in the package, but also whether the item is new or used. Obviously, it is expected that you might have to pay a premium to buy these new. But how much is that premium? Can we estimate its value after controlling for the number of wheels?\nWe will fit a parallel slopes model using lm(). In addition to the data argument, lm() needs to know which variables you want to include in your regression model, and how you want to include them. It accomplishes this using a formula argument. A simple linear regression formula looks like y ~ x, where y is the name of the response variable, and x is the name of the explanatory variable. Here, we will simply extend this formula to include multiple explanatory variables. A parallel slopes model has the form y ~ x + z, where z is a categorical explanatory variable, and x is a numerical explanatory variable.\nThe output from lm() is a model object, which when printed, will show the fitted coefficients.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(openintro)\nlibrary(broom)\nlibrary(pander)\ndata( mariokart, package = \"openintro\")\nmario_kart <- mariokart\n\nmario_kart <- mario_kart %>% mutate(total_pr := ifelse(total_pr > 100, NA, total_pr))\n# Explore the data\nglimpse(mario_kart)\n\nRows: 143\nColumns: 12\n$ id          <dbl> 150377422259, 260483376854, 320432342985, 280405224677, 17…\n$ duration    <int> 3, 7, 3, 3, 1, 3, 1, 1, 3, 7, 1, 1, 1, 1, 7, 7, 3, 3, 1, 7…\n$ n_bids      <int> 20, 13, 16, 18, 20, 19, 13, 15, 29, 8, 15, 15, 13, 16, 6, …\n$ cond        <fct> new, used, new, new, new, new, used, new, used, used, new,…\n$ start_pr    <dbl> 0.99, 0.99, 0.99, 0.99, 0.01, 0.99, 0.01, 1.00, 0.99, 19.9…\n$ ship_pr     <dbl> 4.00, 3.99, 3.50, 0.00, 0.00, 4.00, 0.00, 2.99, 4.00, 4.00…\n$ total_pr    <dbl> 51.55, 37.04, 45.50, 44.00, 71.00, 45.00, 37.02, 53.99, 47…\n$ ship_sp     <fct> standard, firstClass, firstClass, standard, media, standar…\n$ seller_rate <int> 1580, 365, 998, 7, 820, 270144, 7284, 4858, 27, 201, 4858,…\n$ stock_photo <fct> yes, yes, no, yes, yes, yes, yes, yes, yes, no, yes, yes, …\n$ wheels      <int> 1, 1, 1, 1, 2, 0, 0, 2, 1, 1, 2, 2, 2, 2, 1, 0, 1, 1, 2, 2…\n$ title       <fct> \"~~ Wii MARIO KART &amp; WHEEL ~ NINTENDO Wii ~ BRAND NEW …\n\n# fit parallel slopes\n\nmod_mario <- lm(total_pr ~ wheels + cond, data = mario_kart)"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#reasoning-about-two-intercepts",
    "href": "myblog/datacamp/regression_r/regression.html#reasoning-about-two-intercepts",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Reasoning about two intercepts",
    "text": "Reasoning about two intercepts\nThe mario_kart data contains several other variables. The totalPr, startPr, and shipPr variables are numeric, while the cond and stockPhoto variables are categorical.\nWhich formula will result in a parallel slopes model?\n\ntotalPr ~ shipPr + stockPhoto"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#using-geom_line-and-augment",
    "href": "myblog/datacamp/regression_r/regression.html#using-geom_line-and-augment",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Using geom_line() and augment()",
    "text": "Using geom_line() and augment()\nParallel slopes models are so-named because we can visualize these models in the data space as not one line, but two parallel lines. To do this, we’ll draw two things:\na scatterplot showing the data, with color separating the points into groups a line for each value of the categorical variable Our plotting strategy is to compute the fitted values, plot these, and connect the points to form a line. The augment() function from the broom package provides an easy way to add the fitted values to our data frame, and the geom_line() function can then use that data frame to plot the points and connect them.\nNote that this approach has the added benefit of automatically coloring the lines appropriately to match the data.\nYou already know how to use ggplot() and geom_point() to make the scatterplot. The only twist is that now you’ll pass your augment()-ed model as the data argument in your ggplot() call. When you add your geom_line(), instead of letting the y aesthetic inherit its values from the ggplot() call, you can set it to the .fitted column of the augment()-ed model. This has the advantage of automatically coloring the lines for you.\n\n# Augment the model\naugmented_mod <- augment(mod_mario)\nglimpse(augmented_mod)\n\nRows: 141\nColumns: 10\n$ .rownames  <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"1…\n$ total_pr   <dbl> 51.55, 37.04, 45.50, 44.00, 71.00, 45.00, 37.02, 53.99, 47.…\n$ wheels     <int> 1, 1, 1, 1, 2, 0, 0, 2, 1, 1, 2, 2, 2, 2, 1, 0, 1, 1, 2, 0,…\n$ cond       <fct> new, used, new, new, new, new, used, new, used, used, new, …\n$ .fitted    <dbl> 49.60260, 44.01777, 49.60260, 49.60260, 56.83544, 42.36976,…\n$ .resid     <dbl> 1.9473995, -6.9777674, -4.1026005, -5.6026005, 14.1645592, …\n$ .hat       <dbl> 0.02103158, 0.01250410, 0.02103158, 0.02103158, 0.01915635,…\n$ .sigma     <dbl> 4.902339, 4.868399, 4.892414, 4.881308, 4.750591, 4.899816,…\n$ .cooksd    <dbl> 1.161354e-03, 8.712334e-03, 5.154337e-03, 9.612441e-03, 5.5…\n$ .std.resid <dbl> 0.40270893, -1.43671086, -0.84838977, -1.15857953, 2.926332…\n\n# scatterplot, with color\ndata_space <- ggplot(augmented_mod, aes(x = wheels, y = total_pr , color = cond )) + \n  geom_point()\n  \n# single call to geom_line()\ndata_space + \n  geom_line(aes(y = .fitted))"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#intercept-interpretation",
    "href": "myblog/datacamp/regression_r/regression.html#intercept-interpretation",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Intercept interpretation",
    "text": "Intercept interpretation\nRecall that the cond variable is either new or used. Here are the fitted coefficients from your model:\nCall: lm(formula = totalPr ~ wheels + cond, data = mario_kart)\nCoefficients: (Intercept) wheels condused\n42.370 7.233 -5.585\nChoose the correct interpretation of the coefficient on condused:\n\nThe expected price of a used MarioKart is $5.58 less than that of a new one with the same number of wheels.\nFor each additional wheel, the expected price of a MarioKart increases by $7.23 regardless of whether it is new or used.\n\nSyntax from math The babies data set contains observations about the birthweight and other characteristics of children born in the San Francisco Bay area from 1960–1967.\nWe would like to build a model for birthweight as a function of the mother’s age and whether this child was her first (parity == 0). Use the mathematical specification below to code the model in R.\n\\[birthweight = \\beta_0 + \\beta_1 * age  + \\beta_2 * parity + \\epsilon\\]\n\ndata( babies, package = \"openintro\")\n\nmod <- lm(bwt~ age+parity, data = babies)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n118.3\n2.788\n42.43\n3.957e-243\n\n\nage\n0.06315\n0.09577\n0.6594\n0.5097\n\n\nparity\n-1.652\n1.271\n-1.3\n0.1937"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#syntax-from-plot",
    "href": "myblog/datacamp/regression_r/regression.html#syntax-from-plot",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Syntax from plot",
    "text": "Syntax from plot\nThis time, we’d like to build a model for birthweight as a function of the length of gestation and the mother’s smoking status. Use the plot to inform your model specification.\n\nggplot(babies, aes(gestation, bwt, color = factor(smoke)))+\n    geom_point()\n\n\n\nmod <- lm(bwt~ gestation + smoke, data = babies)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.9317\n8.152\n-0.1143\n0.909\n\n\ngestation\n0.4429\n0.02902\n15.26\n3.156e-48\n\n\nsmoke\n-8.088\n0.9527\n-8.49\n5.963e-17\n\n\n\n\n\nR-squared vs. adjusted R-squared Two common measures of how well a model fits to data are \\[R^2\\] (the coefficient of determination) and the adjusted \\[R^2\\] . The former measures the percentage of the variability in the response variable that is explained by the model. To compute this, we define\n\\[R^2 = 1 - \\frac{sse}{sst} \\] where SSE and SST are the sum of the squared residuals, and the total sum of the squares, respectively. One issue with this measure is that the can only decrease as new variable are added to the model, while the SST depends only on the response variable and therefore is not affected by changes to the model. This means that you can increase \\[R^2\\] by adding any additional variable to your model—even random noise.\nThe adjusted \\[R^2\\] includes a term that penalizes a model for each additional explanatory variable (where is the number of explanatory variables). We can see both measures in the output of the summary() function on our model object.\n\n# R^2 and adjusted R^2\nsummary(mod_mario)\n\n\nCall:\nlm(formula = total_pr ~ wheels + cond, data = mario_kart)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.0078  -3.0754  -0.8254   2.9822  14.1646 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  42.3698     1.0651  39.780  < 2e-16 ***\nwheels        7.2328     0.5419  13.347  < 2e-16 ***\ncondused     -5.5848     0.9245  -6.041 1.35e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.887 on 138 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7165,    Adjusted R-squared:  0.7124 \nF-statistic: 174.4 on 2 and 138 DF,  p-value: < 2.2e-16\n\n# add random noise\nmario_kart_noisy <- mario_kart %>% \nmutate(noise = rnorm(n = nrow(mario_kart)))\n  \n# compute new model\nmod2_mario2 <- lm(total_pr ~ wheels + cond+noise, data = mario_kart_noisy)\n\n# new R^2 and adjusted R^2\nsummary(mod2_mario2)\n\n\nCall:\nlm(formula = total_pr ~ wheels + cond + noise, data = mario_kart_noisy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.1221  -3.0761  -0.8246   2.8795  14.3721 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  42.4152     1.0787  39.321  < 2e-16 ***\nwheels        7.2197     0.5454  13.239  < 2e-16 ***\ncondused     -5.6279     0.9380  -6.000 1.67e-08 ***\nnoise         0.1404     0.4546   0.309    0.758    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.904 on 137 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7167,    Adjusted R-squared:  0.7105 \nF-statistic: 115.5 on 3 and 137 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#prediction",
    "href": "myblog/datacamp/regression_r/regression.html#prediction",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Prediction",
    "text": "Prediction\nOnce we have fit a regression model, we can use it to make predictions for unseen observations or retrieve the fitted values. Here, we explore two methods for doing the latter.\nA traditional way to return the fitted values (i.e. the y ’s) is to run the predict() function on the model object. This will return a vector of the fitted values. Note that predict() will take an optional newdata argument that will allow you to make predictions for observations that are not in the original data.\nA newer alternative is the augment() function from the broom package, which returns a data.frame with the response varible (), the relevant explanatory variables (the ’s), the fitted value ( ) and some information about the residuals (). augment() will also take a newdata argument that allows you to make predictions.\n\n# return a vector\nlibrary(knitr)\n\npredict(mod_mario)\n\n       1        2        3        4        5        6        7        8 \n49.60260 44.01777 49.60260 49.60260 56.83544 42.36976 36.78493 56.83544 \n       9       10       11       12       13       14       15       16 \n44.01777 44.01777 56.83544 56.83544 56.83544 56.83544 44.01777 36.78493 \n      17       18       19       21       22       23       24       25 \n49.60260 49.60260 56.83544 36.78493 56.83544 56.83544 56.83544 44.01777 \n      26       27       28       29       30       31       32       33 \n56.83544 36.78493 36.78493 36.78493 49.60260 36.78493 36.78493 44.01777 \n      34       35       36       37       38       39       40       41 \n51.25061 44.01777 44.01777 36.78493 44.01777 56.83544 56.83544 49.60260 \n      42       43       44       45       46       47       48       49 \n44.01777 51.25061 56.83544 56.83544 44.01777 56.83544 36.78493 36.78493 \n      50       51       52       53       54       55       56       57 \n44.01777 56.83544 36.78493 44.01777 42.36976 36.78493 36.78493 44.01777 \n      58       59       60       61       62       63       64       66 \n44.01777 36.78493 36.78493 56.83544 36.78493 56.83544 36.78493 51.25061 \n      67       68       69       70       71       72       73       74 \n56.83544 44.01777 58.48345 51.25061 49.60260 44.01777 49.60260 56.83544 \n      75       76       77       78       79       80       81       82 \n56.83544 51.25061 44.01777 36.78493 36.78493 36.78493 44.01777 56.83544 \n      83       84       85       86       87       88       89       90 \n44.01777 65.71629 44.01777 56.83544 36.78493 49.60260 49.60260 36.78493 \n      91       92       93       94       95       96       97       98 \n44.01777 36.78493 51.25061 44.01777 36.78493 51.25061 42.36976 56.83544 \n      99      100      101      102      103      104      105      106 \n51.25061 44.01777 51.25061 56.83544 56.83544 56.83544 36.78493 49.60260 \n     107      108      109      110      111      112      113      114 \n51.25061 44.01777 56.83544 49.60260 36.78493 44.01777 51.25061 56.83544 \n     115      116      117      118      119      120      121      122 \n64.06828 44.01777 49.60260 44.01777 49.60260 51.25061 42.36976 44.01777 \n     123      124      125      126      127      128      129      130 \n56.83544 44.01777 49.60260 44.01777 51.25061 56.83544 56.83544 49.60260 \n     131      132      133      134      135      136      137      138 \n56.83544 36.78493 44.01777 44.01777 36.78493 56.83544 36.78493 44.01777 \n     139      140      141      142      143 \n36.78493 51.25061 49.60260 36.78493 56.83544 \n\n# return a data frame\n\naugment(mod_mario)%>% head() %>% kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.rownames\ntotal_pr\nwheels\ncond\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n1\n51.55\n1\nnew\n49.60260\n1.947399\n0.0210316\n4.902340\n0.0011614\n0.4027089\n\n\n2\n37.04\n1\nused\n44.01777\n-6.977767\n0.0125041\n4.868399\n0.0087123\n-1.4367109\n\n\n3\n45.50\n1\nnew\n49.60260\n-4.102601\n0.0210316\n4.892414\n0.0051543\n-0.8483898\n\n\n4\n44.00\n1\nnew\n49.60260\n-5.602601\n0.0210316\n4.881308\n0.0096124\n-1.1585795\n\n\n5\n71.00\n2\nnew\n56.83544\n14.164559\n0.0191563\n4.750591\n0.0557493\n2.9263328\n\n\n6\n45.00\n0\nnew\n42.36976\n2.630240\n0.0474932\n4.899816\n0.0050537\n0.5514192"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#thought-experiments",
    "href": "myblog/datacamp/regression_r/regression.html#thought-experiments",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Thought experiments",
    "text": "Thought experiments\nSuppose that after going apple picking you have 12 apples left over. You decide to conduct an experiment to investigate how quickly they will rot under certain conditions. You place six apples in a cool spot in your basement, and leave the other six on the window sill in the kitchen. Every week, you estimate the percentage of the surface area of the apple that is rotten or moldy.\nConsider the following models:\n\\[rot = \\beta_0 + \\beta_1 *t + \\beta_2 * temp + \\epsilon \\]\nand\n\\[rot = \\beta_0 + \\beta_1 *t + \\beta_2 * temp + \\beta_2 * temp *t +  \\epsilon \\]\n\nThe rate at which apples rot will vary based on the temperature."
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#fitting-a-model-with-interaction",
    "href": "myblog/datacamp/regression_r/regression.html#fitting-a-model-with-interaction",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Fitting a model with interaction",
    "text": "Fitting a model with interaction\nIncluding an interaction term in a model is easy—we just have to tell lm() that we want to include that new variable. An expression of the form\nlm(y ~ x + z + x:z, data = mydata)\nwill do the trick. The use of the colon (:) here means that the interaction between and will be a third term in the model.\n\n# include interaction\n\nmod <- lm(total_pr ~cond + duration + cond:duration, data = mario_kart)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n58.27\n1.366\n42.64\n5.832e-81\n\n\ncondused\n-17.12\n2.178\n-7.86\n1.014e-12\n\n\nduration\n-1.966\n0.4488\n-4.38\n2.342e-05\n\n\ncondused:duration\n2.325\n0.5484\n4.239\n4.102e-05"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#visualizing-interaction-models",
    "href": "myblog/datacamp/regression_r/regression.html#visualizing-interaction-models",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Visualizing interaction models",
    "text": "Visualizing interaction models\nInteraction allows the slope of the regression line in each group to vary. In this case, this means that the relationship between the final price and the length of the auction is moderated by the condition of each item.\nInteraction models are easy to visualize in the data space with ggplot2 because they have the same coefficients as if the models were fit independently to each group defined by the level of the categorical variable. In this case, new and used MarioKarts each get their own regression line. To see this, we can set an aesthetic (e.g. color) to the categorical variable, and then add a geom_smooth() layer to overlay the regression line for each color.\n\n# interaction plot\nggplot(mario_kart, aes(duration, total_pr, color = cond)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = 0)"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#consequences-of-simpsons-paradox",
    "href": "myblog/datacamp/regression_r/regression.html#consequences-of-simpsons-paradox",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Consequences of Simpson’s paradox",
    "text": "Consequences of Simpson’s paradox\nIn the simple linear regression model for average SAT score, (total) as a function of average teacher salary (salary), the fitted coefficient was -5.02 points per thousand dollars. This suggests that for every additional thousand dollars of salary for teachers in a particular state, the expected SAT score for a student from that state is about 5 points lower.\nIn the model that includes the percentage of students taking the SAT, the coefficient on salary becomes 1.84 points per thousand dollars. Choose the correct interpretation of this slope coefficient.\n\nFor every additional thousand dollars of salary for teachers in a particular state, the expected SAT score for a student from that state is about 2 points higher, after controlling for the percentage of students taking the SAT."
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#simpsons-paradox-in-action",
    "href": "myblog/datacamp/regression_r/regression.html#simpsons-paradox-in-action",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Simpson’s paradox in action",
    "text": "Simpson’s paradox in action\nA mild version of Simpson’s paradox can be observed in the MarioKart auction data. Consider the relationship between the final auction price and the length of the auction. It seems reasonable to assume that longer auctions would result in higher prices, since—other things being equal—a longer auction gives more bidders more time to see the auction and bid on the item.\nHowever, a simple linear regression model reveals the opposite: longer auctions are associated with lower final prices. The problem is that all other things are not equal. In this case, the new MarioKarts—which people pay a premium for—were mostly sold in one-day auctions, while a plurality of the used MarioKarts were sold in the standard seven-day auctions.\nOur simple linear regression model is misleading, in that it suggests a negative relationship between final auction price and duration. However, for the used MarioKarts, the relationship is positive.\n\nslr <- ggplot(mario_kart, aes(y = total_pr, x = duration)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n# model with one slope\nmod <- lm(total_pr ~ duration, data = mario_kart)\n\n# plot with two slopes\nslr + aes(color = cond)"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#fitting-a-mlr-model",
    "href": "myblog/datacamp/regression_r/regression.html#fitting-a-mlr-model",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Fitting a MLR model",
    "text": "Fitting a MLR model\nIn terms of the R code, fitting a multiple linear regression model is easy: simply add variables to the model formula you specify in the lm() command.\nIn a parallel slopes model, we had two explanatory variables: one was numeric and one was categorical. Here, we will allow both explanatory variables to be numeric.\n\n# Fit the model using duration and startPr\n\nmod <- lm(total_pr~ start_pr + duration, data = mario_kart)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n51.03\n1.179\n43.28\n3.666e-82\n\n\nstart_pr\n0.233\n0.04364\n5.339\n3.756e-07\n\n\nduration\n-1.508\n0.2555\n-5.902\n2.645e-08"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#tiling-the-plane",
    "href": "myblog/datacamp/regression_r/regression.html#tiling-the-plane",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Tiling the plane",
    "text": "Tiling the plane\nOne method for visualizing a multiple linear regression model is to create a heatmap of the fitted values in the plane defined by the two explanatory variables. This heatmap will illustrate how the model output changes over different combinations of the explanatory variables.\nThis is a multistep process:\nFirst, create a grid of the possible pairs of values of the explanatory variables. The grid should be over the actual range of the data present in each variable. We’ve done this for you and stored the result as a data frame called grid. Use augment() with the newdata argument to find the ’s corresponding to the values in grid. Add these to the data_space plot by using the fill aesthetic and geom_tile().\n\n# add predictions to grid\nprice_hats <- augment(mod, newdata = grid)\n\n# tile the plane\ndata_space + \n  geom_tile(data = price_hats, aes(fill = .fitted), alpha = 0.5)"
  },
  {
    "objectID": "myblog/datacamp/regression_r/regression.html#models-in-3d",
    "href": "myblog/datacamp/regression_r/regression.html#models-in-3d",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Models in 3D",
    "text": "Models in 3D\nAn alternative way to visualize a multiple regression model with two numeric explanatory variables is as a plane in three dimensions. This is possible in R using the plotly package.\nWe have created three objects that you will need:\nx: a vector of unique values of duration y: a vector of unique values of startPr plane: a matrix of the fitted values across all combinations of x and y Much like ggplot(), the plot_ly() function will allow you to create a plot object with variables mapped to x, y, and z aesthetics. The add_markers() function is similar to geom_point() in that it allows you to add points to your 3D plot.\nNote that plot_ly uses the pipe (%>%) operator to chain commands together.\n\n# draw the 3D scatterplot\np <- plot_ly(data = mario_kart, z = ~totalPr, x = ~duration, y = ~startPr, opacity = 0.6) %>%\n  add_markers() \n  \n# draw the plane\np %>%\n  add_surface(x = ~x, y = ~y, z = ~plane, showscale = FALSE)"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html",
    "href": "myblog/datacamp/tidymodels/tidymodels.html",
    "title": "Modeling with tidymodels in R",
    "section": "",
    "text": "The rsample package is designed to create training and test datasets. Creating a test dataset is important for estimating how a trained model will likely perform on new data. It also guards against overfitting, where a model memorizes patterns that exist only in the training data and performs poorly on new data.\nIn this exercise, you will create training and test datasets from the home_sales data. This data contains information on homes sold in the Seattle, Washington area between 2015 and 2016.\nThe outcome variable in this data is selling_price.\nThe tidymodels package will be pre-loaded in every exercise in the course. The home_sales tibble has also been loaded for you.\n\n\n\nTidy model packages\n\n\n\nhome_sales <- readRDS(\"home_sales.rds\")\n\n# Create a data split object\nhome_split <- initial_split(home_sales, \n                            prop = 0.7, \n                            strata = selling_price)\n\n# Create the training data\nhome_training <- home_split %>%\n  training()\n\n# Create the test data\nhome_test <- home_split %>% \n  testing()\n\n# Check number of rows in each dataset\nnrow(home_training)\n\n[1] 1042\n\nnrow(home_test)\n\n[1] 450\n\n\nDistribution of outcome variable values Stratifying by the outcome variable when generating training and test datasets ensures that the outcome variable values have a similar range in both datasets.\nSince the original data is split at random, stratification avoids placing all the expensive homes in home_sales into the test dataset, for example. In this case, your model would most likely perform poorly because it was trained on less expensive homes.\nIn this exercise, you will calculate summary statistics for the selling_price variable in the training and test datasets. The home_training and home_test tibbles have been loaded from the previous exercise.\n\n# Distribution of selling_price in training data\nlibrary(knitr)\nsummary_func <- function(df){\n      df %>% summarize(min_sell_price = min(selling_price),\n            max_sell_price = max(selling_price),\n            mean_sell_price = mean(selling_price),\n            sd_sell_price = sd(selling_price)) %>%\n    kable()\n\n}\nhome_training %>% \n    summary_func()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n479438.2\n80925.92\n\n\n\n\nhome_test %>% \n  summary_func()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n478265.5\n81185.76"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#fitting-a-linear-regression-model",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#fitting-a-linear-regression-model",
    "title": "Modeling with tidymodels in R",
    "section": "Fitting a linear regression model",
    "text": "Fitting a linear regression model\nThe parsnip package provides a unified syntax for the model fitting process in R.\nWith parsnip, it is easy to define models using the various packages, or engines, that exist in the R ecosystem.\nIn this exercise, you will define a parsnip linear regression object and train your model to predict selling_price using home_age and sqft_living as predictor variables from the home_sales data.\nThe home_training and home_test tibbles that you created in the previous lesson have been loaded into this session.\n\n# Specify a linear regression model, linear_model\nlinear_model <- linear_reg() %>% \n  # Set the model engine\n  set_engine('lm') %>% \n  # Set the model mode\n  set_mode('regression')\n\n# Train the model with the training data\nlm_fit <- linear_model %>% \n  fit(selling_price~ home_age + sqft_living,\n      data = home_training)\n\n# Print lm_fit to view model information\ntidy(lm_fit) %>%\n    kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n285035.0613\n7691.199132\n37.059899\n0\n\n\nhome_age\n-1231.5792\n178.152470\n-6.913063\n0\n\n\nsqft_living\n103.9248\n2.771416\n37.498823\n0"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#predicting-home-selling-prices",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#predicting-home-selling-prices",
    "title": "Modeling with tidymodels in R",
    "section": "Predicting home selling prices",
    "text": "Predicting home selling prices\nAfter fitting a model using the training data, the next step is to use it to make predictions on the test dataset. The test dataset acts as a new source of data for the model and will allow you to evaluate how well it performs.\nBefore you can evaluate model performance, you must add your predictions to the test dataset.\nIn this exercise, you will use your trained model, lm_fit, to predict selling_price in the home_test dataset.\nYour trained model, lm_fit, as well as the test dataset, home_test have been loaded into your session.\n\n# Predict selling_price\nhome_predictions <- predict(lm_fit,\n                        new_data = home_test)\n\n# View predicted selling prices\n#home_predictions\n\n# Combine test data with predictions\nhome_test_results <- home_test %>% \n  select(selling_price, home_age, sqft_living) %>% \n  bind_cols(home_predictions)\n\n# View results\nhome_test_results %>% \n    head()\n\n# A tibble: 6 × 4\n  selling_price home_age sqft_living   .pred\n          <dbl>    <dbl>       <dbl>   <dbl>\n1        635000        4        3350 628257.\n2        380000       24        2130 476837.\n3        495000       21        1650 430648.\n4        355000       19        1430 410248.\n5        464950       19        2190 489230.\n6        475000        0        2300 524062."
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#model-performance-metrics",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#model-performance-metrics",
    "title": "Modeling with tidymodels in R",
    "section": "Model performance metrics",
    "text": "Model performance metrics\nEvaluating model results is an important step in the modeling process. Model evaluation should be done on the test dataset in order to see how well a model will generalize to new datasets.\nIn the previous exercise, you trained a linear regression model to predict selling_price using home_age and sqft_living as predictor variables. You then created the home_test_results tibble using your trained model on the home_test data.\nIn this exercise, you will calculate the RMSE and R squared metrics using your results in home_test_results.\nThe home_test_results tibble has been loaded into your session.\n\n# Print home_test_results\n#home_test_results\n\n# Caculate the RMSE metric\nhome_test_results %>% \n  rmse(truth = selling_price, estimate =.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      46214.\n\n# Calculate the R squared metric\nhome_test_results %>% \n  rsq(truth = selling_price, estimate =.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.677"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#r-squared-plot",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#r-squared-plot",
    "title": "Modeling with tidymodels in R",
    "section": "R squared plot",
    "text": "R squared plot\nIn the previous exercise, you got an R squared value of 0.651. The R squared metric ranges from 0 to 1, 0 being the worst and 1 the best.\nCalculating the R squared value is only the first step in studying your model’s predictions.\nMaking an R squared plot is extremely important because it will uncover potential problems with your model, such as non-linear patterns or regions where your model is either over or under-predicting the outcome variable.\nIn this exercise, you will create an R squared plot of your model’s performance.\nThe home_test_results tibble has been loaded into your session.\n\n# Create an R squared plot of model performance\nggplot(home_test_results, aes(x = selling_price, y = .pred)) +\n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred()  +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#complete-model-fitting-process-with-last_fit",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#complete-model-fitting-process-with-last_fit",
    "title": "Modeling with tidymodels in R",
    "section": "Complete model fitting process with last_fit()",
    "text": "Complete model fitting process with last_fit()\nIn this exercise, you will train and evaluate the performance of a linear regression model that predicts selling_price using all the predictors available in the home_sales tibble.\nThis exercise will give you a chance to perform the entire model fitting process with tidymodels, from defining your model object to evaluating its performance on the test data.\nEarlier in the chapter, you created an rsample object called home_split by passing the home_sales tibble into initial_split(). The home_split object contains the instructions for randomly splitting home_sales into training and test sets.\nThe home_sales tibble, and home_split object have been loaded into this session.\n\n# Define a linear regression model\nlinear_model <- linear_reg() %>% \n  set_engine('lm') %>% \n  set_mode('regression')\n\n# Train linear_model with last_fit()\nlinear_fit <- linear_model %>% \n  last_fit(selling_price ~ ., split = home_split)\n\n# Collect predictions and view results\npredictions_df <- linear_fit %>% collect_predictions()\npredictions_df %>% head()\n\n# A tibble: 6 × 5\n  id                 .pred  .row selling_price .config             \n  <chr>              <dbl> <int>         <dbl> <chr>               \n1 train/test split 696151.     4        635000 Preprocessor1_Model1\n2 train/test split 408483.     5        380000 Preprocessor1_Model1\n3 train/test split 445489.     6        495000 Preprocessor1_Model1\n4 train/test split 399760.     7        355000 Preprocessor1_Model1\n5 train/test split 474256.     8        464950 Preprocessor1_Model1\n6 train/test split 456075.    16        475000 Preprocessor1_Model1\n\n# Make an R squared plot using predictions_df\nggplot(predictions_df, aes(x = selling_price, y = .pred)) + \n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred() +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#data-resampling",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#data-resampling",
    "title": "Modeling with tidymodels in R",
    "section": "Data resampling",
    "text": "Data resampling\nThe first step in a machine learning project is to create training and test datasets for model fitting and evaluation. The test dataset provides an estimate of how your model will perform on new data and helps to guard against overfitting.\nYou will be working with the telecom_df dataset which contains information on customers of a telecommunications company. The outcome variable is canceled_service and it records whether a customer canceled their contract with the company. The predictor variables contain information about customers’ cell phone and internet usage as well as their contract type and monthly charges.\nThe telecom_df tibble has been loaded into your session.\n\ntelecom_df <- readRDS(\"telecom_df.rds\")\n# Create data split object\ntelecom_split <- initial_split(telecom_df, prop = 0.75,\n                     strata = canceled_service)\n\n# Create the training data\ntelecom_training <- telecom_split %>% \n  training()\n\n# Create the test data\ntelecom_test <- telecom_split %>% \n  testing()\n\n# Check the number of rows\nnrow(telecom_training)\n\n[1] 731\n\nnrow(telecom_test)\n\n[1] 244"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#fitting-a-logistic-regression-model",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#fitting-a-logistic-regression-model",
    "title": "Modeling with tidymodels in R",
    "section": "Fitting a logistic regression model",
    "text": "Fitting a logistic regression model\nIn addition to regression models, the parsnip package also provides a general interface to classification models in R.\nIn this exercise, you will define a parsnip logistic regression object and train your model to predict canceled_service using avg_call_mins, avg_intl_mins, and monthly_charges as predictor variables from the telecom_df data.\nThe telecom_training and telecom_test tibbles that you created in the previous lesson have been loaded into this session.\n\n# Specify a logistic regression model\nlogistic_model <- logistic_reg() %>% \n  # Set the engine\n  set_engine('glm') %>% \n  # Set the mode\n  set_mode('classification')\n\n# Fit to training data\nlogistic_fit <- logistic_model %>% \n  fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n      data = telecom_training)\n\n# Print model fit object\nlogistic_fit %>% tidy()\n\n# A tibble: 4 × 5\n  term            estimate std.error statistic  p.value\n  <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)      2.46      0.590       4.17  3.02e- 5\n2 avg_call_mins   -0.0107    0.00129    -8.29  1.10e-16\n3 avg_intl_mins    0.0209    0.00307     6.82  9.07e-12\n4 monthly_charges -0.00144   0.00484    -0.298 7.65e- 1"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#combining-test-dataset-results",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#combining-test-dataset-results",
    "title": "Modeling with tidymodels in R",
    "section": "Combining test dataset results",
    "text": "Combining test dataset results\nEvaluating your model’s performance on the test dataset gives insights into how well your model predicts on new data sources. These insights will help you communicate your model’s value in solving problems or improving decision making.\nBefore you can calculate classification metrics such as sensitivity or specificity, you must create a results tibble with the required columns for yardstick metric functions.\nIn this exercise, you will use your trained model to predict the outcome variable in the telecom_test dataset and combine it with the true outcome values in the canceled_service column.\nYour trained model, logistic_fit, and test dataset, telecom_test, have been loaded from the previous exercise.\n\n# Predict outcome categories\nclass_preds <- predict(logistic_fit, new_data = telecom_test,\n                       type = 'class')\n\n# Obtain estimated probabilities for each outcome value\nprob_preds <- predict(logistic_fit, new_data = telecom_test, \n                      type = 'prob')\n\n# Combine test set results\ntelecom_results <- telecom_test %>% \n  select(canceled_service) %>% \n  bind_cols(class_preds, prob_preds)\n\n# View results tibble\ntelecom_results %>%\n    head()\n\n# A tibble: 6 × 4\n  canceled_service .pred_class .pred_yes .pred_no\n  <fct>            <fct>           <dbl>    <dbl>\n1 yes              no             0.364     0.636\n2 no               no             0.0166    0.983\n3 no               no             0.252     0.748\n4 yes              no             0.448     0.552\n5 no               no             0.209     0.791\n6 no               yes            0.585     0.415"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#evaluating-performance-with-yardstick",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#evaluating-performance-with-yardstick",
    "title": "Modeling with tidymodels in R",
    "section": "Evaluating performance with yardstick",
    "text": "Evaluating performance with yardstick\nIn the previous exercise, you calculated classification metrics from a sample confusion matrix. The yardstick package was designed to automate this process.\nFor classification models, yardstick functions require a tibble of model results as the first argument. This should include the actual outcome values, predicted outcome values, and estimated probabilities for each value of the outcome variable.\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate performance metrics.\nThe telecom_results tibble has been loaded into your session.\n\n# Calculate the confusion matrix\nconf_mat(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n          Truth\nPrediction yes  no\n       yes  25  11\n       no   57 151\n\n# Calculate the accuracy\naccuracy(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.721\n\n# Calculate the sensitivity\n\nsens(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.305\n\n# Calculate the specificity\n\nspec(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 spec    binary         0.932"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#creating-custom-metric-sets",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#creating-custom-metric-sets",
    "title": "Modeling with tidymodels in R",
    "section": "Creating custom metric sets",
    "text": "Creating custom metric sets\nThe yardstick package also provides the ability to create custom sets of model metrics. In cases where the cost of obtaining false negative errors is different from the cost of false positive errors, it may be important to examine a specific set of performance metrics.\nInstead of calculating accuracy, sensitivity, and specificity separately, you can create your own metric function that calculates all three at the same time.\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate a custom set of performance metrics. You will also use a confusion matrix to calculate all available binary classification metrics in tidymodelsall at once.\nThe telecom_results tibble has been loaded into your session.\n\n# Create a custom metric function\ntelecom_metrics <- metric_set(accuracy, sens, spec)\n\n# Calculate metrics using model results tibble\ntelecom_metrics(telecom_results, \n                truth = canceled_service,\n                estimate = .pred_class)\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.721\n2 sens     binary         0.305\n3 spec     binary         0.932\n\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class) %>% \n  # Pass to the summary() function\n  summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary         0.721\n 2 kap                  binary         0.275\n 3 sens                 binary         0.305\n 4 spec                 binary         0.932\n 5 ppv                  binary         0.694\n 6 npv                  binary         0.726\n 7 mcc                  binary         0.316\n 8 j_index              binary         0.237\n 9 bal_accuracy         binary         0.618\n10 detection_prevalence binary         0.148\n11 precision            binary         0.694\n12 recall               binary         0.305\n13 f_meas               binary         0.424"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#plotting-the-confusion-matrix",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#plotting-the-confusion-matrix",
    "title": "Modeling with tidymodels in R",
    "section": "Plotting the confusion matrix",
    "text": "Plotting the confusion matrix\nCalculating performance metrics with the yardstick package provides insight into how well a classification model is performing on the test dataset. Most yardstick functions return a single number that summarizes classification performance.\nMany times, it is helpful to create visualizations of the confusion matrix to more easily communicate your results.\nIn this exercise, you will make a heat map and mosaic plot of the confusion matrix from your logistic regression model on the telecom_df dataset.\nYour model results tibble, telecom_results, has been loaded into your session.\n\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a heat map\n  autoplot(type = \"heatmap\")\n\n\n\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a mosaic plot\n  autoplot(type = \"mosaic\")"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#roc-curves-and-area-under-the-roc-curve",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#roc-curves-and-area-under-the-roc-curve",
    "title": "Modeling with tidymodels in R",
    "section": "ROC curves and area under the ROC curve",
    "text": "ROC curves and area under the ROC curve\nROC curves are used to visualize the performance of a classification model across a range of probability thresholds. An ROC curve with the majority of points near the upper left corner of the plot indicates that a classification model is able to correctly predict both the positive and negative outcomes correctly across a wide range of probability thresholds.\nThe area under this curve provides a letter grade summary of model performance.\nIn this exercise, you will create an ROC curve from your logistic regression model results and calculate the area under the ROC curve with yardstick.\nYour model results tibble, telecom_results has been loaded into your session.\n\n# Calculate metrics across thresholds\nthreshold_df <- telecom_results %>% \n  roc_curve(truth = canceled_service, .pred_yes)\n\n# View results\nthreshold_df %>%\n  head()\n\n# A tibble: 6 × 3\n  .threshold specificity sensitivity\n       <dbl>       <dbl>       <dbl>\n1  -Inf          0             1    \n2     0.0109     0             1    \n3     0.0166     0             0.988\n4     0.0210     0.00617       0.988\n5     0.0371     0.0123        0.988\n6     0.0443     0.0185        0.988\n\n# Plot ROC curve\nthreshold_df %>% \n  autoplot()\n\n\n\n# Calculate ROC AUC\nroc_auc(telecom_results, truth = canceled_service, .pred_yes)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.751\n\n\nStreamlining the modeling process The last_fit() function is designed to streamline the modeling workflow in tidymodels. Instead of training your model on the training data and building a results tibble using the test data, last_fit() accomplishes this with one function.\nIn this exercise, you will train the same logistic regression model as you fit in the previous exercises, except with the last_fit() function.\nYour data split object, telecom_split, and model specification, logistic_model, have been loaded into your session.\n\n# Train model with last_fit()\ntelecom_last_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n           split = telecom_split)\n\n# View test set metrics\ntelecom_last_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.721 Preprocessor1_Model1\n2 roc_auc  binary         0.751 Preprocessor1_Model1"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#collecting-predictions-and-creating-custom-metrics",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#collecting-predictions-and-creating-custom-metrics",
    "title": "Modeling with tidymodels in R",
    "section": "Collecting predictions and creating custom metrics",
    "text": "Collecting predictions and creating custom metrics\nUsing the last_fit() modeling workflow also saves time in collecting model predictions. Instead of manually creating a tibble of model results, there are helper functions that extract this information automatically.\nIn this exercise, you will use your trained model, telecom_last_fit, to create a tibble of model results on the test dataset as well as calculate custom performance metrics.\nYou trained model, telecom_last_fit, has been loaded into this session.\n\n# Collect predictions\nlast_fit_results <- telecom_last_fit %>% \n  collect_predictions()\n\n# View results\nlast_fit_results %>%\n  head()\n\n# A tibble: 6 × 7\n  id               .pred_yes .pred_no  .row .pred_class canceled_service .config\n  <chr>                <dbl>    <dbl> <int> <fct>       <fct>            <chr>  \n1 train/test split    0.364     0.636     2 no          yes              Prepro…\n2 train/test split    0.0166    0.983    22 no          no               Prepro…\n3 train/test split    0.252     0.748    23 no          no               Prepro…\n4 train/test split    0.448     0.552    30 no          yes              Prepro…\n5 train/test split    0.209     0.791    33 no          no               Prepro…\n6 train/test split    0.585     0.415    43 yes         no               Prepro…\n\n# Custom metrics function\nlast_fit_metrics <- metric_set(accuracy, sens,\n                               spec, roc_auc)\n\n# Calculate metrics\nlast_fit_metrics(last_fit_results,\n                 truth = canceled_service,\n                 estimate = .pred_class,\n                 .pred_yes)\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.721\n2 sens     binary         0.305\n3 spec     binary         0.932\n4 roc_auc  binary         0.751"
  },
  {
    "objectID": "myblog/datacamp/tidymodels/tidymodels.html#complete-modeling-workflow",
    "href": "myblog/datacamp/tidymodels/tidymodels.html#complete-modeling-workflow",
    "title": "Modeling with tidymodels in R",
    "section": "Complete modeling workflow",
    "text": "Complete modeling workflow\nIn this exercise, you will use the last_fit() function to train a logistic regression model and evaluate its performance on the test data by assessing the ROC curve and the area under the ROC curve.\nSimilar to previous exercises, you will predict canceled_service in the telecom_df data, but with an additional predictor variable to see if you can improve model performance.\nThe telecom_df tibble, telecom_split, and logistic_model objects from the previous exercises have been loaded into your workspace. The telecom_split object contains the instructions for randomly splitting the telecom_df tibble into training and test sets. The logistic_model object is a parsnip specification of a logistic regression model.\n\n# Train a logistic regression model\nlogistic_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins + avg_intl_mins + monthly_charges + months_with_company, \n           split = telecom_split)\n\n# Collect metrics\nlogistic_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.811 Preprocessor1_Model1\n2 roc_auc  binary         0.865 Preprocessor1_Model1\n\n# Collect model predictions\nlogistic_fit %>% \n  collect_predictions() %>% \n  # Plot ROC curve\n  roc_curve(truth = canceled_service, .pred_yes) %>% \n  autoplot()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Projects",
    "section": "",
    "text": "Data Science projects I have been working when I’m bored 😊"
  }
]