[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Personal Blog",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShap Calculation R\n\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting loan defaults\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2020\n\n\nmburu\n\n\n\n\n\n\n  \n\n\n\n\nPredict whether the cancer is benign or malignant\n\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2019\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssociation analysis\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2018\n\n\nMburu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "datacamp/regression_r/regression.html",
    "href": "datacamp/regression_r/regression.html",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "",
    "text": "We use the lm() function to fit linear models to data. In this case, we want to understand how the price of MarioKart games sold at auction varies as a function of not only the number of wheels included in the package, but also whether the item is new or used. Obviously, it is expected that you might have to pay a premium to buy these new. But how much is that premium? Can we estimate its value after controlling for the number of wheels?\nWe will fit a parallel slopes model using lm(). In addition to the data argument, lm() needs to know which variables you want to include in your regression model, and how you want to include them. It accomplishes this using a formula argument. A simple linear regression formula looks like y ~ x, where y is the name of the response variable, and x is the name of the explanatory variable. Here, we will simply extend this formula to include multiple explanatory variables. A parallel slopes model has the form y ~ x + z, where z is a categorical explanatory variable, and x is a numerical explanatory variable.\nThe output from lm() is a model object, which when printed, will show the fitted coefficients.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(openintro)\nlibrary(broom)\nlibrary(pander)\ndata( mariokart, package = \"openintro\")\nmario_kart <- mariokart\n\nmario_kart <- mario_kart %>% mutate(total_pr := ifelse(total_pr > 100, NA, total_pr))\n# Explore the data\nglimpse(mario_kart)\n\nRows: 143\nColumns: 12\n$ id          <dbl> 150377422259, 260483376854, 320432342985, 280405224677, 17…\n$ duration    <int> 3, 7, 3, 3, 1, 3, 1, 1, 3, 7, 1, 1, 1, 1, 7, 7, 3, 3, 1, 7…\n$ n_bids      <int> 20, 13, 16, 18, 20, 19, 13, 15, 29, 8, 15, 15, 13, 16, 6, …\n$ cond        <fct> new, used, new, new, new, new, used, new, used, used, new,…\n$ start_pr    <dbl> 0.99, 0.99, 0.99, 0.99, 0.01, 0.99, 0.01, 1.00, 0.99, 19.9…\n$ ship_pr     <dbl> 4.00, 3.99, 3.50, 0.00, 0.00, 4.00, 0.00, 2.99, 4.00, 4.00…\n$ total_pr    <dbl> 51.55, 37.04, 45.50, 44.00, 71.00, 45.00, 37.02, 53.99, 47…\n$ ship_sp     <fct> standard, firstClass, firstClass, standard, media, standar…\n$ seller_rate <int> 1580, 365, 998, 7, 820, 270144, 7284, 4858, 27, 201, 4858,…\n$ stock_photo <fct> yes, yes, no, yes, yes, yes, yes, yes, yes, no, yes, yes, …\n$ wheels      <int> 1, 1, 1, 1, 2, 0, 0, 2, 1, 1, 2, 2, 2, 2, 1, 0, 1, 1, 2, 2…\n$ title       <fct> \"~~ Wii MARIO KART &amp; WHEEL ~ NINTENDO Wii ~ BRAND NEW …\n\n# fit parallel slopes\n\nmod_mario <- lm(total_pr ~ wheels + cond, data = mario_kart)"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#reasoning-about-two-intercepts",
    "href": "datacamp/regression_r/regression.html#reasoning-about-two-intercepts",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Reasoning about two intercepts",
    "text": "Reasoning about two intercepts\nThe mario_kart data contains several other variables. The totalPr, startPr, and shipPr variables are numeric, while the cond and stockPhoto variables are categorical.\nWhich formula will result in a parallel slopes model?\n\ntotalPr ~ shipPr + stockPhoto"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#using-geom_line-and-augment",
    "href": "datacamp/regression_r/regression.html#using-geom_line-and-augment",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Using geom_line() and augment()",
    "text": "Using geom_line() and augment()\nParallel slopes models are so-named because we can visualize these models in the data space as not one line, but two parallel lines. To do this, we’ll draw two things:\na scatterplot showing the data, with color separating the points into groups a line for each value of the categorical variable Our plotting strategy is to compute the fitted values, plot these, and connect the points to form a line. The augment() function from the broom package provides an easy way to add the fitted values to our data frame, and the geom_line() function can then use that data frame to plot the points and connect them.\nNote that this approach has the added benefit of automatically coloring the lines appropriately to match the data.\nYou already know how to use ggplot() and geom_point() to make the scatterplot. The only twist is that now you’ll pass your augment()-ed model as the data argument in your ggplot() call. When you add your geom_line(), instead of letting the y aesthetic inherit its values from the ggplot() call, you can set it to the .fitted column of the augment()-ed model. This has the advantage of automatically coloring the lines for you.\n\n# Augment the model\naugmented_mod <- augment(mod_mario)\nglimpse(augmented_mod)\n\nRows: 141\nColumns: 10\n$ .rownames  <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"1…\n$ total_pr   <dbl> 51.55, 37.04, 45.50, 44.00, 71.00, 45.00, 37.02, 53.99, 47.…\n$ wheels     <int> 1, 1, 1, 1, 2, 0, 0, 2, 1, 1, 2, 2, 2, 2, 1, 0, 1, 1, 2, 0,…\n$ cond       <fct> new, used, new, new, new, new, used, new, used, used, new, …\n$ .fitted    <dbl> 49.60260, 44.01777, 49.60260, 49.60260, 56.83544, 42.36976,…\n$ .resid     <dbl> 1.9473995, -6.9777674, -4.1026005, -5.6026005, 14.1645592, …\n$ .hat       <dbl> 0.02103158, 0.01250410, 0.02103158, 0.02103158, 0.01915635,…\n$ .sigma     <dbl> 4.902339, 4.868399, 4.892414, 4.881308, 4.750591, 4.899816,…\n$ .cooksd    <dbl> 1.161354e-03, 8.712334e-03, 5.154337e-03, 9.612441e-03, 5.5…\n$ .std.resid <dbl> 0.40270893, -1.43671086, -0.84838977, -1.15857953, 2.926332…\n\n# scatterplot, with color\ndata_space <- ggplot(augmented_mod, aes(x = wheels, y = total_pr , color = cond )) + \n  geom_point()\n  \n# single call to geom_line()\ndata_space + \n  geom_line(aes(y = .fitted))"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#intercept-interpretation",
    "href": "datacamp/regression_r/regression.html#intercept-interpretation",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Intercept interpretation",
    "text": "Intercept interpretation\nRecall that the cond variable is either new or used. Here are the fitted coefficients from your model:\nCall: lm(formula = totalPr ~ wheels + cond, data = mario_kart)\nCoefficients: (Intercept) wheels condused\n42.370 7.233 -5.585\nChoose the correct interpretation of the coefficient on condused:\n\nThe expected price of a used MarioKart is $5.58 less than that of a new one with the same number of wheels.\nFor each additional wheel, the expected price of a MarioKart increases by $7.23 regardless of whether it is new or used.\n\nSyntax from math The babies data set contains observations about the birthweight and other characteristics of children born in the San Francisco Bay area from 1960–1967.\nWe would like to build a model for birthweight as a function of the mother’s age and whether this child was her first (parity == 0). Use the mathematical specification below to code the model in R.\n\\[birthweight = \\beta_0 + \\beta_1 * age  + \\beta_2 * parity + \\epsilon\\]\n\ndata( babies, package = \"openintro\")\n\nmod <- lm(bwt~ age+parity, data = babies)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n118.3\n2.788\n42.43\n3.957e-243\n\n\nage\n0.06315\n0.09577\n0.6594\n0.5097\n\n\nparity\n-1.652\n1.271\n-1.3\n0.1937"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#syntax-from-plot",
    "href": "datacamp/regression_r/regression.html#syntax-from-plot",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Syntax from plot",
    "text": "Syntax from plot\nThis time, we’d like to build a model for birthweight as a function of the length of gestation and the mother’s smoking status. Use the plot to inform your model specification.\n\nggplot(babies, aes(gestation, bwt, color = factor(smoke)))+\n    geom_point()\n\n\n\nmod <- lm(bwt~ gestation + smoke, data = babies)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.9317\n8.152\n-0.1143\n0.909\n\n\ngestation\n0.4429\n0.02902\n15.26\n3.156e-48\n\n\nsmoke\n-8.088\n0.9527\n-8.49\n5.963e-17\n\n\n\n\n\nR-squared vs. adjusted R-squared Two common measures of how well a model fits to data are \\[R^2\\] (the coefficient of determination) and the adjusted \\[R^2\\] . The former measures the percentage of the variability in the response variable that is explained by the model. To compute this, we define\n\\[R^2 = 1 - \\frac{sse}{sst} \\] where SSE and SST are the sum of the squared residuals, and the total sum of the squares, respectively. One issue with this measure is that the can only decrease as new variable are added to the model, while the SST depends only on the response variable and therefore is not affected by changes to the model. This means that you can increase \\[R^2\\] by adding any additional variable to your model—even random noise.\nThe adjusted \\[R^2\\] includes a term that penalizes a model for each additional explanatory variable (where is the number of explanatory variables). We can see both measures in the output of the summary() function on our model object.\n\n# R^2 and adjusted R^2\nsummary(mod_mario)\n\n\nCall:\nlm(formula = total_pr ~ wheels + cond, data = mario_kart)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.0078  -3.0754  -0.8254   2.9822  14.1646 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  42.3698     1.0651  39.780  < 2e-16 ***\nwheels        7.2328     0.5419  13.347  < 2e-16 ***\ncondused     -5.5848     0.9245  -6.041 1.35e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.887 on 138 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7165,    Adjusted R-squared:  0.7124 \nF-statistic: 174.4 on 2 and 138 DF,  p-value: < 2.2e-16\n\n# add random noise\nmario_kart_noisy <- mario_kart %>% \nmutate(noise = rnorm(n = nrow(mario_kart)))\n  \n# compute new model\nmod2_mario2 <- lm(total_pr ~ wheels + cond+noise, data = mario_kart_noisy)\n\n# new R^2 and adjusted R^2\nsummary(mod2_mario2)\n\n\nCall:\nlm(formula = total_pr ~ wheels + cond + noise, data = mario_kart_noisy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.3657  -3.3118  -0.7954   2.9268  13.1564 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  42.4557     1.0609  40.019  < 2e-16 ***\nwheels        7.1391     0.5423  13.164  < 2e-16 ***\ncondused     -5.5782     0.9196  -6.066 1.21e-08 ***\nnoise        -0.6503     0.4134  -1.573    0.118    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.862 on 137 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7215,    Adjusted R-squared:  0.7154 \nF-statistic: 118.3 on 3 and 137 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#prediction",
    "href": "datacamp/regression_r/regression.html#prediction",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Prediction",
    "text": "Prediction\nOnce we have fit a regression model, we can use it to make predictions for unseen observations or retrieve the fitted values. Here, we explore two methods for doing the latter.\nA traditional way to return the fitted values (i.e. the y ’s) is to run the predict() function on the model object. This will return a vector of the fitted values. Note that predict() will take an optional newdata argument that will allow you to make predictions for observations that are not in the original data.\nA newer alternative is the augment() function from the broom package, which returns a data.frame with the response varible (), the relevant explanatory variables (the ’s), the fitted value ( ) and some information about the residuals (). augment() will also take a newdata argument that allows you to make predictions.\n\n# return a vector\nlibrary(knitr)\n\npredict(mod_mario)\n\n       1        2        3        4        5        6        7        8 \n49.60260 44.01777 49.60260 49.60260 56.83544 42.36976 36.78493 56.83544 \n       9       10       11       12       13       14       15       16 \n44.01777 44.01777 56.83544 56.83544 56.83544 56.83544 44.01777 36.78493 \n      17       18       19       21       22       23       24       25 \n49.60260 49.60260 56.83544 36.78493 56.83544 56.83544 56.83544 44.01777 \n      26       27       28       29       30       31       32       33 \n56.83544 36.78493 36.78493 36.78493 49.60260 36.78493 36.78493 44.01777 \n      34       35       36       37       38       39       40       41 \n51.25061 44.01777 44.01777 36.78493 44.01777 56.83544 56.83544 49.60260 \n      42       43       44       45       46       47       48       49 \n44.01777 51.25061 56.83544 56.83544 44.01777 56.83544 36.78493 36.78493 \n      50       51       52       53       54       55       56       57 \n44.01777 56.83544 36.78493 44.01777 42.36976 36.78493 36.78493 44.01777 \n      58       59       60       61       62       63       64       66 \n44.01777 36.78493 36.78493 56.83544 36.78493 56.83544 36.78493 51.25061 \n      67       68       69       70       71       72       73       74 \n56.83544 44.01777 58.48345 51.25061 49.60260 44.01777 49.60260 56.83544 \n      75       76       77       78       79       80       81       82 \n56.83544 51.25061 44.01777 36.78493 36.78493 36.78493 44.01777 56.83544 \n      83       84       85       86       87       88       89       90 \n44.01777 65.71629 44.01777 56.83544 36.78493 49.60260 49.60260 36.78493 \n      91       92       93       94       95       96       97       98 \n44.01777 36.78493 51.25061 44.01777 36.78493 51.25061 42.36976 56.83544 \n      99      100      101      102      103      104      105      106 \n51.25061 44.01777 51.25061 56.83544 56.83544 56.83544 36.78493 49.60260 \n     107      108      109      110      111      112      113      114 \n51.25061 44.01777 56.83544 49.60260 36.78493 44.01777 51.25061 56.83544 \n     115      116      117      118      119      120      121      122 \n64.06828 44.01777 49.60260 44.01777 49.60260 51.25061 42.36976 44.01777 \n     123      124      125      126      127      128      129      130 \n56.83544 44.01777 49.60260 44.01777 51.25061 56.83544 56.83544 49.60260 \n     131      132      133      134      135      136      137      138 \n56.83544 36.78493 44.01777 44.01777 36.78493 56.83544 36.78493 44.01777 \n     139      140      141      142      143 \n36.78493 51.25061 49.60260 36.78493 56.83544 \n\n# return a data frame\n\naugment(mod_mario)%>% head() %>% kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.rownames\ntotal_pr\nwheels\ncond\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n1\n51.55\n1\nnew\n49.60260\n1.947399\n0.0210316\n4.902340\n0.0011614\n0.4027089\n\n\n2\n37.04\n1\nused\n44.01777\n-6.977767\n0.0125041\n4.868399\n0.0087123\n-1.4367109\n\n\n3\n45.50\n1\nnew\n49.60260\n-4.102601\n0.0210316\n4.892414\n0.0051543\n-0.8483898\n\n\n4\n44.00\n1\nnew\n49.60260\n-5.602601\n0.0210316\n4.881308\n0.0096124\n-1.1585795\n\n\n5\n71.00\n2\nnew\n56.83544\n14.164559\n0.0191563\n4.750591\n0.0557493\n2.9263328\n\n\n6\n45.00\n0\nnew\n42.36976\n2.630240\n0.0474932\n4.899816\n0.0050537\n0.5514192"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#thought-experiments",
    "href": "datacamp/regression_r/regression.html#thought-experiments",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Thought experiments",
    "text": "Thought experiments\nSuppose that after going apple picking you have 12 apples left over. You decide to conduct an experiment to investigate how quickly they will rot under certain conditions. You place six apples in a cool spot in your basement, and leave the other six on the window sill in the kitchen. Every week, you estimate the percentage of the surface area of the apple that is rotten or moldy.\nConsider the following models:\n\\[rot = \\beta_0 + \\beta_1 *t + \\beta_2 * temp + \\epsilon \\]\nand\n\\[rot = \\beta_0 + \\beta_1 *t + \\beta_2 * temp + \\beta_2 * temp *t +  \\epsilon \\]\n\nThe rate at which apples rot will vary based on the temperature."
  },
  {
    "objectID": "datacamp/regression_r/regression.html#fitting-a-model-with-interaction",
    "href": "datacamp/regression_r/regression.html#fitting-a-model-with-interaction",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Fitting a model with interaction",
    "text": "Fitting a model with interaction\nIncluding an interaction term in a model is easy—we just have to tell lm() that we want to include that new variable. An expression of the form\nlm(y ~ x + z + x:z, data = mydata)\nwill do the trick. The use of the colon (:) here means that the interaction between and will be a third term in the model.\n\n# include interaction\n\nmod <- lm(total_pr ~cond + duration + cond:duration, data = mario_kart)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n58.27\n1.366\n42.64\n5.832e-81\n\n\ncondused\n-17.12\n2.178\n-7.86\n1.014e-12\n\n\nduration\n-1.966\n0.4488\n-4.38\n2.342e-05\n\n\ncondused:duration\n2.325\n0.5484\n4.239\n4.102e-05"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#visualizing-interaction-models",
    "href": "datacamp/regression_r/regression.html#visualizing-interaction-models",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Visualizing interaction models",
    "text": "Visualizing interaction models\nInteraction allows the slope of the regression line in each group to vary. In this case, this means that the relationship between the final price and the length of the auction is moderated by the condition of each item.\nInteraction models are easy to visualize in the data space with ggplot2 because they have the same coefficients as if the models were fit independently to each group defined by the level of the categorical variable. In this case, new and used MarioKarts each get their own regression line. To see this, we can set an aesthetic (e.g. color) to the categorical variable, and then add a geom_smooth() layer to overlay the regression line for each color.\n\n# interaction plot\nggplot(mario_kart, aes(duration, total_pr, color = cond)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = 0)"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#consequences-of-simpsons-paradox",
    "href": "datacamp/regression_r/regression.html#consequences-of-simpsons-paradox",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Consequences of Simpson’s paradox",
    "text": "Consequences of Simpson’s paradox\nIn the simple linear regression model for average SAT score, (total) as a function of average teacher salary (salary), the fitted coefficient was -5.02 points per thousand dollars. This suggests that for every additional thousand dollars of salary for teachers in a particular state, the expected SAT score for a student from that state is about 5 points lower.\nIn the model that includes the percentage of students taking the SAT, the coefficient on salary becomes 1.84 points per thousand dollars. Choose the correct interpretation of this slope coefficient.\n\nFor every additional thousand dollars of salary for teachers in a particular state, the expected SAT score for a student from that state is about 2 points higher, after controlling for the percentage of students taking the SAT."
  },
  {
    "objectID": "datacamp/regression_r/regression.html#simpsons-paradox-in-action",
    "href": "datacamp/regression_r/regression.html#simpsons-paradox-in-action",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Simpson’s paradox in action",
    "text": "Simpson’s paradox in action\nA mild version of Simpson’s paradox can be observed in the MarioKart auction data. Consider the relationship between the final auction price and the length of the auction. It seems reasonable to assume that longer auctions would result in higher prices, since—other things being equal—a longer auction gives more bidders more time to see the auction and bid on the item.\nHowever, a simple linear regression model reveals the opposite: longer auctions are associated with lower final prices. The problem is that all other things are not equal. In this case, the new MarioKarts—which people pay a premium for—were mostly sold in one-day auctions, while a plurality of the used MarioKarts were sold in the standard seven-day auctions.\nOur simple linear regression model is misleading, in that it suggests a negative relationship between final auction price and duration. However, for the used MarioKarts, the relationship is positive.\n\nslr <- ggplot(mario_kart, aes(y = total_pr, x = duration)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n# model with one slope\nmod <- lm(total_pr ~ duration, data = mario_kart)\n\n# plot with two slopes\nslr + aes(color = cond)"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#fitting-a-mlr-model",
    "href": "datacamp/regression_r/regression.html#fitting-a-mlr-model",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Fitting a MLR model",
    "text": "Fitting a MLR model\nIn terms of the R code, fitting a multiple linear regression model is easy: simply add variables to the model formula you specify in the lm() command.\nIn a parallel slopes model, we had two explanatory variables: one was numeric and one was categorical. Here, we will allow both explanatory variables to be numeric.\n\n# Fit the model using duration and startPr\n\nmod <- lm(total_pr~ start_pr + duration, data = mario_kart)\n\ntidy(mod) %>% pander()\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n51.03\n1.179\n43.28\n3.666e-82\n\n\nstart_pr\n0.233\n0.04364\n5.339\n3.756e-07\n\n\nduration\n-1.508\n0.2555\n-5.902\n2.645e-08"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#tiling-the-plane",
    "href": "datacamp/regression_r/regression.html#tiling-the-plane",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Tiling the plane",
    "text": "Tiling the plane\nOne method for visualizing a multiple linear regression model is to create a heatmap of the fitted values in the plane defined by the two explanatory variables. This heatmap will illustrate how the model output changes over different combinations of the explanatory variables.\nThis is a multistep process:\nFirst, create a grid of the possible pairs of values of the explanatory variables. The grid should be over the actual range of the data present in each variable. We’ve done this for you and stored the result as a data frame called grid. Use augment() with the newdata argument to find the ’s corresponding to the values in grid. Add these to the data_space plot by using the fill aesthetic and geom_tile().\n\n# add predictions to grid\nprice_hats <- augment(mod, newdata = grid)\n\n# tile the plane\ndata_space + \n  geom_tile(data = price_hats, aes(fill = .fitted), alpha = 0.5)"
  },
  {
    "objectID": "datacamp/regression_r/regression.html#models-in-3d",
    "href": "datacamp/regression_r/regression.html#models-in-3d",
    "title": "Multiple Linear and Logistic Regression in R",
    "section": "Models in 3D",
    "text": "Models in 3D\nAn alternative way to visualize a multiple regression model with two numeric explanatory variables is as a plane in three dimensions. This is possible in R using the plotly package.\nWe have created three objects that you will need:\nx: a vector of unique values of duration y: a vector of unique values of startPr plane: a matrix of the fitted values across all combinations of x and y Much like ggplot(), the plot_ly() function will allow you to create a plot object with variables mapped to x, y, and z aesthetics. The add_markers() function is similar to geom_point() in that it allows you to add points to your 3D plot.\nNote that plot_ly uses the pipe (%>%) operator to chain commands together.\n\n# draw the 3D scatterplot\np <- plot_ly(data = mario_kart, z = ~totalPr, x = ~duration, y = ~startPr, opacity = 0.6) %>%\n  add_markers() \n  \n# draw the plane\np %>%\n  add_surface(x = ~x, y = ~y, z = ~plane, showscale = FALSE)"
  },
  {
    "objectID": "datacamp/scalable_data_R/scalable_data_r.html",
    "href": "datacamp/scalable_data_R/scalable_data_r.html",
    "title": "Scalable Data Processing in R",
    "section": "",
    "text": "If you are processing all elements of two data sets, and one data set is bigger, then the bigger data set will take longer to process. However, it’s important to realize that how much longer it takes is not always directly proportional to how much bigger it is. That is, if you have two data sets and one is two times the size of the other, it is not guaranteed that the larger one will take twice as long to process. It could take 1.5 times longer or even four times longer. It depends on which operations are used to process the data set.\nIn this exercise, you’ll use the microbenchmark package, which was covered in the Writing Efficient R Code course.\nNote: Numbers are specified using scientific notation\n\n# Load the microbenchmark package\nlibrary(microbenchmark)\n\n# Compare the timings for sorting different sizes of vector\nmb <- microbenchmark(\n  # Sort a random normal vector length 1e5\n  \"1e5\" = sort(rnorm(1e5)),\n  # Sort a random normal vector length 2.5e5\n  \"2.5e5\" = sort(rnorm(2.5e5)),\n  # Sort a random normal vector length 5e5\n  \"5e5\" = sort(rnorm(5e5)),\n  \"7.5e5\" = sort(rnorm(7.5e5)),\n  \"1e6\" = sort(rnorm(1e6)),\n  times = 10\n)\n\n# Plot the resulting benchmark object\nplot(mb)"
  },
  {
    "objectID": "datacamp/scalable_data_R/scalable_data_r.html#reading-a-big.matrix-object",
    "href": "datacamp/scalable_data_R/scalable_data_r.html#reading-a-big.matrix-object",
    "title": "Scalable Data Processing in R",
    "section": "Reading a big.matrix object",
    "text": "Reading a big.matrix object\nIn this exercise, you’ll create your first file-backed big.matrix object using the read.big.matrix() function. The function is meant to look similar to read.table() but, in addition, it needs to know what type of numeric values you want to read (“char”, “short”, “integer”, “double”), it needs the name of the file that will hold the matrix’s data (the backing file), and it needs the name of the file to hold information about the matrix (a descriptor file). The result will be a file on the disk holding the value read in along with a descriptor file which holds extra information (like the number of columns and rows) about the resulting big.matrix object.\n\n# Load the bigmemory package\nlibrary(bigmemory)\n\n# Create the big.matrix object: x\nx <- read.big.matrix(\"mortgage-sample.csv\", header = TRUE, \n                     type = \"integer\", \n                     backingfile = \"mortgage-sample.bin\", \n                     descriptorfile = \"mortgage-sample.desc\")\n    \n# Find the dimensions of x\ndim(x)\n\n[1] 70000    16"
  },
  {
    "objectID": "datacamp/scalable_data_R/scalable_data_r.html#attaching-a-big.matrix-object",
    "href": "datacamp/scalable_data_R/scalable_data_r.html#attaching-a-big.matrix-object",
    "title": "Scalable Data Processing in R",
    "section": "Attaching a big.matrix object",
    "text": "Attaching a big.matrix object\nNow that the big.matrix object is on the disk, we can use the information stored in the descriptor file to instantly make it available during an R session. This means that you don’t have to reimport the data set, which takes more time for larger files. You can simply point the bigmemory package at the existing structures on the disk and begin accessing data without the wait.\n\n# Attach mortgage-sample.desc\nmort <- attach.big.matrix(\"mortgage-sample.desc\")\n\n# Find the dimensions of mort\ndim(mort)\n\n[1] 70000    16\n\n# Look at the first 6 rows of mort\nhead(mort)\n\n     enterprise record_number msa perc_minority tract_income_ratio\n[1,]          1           566   1             1                  3\n[2,]          1           116   1             3                  2\n[3,]          1           239   1             2                  2\n[4,]          1            62   1             2                  3\n[5,]          1           106   1             2                  3\n[6,]          1           759   1             3                  3\n     borrower_income_ratio loan_purpose federal_guarantee borrower_race\n[1,]                     1            2                 4             3\n[2,]                     1            2                 4             5\n[3,]                     3            8                 4             5\n[4,]                     3            2                 4             5\n[5,]                     3            2                 4             9\n[6,]                     2            2                 4             9\n     co_borrower_race borrower_gender co_borrower_gender num_units\n[1,]                9               2                  4         1\n[2,]                9               1                  4         1\n[3,]                5               1                  2         1\n[4,]                9               2                  4         1\n[5,]                9               3                  4         1\n[6,]                9               1                  2         2\n     affordability year type\n[1,]             3 2010    1\n[2,]             3 2008    1\n[3,]             4 2014    0\n[4,]             4 2009    1\n[5,]             4 2013    1\n[6,]             4 2010    1"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html",
    "href": "datacamp/tidymodels/tidymodels.html",
    "title": "Modeling with tidymodels in R",
    "section": "",
    "text": "The rsample package is designed to create training and test datasets. Creating a test dataset is important for estimating how a trained model will likely perform on new data. It also guards against overfitting, where a model memorizes patterns that exist only in the training data and performs poorly on new data.\nIn this exercise, you will create training and test datasets from the home_sales data. This data contains information on homes sold in the Seattle, Washington area between 2015 and 2016.\nThe outcome variable in this data is selling_price.\nThe tidymodels package will be pre-loaded in every exercise in the course. The home_sales tibble has also been loaded for you.\n\n\n\nTidy model packages\n\n\n\nhome_sales <- readRDS(\"home_sales.rds\")\n\n# Create a data split object\nhome_split <- initial_split(home_sales, \n                            prop = 0.7, \n                            strata = selling_price)\n\n# Create the training data\nhome_training <- home_split %>%\n  training()\n\n# Create the test data\nhome_test <- home_split %>% \n  testing()\n\n# Check number of rows in each dataset\nnrow(home_training)\n\n[1] 1042\n\nnrow(home_test)\n\n[1] 450\n\n\nDistribution of outcome variable values Stratifying by the outcome variable when generating training and test datasets ensures that the outcome variable values have a similar range in both datasets.\nSince the original data is split at random, stratification avoids placing all the expensive homes in home_sales into the test dataset, for example. In this case, your model would most likely perform poorly because it was trained on less expensive homes.\nIn this exercise, you will calculate summary statistics for the selling_price variable in the training and test datasets. The home_training and home_test tibbles have been loaded from the previous exercise.\n\n# Distribution of selling_price in training data\nlibrary(knitr)\nsummary_func <- function(df){\n      df %>% summarize(min_sell_price = min(selling_price),\n            max_sell_price = max(selling_price),\n            mean_sell_price = mean(selling_price),\n            sd_sell_price = sd(selling_price)) %>%\n    kable()\n\n}\nhome_training %>% \n    summary_func()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n478681.5\n80542.75\n\n\n\n\nhome_test %>% \n  summary_func()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n480017.6\n82062.69"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#fitting-a-linear-regression-model",
    "href": "datacamp/tidymodels/tidymodels.html#fitting-a-linear-regression-model",
    "title": "Modeling with tidymodels in R",
    "section": "Fitting a linear regression model",
    "text": "Fitting a linear regression model\nThe parsnip package provides a unified syntax for the model fitting process in R.\nWith parsnip, it is easy to define models using the various packages, or engines, that exist in the R ecosystem.\nIn this exercise, you will define a parsnip linear regression object and train your model to predict selling_price using home_age and sqft_living as predictor variables from the home_sales data.\nThe home_training and home_test tibbles that you created in the previous lesson have been loaded into this session.\n\n# Specify a linear regression model, linear_model\nlinear_model <- linear_reg() %>% \n  # Set the model engine\n  set_engine('lm') %>% \n  # Set the model mode\n  set_mode('regression')\n\n# Train the model with the training data\nlm_fit <- linear_model %>% \n  fit(selling_price~ home_age + sqft_living,\n      data = home_training)\n\n# Print lm_fit to view model information\ntidy(lm_fit) %>%\n    kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n291959.1998\n7409.281062\n39.40452\n0\n\n\nhome_age\n-1501.8965\n170.875877\n-8.78940\n0\n\n\nsqft_living\n103.6533\n2.698268\n38.41474\n0"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#predicting-home-selling-prices",
    "href": "datacamp/tidymodels/tidymodels.html#predicting-home-selling-prices",
    "title": "Modeling with tidymodels in R",
    "section": "Predicting home selling prices",
    "text": "Predicting home selling prices\nAfter fitting a model using the training data, the next step is to use it to make predictions on the test dataset. The test dataset acts as a new source of data for the model and will allow you to evaluate how well it performs.\nBefore you can evaluate model performance, you must add your predictions to the test dataset.\nIn this exercise, you will use your trained model, lm_fit, to predict selling_price in the home_test dataset.\nYour trained model, lm_fit, as well as the test dataset, home_test have been loaded into your session.\n\n# Predict selling_price\nhome_predictions <- predict(lm_fit,\n                        new_data = home_test)\n\n# View predicted selling prices\n#home_predictions\n\n# Combine test data with predictions\nhome_test_results <- home_test %>% \n  select(selling_price, home_age, sqft_living) %>% \n  bind_cols(home_predictions)\n\n# View results\nhome_test_results %>% \n    head()\n\n# A tibble: 6 × 4\n  selling_price home_age sqft_living   .pred\n          <dbl>    <dbl>       <dbl>   <dbl>\n1        356000       24        1430 404138.\n2        552321       29        1960 451565.\n3        525000       28        2450 503857.\n4        450000       25        1990 460682.\n5        624000       26        3570 622952.\n6        452000       27        1530 409997."
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#model-performance-metrics",
    "href": "datacamp/tidymodels/tidymodels.html#model-performance-metrics",
    "title": "Modeling with tidymodels in R",
    "section": "Model performance metrics",
    "text": "Model performance metrics\nEvaluating model results is an important step in the modeling process. Model evaluation should be done on the test dataset in order to see how well a model will generalize to new datasets.\nIn the previous exercise, you trained a linear regression model to predict selling_price using home_age and sqft_living as predictor variables. You then created the home_test_results tibble using your trained model on the home_test data.\nIn this exercise, you will calculate the RMSE and R squared metrics using your results in home_test_results.\nThe home_test_results tibble has been loaded into your session.\n\n# Print home_test_results\n#home_test_results\n\n# Caculate the RMSE metric\nhome_test_results %>% \n  rmse(truth = selling_price, estimate =.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      49066.\n\n# Calculate the R squared metric\nhome_test_results %>% \n  rsq(truth = selling_price, estimate =.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.645"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#r-squared-plot",
    "href": "datacamp/tidymodels/tidymodels.html#r-squared-plot",
    "title": "Modeling with tidymodels in R",
    "section": "R squared plot",
    "text": "R squared plot\nIn the previous exercise, you got an R squared value of 0.651. The R squared metric ranges from 0 to 1, 0 being the worst and 1 the best.\nCalculating the R squared value is only the first step in studying your model’s predictions.\nMaking an R squared plot is extremely important because it will uncover potential problems with your model, such as non-linear patterns or regions where your model is either over or under-predicting the outcome variable.\nIn this exercise, you will create an R squared plot of your model’s performance.\nThe home_test_results tibble has been loaded into your session.\n\n# Create an R squared plot of model performance\nggplot(home_test_results, aes(x = selling_price, y = .pred)) +\n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred()  +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#complete-model-fitting-process-with-last_fit",
    "href": "datacamp/tidymodels/tidymodels.html#complete-model-fitting-process-with-last_fit",
    "title": "Modeling with tidymodels in R",
    "section": "Complete model fitting process with last_fit()",
    "text": "Complete model fitting process with last_fit()\nIn this exercise, you will train and evaluate the performance of a linear regression model that predicts selling_price using all the predictors available in the home_sales tibble.\nThis exercise will give you a chance to perform the entire model fitting process with tidymodels, from defining your model object to evaluating its performance on the test data.\nEarlier in the chapter, you created an rsample object called home_split by passing the home_sales tibble into initial_split(). The home_split object contains the instructions for randomly splitting home_sales into training and test sets.\nThe home_sales tibble, and home_split object have been loaded into this session.\n\n# Define a linear regression model\nlinear_model <- linear_reg() %>% \n  set_engine('lm') %>% \n  set_mode('regression')\n\n# Train linear_model with last_fit()\nlinear_fit <- linear_model %>% \n  last_fit(selling_price ~ ., split = home_split)\n\n# Collect predictions and view results\npredictions_df <- linear_fit %>% collect_predictions()\npredictions_df %>% head()\n\n# A tibble: 6 × 5\n  id                 .pred  .row selling_price .config             \n  <chr>              <dbl> <int>         <dbl> <chr>               \n1 train/test split 441486.    11        356000 Preprocessor1_Model1\n2 train/test split 469665.    15        552321 Preprocessor1_Model1\n3 train/test split 459006.    19        525000 Preprocessor1_Model1\n4 train/test split 478718.    21        450000 Preprocessor1_Model1\n5 train/test split 629032.    22        624000 Preprocessor1_Model1\n6 train/test split 409903.    23        452000 Preprocessor1_Model1\n\n# Make an R squared plot using predictions_df\nggplot(predictions_df, aes(x = selling_price, y = .pred)) + \n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred() +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#data-resampling",
    "href": "datacamp/tidymodels/tidymodels.html#data-resampling",
    "title": "Modeling with tidymodels in R",
    "section": "Data resampling",
    "text": "Data resampling\nThe first step in a machine learning project is to create training and test datasets for model fitting and evaluation. The test dataset provides an estimate of how your model will perform on new data and helps to guard against overfitting.\nYou will be working with the telecom_df dataset which contains information on customers of a telecommunications company. The outcome variable is canceled_service and it records whether a customer canceled their contract with the company. The predictor variables contain information about customers’ cell phone and internet usage as well as their contract type and monthly charges.\nThe telecom_df tibble has been loaded into your session.\n\ntelecom_df <- readRDS(\"telecom_df.rds\")\n# Create data split object\ntelecom_split <- initial_split(telecom_df, prop = 0.75,\n                     strata = canceled_service)\n\n# Create the training data\ntelecom_training <- telecom_split %>% \n  training()\n\n# Create the test data\ntelecom_test <- telecom_split %>% \n  testing()\n\n# Check the number of rows\nnrow(telecom_training)\n\n[1] 731\n\nnrow(telecom_test)\n\n[1] 244"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#fitting-a-logistic-regression-model",
    "href": "datacamp/tidymodels/tidymodels.html#fitting-a-logistic-regression-model",
    "title": "Modeling with tidymodels in R",
    "section": "Fitting a logistic regression model",
    "text": "Fitting a logistic regression model\nIn addition to regression models, the parsnip package also provides a general interface to classification models in R.\nIn this exercise, you will define a parsnip logistic regression object and train your model to predict canceled_service using avg_call_mins, avg_intl_mins, and monthly_charges as predictor variables from the telecom_df data.\nThe telecom_training and telecom_test tibbles that you created in the previous lesson have been loaded into this session.\n\n# Specify a logistic regression model\nlogistic_model <- logistic_reg() %>% \n  # Set the engine\n  set_engine('glm') %>% \n  # Set the mode\n  set_mode('classification')\n\n# Fit to training data\nlogistic_fit <- logistic_model %>% \n  fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n      data = telecom_training)\n\n# Print model fit object\nlogistic_fit %>% tidy()\n\n# A tibble: 4 × 5\n  term            estimate std.error statistic  p.value\n  <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)      1.73      0.600        2.89 3.91e- 3\n2 avg_call_mins   -0.0112    0.00133     -8.43 3.42e-17\n3 avg_intl_mins    0.0230    0.00319      7.23 4.95e-13\n4 monthly_charges  0.00672   0.00476      1.41 1.58e- 1"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#combining-test-dataset-results",
    "href": "datacamp/tidymodels/tidymodels.html#combining-test-dataset-results",
    "title": "Modeling with tidymodels in R",
    "section": "Combining test dataset results",
    "text": "Combining test dataset results\nEvaluating your model’s performance on the test dataset gives insights into how well your model predicts on new data sources. These insights will help you communicate your model’s value in solving problems or improving decision making.\nBefore you can calculate classification metrics such as sensitivity or specificity, you must create a results tibble with the required columns for yardstick metric functions.\nIn this exercise, you will use your trained model to predict the outcome variable in the telecom_test dataset and combine it with the true outcome values in the canceled_service column.\nYour trained model, logistic_fit, and test dataset, telecom_test, have been loaded from the previous exercise.\n\n# Predict outcome categories\nclass_preds <- predict(logistic_fit, new_data = telecom_test,\n                       type = 'class')\n\n# Obtain estimated probabilities for each outcome value\nprob_preds <- predict(logistic_fit, new_data = telecom_test, \n                      type = 'prob')\n\n# Combine test set results\ntelecom_results <- telecom_test %>% \n  select(canceled_service) %>% \n  bind_cols(class_preds, prob_preds)\n\n# View results tibble\ntelecom_results %>%\n    head()\n\n# A tibble: 6 × 4\n  canceled_service .pred_class .pred_yes .pred_no\n  <fct>            <fct>           <dbl>    <dbl>\n1 yes              yes            0.796     0.204\n2 yes              no             0.168     0.832\n3 no               no             0.478     0.522\n4 no               yes            0.642     0.358\n5 no               no             0.0133    0.987\n6 no               no             0.223     0.777"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#evaluating-performance-with-yardstick",
    "href": "datacamp/tidymodels/tidymodels.html#evaluating-performance-with-yardstick",
    "title": "Modeling with tidymodels in R",
    "section": "Evaluating performance with yardstick",
    "text": "Evaluating performance with yardstick\nIn the previous exercise, you calculated classification metrics from a sample confusion matrix. The yardstick package was designed to automate this process.\nFor classification models, yardstick functions require a tibble of model results as the first argument. This should include the actual outcome values, predicted outcome values, and estimated probabilities for each value of the outcome variable.\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate performance metrics.\nThe telecom_results tibble has been loaded into your session.\n\n# Calculate the confusion matrix\nconf_mat(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n          Truth\nPrediction yes  no\n       yes  28  21\n       no   54 141\n\n# Calculate the accuracy\naccuracy(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.693\n\n# Calculate the sensitivity\n\nsens(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.341\n\n# Calculate the specificity\n\nspec(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 spec    binary         0.870"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#creating-custom-metric-sets",
    "href": "datacamp/tidymodels/tidymodels.html#creating-custom-metric-sets",
    "title": "Modeling with tidymodels in R",
    "section": "Creating custom metric sets",
    "text": "Creating custom metric sets\nThe yardstick package also provides the ability to create custom sets of model metrics. In cases where the cost of obtaining false negative errors is different from the cost of false positive errors, it may be important to examine a specific set of performance metrics.\nInstead of calculating accuracy, sensitivity, and specificity separately, you can create your own metric function that calculates all three at the same time.\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate a custom set of performance metrics. You will also use a confusion matrix to calculate all available binary classification metrics in tidymodelsall at once.\nThe telecom_results tibble has been loaded into your session.\n\n# Create a custom metric function\ntelecom_metrics <- metric_set(accuracy, sens, spec)\n\n# Calculate metrics using model results tibble\ntelecom_metrics(telecom_results, \n                truth = canceled_service,\n                estimate = .pred_class)\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.693\n2 sens     binary         0.341\n3 spec     binary         0.870\n\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class) %>% \n  # Pass to the summary() function\n  summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary         0.693\n 2 kap                  binary         0.235\n 3 sens                 binary         0.341\n 4 spec                 binary         0.870\n 5 ppv                  binary         0.571\n 6 npv                  binary         0.723\n 7 mcc                  binary         0.250\n 8 j_index              binary         0.212\n 9 bal_accuracy         binary         0.606\n10 detection_prevalence binary         0.201\n11 precision            binary         0.571\n12 recall               binary         0.341\n13 f_meas               binary         0.427"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#plotting-the-confusion-matrix",
    "href": "datacamp/tidymodels/tidymodels.html#plotting-the-confusion-matrix",
    "title": "Modeling with tidymodels in R",
    "section": "Plotting the confusion matrix",
    "text": "Plotting the confusion matrix\nCalculating performance metrics with the yardstick package provides insight into how well a classification model is performing on the test dataset. Most yardstick functions return a single number that summarizes classification performance.\nMany times, it is helpful to create visualizations of the confusion matrix to more easily communicate your results.\nIn this exercise, you will make a heat map and mosaic plot of the confusion matrix from your logistic regression model on the telecom_df dataset.\nYour model results tibble, telecom_results, has been loaded into your session.\n\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a heat map\n  autoplot(type = \"heatmap\")\n\n\n\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a mosaic plot\n  autoplot(type = \"mosaic\")"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#roc-curves-and-area-under-the-roc-curve",
    "href": "datacamp/tidymodels/tidymodels.html#roc-curves-and-area-under-the-roc-curve",
    "title": "Modeling with tidymodels in R",
    "section": "ROC curves and area under the ROC curve",
    "text": "ROC curves and area under the ROC curve\nROC curves are used to visualize the performance of a classification model across a range of probability thresholds. An ROC curve with the majority of points near the upper left corner of the plot indicates that a classification model is able to correctly predict both the positive and negative outcomes correctly across a wide range of probability thresholds.\nThe area under this curve provides a letter grade summary of model performance.\nIn this exercise, you will create an ROC curve from your logistic regression model results and calculate the area under the ROC curve with yardstick.\nYour model results tibble, telecom_results has been loaded into your session.\n\n# Calculate metrics across thresholds\nthreshold_df <- telecom_results %>% \n  roc_curve(truth = canceled_service, .pred_yes)\n\n# View results\nthreshold_df %>%\n  head()\n\n# A tibble: 6 × 3\n  .threshold specificity sensitivity\n       <dbl>       <dbl>       <dbl>\n1  -Inf          0                 1\n2     0.0122     0                 1\n3     0.0133     0.00617           1\n4     0.0237     0.0123            1\n5     0.0288     0.0185            1\n6     0.0324     0.0247            1\n\n# Plot ROC curve\nthreshold_df %>% \n  autoplot()\n\n\n\n# Calculate ROC AUC\nroc_auc(telecom_results, truth = canceled_service, .pred_yes)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.719\n\n\nStreamlining the modeling process The last_fit() function is designed to streamline the modeling workflow in tidymodels. Instead of training your model on the training data and building a results tibble using the test data, last_fit() accomplishes this with one function.\nIn this exercise, you will train the same logistic regression model as you fit in the previous exercises, except with the last_fit() function.\nYour data split object, telecom_split, and model specification, logistic_model, have been loaded into your session.\n\n# Train model with last_fit()\ntelecom_last_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n           split = telecom_split)\n\n# View test set metrics\ntelecom_last_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.693 Preprocessor1_Model1\n2 roc_auc  binary         0.719 Preprocessor1_Model1"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#collecting-predictions-and-creating-custom-metrics",
    "href": "datacamp/tidymodels/tidymodels.html#collecting-predictions-and-creating-custom-metrics",
    "title": "Modeling with tidymodels in R",
    "section": "Collecting predictions and creating custom metrics",
    "text": "Collecting predictions and creating custom metrics\nUsing the last_fit() modeling workflow also saves time in collecting model predictions. Instead of manually creating a tibble of model results, there are helper functions that extract this information automatically.\nIn this exercise, you will use your trained model, telecom_last_fit, to create a tibble of model results on the test dataset as well as calculate custom performance metrics.\nYou trained model, telecom_last_fit, has been loaded into this session.\n\n# Collect predictions\nlast_fit_results <- telecom_last_fit %>% \n  collect_predictions()\n\n# View results\nlast_fit_results %>%\n  head()\n\n# A tibble: 6 × 7\n  id               .pred_yes .pred_no  .row .pred_class canceled_service .config\n  <chr>                <dbl>    <dbl> <int> <fct>       <fct>            <chr>  \n1 train/test split    0.796     0.204     7 yes         yes              Prepro…\n2 train/test split    0.168     0.832    14 no          yes              Prepro…\n3 train/test split    0.478     0.522    16 no          no               Prepro…\n4 train/test split    0.642     0.358    18 yes         no               Prepro…\n5 train/test split    0.0133    0.987    22 no          no               Prepro…\n6 train/test split    0.223     0.777    23 no          no               Prepro…\n\n# Custom metrics function\nlast_fit_metrics <- metric_set(accuracy, sens,\n                               spec, roc_auc)\n\n# Calculate metrics\nlast_fit_metrics(last_fit_results,\n                 truth = canceled_service,\n                 estimate = .pred_class,\n                 .pred_yes)\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.693\n2 sens     binary         0.341\n3 spec     binary         0.870\n4 roc_auc  binary         0.719"
  },
  {
    "objectID": "datacamp/tidymodels/tidymodels.html#complete-modeling-workflow",
    "href": "datacamp/tidymodels/tidymodels.html#complete-modeling-workflow",
    "title": "Modeling with tidymodels in R",
    "section": "Complete modeling workflow",
    "text": "Complete modeling workflow\nIn this exercise, you will use the last_fit() function to train a logistic regression model and evaluate its performance on the test data by assessing the ROC curve and the area under the ROC curve.\nSimilar to previous exercises, you will predict canceled_service in the telecom_df data, but with an additional predictor variable to see if you can improve model performance.\nThe telecom_df tibble, telecom_split, and logistic_model objects from the previous exercises have been loaded into your workspace. The telecom_split object contains the instructions for randomly splitting the telecom_df tibble into training and test sets. The logistic_model object is a parsnip specification of a logistic regression model.\n\n# Train a logistic regression model\nlogistic_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins + avg_intl_mins + monthly_charges + months_with_company, \n           split = telecom_split)\n\n# Collect metrics\nlogistic_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.758 Preprocessor1_Model1\n2 roc_auc  binary         0.820 Preprocessor1_Model1\n\n# Collect model predictions\nlogistic_fit %>% \n  collect_predictions() %>% \n  # Plot ROC curve\n  roc_curve(truth = canceled_service, .pred_yes) %>% \n  autoplot()"
  },
  {
    "objectID": "datacamp/us_census/us_census.html",
    "href": "datacamp/us_census/us_census.html",
    "title": "Census data in r with tidycensus",
    "section": "",
    "text": "tidycensus is an R package designed to return data from the US Census Bureau ready for use within the Tidyverse.\nTo acquire data from the US Census Bureau using the tidycensus R package, you must first acquire and set a Census API key. After obtaining your key, you can install it for future use with the census_api_key() function in tidycensus.\nThis exercise uses a fake API key for purposes of illustration.\n\n# Load the tidycensus package into your R session\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(tigris)\nlibrary(here)\n\n# Define your Census API key and set it with census_api_key()\napi_key <- Sys.getenv(\"CENSUS_API_KEY\")\n\ncensus_api_key(api_key)\n\n##Setting a cache directory Spatial data from the US Census Bureau can get very big - sometimes hundreds of megabytes in size. By default, tigris functions download data from the US Census Bureau’s website - but this can get tiresome if downloading the same large datasets over and over. To resolve this, tigris includes an option to cache downloaded data on a user’s computer for future use, meaning that files only have to be downloaded from the Census website once. In this exercise, you’ll get acquainted with the caching functionality in tigris.\n\n# Set the cache directory\ntigris_cache_dir(here(\"tigris_cache\"))\n\n# Set the tigris_use_cache option\noptions(tigris_use_cache = TRUE)\n\n# Check to see that you've modified the option correctly\ngetOption(\"tigris_use_cache\")\n\n[1] TRUE"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#getting-census-data-with-tidycensus",
    "href": "datacamp/us_census/us_census.html#getting-census-data-with-tidycensus",
    "title": "Census data in r with tidycensus",
    "section": "Getting Census data with tidycensus",
    "text": "Getting Census data with tidycensus\nIn this exercise, you will load and inspect data from the 2010 US Census and 2012-2016 American Community Survey. The core functions of get_decennial() and get_acs() in tidycensus are used to obtain data from these sources; the 2010 Census and 2012-2016 ACS are the defaults for these functions, respectively.\nBy inspecting the data, you’ll get a sense of differences between decennial US Census data and data from the ACS, which is based on a sample and subject to a margin of error. Whereas get_decennial() returns a data value for each row, get_acs() returns estimate and moe columns representing the ACS estimate and margin of error.\n\n# Obtain and view state populations from the 2010 US Census\nstate_pop <- get_decennial(geography = \"state\", \n                           variables = \"P001001\")\n\nhead(state_pop)\n\n# A tibble: 6 × 4\n  GEOID NAME       variable    value\n  <chr> <chr>      <chr>       <dbl>\n1 01    Alabama    P001001   4779736\n2 02    Alaska     P001001    710231\n3 04    Arizona    P001001   6392017\n4 05    Arkansas   P001001   2915918\n5 06    California P001001  37253956\n6 22    Louisiana  P001001   4533372\n\n# Obtain and view state median household income from the 2012-2016 American Community Survey\nstate_income <- get_acs(geography = \"state\", \n                        variables = \"B19013_001\")\n\nhead(state_income)\n\n# A tibble: 6 × 5\n  GEOID NAME       variable   estimate   moe\n  <chr> <chr>      <chr>         <dbl> <dbl>\n1 01    Alabama    B19013_001    54943   377\n2 02    Alaska     B19013_001    80287  1113\n3 04    Arizona    B19013_001    65913   387\n4 05    Arkansas   B19013_001    52123   458\n5 06    California B19013_001    84097   236\n6 08    Colorado   B19013_001    80184   450"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#understanding-tidycensus-options",
    "href": "datacamp/us_census/us_census.html#understanding-tidycensus-options",
    "title": "Census data in r with tidycensus",
    "section": "Understanding tidycensus options",
    "text": "Understanding tidycensus options\nAs discussed in this lesson, Census data comprise thousands of variables available across dozens of geographies! Most of these geography-variable combinations are accessible with tidycensus; however, it helps to understand the package options.\nSome data, like Census tracts, are only available by state, and users might want to subset by county; tidycensus facilitates this with state and county parameters when appropriate. Additionally, tidycensus includes the Census variable ID in the variable column; however, a user might want to supply her own variable name, which can be accomplished with a named vector.\nYou’ll be using the Census variable B19013_001 here, which refers to median household income.\n\n# Get an ACS dataset for Census tracts in Texas by setting the state\ntx_income <- get_acs(geography = \"tract\",\n                     variables = \"B19013_001\",\n                     state = \"TX\")\n\n# Inspect the dataset\nhead(tx_income)\n\n# A tibble: 6 × 5\n  GEOID       NAME                                       variable estimate   moe\n  <chr>       <chr>                                      <chr>       <dbl> <dbl>\n1 48001950100 Census Tract 9501, Anderson County, Texas  B19013_…    61325  9171\n2 48001950401 Census Tract 9504.01, Anderson County, Te… B19013_…    92813 45136\n3 48001950402 Census Tract 9504.02, Anderson County, Te… B19013_…       NA    NA\n4 48001950500 Census Tract 9505, Anderson County, Texas  B19013_…    41713  6650\n5 48001950600 Census Tract 9506, Anderson County, Texas  B19013_…    32552 12274\n6 48001950700 Census Tract 9507, Anderson County, Texas  B19013_…    35811  5573\n\n# Get an ACS dataset for Census tracts in Travis County, TX\ntravis_income <- get_acs(geography = \"tract\",\n                         variables = \"B19013_001\", \n                         state = \"TX\",\n                         county = \"Travis\")\n\n# Inspect the dataset\nhead(travis_income)\n\n# A tibble: 6 × 5\n  GEOID       NAME                                    variable   estimate   moe\n  <chr>       <chr>                                   <chr>         <dbl> <dbl>\n1 48453000101 Census Tract 1.01, Travis County, Texas B19013_001   121964 31935\n2 48453000102 Census Tract 1.02, Travis County, Texas B19013_001   201417 26672\n3 48453000203 Census Tract 2.03, Travis County, Texas B19013_001    81994 14344\n4 48453000204 Census Tract 2.04, Travis County, Texas B19013_001    93219 26118\n5 48453000205 Census Tract 2.05, Travis County, Texas B19013_001    75000 24198\n6 48453000206 Census Tract 2.06, Travis County, Texas B19013_001    88342 10549\n\n# Supply custom variable names\ntravis_income2 <- get_acs(geography = \"tract\", \n                          variables = c(hhincome = \"B19013_001\"), \n                          state = \"TX\",\n                          county = \"Travis\")\n\n# Inspect the dataset\nhead(travis_income2)\n\n# A tibble: 6 × 5\n  GEOID       NAME                                    variable estimate   moe\n  <chr>       <chr>                                   <chr>       <dbl> <dbl>\n1 48453000101 Census Tract 1.01, Travis County, Texas hhincome   121964 31935\n2 48453000102 Census Tract 1.02, Travis County, Texas hhincome   201417 26672\n3 48453000203 Census Tract 2.03, Travis County, Texas hhincome    81994 14344\n4 48453000204 Census Tract 2.04, Travis County, Texas hhincome    93219 26118\n5 48453000205 Census Tract 2.05, Travis County, Texas hhincome    75000 24198\n6 48453000206 Census Tract 2.06, Travis County, Texas hhincome    88342 10549"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#tidy-and-wide-data-in-tidycensus",
    "href": "datacamp/us_census/us_census.html#tidy-and-wide-data-in-tidycensus",
    "title": "Census data in r with tidycensus",
    "section": "Tidy and wide data in tidycensus",
    "text": "Tidy and wide data in tidycensus\nBy default, tidycensus functions return tidy data frames, in which each row represents a unique unit-variable combination. However, at times it is useful to have each Census variable in its own column for some methods of visualization and analysis. To accomplish this, you can set output = “wide” in your calls to get_acs() or get_decennial(), which will place estimates/values and margins of error in their own columns.\n\n# Return county data in wide format\nor_wide <- get_acs(geography = \"county\", \n                     state = \"OR\",\n                     variables = c(hhincome = \"B19013_001\", \n                            medage = \"B01002_001\"), \n                     output = \"wide\")\n\n# Compare output to the tidy format from previous exercises\nhead(or_wide)\n\n# A tibble: 6 × 6\n  GEOID NAME                     hhincomeE hhincomeM medageE medageM\n  <chr> <chr>                        <dbl>     <dbl>   <dbl>   <dbl>\n1 41001 Baker County, Oregon         46922      3271    47.7     0.9\n2 41003 Benton County, Oregon        68732      2689    33.3     0.3\n3 41005 Clackamas County, Oregon     88517      1424    41.6     0.2\n4 41007 Clatsop County, Oregon       61846      2651    44.5     0.4\n5 41009 Columbia County, Oregon      73909      3517    43.3     0.4\n6 41011 Coos County, Oregon          52548      3145    48.4     0.3\n\n# Create a scatterplot\nplot(or_wide$hhincomeE, or_wide$medageE)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#loading-variables-in-tidycensus",
    "href": "datacamp/us_census/us_census.html#loading-variables-in-tidycensus",
    "title": "Census data in r with tidycensus",
    "section": "Loading variables in tidycensus",
    "text": "Loading variables in tidycensus\nThere are hundreds of thousands of variables in the decennial Census and American Community Survey samples, which can make it difficult to know which variable codes to use! tidycensus aims to make this easier with the load_variables() function, which obtains a dataset of variables from a specified sample and loads it into R as a browsable data frame.\n\n# Load variables from the 2012-2016 ACS\nv16 <- load_variables(year = 2016,\n           dataset = \"acs5\",\n           cache = TRUE)\n\n# Get variables from the ACS Data Profile\nv16p <- load_variables(year = 2016,\n                       dataset = \"acs5/profile\",\n                       cache = TRUE)\n\n# Set year and dataset to get variables from the 2000 Census SF3\nv00 <- load_variables(year = 2000,\n                      dataset = \"sf3\",\n                      cache = TRUE)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#exploring-variables-with-tidyverse-tools",
    "href": "datacamp/us_census/us_census.html#exploring-variables-with-tidyverse-tools",
    "title": "Census data in r with tidycensus",
    "section": "Exploring variables with tidyverse tools",
    "text": "Exploring variables with tidyverse tools\nOnce loaded, your dataset of Census or ACS variables might contain thousands of rows. In RStudio, it is recommended to use the View() function to interactively search for these variables. Outside of RStudio, these datasets can be browsed using tidyverse filtering tools.\n\n# Filter for table B19001\nfilter(v16, str_detect(name, \"B19001\"))\n\n# A tibble: 170 × 4\n   name        label                               concept             geography\n   <chr>       <chr>                               <chr>               <chr>    \n 1 B19001A_001 Estimate!!Total                     HOUSEHOLD INCOME I… tract    \n 2 B19001A_002 Estimate!!Total!!Less than $10,000  HOUSEHOLD INCOME I… tract    \n 3 B19001A_003 Estimate!!Total!!$10,000 to $14,999 HOUSEHOLD INCOME I… tract    \n 4 B19001A_004 Estimate!!Total!!$15,000 to $19,999 HOUSEHOLD INCOME I… tract    \n 5 B19001A_005 Estimate!!Total!!$20,000 to $24,999 HOUSEHOLD INCOME I… tract    \n 6 B19001A_006 Estimate!!Total!!$25,000 to $29,999 HOUSEHOLD INCOME I… tract    \n 7 B19001A_007 Estimate!!Total!!$30,000 to $34,999 HOUSEHOLD INCOME I… tract    \n 8 B19001A_008 Estimate!!Total!!$35,000 to $39,999 HOUSEHOLD INCOME I… tract    \n 9 B19001A_009 Estimate!!Total!!$40,000 to $44,999 HOUSEHOLD INCOME I… tract    \n10 B19001A_010 Estimate!!Total!!$45,000 to $49,999 HOUSEHOLD INCOME I… tract    \n# ℹ 160 more rows\n\n# Use public transportation to search for related variables\nfilter(v16p, str_detect(label, fixed(\"public transportation\", \n                                ignore_case = TRUE)))\n\n# A tibble: 2 × 3\n  name       label                                                       concept\n  <chr>      <chr>                                                       <chr>  \n1 DP03_0021  Estimate!!COMMUTING TO WORK!!Workers 16 years and over!!Pu… SELECT…\n2 DP03_0021P Percent!!COMMUTING TO WORK!!Workers 16 years and over!!Pub… SELECT…"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#comparing-geographies-with-ggplot2-visualizations",
    "href": "datacamp/us_census/us_census.html#comparing-geographies-with-ggplot2-visualizations",
    "title": "Census data in r with tidycensus",
    "section": "Comparing geographies with ggplot2 visualizations",
    "text": "Comparing geographies with ggplot2 visualizations\nWhen exploring Census or ACS data, you’ll often want to know how data varies among different geographic units. For example - which US states have higher - or lower - median household incomes? This can be accomplished through visualization using dot plots, which are particularly effective for showing ranks visually. In this exercise, you’ll use the popular ggplot2 data visualization package to accomplish this.\n\n# Access the 1-year ACS  with the survey parameter\nne_income <- get_acs(geography = \"state\",\n                     variables = \"B19013_001\", \n                     survey = \"acs1\", \n                     state = c(\"ME\", \"NH\", \"VT\", \"MA\", \n                               \"RI\", \"CT\", \"NY\"))\n\n# Create a dot plot\n\n  \n# Reorder the states in descending order of estimates\nggplot(ne_income, aes(x = estimate, y = reorder(NAME, estimate))) + \n  geom_point()"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#customizing-ggplot2-visualizations-of-acs-data",
    "href": "datacamp/us_census/us_census.html#customizing-ggplot2-visualizations-of-acs-data",
    "title": "Census data in r with tidycensus",
    "section": "Customizing ggplot2 visualizations of ACS data",
    "text": "Customizing ggplot2 visualizations of ACS data\nWhile the ggplot2 defaults are excellent for exploratory visualization of data, you’ll likely want to customize your charts before sharing them with others. In this exercise, you’ll customize your tidycensus dot plot by modifying the chart colors, tick labels, and axis labels. You’ll also learn how to format labels using the scales package, as label formatters can be imported using the :: syntax.\n\n# Set dot color and size\ng_color <- ggplot(ne_income, aes(x = estimate, y = reorder(NAME, estimate))) + \n  geom_point(color = \"navy\", size = 4)\n\n# Format the x-axis labels\ng_scale <- g_color + \n  scale_x_continuous(labels = scales::dollar) + \n  theme_minimal(base_size = 12) \n\n# Label your x-axis, y-axis, and title your chart\ng_label <- g_scale + \n  labs(x =\"2016 ACS estimate\", \n       y = \"\", \n       title = \"Median household income by state\")\n  \ng_label"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#download-and-view-a-table-of-data-from-the-acs",
    "href": "datacamp/us_census/us_census.html#download-and-view-a-table-of-data-from-the-acs",
    "title": "Census data in r with tidycensus",
    "section": "Download and view a table of data from the ACS",
    "text": "Download and view a table of data from the ACS\nVariables in the decennial Census and American Community Survey are organized into tables, within which they share a common prefix. Commonly, analysts will want to work with all variables in a given table, as these variables might represent different aspects of a common characteristic (such as race or income levels). To request data for an entire table in tidycensus, users can specify a table argument with the table prefix, and optionally cache a dataset of table codes to speed up table searching in future requests. In this exercise, you’ll acquire a table of variables representing different income bands, then filter out the denominator rows.\n\n# Download table \"B19001\"\nwa_income <- get_acs(geography = \"county\", \n                 state = \"WA\", \n                 table = \"B19001\")\n\n# Check out the first few rows of wa_income\nhead(wa_income)\n\n# A tibble: 6 × 5\n  GEOID NAME                     variable   estimate   moe\n  <chr> <chr>                    <chr>         <dbl> <dbl>\n1 53001 Adams County, Washington B19001_001     6158   123\n2 53001 Adams County, Washington B19001_002      474   171\n3 53001 Adams County, Washington B19001_003      255   107\n4 53001 Adams County, Washington B19001_004      204    90\n5 53001 Adams County, Washington B19001_005      393   134\n6 53001 Adams County, Washington B19001_006      358   159"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#get-a-summary-variable-and-calculate-percentages",
    "href": "datacamp/us_census/us_census.html#get-a-summary-variable-and-calculate-percentages",
    "title": "Census data in r with tidycensus",
    "section": "Get a summary variable and calculate percentages",
    "text": "Get a summary variable and calculate percentages\nMany variables in the Census and American Community Survey are represented as counts or estimated counts. While count data is useful for some applications, it is often good practice to normalize count data by its denominator to convert it to a proportion or percentage to make clearer comparisons. This is facilitated in tidycensus with the summary_var argument, which allows users to request that a variable is given its own column in a tidy Census dataset. This value can then be used as the denominator for subsequent calculations of percentages.\nSummary question: When the summary_var parameter is requested in get_acs(), what information is returned by the function?\n\n# Assign Census variables vector to race_vars \nrace_vars <- c(White = \"B03002_003\", Black = \"B03002_004\", Native = \"B03002_005\", \n               Asian = \"B03002_006\", HIPI = \"B03002_007\", Hispanic = \"B03002_012\")\n\n# Request a summary variable from the ACS\nca_race <- get_acs(geography = \"county\", \n                   state = \"CA\",\n                   variables = race_vars, \n                   summary_var = \"B03002_001\")\n\n# Calculate a new percentage column and check the result\nca_race_pct <- ca_race %>%\n  mutate(pct = 100 * (estimate / summary_est))\n\nhead(ca_race_pct)\n\n# A tibble: 6 × 8\n  GEOID NAME              variable estimate   moe summary_est summary_moe    pct\n  <chr> <chr>             <chr>       <dbl> <dbl>       <dbl>       <dbl>  <dbl>\n1 06001 Alameda County, … White      499730   988     1673133          NA 29.9  \n2 06001 Alameda County, … Black      166017  1837     1673133          NA  9.92 \n3 06001 Alameda County, … Native       5248   318     1673133          NA  0.314\n4 06001 Alameda County, … Asian      524980  2437     1673133          NA 31.4  \n5 06001 Alameda County, … HIPI        12699   566     1673133          NA  0.759\n6 06001 Alameda County, … Hispanic   374542    NA     1673133          NA 22.4"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#finding-the-largest-group-by-county",
    "href": "datacamp/us_census/us_census.html#finding-the-largest-group-by-county",
    "title": "Census data in r with tidycensus",
    "section": "Finding the largest group by county",
    "text": "Finding the largest group by county\ntidyverse data wrangling tools in packages like dplyr and purrr are extremely powerful for exploring Census data. tidycensus is specifically designed with data exploration within the tidyverse in mind. For example, users might be interested in finding out the largest racial/ethnic group within each county for a given state. This can be accomplished using dplyr grouping capabilities, which allow users to identify the largest ACS group estimate and filter to retain the rows that match that group.\n\n# Group the dataset and filter the estimate\nca_largest <- ca_race %>%\n  group_by(GEOID) %>%\n  filter(estimate == max(estimate)) \n\nhead(ca_largest)\n\n# A tibble: 6 × 7\n# Groups:   GEOID [6]\n  GEOID NAME                     variable estimate   moe summary_est summary_moe\n  <chr> <chr>                    <chr>       <dbl> <dbl>       <dbl>       <dbl>\n1 06001 Alameda County, Califor… Asian      524980  2437     1673133          NA\n2 06003 Alpine County, Californ… White         730   153        1344         228\n3 06005 Amador County, Californ… White       30081   412       40095          NA\n4 06007 Butte County, California White      153153   300      217884          NA\n5 06009 Calaveras County, Calif… White       35925   129       45349          NA\n6 06011 Colusa County, Californ… Hispanic    13177    NA       21780          NA\n\n# Group the dataset and get a breakdown of the results\nca_largest %>% \n  group_by(variable) %>%\n  tally()\n\n# A tibble: 3 × 2\n  variable     n\n  <chr>    <int>\n1 Asian        2\n2 Hispanic    16\n3 White       40"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#recoding-variables-and-calculating-group-sums",
    "href": "datacamp/us_census/us_census.html#recoding-variables-and-calculating-group-sums",
    "title": "Census data in r with tidycensus",
    "section": "Recoding variables and calculating group sums",
    "text": "Recoding variables and calculating group sums\ndplyr, one of the core packages within the tidyverse, includes numerous functions for data wrangling. This functionality allows users to recode datasets, define groups within those datasets, and perform calculations over those groups. Such operations commonly take place within a pipe, denoted with the %>% operator.\nIn this exercise, you’ll work with ACS data in just such a tidyverse workflow. You’ll be identifying median household income variables in ACS table B19001 that are below $35,000; between $35,000 and $75,000; and above $75,000. You’ll then tabulate the number of households that fall into each group for counties in Washington.\n\n# Use a tidy workflow to wrangle ACS data\nwa_grouped <- wa_income %>%\n  filter(variable != \"B19001_001\") %>%\n  mutate(incgroup = case_when(\n    variable < \"B19001_008\" ~ \"below35k\", \n    variable < \"B19001_013\" ~ \"35kto75k\", \n    TRUE ~ \"above75k\"\n  )) %>%\n  group_by(NAME, incgroup) %>%\n  summarize(group_est = sum(estimate))\n\nwa_grouped\n\n# A tibble: 117 × 3\n# Groups:   NAME [39]\n   NAME                      incgroup group_est\n   <chr>                     <chr>        <dbl>\n 1 Adams County, Washington  35kto75k      2156\n 2 Adams County, Washington  above75k      2094\n 3 Adams County, Washington  below35k      1908\n 4 Asotin County, Washington 35kto75k      3215\n 5 Asotin County, Washington above75k      3537\n 6 Asotin County, Washington below35k      2535\n 7 Benton County, Washington 35kto75k     21285\n 8 Benton County, Washington above75k     37911\n 9 Benton County, Washington below35k     15094\n10 Chelan County, Washington 35kto75k      9163\n# ℹ 107 more rows"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#comparing-acs-estimates-for-multiple-years",
    "href": "datacamp/us_census/us_census.html#comparing-acs-estimates-for-multiple-years",
    "title": "Census data in r with tidycensus",
    "section": "Comparing ACS estimates for multiple years",
    "text": "Comparing ACS estimates for multiple years\nThe American Community Survey is updated every year, which allows researchers to use ACS datasets to study demographic changes over time.\nIn this exercise, you’ll learn how to use the tidyverse function map_df() to work with multi-year ACS data. map_df() helps analysts iterate through a sequence of values, compute a process for each of those values, then combine the results into a single data frame. You’ll be using map_df() in this way with ACS data, as you iterate through a vector of years, retrieve ACS data for each year, and combine the results. This will allow you to view how ACS estimates have changed over time.\n\n# Map through ACS1 estimates to see how they change through the years\nmi_cities <- map_df(2012:2016, function(x) {\n  get_acs(geography = \"place\", \n          variables = c(totalpop = \"B01003_001\"), \n          state = \"MI\", \n          survey = \"acs1\", \n          year = x) %>%\n    mutate(year = x)\n})\n\nmi_cities %>% arrange(NAME, year)\n\n# A tibble: 80 × 6\n   GEOID   NAME                     variable estimate   moe  year\n   <chr>   <chr>                    <chr>       <dbl> <dbl> <int>\n 1 2603000 Ann Arbor city, Michigan totalpop   116128    35  2012\n 2 2603000 Ann Arbor city, Michigan totalpop   117034    43  2013\n 3 2603000 Ann Arbor city, Michigan totalpop   117759    44  2014\n 4 2603000 Ann Arbor city, Michigan totalpop   117070    33  2015\n 5 2603000 Ann Arbor city, Michigan totalpop   120777    33  2016\n 6 2621000 Dearborn city, Michigan  totalpop    96470    28  2012\n 7 2621000 Dearborn city, Michigan  totalpop    95888    35  2013\n 8 2621000 Dearborn city, Michigan  totalpop    95546    48  2014\n 9 2621000 Dearborn city, Michigan  totalpop    95180    40  2015\n10 2621000 Dearborn city, Michigan  totalpop    94430    52  2016\n# ℹ 70 more rows"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#inspecting-margins-of-error",
    "href": "datacamp/us_census/us_census.html#inspecting-margins-of-error",
    "title": "Census data in r with tidycensus",
    "section": "Inspecting margins of error",
    "text": "Inspecting margins of error\nACS data are distinct from decennial Census data in that they represent estimates with an associated margin of error. ACS margins of error by default represent a 90 percent confidence level around an estimate, which means that we are 90 percent sure that the true value falls within a range of the reported estimate plus or minus the reported margin of error.\nIn this exercise, you’ll get some experience working with data that has high margins of error relative to their estimates. We’ll use the example of poverty for the population aged 75 and above for Census tracts in Vermont.\n\n# Get data on elderly poverty by Census tract in Vermont\nvt_eldpov <- get_acs(geography = \"tract\", \n                     variables = c(eldpovm = \"B17001_016\", \n                                   eldpovf = \"B17001_030\"), \n                     state = \"VT\")\n\nvt_eldpov\n\n# A tibble: 386 × 5\n   GEOID       NAME                                      variable estimate   moe\n   <chr>       <chr>                                     <chr>       <dbl> <dbl>\n 1 50001960100 Census Tract 9601, Addison County, Vermo… eldpovm         2     5\n 2 50001960100 Census Tract 9601, Addison County, Vermo… eldpovf         8     7\n 3 50001960200 Census Tract 9602, Addison County, Vermo… eldpovm         4     6\n 4 50001960200 Census Tract 9602, Addison County, Vermo… eldpovf         0    10\n 5 50001960300 Census Tract 9603, Addison County, Vermo… eldpovm         0    10\n 6 50001960300 Census Tract 9603, Addison County, Vermo… eldpovf         7     9\n 7 50001960400 Census Tract 9604, Addison County, Vermo… eldpovm         7    10\n 8 50001960400 Census Tract 9604, Addison County, Vermo… eldpovf        15    11\n 9 50001960500 Census Tract 9605, Addison County, Vermo… eldpovm         6    10\n10 50001960500 Census Tract 9605, Addison County, Vermo… eldpovf        14    16\n# ℹ 376 more rows\n\n# Identify rows with greater margins of error than their estimates\nmoe_check <- filter(vt_eldpov, moe > estimate)\n\n# Check proportion of rows where the margin of error exceeds the estimate\nnrow(moe_check) / nrow(vt_eldpov)\n\n[1] 0.7927461"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#using-margin-of-error-functions-in-tidycensus",
    "href": "datacamp/us_census/us_census.html#using-margin-of-error-functions-in-tidycensus",
    "title": "Census data in r with tidycensus",
    "section": "Using margin of error functions in tidycensus",
    "text": "Using margin of error functions in tidycensus\nWhile the Census Bureau API and tidycensus return pre-computed margins of error for you, you may want to derive new estimates from downloaded ACS data and in turn understand the margins of error around these derived estimates. tidycensus includes four functions (listed below) to help you with these tasks, each of which incorporates the recommended formulas from the US Census Bureau.\nmoe_sum() moe_product() moe_ratio() moe_prop()\n\n# Calculate a margin of error for a sum\nmoe_sum(moe = c(55, 33, 44, 12, 4))\n\n[1] 78.80355\n\n# Calculate a margin of error for a product\nmoe_product(est1 = 55,\n    est2 = 33,\n    moe1 = 12,\n    moe2 = 9)\n\n[1] 633.9093\n\n# Calculate a margin of error for a ratio\nmoe_ratio(num = 1000,\n    denom = 950,\n    moe_num = 200,\n    moe_denom = 177)\n\n[1] 0.287724\n\n# Calculate a margin of error for a proportion\nmoe_prop(num = 374,\n    denom = 1200,\n    moe_num = 122,\n    moe_denom = 333)\n\n[1] 0.05344178"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#calculating-group-wise-margins-of-error",
    "href": "datacamp/us_census/us_census.html#calculating-group-wise-margins-of-error",
    "title": "Census data in r with tidycensus",
    "section": "Calculating group-wise margins of error",
    "text": "Calculating group-wise margins of error\nOne way to reduce margins of error in an ACS analysis is to combine estimates when appropriate. This can be accomplished using tidyverse group-wise data analysis tools. In this exercise, you’ll combine estimates for male and female elderly poverty in Vermont, and use the moe_sum() function as part of this group-wise analysis. While you may lose some detail with this type of approach, your estimates will be more reliable relative to their margins of error than before you combined them.\n\n# Group the dataset and calculate a derived margin of error\nvt_eldpov2 <- vt_eldpov %>%\n  group_by(GEOID) %>%\n  summarize(\n    estmf = sum(estimate), \n    moemf = moe_sum(moe = moe, estimate = estimate)\n  )\n\n# Filter rows where newly-derived margin of error exceeds newly-derived estimate\nmoe_check2 <- filter(vt_eldpov2, moemf > estmf)\n\n# Check proportion of rows where margin of error exceeds estimate\nnrow(moe_check2) / nrow(vt_eldpov2)\n\n[1] 0.626943"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#quick-visual-exploration-of-acs-margins-of-error",
    "href": "datacamp/us_census/us_census.html#quick-visual-exploration-of-acs-margins-of-error",
    "title": "Census data in r with tidycensus",
    "section": "Quick visual exploration of ACS margins of error",
    "text": "Quick visual exploration of ACS margins of error\nIn Chapter 1, you learned how to create a dot plot of ACS income estimates. In this chapter, you’ve also learned about the importance of taking margins of error into account in ACS analyses. While margins of error are likely minimal for state-level estimates, they may be more significant for sub-state estimates, like counties. In this exercise, you’ll learn how to visualize margins of error around estimates with ggplot2.\n\n# Request median household income data\nmaine_inc <- get_acs(geography = \"county\", \n                     variables = c(hhincome = \"B19013_001\"), \n                     state = \"ME\") \n\n# Generate horizontal error bars with dots\nggplot(maine_inc, aes(x = estimate, y = NAME)) + \n  geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) + \n  geom_point()"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#customizing-a-ggplot2-margin-of-error-plot",
    "href": "datacamp/us_census/us_census.html#customizing-a-ggplot2-margin-of-error-plot",
    "title": "Census data in r with tidycensus",
    "section": "Customizing a ggplot2 margin of error plot",
    "text": "Customizing a ggplot2 margin of error plot\nYou’ve hopefully identified some problems with the chart you created in the previous exercise. As the counties are not ordered, patterns in the data are difficult for a viewer to parse. Specifically, margin of error plots are much more effective when dots are ordered as the ordering allows viewers to understand the uncertainty in estimate values relative to other estimates. Additionally, the lack of plot formatting makes it difficult for chart viewers to understand the chart’s content. In this exercise, you’ll clean up your ggplot2 code to create a much more visually appealing margin of error chart.\n\n# Remove unnecessary content from the county's name\nmaine_inc2 <- maine_inc %>%\n  mutate(NAME = str_replace(NAME, \" County, Maine\", \"\"))\n\n# Build a margin of error plot incorporating your modifications\nggplot(maine_inc2, aes(x = estimate, y = reorder(NAME, estimate))) + \n  geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) + \n  geom_point(size = 3, color = \"darkgreen\") + \n  theme_grey(base_size = 14) + \n  labs(title = \"Median household income\", \n       subtitle = \"Counties in Maine\", \n       x = \"ACS estimate (bars represent margins of error)\", \n       y = \"\") + \n  scale_x_continuous(labels = scales::dollar)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#getting-census-boundary-files-with-tigris",
    "href": "datacamp/us_census/us_census.html#getting-census-boundary-files-with-tigris",
    "title": "Census data in r with tidycensus",
    "section": "Getting Census boundary files with tigris",
    "text": "Getting Census boundary files with tigris\nThe US Census Bureau’s TIGER/Line shapefiles include boundary files for the geography at which decennial Census and ACS data are aggregated. These geographies include legal entities that have legal standing in the U.S., such as states and counties, and statistical entities used for data tabulation such as Census tracts and block groups. In this exercise, you’ll use the tigris package to acquire such boundary files for counties in Colorado and Census tracts for Colorado’s Denver County, which covers the city of Denver.\n\n# Get a counties dataset for Colorado and plot it\nco_counties <- counties(state = \"CO\")\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |=================================================================     |  94%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\nplot(co_counties)\n\n\n\n# Get a Census tracts dataset for Denver County, Colorado and plot it\ndenver_tracts <- tracts(state = \"CO\", county = \"Denver\")\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |=================================================================     |  94%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\nplot(denver_tracts)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#getting-geographic-features-with-tigris",
    "href": "datacamp/us_census/us_census.html#getting-geographic-features-with-tigris",
    "title": "Census data in r with tidycensus",
    "section": "Getting geographic features with tigris",
    "text": "Getting geographic features with tigris\nIn addition to enumeration units, the TIGER/Line database produced by the Census Bureau includes geographic features. These features consist of several datasets for use in thematic mapping and spatial analysis, such as transportation infrastructure and water features. In this exercise, you’ll acquire and plot roads and water data with tigris.\n\n# Plot area water features for Lane County, Oregon\nlane_water <- area_water(state = \"OR\", county = \"Lane\")\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |======================================================================| 100%\n\nplot(lane_water)\n\n\n\n# Plot primary & secondary roads for the state of New Hampshire\nnh_roads <- primary_secondary_roads(state = \"NH\")\n\n\nDownloading: 28 kB     \nDownloading: 28 kB     \nDownloading: 40 kB     \nDownloading: 40 kB     \nDownloading: 49 kB     \nDownloading: 49 kB     \nDownloading: 56 kB     \nDownloading: 56 kB     \nDownloading: 65 kB     \nDownloading: 65 kB     \nDownloading: 68 kB     \nDownloading: 68 kB     \nDownloading: 72 kB     \nDownloading: 72 kB     \nDownloading: 77 kB     \nDownloading: 77 kB     \nDownloading: 89 kB     \nDownloading: 89 kB     \nDownloading: 93 kB     \nDownloading: 93 kB     \nDownloading: 97 kB     \nDownloading: 97 kB     \nDownloading: 97 kB     \nDownloading: 97 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 160 kB     \nDownloading: 160 kB     \nDownloading: 160 kB     \nDownloading: 160 kB     \nDownloading: 170 kB     \nDownloading: 170 kB     \nDownloading: 180 kB     \nDownloading: 180 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 200 kB     \nDownloading: 200 kB     \nDownloading: 210 kB     \nDownloading: 210 kB     \nDownloading: 210 kB     \nDownloading: 210 kB     \nDownloading: 220 kB     \nDownloading: 220 kB     \nDownloading: 220 kB     \nDownloading: 220 kB     \nDownloading: 230 kB     \nDownloading: 230 kB     \nDownloading: 240 kB     \nDownloading: 240 kB     \nDownloading: 240 kB     \nDownloading: 240 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 280 kB     \nDownloading: 280 kB     \nDownloading: 280 kB     \nDownloading: 280 kB     \nDownloading: 280 kB     \nDownloading: 280 kB     \nDownloading: 290 kB     \nDownloading: 290 kB     \nDownloading: 300 kB     \nDownloading: 300 kB     \nDownloading: 310 kB     \nDownloading: 310 kB     \nDownloading: 320 kB     \nDownloading: 320 kB     \nDownloading: 320 kB     \nDownloading: 320 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 370 kB     \nDownloading: 370 kB     \nDownloading: 370 kB     \nDownloading: 370 kB     \nDownloading: 380 kB     \nDownloading: 380 kB     \nDownloading: 380 kB     \nDownloading: 380 kB     \nDownloading: 390 kB     \nDownloading: 390 kB     \nDownloading: 390 kB     \nDownloading: 390 kB     \nDownloading: 390 kB     \nDownloading: 390 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 420 kB     \nDownloading: 420 kB     \nDownloading: 430 kB     \nDownloading: 430 kB     \nDownloading: 430 kB     \nDownloading: 430 kB     \nDownloading: 430 kB     \nDownloading: 430 kB     \nDownloading: 440 kB     \nDownloading: 440 kB     \nDownloading: 450 kB     \nDownloading: 450 kB     \nDownloading: 450 kB     \nDownloading: 450 kB     \nDownloading: 470 kB     \nDownloading: 470 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 500 kB     \nDownloading: 500 kB     \nDownloading: 500 kB     \nDownloading: 500 kB     \nDownloading: 510 kB     \nDownloading: 510 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 540 kB     \nDownloading: 540 kB     \nDownloading: 540 kB     \nDownloading: 540 kB     \nDownloading: 550 kB     \nDownloading: 550 kB     \nDownloading: 560 kB     \nDownloading: 560 kB     \nDownloading: 560 kB     \nDownloading: 560 kB     \nDownloading: 570 kB     \nDownloading: 570 kB     \nDownloading: 570 kB     \nDownloading: 570 kB     \nDownloading: 580 kB     \nDownloading: 580 kB     \nDownloading: 590 kB     \nDownloading: 590 kB     \nDownloading: 590 kB     \nDownloading: 590 kB     \nDownloading: 600 kB     \nDownloading: 600 kB     \nDownloading: 620 kB     \nDownloading: 620 kB     \nDownloading: 630 kB     \nDownloading: 630 kB     \nDownloading: 630 kB     \nDownloading: 630 kB     \nDownloading: 630 kB     \nDownloading: 630 kB     \nDownloading: 640 kB     \nDownloading: 640 kB     \nDownloading: 640 kB     \nDownloading: 640 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 660 kB     \nDownloading: 660 kB     \nDownloading: 670 kB     \nDownloading: 670 kB     \nDownloading: 670 kB     \nDownloading: 670 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 690 kB     \nDownloading: 690 kB     \nDownloading: 700 kB     \nDownloading: 700 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 720 kB     \nDownloading: 720 kB     \nDownloading: 720 kB     \nDownloading: 720 kB     \nDownloading: 730 kB     \nDownloading: 730 kB     \nDownloading: 740 kB     \nDownloading: 740 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 770 kB     \nDownloading: 770 kB     \nDownloading: 770 kB     \nDownloading: 770 kB     \nDownloading: 780 kB     \nDownloading: 780 kB     \nDownloading: 790 kB     \nDownloading: 790 kB     \nDownloading: 790 kB     \nDownloading: 790 kB     \nDownloading: 810 kB     \nDownloading: 810 kB     \nDownloading: 810 kB     \nDownloading: 810 kB     \nDownloading: 810 kB     \nDownloading: 810 kB     \nDownloading: 820 kB     \nDownloading: 820 kB     \nDownloading: 820 kB     \nDownloading: 820 kB     \nDownloading: 820 kB     \nDownloading: 820 kB     \nDownloading: 830 kB     \nDownloading: 830 kB     \nDownloading: 840 kB     \nDownloading: 840 kB     \nDownloading: 840 kB     \nDownloading: 840 kB     \nDownloading: 840 kB     \nDownloading: 840 kB     \nDownloading: 850 kB     \nDownloading: 850 kB     \nDownloading: 860 kB     \nDownloading: 860 kB     \nDownloading: 860 kB     \nDownloading: 860 kB     \nDownloading: 870 kB     \nDownloading: 870 kB     \nDownloading: 870 kB     \nDownloading: 870 kB     \nDownloading: 870 kB     \nDownloading: 870 kB     \nDownloading: 870 kB     \nDownloading: 870 kB     \nDownloading: 890 kB     \nDownloading: 890 kB     \nDownloading: 900 kB     \nDownloading: 900 kB     \nDownloading: 910 kB     \nDownloading: 910 kB     \nDownloading: 910 kB     \nDownloading: 910 kB     \nDownloading: 920 kB     \nDownloading: 920 kB     \nDownloading: 920 kB     \nDownloading: 920 kB     \nDownloading: 930 kB     \nDownloading: 930 kB     \nDownloading: 930 kB     \nDownloading: 930 kB     \nDownloading: 950 kB     \nDownloading: 950 kB     \nDownloading: 950 kB     \nDownloading: 950 kB     \nDownloading: 970 kB     \nDownloading: 970 kB     \nDownloading: 970 kB     \nDownloading: 970 kB     \nDownloading: 980 kB     \nDownloading: 980 kB     \nDownloading: 980 kB     \nDownloading: 980 kB     \nDownloading: 980 kB     \nDownloading: 980 kB     \nDownloading: 990 kB     \nDownloading: 990 kB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.6 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.3 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \nDownloading: 2.5 MB     \n\nplot(nh_roads)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#understanding-the-structure-of-tigris-objects",
    "href": "datacamp/us_census/us_census.html#understanding-the-structure-of-tigris-objects",
    "title": "Census data in r with tidycensus",
    "section": "Understanding the structure of tigris objects",
    "text": "Understanding the structure of tigris objects\nBy default, tigris returns objects of class SpatialDataFrame from the sp package. Objects of class Spatial represent components of spatial data in different slots, which include descriptions of the object’s geometry, attributes, and coordinate system. In this exercise, we’ll briefly examine the structure of objects returned by tigris functions.\n\n# Check the class of the data\nclass(co_counties)\n\n[1] \"sf\"         \"data.frame\"\n\n# Take a look at the information in the data slot\nhead(co_counties)\n\nSimple feature collection with 6 features and 17 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -108.3811 ymin: 36.99961 xmax: -102.0448 ymax: 41.0026\nGeodetic CRS:  NAD83\n    STATEFP COUNTYFP COUNTYNS GEOID     NAME        NAMELSAD LSAD CLASSFP MTFCC\n23       08      109 00198170 08109 Saguache Saguache County   06      H1 G4020\n107      08      115 00198173 08115 Sedgwick Sedgwick County   06      H1 G4020\n124      08      017 00198124 08017 Cheyenne Cheyenne County   06      H1 G4020\n163      08      027 00198129 08027   Custer   Custer County   06      H1 G4020\n200      08      067 00198148 08067 La Plata La Plata County   06      H1 G4020\n228      08      111 00198171 08111 San Juan San Juan County   06      H1 G4020\n    CSAFP CBSAFP METDIVFP FUNCSTAT      ALAND   AWATER    INTPTLAT     INTPTLON\n23   <NA>   <NA>     <NA>        A 8206547699  4454510 +38.0316514 -106.2346662\n107  <NA>   <NA>     <NA>        A 1419419015  3530746 +40.8715679 -102.3553579\n124  <NA>   <NA>     <NA>        A 4605713960  8166129 +38.8356456 -102.6017914\n163  <NA>   <NA>     <NA>        A 1913031975  3364150 +38.1019955 -105.3735123\n200  <NA>  20420     <NA>        A 4376255277 25642579 +37.2873673 -107.8397178\n228  <NA>   <NA>     <NA>        A 1003660672  2035929 +37.7810492 -107.6702567\n                          geometry\n23  MULTIPOLYGON (((-105.8093 3...\n107 MULTIPOLYGON (((-102.2091 4...\n124 MULTIPOLYGON (((-102.547 38...\n163 MULTIPOLYGON (((-105.7969 3...\n200 MULTIPOLYGON (((-107.7124 3...\n228 MULTIPOLYGON (((-107.9751 3...\n\n# Check the coordinate system of the data"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#tigerline-and-cartographic-boundary-files",
    "href": "datacamp/us_census/us_census.html#tigerline-and-cartographic-boundary-files",
    "title": "Census data in r with tidycensus",
    "section": "TIGER/Line and cartographic boundary files",
    "text": "TIGER/Line and cartographic boundary files\nIn addition to its TIGER/Line shapefiles, the US Census Bureau releases cartographic boundary shapefiles for enumeration units. TIGER/Line shapefiles correspond to legal boundaries of units, which can include water area and in turn, may not be preferable for thematic mapping. The Census Bureau’s cartographic boundary shapefiles are clipped to the US shoreline and are generalized, which can make them superior for mapping projects. In this exercise, you’ll compare the TIGER/Line and cartographic boundary representations of the US state of Michigan.\n\n# Get a counties dataset for Michigan\nmi_tiger <- counties(\"MI\")\n\n# Get the equivalent cartographic boundary shapefile\nmi_cb <- counties(\"MI\", cb = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |=================================================================     |  94%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n# Overlay the two on a plot to make a comparison\nplot(mi_tiger)\nplot(mi_cb, add = TRUE, border = \"red\")"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#getting-data-as-simple-features-objects",
    "href": "datacamp/us_census/us_census.html#getting-data-as-simple-features-objects",
    "title": "Census data in r with tidycensus",
    "section": "Getting data as simple features objects",
    "text": "Getting data as simple features objects\nThe sf package, which stands for simple features, promises to revolutionize the way that vector spatial data are handled within R. sf objects represent spatial data much like regular data frames, with a list-column that contains the geometry of the geographic dataset. tigris can return spatial data as simple features objects either by declaring class = “sf” within a function call or by setting as a global option. In this exercise, you’ll get acquainted with simple features in tigris.\n\n# Get data from tigris as simple features\noptions(tigris_class = \"sf\")\n\n# Get countries from Colorado and view the first few rows\ncolorado_sf <- counties(\"CO\")\nhead(colorado_sf)\n\nSimple feature collection with 6 features and 17 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -108.3811 ymin: 36.99961 xmax: -102.0448 ymax: 41.0026\nGeodetic CRS:  NAD83\n    STATEFP COUNTYFP COUNTYNS GEOID     NAME        NAMELSAD LSAD CLASSFP MTFCC\n23       08      109 00198170 08109 Saguache Saguache County   06      H1 G4020\n107      08      115 00198173 08115 Sedgwick Sedgwick County   06      H1 G4020\n124      08      017 00198124 08017 Cheyenne Cheyenne County   06      H1 G4020\n163      08      027 00198129 08027   Custer   Custer County   06      H1 G4020\n200      08      067 00198148 08067 La Plata La Plata County   06      H1 G4020\n228      08      111 00198171 08111 San Juan San Juan County   06      H1 G4020\n    CSAFP CBSAFP METDIVFP FUNCSTAT      ALAND   AWATER    INTPTLAT     INTPTLON\n23   <NA>   <NA>     <NA>        A 8206547699  4454510 +38.0316514 -106.2346662\n107  <NA>   <NA>     <NA>        A 1419419015  3530746 +40.8715679 -102.3553579\n124  <NA>   <NA>     <NA>        A 4605713960  8166129 +38.8356456 -102.6017914\n163  <NA>   <NA>     <NA>        A 1913031975  3364150 +38.1019955 -105.3735123\n200  <NA>  20420     <NA>        A 4376255277 25642579 +37.2873673 -107.8397178\n228  <NA>   <NA>     <NA>        A 1003660672  2035929 +37.7810492 -107.6702567\n                          geometry\n23  MULTIPOLYGON (((-105.8093 3...\n107 MULTIPOLYGON (((-102.2091 4...\n124 MULTIPOLYGON (((-102.547 38...\n163 MULTIPOLYGON (((-105.7969 3...\n200 MULTIPOLYGON (((-107.7124 3...\n228 MULTIPOLYGON (((-107.9751 3...\n\n# Plot its geometry column\nplot(colorado_sf$geometry)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#working-with-historic-shapefiles",
    "href": "datacamp/us_census/us_census.html#working-with-historic-shapefiles",
    "title": "Census data in r with tidycensus",
    "section": "Working with historic shapefiles",
    "text": "Working with historic shapefiles\nTo ensure clean integration with the tidycensus package - which you’ll learn about in the next chapter - tigris defaults to returning shapefiles that correspond to the year of the most recently-released ACS data. However, you may want boundary files for other years. tigris allows R users to obtain shapefiles for 1990, 2000, and 2010 through 2017, which represent many boundary changes over time. In this exercise, you’ll use tigris to explore how Census tract boundaries have changed in Williamson County, Texas between 1990 and 2016.\n\n# Get a historic Census tract shapefile from 1990 for Williamson County, Texas\nwilliamson90 <- tracts(state = \"TX\", county = \"Williamson\", \n                       cb = TRUE, year = 1990)\n\n\nDownloading: 28 kB     \nDownloading: 28 kB     \nDownloading: 89 kB     \nDownloading: 89 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 200 kB     \nDownloading: 200 kB     \nDownloading: 220 kB     \nDownloading: 220 kB     \nDownloading: 230 kB     \nDownloading: 230 kB     \nDownloading: 230 kB     \nDownloading: 230 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 260 kB     \nDownloading: 270 kB     \nDownloading: 270 kB     \nDownloading: 280 kB     \nDownloading: 280 kB     \nDownloading: 280 kB     \nDownloading: 280 kB     \nDownloading: 280 kB     \nDownloading: 280 kB     \nDownloading: 300 kB     \nDownloading: 300 kB     \nDownloading: 300 kB     \nDownloading: 300 kB     \nDownloading: 300 kB     \nDownloading: 300 kB     \nDownloading: 310 kB     \nDownloading: 310 kB     \nDownloading: 320 kB     \nDownloading: 320 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 340 kB     \nDownloading: 340 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 360 kB     \nDownloading: 360 kB     \nDownloading: 360 kB     \nDownloading: 360 kB     \nDownloading: 370 kB     \nDownloading: 370 kB     \nDownloading: 370 kB     \nDownloading: 370 kB     \nDownloading: 380 kB     \nDownloading: 380 kB     \nDownloading: 390 kB     \nDownloading: 390 kB     \nDownloading: 390 kB     \nDownloading: 390 kB     \nDownloading: 390 kB     \nDownloading: 390 kB     \nDownloading: 390 kB     \nDownloading: 390 kB     \nDownloading: 420 kB     \nDownloading: 420 kB     \nDownloading: 440 kB     \nDownloading: 440 kB     \nDownloading: 440 kB     \nDownloading: 440 kB     \nDownloading: 460 kB     \nDownloading: 460 kB     \nDownloading: 470 kB     \nDownloading: 470 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 490 kB     \nDownloading: 490 kB     \nDownloading: 500 kB     \nDownloading: 500 kB     \nDownloading: 510 kB     \nDownloading: 510 kB     \nDownloading: 510 kB     \nDownloading: 510 kB     \nDownloading: 520 kB     \nDownloading: 520 kB     \nDownloading: 520 kB     \nDownloading: 520 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 540 kB     \nDownloading: 540 kB     \nDownloading: 540 kB     \nDownloading: 540 kB     \nDownloading: 560 kB     \nDownloading: 560 kB     \nDownloading: 560 kB     \nDownloading: 560 kB     \nDownloading: 570 kB     \nDownloading: 570 kB     \nDownloading: 580 kB     \nDownloading: 580 kB     \nDownloading: 590 kB     \nDownloading: 590 kB     \nDownloading: 590 kB     \nDownloading: 590 kB     \nDownloading: 590 kB     \nDownloading: 590 kB     \nDownloading: 610 kB     \nDownloading: 610 kB     \nDownloading: 610 kB     \nDownloading: 610 kB     \nDownloading: 610 kB     \nDownloading: 610 kB     \nDownloading: 620 kB     \nDownloading: 620 kB     \nDownloading: 640 kB     \nDownloading: 640 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 660 kB     \nDownloading: 660 kB     \nDownloading: 670 kB     \nDownloading: 670 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 690 kB     \nDownloading: 690 kB     \nDownloading: 700 kB     \nDownloading: 700 kB     \nDownloading: 700 kB     \nDownloading: 700 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 720 kB     \nDownloading: 720 kB     \nDownloading: 730 kB     \nDownloading: 730 kB     \nDownloading: 740 kB     \nDownloading: 740 kB     \nDownloading: 740 kB     \nDownloading: 740 kB     \nDownloading: 740 kB     \nDownloading: 740 kB     \nDownloading: 750 kB     \nDownloading: 750 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 770 kB     \nDownloading: 770 kB     \nDownloading: 780 kB     \nDownloading: 780 kB     \nDownloading: 790 kB     \nDownloading: 790 kB     \nDownloading: 790 kB     \nDownloading: 790 kB     \nDownloading: 810 kB     \nDownloading: 810 kB     \nDownloading: 810 kB     \nDownloading: 810 kB     \nDownloading: 810 kB     \nDownloading: 810 kB     \nDownloading: 810 kB     \nDownloading: 810 kB     \nDownloading: 820 kB     \nDownloading: 820 kB     \nDownloading: 820 kB     \nDownloading: 820 kB     \nDownloading: 820 kB     \nDownloading: 820 kB     \nDownloading: 830 kB     \nDownloading: 830 kB     \nDownloading: 840 kB     \nDownloading: 840 kB     \nDownloading: 850 kB     \nDownloading: 850 kB     \nDownloading: 870 kB     \nDownloading: 870 kB     \nDownloading: 890 kB     \nDownloading: 890 kB     \nDownloading: 900 kB     \nDownloading: 900 kB     \nDownloading: 900 kB     \nDownloading: 900 kB     \nDownloading: 920 kB     \nDownloading: 920 kB     \nDownloading: 920 kB     \nDownloading: 920 kB     \nDownloading: 930 kB     \nDownloading: 930 kB     \nDownloading: 930 kB     \nDownloading: 930 kB     \nDownloading: 940 kB     \nDownloading: 940 kB     \nDownloading: 940 kB     \nDownloading: 940 kB     \nDownloading: 940 kB     \nDownloading: 940 kB     \nDownloading: 950 kB     \nDownloading: 950 kB     \nDownloading: 960 kB     \nDownloading: 960 kB     \nDownloading: 970 kB     \nDownloading: 970 kB     \nDownloading: 970 kB     \nDownloading: 970 kB     \nDownloading: 970 kB     \nDownloading: 970 kB     \nDownloading: 980 kB     \nDownloading: 980 kB     \nDownloading: 990 kB     \nDownloading: 990 kB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \n\n# Compare with a current dataset for 2016\nwilliamson16 <- tracts(state = \"TX\", county = \"Williamson\", \n                       cb = TRUE, year = 2016)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n# Plot the geometry to compare the results                       \npar(mfrow = c(1, 2))\nplot(williamson90$geometry)\nplot(williamson16$geometry)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#combining-datasets-of-the-same-tigris-type",
    "href": "datacamp/us_census/us_census.html#combining-datasets-of-the-same-tigris-type",
    "title": "Census data in r with tidycensus",
    "section": "Combining datasets of the same tigris type",
    "text": "Combining datasets of the same tigris type\nOften, datasets from the US Census Bureau are available by state, which means they are available by state from tigris as well. In many instances, you’ll want to combine datasets for multiple states. For example, an analysis of the Portland, Oregon metropolitan area would include areas in both Oregon and Washington north of the Columbia River; however, these areas are represented in different Census files. In this exercise, you’ll learn how to combine datasets with the rbind_tigris() function.\n\n# Get Census tract boundaries for Oregon and Washington\nor_tracts <- tracts(\"OR\", cb = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\nwa_tracts <- tracts(\"WA\", cb = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n# Check the tigris attributes of each object\nattr(or_tracts, \"tigris\")\n\n[1] \"tract\"\n\nattr(wa_tracts, \"tigris\")\n\n[1] \"tract\"\n\n# Combine the datasets then plot the result\nor_wa_tracts <- rbind_tigris(or_tracts, wa_tracts)\nplot(or_wa_tracts$geometry)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#getting-data-for-multiple-states",
    "href": "datacamp/us_census/us_census.html#getting-data-for-multiple-states",
    "title": "Census data in r with tidycensus",
    "section": "Getting data for multiple states",
    "text": "Getting data for multiple states\nIn the previous exercise, you learned how to combine datasets with the rbind_tigris() function. If you need data for more than two states, however, this process can get tedious. In this exercise, you’ll learn how to generate a list of datasets for multiple states with the tidyverse map() function, and combine those datasets with rbind_tigris().\n\n# Generate a vector of state codes and assign to new_england\nnew_england <- c(\"ME\", \"NH\", \"VT\", \"MA\")\n\n# Iterate through the states and request tract data for state\nne_tracts <- map(new_england, function(x) {\n  tracts(state = x, cb = TRUE)\n}) %>%\n  rbind_tigris()\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |======================================================================| 100%\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\nplot(ne_tracts$geometry)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#joining-data-from-an-external-data-frame",
    "href": "datacamp/us_census/us_census.html#joining-data-from-an-external-data-frame",
    "title": "Census data in r with tidycensus",
    "section": "Joining data from an external data frame",
    "text": "Joining data from an external data frame\nWhen working with geographic data in R, you’ll commonly want to join attribute information from an external dataset to it for mapping and spatial analysis. The sf package enables the use of the tidyverse *_join() functions for simple features objects for this purpose. In this exercise, you’ll learn how to join data to a spatial dataset of legislative boundaries for the Texas House of Representatives that you’ve obtained using tigris.\n\n# Get boundaries for Texas and set the house parameter\ntx_house <- state_legislative_districts(state = \"TX\", house = \"lower\", cb = TRUE)\n\n# Merge data on legislators to their corresponding boundaries\ntx_joined <- left_join(tx_house, tx_members, by = c(\"NAME\" = \"District\"))\n\nhead(tx_joined)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#plotting-simple-features-with-geom_sf",
    "href": "datacamp/us_census/us_census.html#plotting-simple-features-with-geom_sf",
    "title": "Census data in r with tidycensus",
    "section": "Plotting simple features with geom_sf()",
    "text": "Plotting simple features with geom_sf()\nThe newest version of ggplot2 includes a geom_sf() function to plot simple features objects natively. This allows you to make maps using familiar ggplot2 syntax! In this exercise, you’ll walk through the process of creating a map with ggplot2 step-by-step.\n\n# Plot the legislative district boundaries\nggplot(tx_joined) + \n  geom_sf()\n\n# Set fill aesthetic to map areas represented by Republicans and Democrats\nggplot(tx_joined, aes(fill = Party)) + \n  geom_sf()\n\n# Set values so that Republican areas are red and Democratic areas are blue\nggplot(tx_joined, aes(fill = Party)) + \n  geom_sf() + \n  scale_fill_manual(values = c(\"R\" = \"red\", \"D\" = \"blue\"))"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#customizing-geom_sf-plots",
    "href": "datacamp/us_census/us_census.html#customizing-geom_sf-plots",
    "title": "Census data in r with tidycensus",
    "section": "Customizing geom_sf() plots",
    "text": "Customizing geom_sf() plots\nAs you’ve learned in previous chapters, it is a good idea to clean up and format your ggplot2 visualizations before sharing with others. In this exercise, you’ll make some modifications to your map of Texas House districts such as removing the gridlines and adding an informative title.\n\n# Draw a ggplot without gridlines and with an informative title\nggplot(tx_joined, aes(fill = Party)) + \n  geom_sf() + \n  coord_sf(crs = 3083, datum = NA) + \n  scale_fill_manual(values = c(\"R\" = \"red\", \"D\" = \"blue\")) + \n  theme_minimal(base_size = 16) + \n  labs(title = \"State House Districts in Texas\")"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#getting-simple-feature-geometry",
    "href": "datacamp/us_census/us_census.html#getting-simple-feature-geometry",
    "title": "Census data in r with tidycensus",
    "section": "Getting simple feature geometry",
    "text": "Getting simple feature geometry\ntidycensus can obtain simple feature geometry for many geographies by adding the argument geometry = TRUE. In this exercise, you’ll obtain a dataset of median housing values for owner-occupied units by Census tract in Orange County, California with simple feature geometry included.\n\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(sf)\n\n# Get dataset with geometry set to TRUE\norange_value <- get_acs(geography = \"tract\", state = \"CA\", \n                    county = \"Orange\", \n                    variables = \"B25077_001\", \n                    geometry = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |=================================================================     |  94%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n# Plot the estimate to view a map of the data\nplot(orange_value[\"estimate\"])"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#joining-data-from-tigris-and-tidycensus",
    "href": "datacamp/us_census/us_census.html#joining-data-from-tigris-and-tidycensus",
    "title": "Census data in r with tidycensus",
    "section": "Joining data from tigris and tidycensus",
    "text": "Joining data from tigris and tidycensus\nGeometry is currently supported in tidycensus for geographies in the core Census hierarchy - state, county, Census tract, block group, and block - as well as zip code tabulation areas. However, you may be interested in mapping data for other geographies. In this case, you can download the equivalent boundary file from the Census Bureau using the tigris package and join your demographic data to it for mapping.\n\n# Get an income dataset for Idaho by school district\nidaho_income <- get_acs(geography = \"school district (unified)\", \n                        variables = \"B19013_001\", \n                        state = \"ID\")\n\n# Get a school district dataset for Idaho\nidaho_school <- school_districts(state = \"ID\", type = \"unified\", class = \"sf\")\n\n\nDownloading: 7.3 kB     \nDownloading: 7.3 kB     \nDownloading: 11 kB     \nDownloading: 11 kB     \nDownloading: 11 kB     \nDownloading: 11 kB     \nDownloading: 15 kB     \nDownloading: 15 kB     \nDownloading: 20 kB     \nDownloading: 20 kB     \nDownloading: 24 kB     \nDownloading: 24 kB     \nDownloading: 25 kB     \nDownloading: 25 kB     \nDownloading: 28 kB     \nDownloading: 28 kB     \nDownloading: 28 kB     \nDownloading: 28 kB     \nDownloading: 40 kB     \nDownloading: 40 kB     \nDownloading: 44 kB     \nDownloading: 44 kB     \nDownloading: 44 kB     \nDownloading: 44 kB     \nDownloading: 60 kB     \nDownloading: 60 kB     \nDownloading: 74 kB     \nDownloading: 74 kB     \nDownloading: 77 kB     \nDownloading: 77 kB     \nDownloading: 89 kB     \nDownloading: 89 kB     \nDownloading: 89 kB     \nDownloading: 89 kB     \nDownloading: 100 kB     \nDownloading: 100 kB     \nDownloading: 160 kB     \nDownloading: 160 kB     \nDownloading: 170 kB     \nDownloading: 170 kB     \nDownloading: 220 kB     \nDownloading: 220 kB     \nDownloading: 280 kB     \nDownloading: 280 kB     \nDownloading: 290 kB     \nDownloading: 290 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 340 kB     \nDownloading: 340 kB     \nDownloading: 340 kB     \nDownloading: 340 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 370 kB     \nDownloading: 370 kB     \nDownloading: 380 kB     \nDownloading: 380 kB     \nDownloading: 380 kB     \nDownloading: 380 kB     \nDownloading: 390 kB     \nDownloading: 390 kB     \nDownloading: 390 kB     \nDownloading: 390 kB     \nDownloading: 400 kB     \nDownloading: 400 kB     \nDownloading: 400 kB     \nDownloading: 400 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 420 kB     \nDownloading: 420 kB     \nDownloading: 440 kB     \nDownloading: 440 kB     \nDownloading: 450 kB     \nDownloading: 450 kB     \nDownloading: 450 kB     \nDownloading: 450 kB     \nDownloading: 460 kB     \nDownloading: 460 kB     \nDownloading: 470 kB     \nDownloading: 470 kB     \nDownloading: 470 kB     \nDownloading: 470 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 490 kB     \nDownloading: 490 kB     \nDownloading: 500 kB     \nDownloading: 500 kB     \nDownloading: 500 kB     \nDownloading: 500 kB     \nDownloading: 510 kB     \nDownloading: 510 kB     \nDownloading: 560 kB     \nDownloading: 560 kB     \nDownloading: 570 kB     \nDownloading: 570 kB     \nDownloading: 580 kB     \nDownloading: 580 kB     \nDownloading: 590 kB     \nDownloading: 590 kB     \nDownloading: 590 kB     \nDownloading: 590 kB     \nDownloading: 590 kB     \nDownloading: 590 kB     \nDownloading: 600 kB     \nDownloading: 600 kB     \nDownloading: 600 kB     \nDownloading: 600 kB     \nDownloading: 610 kB     \nDownloading: 610 kB     \nDownloading: 620 kB     \nDownloading: 620 kB     \nDownloading: 620 kB     \nDownloading: 620 kB     \nDownloading: 630 kB     \nDownloading: 630 kB     \nDownloading: 630 kB     \nDownloading: 630 kB     \nDownloading: 630 kB     \nDownloading: 630 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 660 kB     \nDownloading: 660 kB     \nDownloading: 660 kB     \nDownloading: 660 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 700 kB     \nDownloading: 700 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 720 kB     \nDownloading: 720 kB     \nDownloading: 720 kB     \nDownloading: 720 kB     \nDownloading: 730 kB     \nDownloading: 730 kB     \nDownloading: 740 kB     \nDownloading: 740 kB     \nDownloading: 750 kB     \nDownloading: 750 kB     \nDownloading: 750 kB     \nDownloading: 750 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 770 kB     \nDownloading: 770 kB     \nDownloading: 770 kB     \nDownloading: 770 kB     \nDownloading: 780 kB     \nDownloading: 780 kB     \nDownloading: 780 kB     \nDownloading: 780 kB     \nDownloading: 790 kB     \nDownloading: 790 kB     \nDownloading: 790 kB     \nDownloading: 790 kB     \nDownloading: 810 kB     \nDownloading: 810 kB     \nDownloading: 820 kB     \nDownloading: 820 kB     \nDownloading: 820 kB     \nDownloading: 820 kB     \nDownloading: 830 kB     \nDownloading: 830 kB     \nDownloading: 830 kB     \nDownloading: 830 kB     \nDownloading: 850 kB     \nDownloading: 850 kB     \nDownloading: 850 kB     \nDownloading: 850 kB     \nDownloading: 860 kB     \nDownloading: 860 kB     \nDownloading: 1.2 MB     \nDownloading: 1.2 MB     \nDownloading: 1.4 MB     \nDownloading: 1.4 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 1.9 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \n\n# Join the income dataset to the boundaries dataset\nid_school_joined <- left_join(idaho_school, idaho_income, by = \"GEOID\")\n\nplot(id_school_joined[\"estimate\"])"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#shifting-alaska-and-hawaii-geometry",
    "href": "datacamp/us_census/us_census.html#shifting-alaska-and-hawaii-geometry",
    "title": "Census data in r with tidycensus",
    "section": "Shifting Alaska and Hawaii geometry",
    "text": "Shifting Alaska and Hawaii geometry\nAnalysts will commonly want to map data for the entire United States by state or county; however, this can be difficult by default as Alaska and Hawaii are distant from the continental United States. A common solution is to rescale and shift Alaska and Hawaii for mapping purposes, which is supported in tidycensus. You’ll learn how to do this in this exercise.\n\n# Get a dataset of median home values from the 1-year ACS\nstate_value <- get_acs(geography = \"state\", \n                       variables = \"B25077_001\", \n                       survey = \"acs1\", \n                       geometry = TRUE, \n                       shift_geo = TRUE)\n\n# Plot the dataset to view the shifted geometry\nplot(state_value[\"estimate\"])"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#making-a-choropleth-map",
    "href": "datacamp/us_census/us_census.html#making-a-choropleth-map",
    "title": "Census data in r with tidycensus",
    "section": "Making a choropleth map",
    "text": "Making a choropleth map\nChoropleth maps, which visualize statistical variation through the shading of areas, are among the most popular ways to map demographic data. Census or ACS data acquired with tidycensus can be mapped in this way in ggplot2 with geom_sf using the estimate column as a fill aesthetic. In this exercise, you’ll make a choropleth map with ggplot2 of median owner-occupied home values by Census tract for Marin County, California.\n\n# Create a choropleth map with ggplot\nggplot(marin_value, aes(fill = estimate)) + \n  geom_sf()"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#modifying-map-colors",
    "href": "datacamp/us_census/us_census.html#modifying-map-colors",
    "title": "Census data in r with tidycensus",
    "section": "Modifying map colors",
    "text": "Modifying map colors\nggplot2 version 3.0 integrated the viridis color palettes, which are perceptually uniform and legible to colorblind individuals and in black and white. For these reasons, the viridis palettes have become very popular for data visualization, including for choropleth mapping. In this exercise, you’ll learn how to use the viridis palettes for choropleth mapping in ggplot2.\n\n# Set continuous viridis palettes for your map\nggplot(marin_value, aes(fill = estimate, color = estimate)) + \n  geom_sf() + \n  scale_fill_viridis_c() +  \n  scale_color_viridis_c()"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#customizing-the-map-output",
    "href": "datacamp/us_census/us_census.html#customizing-the-map-output",
    "title": "Census data in r with tidycensus",
    "section": "Customizing the map output",
    "text": "Customizing the map output\nNow that you’ve chosen an appropriate color palette for your choropleth map of median home values by Census tract in Marin County, you’ll want to customize the output. In this exercise, you’ll clean up some map elements and add some descriptive information to provide context to your map.\n\n# Set the color guide to FALSE and add a subtitle and caption to your map\nggplot(marin_value, aes(fill = estimate, color = estimate)) + \n  geom_sf() + \n  scale_fill_viridis_c(labels = scales::dollar) +  \n  scale_color_viridis_c(guide = FALSE) + \n  theme_minimal() + \n  coord_sf(crs = 26911, datum = NA) + \n  labs(title = \"Median owner-occupied housing value by Census tract\", \n       subtitle = \"Marin County, California\", \n       caption = \"Data source: 2012-2016 ACS.\\nData acquired with the R tidycensus package.\", \n       fill = \"ACS estimate\")"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#graduated-symbol-maps",
    "href": "datacamp/us_census/us_census.html#graduated-symbol-maps",
    "title": "Census data in r with tidycensus",
    "section": "Graduated symbol maps",
    "text": "Graduated symbol maps\nThere are many other effective ways to map statistical data besides choropleth maps. One such example is the graduated symbol map, which represents statistical variation by differently-sized symbols. In this exercise, you’ll learn how to use the st_centroid() tool in the sf package to create points at the center of each state to be used as inputs to a graduated symbol map in ggplot2.\n\nlibrary(sf)\n\n# Generate point centers\ncenters <- st_centroid(state_value)\n\n# Set size parameter and the size range\nggplot() + \n  geom_sf(data = state_value, fill = \"white\") + \n  geom_sf(data = centers, aes(size = estimate), shape = 21, \n          fill = \"lightblue\", alpha = 0.7, show.legend = \"point\") + \n  scale_size_continuous(range = c(1, 20))"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#faceted-maps-with-ggplot2",
    "href": "datacamp/us_census/us_census.html#faceted-maps-with-ggplot2",
    "title": "Census data in r with tidycensus",
    "section": "Faceted maps with ggplot2",
    "text": "Faceted maps with ggplot2\nOne of the most powerful features of ggplot2 is its support for faceted plotting, in which multiple plots are generated for unique values of a column in the data. Faceted maps can be produced with geom_sf() in this way as well if tidycensus data are in tidy format. In this exercise, you’ll produce faceted maps that show the racial and ethnic geography of Washington, DC from the 2010 decennial Census.\n\n# Check the first few rows of the loaded dataset dc_race\nhead(dc_race)\n\n# Remove the gridlines and generate faceted maps\nggplot(dc_race, aes(fill = percent, color = percent)) + \n  geom_sf() + \n  coord_sf(datum = NA) + \n  facet_wrap(~variable)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#interactive-visualization-with-mapview",
    "href": "datacamp/us_census/us_census.html#interactive-visualization-with-mapview",
    "title": "Census data in r with tidycensus",
    "section": "Interactive visualization with mapview",
    "text": "Interactive visualization with mapview\nThe mapview R package allows R users to interactively map spatial datasets in one line of R code. This makes it an essential tool for exploratory spatial data analysis in R. In this exercise, you’ll learn how to quickly map tidycensus data interactively using mapview and your Orange County, California median housing values dataset.\n\n# Add a legend to your map\nm <- mapview(orange_value, \n         zcol = \"estimate\", \n         legend = TRUE)\nm@map"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#generating-random-dots-with-sf",
    "href": "datacamp/us_census/us_census.html#generating-random-dots-with-sf",
    "title": "Census data in r with tidycensus",
    "section": "Generating random dots with sf",
    "text": "Generating random dots with sf\nDot-density maps are created by randomly placing dots within areas where each dot is proportional to a certain number of observations. In this exercise, you’ll learn how to create dots in this way with the sf package using the st_sample() function. You will generate dots that are proportional to about 100 people in the decennial Census, and then you will group the dots to speed up plotting with ggplot2.\n\n# Generate dots, create a group column, and group by group column\ndc_dots <- map(c(\"White\", \"Black\", \"Hispanic\", \"Asian\"), function(group) {\n  dc_race %>%\n    filter(variable == group) %>%\n    st_sample(., size = .$value / 100) %>%\n    st_sf() %>%\n    group_by(group = group) \n}) %>%\n  reduce(rbind) %>%\n  group_by(group) %>%\n  summarize()"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#obtaining-data-for-cartography-with-tigris",
    "href": "datacamp/us_census/us_census.html#obtaining-data-for-cartography-with-tigris",
    "title": "Census data in r with tidycensus",
    "section": "Obtaining data for cartography with tigris",
    "text": "Obtaining data for cartography with tigris\nBefore making your dot-density map of Washington, DC with ggplot2, it will be useful to acquire some ancillary cartographic data with the tigris package that will help map viewers understand what you’ve visualized. These datasets will include major roads in DC; area water features; and the boundary of the District of Columbia, which you’ll use as a background in your map.\n\n# Filter the DC roads object for major roads only\ndc_roads <- roads(\"DC\", \"District of Columbia\") %>%\n  filter(RTTYP %in% c(\"I\", \"S\", \"U\"))\n\n# Get an area water dataset for DC\ndc_water <- area_water(\"DC\", \"District of Columbia\")\n\n# Get the boundary of DC\ndc_boundary <- counties(\"DC\", cb = TRUE)"
  },
  {
    "objectID": "datacamp/us_census/us_census.html#making-a-dot-density-map-with-ggplot2",
    "href": "datacamp/us_census/us_census.html#making-a-dot-density-map-with-ggplot2",
    "title": "Census data in r with tidycensus",
    "section": "Making a dot-density map with ggplot2",
    "text": "Making a dot-density map with ggplot2\nIn your final exercise of this course, you are going to put together the datasets you’ve acquired and generated into a dot-density map with ggplot2. You’ll plot your generated dots and ancillary datasets with geom_sf(), and add some informative map elements to create a cartographic product.\n\n# Plot your datasets and give your map an informative caption\nggplot() + \n  geom_sf(data = dc_boundary, color = NA, fill = \"white\") + \n  geom_sf(data = dc_dots, aes(color = group, fill = group), size = 0.1) + \n  geom_sf(data = dc_roads, color = \"lightblue\", fill = \"lightblue\") + \n  geom_sf(data = dc_water, color = \"grey\") + \n  coord_sf(crs = 26918, datum = NA) + \n  scale_color_brewer(palette = \"Set1\", guide = FALSE) +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(title = \"The racial geography of Washington, DC\", \n       subtitle = \"2010 decennial U.S. Census\", \n       fill = \"\", \n       caption = \"1 dot = approximately 100 people.\\nData acquired with the R tidycensus and tigris packages.\")"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html",
    "href": "datacamp/sampling_R/sampling_R.html",
    "title": "Sampling in R",
    "section": "",
    "text": "Sampling is an important technique in your statistical arsenal. It isn’t always appropriate though—you need to know when to use it and when to work with the whole dataset.\n\nWhich of the following is not a good scenario to use sampling?\nwhen data set is small"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html#simple-sampling-with-dplyr",
    "href": "datacamp/sampling_R/sampling_R.html#simple-sampling-with-dplyr",
    "title": "Sampling in R",
    "section": "Simple sampling with dplyr",
    "text": "Simple sampling with dplyr\nThroughout this chapter you’ll be exploring song data from Spotify. Each row of the dataset represents a song, and there are 41656 rows. Columns include the name of the song, the artists who performed it, the release year, and attributes of the song like its duration, tempo, and danceability. We’ll start by looking at the durations.\nYour first task is to sample the song dataset and compare a calculation on the whole population and on a sample.\nspotify_population is available and dplyr is loaded.\n\nlibrary(tidyverse)\nlibrary(fst)\nlibrary(knitr)\nspotify_population <- read_fst(\"data/spotify_2000_2020.fst\")\n# View the whole population dataset\n\n# Sample 1000 rows from spotify_population\nspotify_sample <- slice_sample(spotify_population, n = 10)\n\n\n# See the result\nkable(spotify_sample)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nacousticness\nartists\ndanceability\nduration_ms\nduration_minutes\nenergy\nexplicit\nid\ninstrumentalness\nkey\nliveness\nloudness\nmode\nname\npopularity\nrelease_date\nspeechiness\ntempo\nvalence\nyear\n\n\n\n\n0.067800\n[‘Keane’]\n0.442\n235880\n3.931333\n0.6060\n0\n7AQAlklmptrrkBSeujkXsD\n1.43e-05\n9\n0.0973\n-5.825\n1\nSomewhere Only We Know\n54\n2013-01-01\n0.0259\n171.982\n0.3590\n2013\n\n\n0.096400\n[‘NF’, ‘Ruelle’]\n0.620\n217107\n3.618450\n0.8350\n0\n68biLwi894rMQPeIiSky2t\n1.23e-05\n5\n0.2690\n-6.634\n0\n10 Feet Down\n64\n2017-10-06\n0.0984\n96.090\n0.0535\n2017\n\n\n0.114000\n[‘Lifehouse’]\n0.459\n195493\n3.258217\n0.4270\n0\n0815caqt2Lytro5EIzMufT\n0.00e+00\n7\n0.1680\n-7.734\n1\nYou And Me\n75\n2005-03-22\n0.0267\n139.902\n0.3540\n2005\n\n\n0.064100\n[‘Young Thug’]\n0.710\n226733\n3.778883\n0.4860\n1\n4BP0An7SXctTY4kF41JxIQ\n0.00e+00\n11\n0.1120\n-6.221\n0\nHalftime\n56\n2015-04-16\n0.2510\n133.284\n0.1700\n2015\n\n\n0.257000\n[‘Ski Mask The Slump God’]\n0.881\n129547\n2.159117\n0.6230\n1\n3EikYy40GMSp8l5mDV6IQo\n1.50e-06\n1\n0.0964\n-9.662\n1\nFoot Fungus\n71\n2018-11-30\n0.1190\n88.953\n0.7450\n2018\n\n\n0.088300\n[‘George Strait’]\n0.615\n182120\n3.035333\n0.7320\n0\n23CzqFXnFgaJo2L9vJXWHN\n5.90e-06\n6\n0.1070\n-8.740\n1\nHere For A Good Time\n49\n2011-01-01\n0.0348\n127.881\n0.5570\n2011\n\n\n0.995000\n[‘Yue Yan Jen’]\n0.181\n160500\n2.675000\n0.0287\n0\n2hkPUEsERVGoI4DxfZjUj5\n9.35e-01\n8\n0.0969\n-33.787\n0\n梦蝶 - Dream to be a butterfly\n67\n2019-08-24\n0.0378\n107.222\n0.1110\n2019\n\n\n0.014100\n[‘Interpol’]\n0.171\n259373\n4.322883\n0.7530\n0\n2ZBYcZBEjn3Mih9ItJx6jT\n4.81e-02\n0\n0.1100\n-4.871\n1\nNYC\n47\n2002\n0.0312\n77.760\n0.2840\n2002\n\n\n0.061200\n[‘Jelly Roll’]\n0.736\n163104\n2.718400\n0.8660\n1\n2MZyHeZDXnvF6TWfCw18wB\n0.00e+00\n11\n0.0465\n-5.714\n0\nHate Goes On\n59\n2017-04-21\n0.1090\n110.050\n0.6460\n2017\n\n\n0.000105\n[‘Goldfinger’]\n0.431\n158840\n2.647333\n0.8990\n0\n0f7BrBVTDAv7napz463Fwb\n0.00e+00\n6\n0.0577\n-5.531\n0\nMy Everything\n49\n2005-02-15\n0.1120\n179.970\n0.5700\n2005"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html#simple-sampling-with-dplyr-1",
    "href": "datacamp/sampling_R/sampling_R.html#simple-sampling-with-dplyr-1",
    "title": "Sampling in R",
    "section": "Simple sampling with dplyr",
    "text": "Simple sampling with dplyr\nThroughout this chapter you’ll be exploring song data from Spotify. Each row of the dataset represents a song, and there are 41656 rows. Columns include the name of the song, the artists who performed it, the release year, and attributes of the song like its duration, tempo, and danceability. We’ll start by looking at the durations.\nYour first task is to sample the song dataset and compare a calculation on the whole population and on a sample.\nspotify_population is available and dplyr is loaded.\n\n# Calculate the mean duration in mins from spotify_population\nmean_dur_pop <- summarize(spotify_population, mean(duration_minutes))\n\n\n# Calculate the mean duration in mins from spotify_sample\nmean_dur_samp <- summarize(spotify_sample, mean(duration_minutes))\n\n\n# See the results\nmean_dur_pop\n\n  mean(duration_minutes)\n1               3.852152\n\nmean_dur_samp\n\n  mean(duration_minutes)\n1               3.214495"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html#simple-sampling-with-base-r",
    "href": "datacamp/sampling_R/sampling_R.html#simple-sampling-with-base-r",
    "title": "Sampling in R",
    "section": "Simple sampling with base-R",
    "text": "Simple sampling with base-R\nWhile dplyr provides great tools for sampling data frames, if you want to work with vectors you can use base-R.\nLet’s turn it up to eleven and look at the loudness property of each song.\nspotify_population is available.\n\n# From previous step\nloudness_pop <- spotify_population$loudness\nloudness_samp <- sample(loudness_pop, size = 100)\n\n# Calculate the standard deviation of loudness_pop\nsd_loudness_pop <- sd(loudness_pop)\n\n# Calculate the standard deviation of loudness_samp\nsd_loudness_samp <- sd(loudness_samp)\n\n# See the results\nsd_loudness_pop\n\n[1] 4.524076\n\nsd_loudness_samp\n\n[1] 5.972561"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html#are-findings-from-the-sample-generalizable",
    "href": "datacamp/sampling_R/sampling_R.html#are-findings-from-the-sample-generalizable",
    "title": "Sampling in R",
    "section": "Are findings from the sample generalizable?",
    "text": "Are findings from the sample generalizable?\nYou just saw how convenience sampling—collecting data via the easiest method can result in samples that aren’t representative of the whole population. Equivalently, this means findings from the sample are not generalizable to the whole population. Visualizing the distributions of the population and the sample can help determine whether or not the sample is representative of the population.\nThe Spotify dataset contains a column named acousticness, which is a confidence measure from zero to one of whether the track is acoustic, that is, it was made with instruments that aren’t plugged in. Here, you’ll look at acousticness in the total population of songs, and in a sample of those songs.\nspotify_population and spotify_mysterious_sample are available; dplyr and ggplot2 are loaded.\n\nggplot(spotify_population, aes(acousticness))+\n    geom_histogram(binwidth = 0.01)\n\n\n\n\n\nggplot(spotify_population, aes(duration_minutes))+\n    geom_histogram(binwidth = 0.5)"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html#generating-random-numbers",
    "href": "datacamp/sampling_R/sampling_R.html#generating-random-numbers",
    "title": "Sampling in R",
    "section": "Generating random numbers",
    "text": "Generating random numbers\nYou’ve seen sample() and it’s dplyr cousin, slice_sample() for generating pseudo-random numbers from a set of values. A related task is to generate random numbers that follow a statistical distribution, like the uniform distribution or the normal distribution.\nEach random number generation function has a name beginning with “r”. It’s first argument is the number of numbers to generate, but other arguments are distribution-specific. Free hint: Try args(runif) and args(rnorm) to see what arguments you need to pass to those functions.\nn_numbers is available and set to 5000; ggplot2 is loaded.\n\nn_numbers <- 5000\n# Generate random numbers from ...\nrandoms <- data.frame(\n  # a uniform distribution from -3 to 3\n  uniform =runif(n_numbers, -3, 3),\n  # a normal distribution with mean 5 and sd 2\n  normal = rnorm(n_numbers, mean = 5, sd = 2)\n)\n\n\n# Plot a histogram of uniform values, binwidth 0.25\nggplot(randoms, aes(uniform)) +\n    geom_histogram(binwidth = 0.25)\n\n\n\n# Plot a histogram of normal values, binwidth 0.5\nggplot(randoms, aes(normal)) +\n    geom_histogram(binwidth = 0.5)"
  },
  {
    "objectID": "datacamp/sampling_R/sampling_R.html#understanding-random-seeds",
    "href": "datacamp/sampling_R/sampling_R.html#understanding-random-seeds",
    "title": "Sampling in R",
    "section": "Understanding random seeds",
    "text": "Understanding random seeds\nWhile random numbers are important for many analyses, they create a problem: the results you get can vary slightly. This can cause awkward conversations with your boss when your script for calculating the sales forecast gives different answers each time.\nSetting the seed to R’s random number generator helps avoid such problems by making the random number generation reproducible. - The values of x are different to those of y.\n\nset.seed(123)\nx <- rnorm(5)\ny <- rnorm(5)\nx\n\n[1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774\n\ny\n\n[1]  1.7150650  0.4609162 -1.2650612 -0.6868529 -0.4456620\n\n\n\nx and y have identical values.\n\n\nset.seed(123)\nx <- rnorm(5)\nset.seed(123)\ny <- rnorm(5)\nx\n\n[1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774\n\ny\n\n[1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774\n\n\n\nx and y have identical values.\n\n\nset.seed(123)\nx <- c(rnorm(5), rnorm(5))\nset.seed(123)\ny <- rnorm(10)\nx\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197\n\ny\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "",
    "text": "You will use the MNIST dataset in several exercises through the course. Let’s do some data exploration to gain a better understanding. Remember that the MNIST dataset contains a set of records that represent handwritten digits using 28x28 features, which are stored into a 784-dimensional vector.\nmnistInput  Each record of the MNIST dataset corresponds to a handwritten digit and each feature represents one pixel of the digit image. In this exercise, a sample of 200 records of the MNIST dataset named mnist_sample is loaded for you.\n\nload(\"mnist-sample-200.RData\")\n# Have a look at the MNIST dataset names\n#names(mnist_sample)\n\n# Show the first records\n#str(mnist_sample)\n\n# Labels of the first 6 digits\nhead(mnist_sample$label)\n\n[1] 5 0 7 0 9 3"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#digits-features",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#digits-features",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Digits features",
    "text": "Digits features\nLet’s continue exploring the dataset. Firstly, it would be helpful to know how many different digits are present by computing a histogram of the labels. Next, the basic statistics (min, mean, median, maximum) of the features for all digits can be calculated. Finally, you will compute the basic statistics for only those digits with label 0. The MNIST sample data is loaded for you as mnist_sample.\n\n# Plot the histogram of the digit labels\nhist(mnist_sample$label)\n\n\n\n# Compute the basic statistics of all records\n#summary(mnist_sample)\n\n# Compute the basic statistics of digits with label 0\n#summary(mnist_sample[, mnist_sample$label == 0])"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#euclidean-distance",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#euclidean-distance",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Euclidean distance",
    "text": "Euclidean distance\nEuclidean distance is the basis of many measures of similarity and is the most important distance metric. You can compute the Euclidean distance in R using the dist() function. In this exercise, you will compute the Euclidean distance between the first 10 records of the MNIST sample data.\nThe mnist_sample object is loaded for you.\n\n# Show the labels of the first 10 records\nmnist_sample$label[1:10]\n\n [1] 5 0 7 0 9 3 4 1 2 6\n\n# Compute the Euclidean distance of the first 10 records\ndistances <- dist(mnist_sample[1:10, -1])\n\n# Show the distances values\ndistances\n\n          1        2        3        4        5        6        7        8\n2  2185.551                                                               \n3  2656.407 2869.979                                                      \n4  2547.027 2341.249 2936.772                                             \n5  2406.509 2959.108 1976.406 2870.928                                    \n6  2343.982 2759.681 2452.568 2739.470 2125.723                           \n7  2464.388 2784.158 2573.667 2870.918 2174.322 2653.703                  \n8  2149.872 2668.903 1999.892 2585.980 2067.044 2273.248 2407.962         \n9  2959.129 3209.677 2935.262 3413.821 2870.746 3114.508 2980.663 2833.447\n10 2728.657 3009.771 2574.752 2832.521 2395.589 2655.864 2464.301 2550.126\n          9\n2          \n3          \n4          \n5          \n6          \n7          \n8          \n9          \n10 2695.166\n\n# Plot the numeric matrix of the distances in a heatmap\nheatmap(as.matrix(distances), \n        Rowv = NA, symm = TRUE, \n        labRow = mnist_sample$label[1:10], \n        labCol = mnist_sample$label[1:10])"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#minkowsky-distance",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#minkowsky-distance",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Minkowsky distance",
    "text": "Minkowsky distance\nThere are other well-known distance metrics besides the Euclidean distance, like the Minkowski distance. This metric can be considered a generalisation of both the Euclidean and Manhattan distance. In R, you can calculate the Minkowsky distance of order p by using dist(…, method = “minkowski”, p).\nThe MNIST sample data is loaded for you as mnist_sample\n\n# Minkowski distance or order 3\ndistances_3 <- dist(mnist_sample[1:10, -1], method = \"minkowski\", p = 3)\n\ndistances_3 \n\n           1         2         3         4         5         6         7\n2  1002.6468                                                            \n3  1169.6470 1228.8295                                                  \n4  1127.4919 1044.9182 1249.6133                                        \n5  1091.3114 1260.3549  941.1654 1231.7432                              \n6  1063.7026 1194.1212 1104.2581 1189.9558  996.2687                    \n7  1098.4279 1198.8891 1131.4498 1227.7888 1005.7588 1165.4475          \n8  1006.9070 1169.4720  950.6812 1143.3503  980.6450 1056.1814 1083.2255\n9  1270.0240 1337.2068 1257.4052 1401.2461 1248.0777 1319.2768 1271.7095\n10 1186.9620 1268.1539 1134.0371 1219.1388 1084.5416 1166.9129 1096.3586\n           8         9\n2                     \n3                     \n4                     \n5                     \n6                     \n7                     \n8                     \n9  1236.9178          \n10 1133.2929 1180.7970\n\nheatmap(as.matrix(distances_3 ), \n        Rowv = NA, symm = TRUE, \n        labRow = mnist_sample$label[1:10], \n        labCol = mnist_sample$label[1:10])\n\n\n\n# Minkowski distance of order 2\ndistances_2 <- dist(mnist_sample[1:10, -1], method = \"minkowski\", p = 2)\ndistances_2\n\n          1        2        3        4        5        6        7        8\n2  2185.551                                                               \n3  2656.407 2869.979                                                      \n4  2547.027 2341.249 2936.772                                             \n5  2406.509 2959.108 1976.406 2870.928                                    \n6  2343.982 2759.681 2452.568 2739.470 2125.723                           \n7  2464.388 2784.158 2573.667 2870.918 2174.322 2653.703                  \n8  2149.872 2668.903 1999.892 2585.980 2067.044 2273.248 2407.962         \n9  2959.129 3209.677 2935.262 3413.821 2870.746 3114.508 2980.663 2833.447\n10 2728.657 3009.771 2574.752 2832.521 2395.589 2655.864 2464.301 2550.126\n          9\n2          \n3          \n4          \n5          \n6          \n7          \n8          \n9          \n10 2695.166\n\nheatmap(as.matrix(distances_2), \n        Rowv = NA, symm = TRUE, \n        labRow = mnist_sample$label[1:10], \n        labCol = mnist_sample$label[1:10])\n\n\n\n\n\nVery Good! As you can see, when using Minkowski distance of order 2 the most similar digits are in positions 3 and 5 of the heatmap grid which corresponds to digits 7 and 9."
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#kl-divergence",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#kl-divergence",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "KL divergence",
    "text": "KL divergence\nThere are more distance metrics that can be used to compute how similar two feature vectors are. For instance, the philentropy package has the function distance(), which implements 46 different distance metrics. For more information, use ?distance in the console. In this exercise, you will compute the KL divergence and check if the results differ from the previous metrics. Since the KL divergence is a measure of the difference between probability distributions you need to rescale the input data by dividing each input feature by the total pixel intensities of that digit. The philentropy package and mnist_sample data have been loaded.\n\nlibrary(philentropy)\nlibrary(tidyverse)\n# Get the first 10 records\nmnist_10 <- mnist_sample[1:10, -1]\n\n# Add 1 to avoid NaN when rescaling\nmnist_10_prep <- mnist_10 + 1 \n\n# Compute the sums per row\nsums <- rowSums(mnist_10_prep)\n\n# Compute KL divergence\ndistances <- distance(mnist_10_prep/sums, method = \"kullback-leibler\")\nheatmap(as.matrix(distances), \n        Rowv = NA, symm = TRUE, \n        labRow = mnist_sample$label[1:10], \n        labCol = mnist_sample$label[1:10])"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#generating-pca-from-mnist-sample",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#generating-pca-from-mnist-sample",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Generating PCA from MNIST sample",
    "text": "Generating PCA from MNIST sample\nYou are going to compute a PCA with the previous mnist_sample dataset. The goal is to have a good representation of each digit in a lower dimensional space. PCA will give you a set of variables, named principal components, that are a linear combination of the input variables. These principal components are ordered in terms of the variance they capture from the original data. So, if you plot the first two principal components you can see the digits in a 2-dimensional space. A sample of 200 records of the MNIST dataset named mnist_sample is loaded for you.\n\n# Get the principal components from PCA\npca_output <- prcomp(mnist_sample[, -1])\n\n# Observe a summary of the output\n#summary(pca_output)\n\n# Store the first two coordinates and the label in a data frame\npca_plot <- data.frame(pca_x = pca_output$x[, \"PC1\"], pca_y = pca_output$x[, \"PC2\"], \n                       label = as.factor(mnist_sample$label))\n\n# Plot the first two principal components using the true labels as color and shape\nggplot(pca_plot, aes(x = pca_x, y = pca_y, color = label)) + \n    ggtitle(\"PCA of MNIST sample\") + \n    geom_text(aes(label = label)) + \n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#t-sne-output-from-mnist-sample",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#t-sne-output-from-mnist-sample",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "t-SNE output from MNIST sample",
    "text": "t-SNE output from MNIST sample\nYou have seen that PCA has some limitations in correctly classifying digits, mainly due to its linear nature. In this exercise, you are going to use the output from the t-SNE algorithm on the MNIST sample data, named tsne_output and visualize the obtained results. In the next chapter, you will focus on the t-SNE algorithm and learn more about how to use it! The MNIST sample dataset mnist_sample as well as the tsne_output are available in your workspace.\n\n# Explore the tsne_output structure\nlibrary(Rtsne)\nlibrary(tidyverse)\ntsne_output <- Rtsne(mnist_sample[, -1])\n#str(tsne_output)\n\n# Have a look at the first records from the t-SNE output\n#head(tsne_output)\n\n# Store the first two coordinates and the label in a data.frame\ntsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], \n                        label = as.factor(mnist_sample$label))\n\n# Plot the t-SNE embedding using the true labels as color and shape\nggplot(tsne_plot, aes(x =tsne_x, y = tsne_y, color = label)) + \n    ggtitle(\"T-Sne output\") + \n    geom_text(aes(label = label)) + \n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-t-sne",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-t-sne",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Computing t-SNE",
    "text": "Computing t-SNE\nAs we have seen, the t-SNE embedding can be computed in R using the Rtsne() function from the Rtsne package in CRAN. Performing a PCA is a common step before running the t-SNE algorithm, but we can skip this step by setting the parameter PCA to FALSE. The dimensionality of the embedding generated by t-SNE can be indicated with the dims parameter. In this exercise, we will generate a three-dimensional embedding from the mnist_sample dataset without doing the PCA step and then, we will plot the first two dimensions. The MNIST sample dataset mnist_sample, as well as the Rtsne and ggplot2 packages, are already loaded.\n\n# Compute t-SNE without doing the PCA step\ntsne_output <- Rtsne(mnist_sample[,-1], PCA = FALSE, dim = 3)\n\n# Show the obtained embedding coordinates\nhead(tsne_output$Y)\n\n           [,1]      [,2]       [,3]\n[1,]  8.5130585 -0.012040 -0.5515124\n[2,]  9.1824788  6.818073 -0.1714140\n[3,] -0.7071571 -9.275270 -1.4029525\n[4,] 12.7867513  4.224923  1.2625483\n[5,] -2.8332174 -5.665389 -2.9483576\n[6,]  7.5231512 -6.763459  4.6436595\n\n# Store the first two coordinates and plot them \ntsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], \n                        digit = as.factor(mnist_sample$label))\n\n# Plot the coordinates\nggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = digit)) + \n    ggtitle(\"t-SNE of MNIST sample\") + \n    geom_text(aes(label = digit)) + \n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#understanding-t-sne-output",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#understanding-t-sne-output",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Understanding t-SNE output",
    "text": "Understanding t-SNE output\nThe most important t-SNE output are those related to the K-L divergence of the points in the original high dimensions and in the new lower dimensional space. Remember that the goal of t-SNE is to minimize the K-L divergence between the original space and the new one. In the returned object, the itercosts structure indicates the total cost from the K-L divergence of all the objects in each 50th iteration and the cost structure indicates the K-L divergence of each record in the final iteration. The Rtsne package and the tsne_output object have been loaded for you.\n\n# Inspect the output object's structure\nstr(tsne_output)\n\nList of 14\n $ N                  : int 200\n $ Y                  : num [1:200, 1:3] 8.513 9.182 -0.707 12.787 -2.833 ...\n $ costs              : num [1:200] 0.003148 0.001657 0.000909 0.001884 0.002727 ...\n $ itercosts          : num [1:20] 52.4 54.5 53.7 52.9 53.3 ...\n $ origD              : int 50\n $ perplexity         : num 30\n $ theta              : num 0.5\n $ max_iter           : num 1000\n $ stop_lying_iter    : int 250\n $ mom_switch_iter    : int 250\n $ momentum           : num 0.5\n $ final_momentum     : num 0.8\n $ eta                : num 200\n $ exaggeration_factor: num 12\n - attr(*, \"class\")= chr [1:2] \"Rtsne\" \"list\"\n\n# Show total costs after each 50th iteration\ntsne_output$itercosts\n\n [1] 52.3726927 54.5336671 53.6914934 52.8764083 53.3387007  1.0203964\n [7]  0.6191073  0.5653559  0.5283445  0.5134371  0.5090093  0.5043607\n[13]  0.5048437  0.5056413  0.5046064  0.5026324  0.5020231  0.5027582\n[19]  0.5023956  0.5012762\n\n# Plot the evolution of the KL divergence at each 50th iteration\nplot(tsne_output$itercosts, type = \"l\")\n\n\n\n# Inspect the output object's structure\nstr(tsne_output)\n\nList of 14\n $ N                  : int 200\n $ Y                  : num [1:200, 1:3] 8.513 9.182 -0.707 12.787 -2.833 ...\n $ costs              : num [1:200] 0.003148 0.001657 0.000909 0.001884 0.002727 ...\n $ itercosts          : num [1:20] 52.4 54.5 53.7 52.9 53.3 ...\n $ origD              : int 50\n $ perplexity         : num 30\n $ theta              : num 0.5\n $ max_iter           : num 1000\n $ stop_lying_iter    : int 250\n $ mom_switch_iter    : int 250\n $ momentum           : num 0.5\n $ final_momentum     : num 0.8\n $ eta                : num 200\n $ exaggeration_factor: num 12\n - attr(*, \"class\")= chr [1:2] \"Rtsne\" \"list\"\n\n# Show the K-L divergence of each record after the final iteration\ntsne_output$costs\n\n  [1]  3.147977e-03  1.657133e-03  9.094168e-04  1.883642e-03  2.727292e-03\n  [6]  5.981636e-03  2.961496e-03  1.149274e-03  9.814474e-04  3.566301e-03\n [11]  2.149702e-03  2.731169e-03  2.540845e-03  1.373162e-03  3.517077e-03\n [16]  1.093367e-03  3.272366e-03  2.950097e-03  9.629194e-04  2.587849e-03\n [21]  1.821698e-03  1.910042e-03  3.428901e-03  1.970162e-03  2.811635e-03\n [26]  2.295839e-03  2.482478e-03  4.181399e-03  2.593145e-03  5.952338e-04\n [31]  3.647138e-03  2.995405e-03  5.705253e-03  2.913593e-03  2.089385e-03\n [36]  3.725846e-03  5.051920e-03  2.778255e-03  1.146354e-03  3.724955e-03\n [41]  3.746930e-03  2.539002e-03  2.545464e-03  1.136219e-03  2.599992e-03\n [46]  3.142187e-03  2.523337e-03  5.560204e-03  1.224284e-03  4.086904e-03\n [51]  2.648273e-03  4.079952e-03  1.457145e-03  4.767710e-03  2.043155e-03\n [56]  6.357258e-03  1.432891e-03  4.169183e-03  5.687548e-04  1.564242e-03\n [61]  5.151411e-03  1.567611e-03  1.105690e-03  1.778718e-03  2.617975e-03\n [66]  5.349474e-04  7.656693e-04  1.153715e-03  2.207745e-03  1.252527e-03\n [71]  2.964990e-03  4.391129e-03  6.054599e-04  3.139128e-03  1.357688e-03\n [76]  2.763932e-03  3.609703e-04  2.441988e-03  1.312005e-03  1.429704e-03\n [81]  3.138023e-03  1.858109e-03  3.062706e-03  1.287617e-03  4.929857e-03\n [86]  2.223998e-03  1.386267e-03  3.148082e-03  9.523172e-04  2.810307e-03\n [91]  4.207387e-03  6.719745e-04  3.557881e-03  6.957375e-04  7.828391e-03\n [96]  3.011792e-03  1.537943e-03  1.288109e-03  2.666971e-03  1.762996e-03\n[101]  1.810986e-03  2.795898e-03  1.777009e-03  1.422414e-03  1.271262e-03\n[106]  2.626858e-03  1.455897e-03  3.397300e-03  2.166585e-03  2.316651e-03\n[111]  2.071246e-03  1.925355e-03  1.751107e-03  2.373877e-03  5.087640e-03\n[116]  3.132379e-03  1.860208e-03  3.079543e-03  7.759835e-05  5.291170e-03\n[121]  2.419754e-03  3.796683e-03  6.202756e-04  1.514771e-03  4.007155e-03\n[126]  2.017814e-03  3.729397e-03  1.821870e-03  2.728735e-03  2.495229e-03\n[131]  9.011142e-03  2.814947e-03  2.386796e-03  1.740721e-03  1.336325e-03\n[136]  3.181815e-03  5.448146e-03  8.346276e-04  1.350390e-03  2.620289e-03\n[141]  1.222717e-03  2.936514e-03  3.322775e-03  1.345822e-03  1.607994e-03\n[146]  4.641212e-03  2.407053e-03  9.286697e-04  2.743769e-03  2.071098e-03\n[151] -2.657814e-04  1.840861e-03  2.507302e-03  2.623730e-03  5.982988e-03\n[156]  3.248133e-03  5.756869e-03  5.601611e-03  1.538261e-04  9.860545e-04\n[161]  1.425812e-03  1.063799e-03  2.782750e-03  1.690733e-03  1.465921e-03\n[166]  2.003639e-03  1.616896e-03  2.028894e-03  2.939394e-03  1.643062e-03\n[171]  5.332427e-04  1.439077e-03  2.318259e-03  2.606944e-03  4.756881e-03\n[176]  4.211307e-03  2.062096e-03  2.740272e-03  5.025564e-03  2.396387e-03\n[181]  1.083252e-03  1.781207e-03  3.484742e-03  8.881287e-04  2.144355e-03\n[186]  1.098430e-03  1.564275e-03  2.496889e-03  4.021953e-03  1.723347e-03\n[191]  2.774782e-03  2.583104e-03  2.217648e-03  4.212547e-03  2.356833e-03\n[196]  1.382957e-03  1.347242e-03  2.425309e-03  2.925207e-03  1.646659e-03\n\n# Plot the K-L divergence of each record after the final iteration\nplot(tsne_output$costs, type = \"l\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#reproducing-results",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#reproducing-results",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Reproducing results",
    "text": "Reproducing results\nt-SNE is a stochastic algorithm, meaning there is some randomness inherent to the process. To ensure reproducible results it is necessary to fix a seed before every new execution. This way, you can tune the algorithm hyper-parameters and isolate the effect of the randomness. In this exercise, the goal is to generate two embeddings and check that they are identical. The mnist_sample dataset is available in your workspace.\n\n# Generate a three-dimensional t-SNE embedding without PCA\ntsne_output <- Rtsne(mnist_sample[, -1], PCA = FALSE, dim = 3)\n\n# Generate a new t-SNE embedding with the same hyper-parameter values\ntsne_output_new <- Rtsne(mnist_sample[, -1], PCA = FALSE, dim = 3)\n\n# Check if the two outputs are identical\nidentical(tsne_output, tsne_output_new)\n\n[1] FALSE\n\n# Generate a three-dimensional t-SNE embedding without PCA\nset.seed(1234)\ntsne_output <- Rtsne(mnist_sample[, -1], PCA = FALSE, dims = 3)\n\n# Generate a new t-SNE embedding with the same hyper-parameter values\nset.seed(1234)\ntsne_output_new <- Rtsne(mnist_sample[, -1], PCA = FALSE, dims = 3)\n\n# Check if the two outputs are identical\nidentical(tsne_output, tsne_output_new)\n\n[1] TRUE"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#optimal-number-of-iterations",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#optimal-number-of-iterations",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Optimal number of iterations",
    "text": "Optimal number of iterations\nA common hyper-parameter to optimize in t-SNE is the optimal number of iterations. As you have seen before it is important to always use the same seed before you can compare different executions. To optimize the number of iterations, you can increase the max_iter parameter of Rtsne() and observe the returned itercosts to find the minimum K-L divergence. The mnist_sample dataset and the Rtsne package have been loaded for you.\n\n# Set seed to ensure reproducible results\nset.seed(1234)\n\n# Execute a t-SNE with 2000 iterations\ntsne_output <- Rtsne(mnist_sample[, -1], max_iter = 2000,PCA = TRUE, dims = 2)\n\n# Observe the output costs \ntsne_output$itercosts\n\n [1] 53.2661477 51.4609512 52.6459945 52.0146489 51.9524842  0.9740208\n [7]  0.7721006  0.7459989  0.7159438  0.7086349  0.7035461  0.6974368\n[13]  0.6951862  0.6965771  0.6956987  0.6958303  0.6860614  0.6845716\n[19]  0.6844257  0.6842720  0.6820150  0.6813309  0.6812841  0.6833337\n[25]  0.6834150  0.6823819  0.6822590  0.6829398  0.6819058  0.6823040\n[31]  0.6824528  0.6831008  0.6819460  0.6829194  0.6828687  0.6829107\n[37]  0.6821132  0.6813898  0.6818957  0.6809017\n\n# Get the 50th iteration with the minimum K-L cost\nwhich.min(tsne_output$itercosts)\n\n[1] 40"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#perplexity-of-mnist-sample",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#perplexity-of-mnist-sample",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Perplexity of MNIST sample",
    "text": "Perplexity of MNIST sample\nThe perplexity parameter indicates the balance between the local and global aspect of the input data. The parameter is an estimate of the number of close neighbors of each original point. Typical values of this parameter fall in the range of 5 to 50. We will generate three different t-SNE executions with the same number of iterations and perplexity values of 5, 20, and 50 and observe the differences in the K-L divergence costs. The optimal number of iterations we found in the last exercise (1200) will be used here. The mnist_sample dataset and the Rtsne package have been loaded for you.\n\n# Set seed to ensure reproducible results\npar(mfrow = c(3, 1))\nset.seed(1234)\n\nperp <- c(5, 20, 50)\nmodels <- list()\nfor (i in 1:length(perp)) {\n        \n        # Execute a t-SNE with perplexity 5\n        perplexity  = perp[i]\n        tsne_output <- Rtsne(mnist_sample[, -1], perplexity = perplexity, max_iter = 1300)\n        # Observe the returned K-L divergence costs at every 50th iteration\n        models[[i]] <- tsne_output\n        plot(tsne_output$itercosts,\n             main = paste(\"Perplexity\", perplexity),\n             type = \"l\", ylab = \"itercosts\")\n}\n\n\n\nnames(models) <- paste0(\"perplexity\",perp)"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#perplexity-of-bigger-mnist-dataset",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#perplexity-of-bigger-mnist-dataset",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Perplexity of bigger MNIST dataset",
    "text": "Perplexity of bigger MNIST dataset\nNow, let’s investigate the effect of the perplexity values with a bigger MNIST dataset of 10.000 records. It would take a lot of time to execute t-SNE for this many records on the DataCamp platform. This is why the pre-loaded output of two t-SNE embeddings with perplexity values of 5 and 50, named tsne_output_5 and tsne_output_50 are available in the workspace. We will look at the K-L costs and plot them using the digit label from the mnist_10k dataset, which is also available in the environment. The Rtsne and ggplot2 packages have been loaded.\n\nI used mnist smaller data set\n\n\n# Observe the K-L divergence costs with perplexity 5 and 50\ntsne_output_5 <- models$perplexity5\ntsne_output_50  <- models$perplexity50\n# Generate the data frame to visualize the embedding\ntsne_plot_5 <- data.frame(tsne_x = tsne_output_5$Y[, 1], tsne_y = tsne_output_5$Y[, 2], digit = as.factor(mnist_sample$label))\n\ntsne_plot_50 <- data.frame(tsne_x = tsne_output_50$Y[, 1], tsne_y = tsne_output_50$Y[, 2], digit = as.factor(mnist_sample$label))\n\n# Plot the obtained embeddings\nggplot(tsne_plot_5, aes(x = tsne_x, y = tsne_y, color = digit)) + \n    ggtitle(\"MNIST t-SNE with 1300 iter and Perplexity=5\") +\n    geom_text(aes(label = digit)) + \n    theme(legend.position=\"none\")\n\n\n\nggplot(tsne_plot_50, aes(x = tsne_x, y = tsne_y, color = digit)) + \n    ggtitle(\"MNIST t-SNE with 1300 iter and Perplexity=50\") + \n    geom_text(aes(label = digit)) + \n    theme(legend.position=\"none\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#plotting-spatial-distribution-of-true-classes",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#plotting-spatial-distribution-of-true-classes",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Plotting spatial distribution of true classes",
    "text": "Plotting spatial distribution of true classes\nAs seen in the video, you can use the obtained representation of t-SNE in a lower dimension space to classify new digits based on the Euclidean distance to known clusters of digits. For this task, let’s start with plotting the spatial distribution of the digit labels in the embedding space. You are going to use the output of a t-SNE execution of 10K MNIST records named tsne and the true labels can be found in a dataset named mnist_10k. In this exercise, you will use the first 5K records of tsne and mnist_10k datasets and the goal is to visualize the obtained t-SNE embedding. The ggplot2 package has been loaded for you.\n\nlibrary(data.table)\nmnist_10k <- readRDS(\"mnist_10k.rds\") %>% setDT()\ntsne <- Rtsne(mnist_10k[, -1], perplexity = 50, max_iter = 1500)\n# Prepare the data.frame\ntsne_plot <- data.frame(tsne_x = tsne$Y[1:5000, 1], \n                        tsne_y = tsne$Y[1:5000, 2], \n                        digit = as.factor(mnist_10k[1:5000, ]$label))\n\n# Plot the obtained embedding\nggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = digit)) + \n    ggtitle(\"MNIST embedding of the first 5K digits\") + \n    geom_text(aes(label = digit)) + \n    theme(legend.position=\"none\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-the-centroids-of-each-class",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-the-centroids-of-each-class",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Computing the centroids of each class",
    "text": "Computing the centroids of each class\nSince the previous visual representation of the digit in a low dimensional space makes sense, you want to compute the centroid of each class in this lower dimensional space. This centroid can be used as a prototype of the digit and you can classify new digits based on their Euclidean distance to these ones. The MNIST data mnist_10k and t-SNE output tsne are available in the workspace. The data.table package has been loaded for you.\n\n# Get the first 5K records and set the column names\ndt_prototypes <- as.data.table(tsne$Y[1:5000,])\nsetnames(dt_prototypes, c(\"X\",\"Y\"))\n\n# Paste the label column as factor\ndt_prototypes[, label := as.factor(mnist_10k[1:5000,]$label)]\n\n# Compute the centroids per label\ndt_prototypes[, mean_X := mean(X), by = label]\ndt_prototypes[, mean_Y := mean(Y), by = label]\n\n# Get the unique records per label\ndt_prototypes <- unique(dt_prototypes, by = \"label\")\ndt_prototypes\n\n             X          Y label     mean_X     mean_Y\n 1:  34.057637  16.347760     7  29.468898  13.368839\n 2:  15.930599   3.488559     4  22.629900  -4.840852\n 3:   7.778437  23.683282     1   1.100101  33.723745\n 4:   9.362583 -26.758608     6   3.611878 -27.625343\n 5: -10.993721  -6.656823     5  -5.993067  -8.892714\n 6:  -5.713590   7.008925     8  -8.308147   5.869606\n 7: -33.804912   2.023328     3 -25.422405  -1.426823\n 8: -29.043160  17.139066     2 -19.258460  21.030112\n 9: -19.477592 -28.485313     0 -18.196709 -30.347110\n10:  27.006914   2.946410     9  20.031918  -3.660056"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-similarities-of-digits-1-and-0",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-similarities-of-digits-1-and-0",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Computing similarities of digits 1 and 0",
    "text": "Computing similarities of digits 1 and 0\nOne way to measure the label similarity for each digit is by computing the Euclidean distance in the lower dimensional space obtained from the t-SNE algorithm. You need to use the previously calculated centroids stored in dt_prototypes and compute the Euclidean distance to the centroid of digit 1 for the last 5000 records from tsne and mnist_10k datasets that are labeled either as 1 or 0. Note that the last 5000 records of tsne were not used before. The MNIST data mnist_10k and t-SNE output tsne are available in the workspace. The data.table package has been loaded for you.\n\n# Store the last 5000 records in distances and set column names\ndistances <- as.data.table(tsne$Y[5001:10000,])\nsetnames(distances, c(\"X\", \"Y\"))\n# Paste the true label\ndistances[, label := mnist_10k[5001:10000,]$label]\ndistances[, mean_X := mean(X), by = label]\ndistances[, mean_Y := mean(Y), by = label]\n\n\n# Filter only those labels that are 1 or 0 \ndistances_filtered <- distances[label == 1 | label == 0]\n\n# Compute Euclidean distance to prototype of digit 1\ndistances_filtered[, dist_1 := sqrt( (X - dt_prototypes[label == 1,]$mean_X)^2 + \n                             (Y - dt_prototypes[label == 1,]$mean_Y)^2)]"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#plotting-similarities-of-digits-1-and-0",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#plotting-similarities-of-digits-1-and-0",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Plotting similarities of digits 1 and 0",
    "text": "Plotting similarities of digits 1 and 0\nIn distances, the distances of 1108 records to the centroid of digit 1 are stored in dist_1. Those records correspond to digits you already know are 1’s or 0’s. You can have a look at the basic statistics of the distances from records that you know are 0 and 1 (label column) to the centroid of class 1 using summary(). Also, if you plot a histogram of those distances and fill them with the label you can check if you are doing a good job identifying the two classes with this t-SNE classifier. The data.table and ggplot2 packages, as well as the distances object, have been loaded for you.\n\n# Compute the basic statistics of distances from records of class 1\nsummary(distances_filtered[label == 1]$dist_1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.003   6.542   8.499   8.978  11.493  55.041 \n\n# Compute the basic statistics of distances from records of class 0\nsummary(distances_filtered[label == 0]$dist_1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  49.63   62.94   66.27   66.95   70.98   76.45 \n\n# Plot the histogram of distances of each class\nggplot(distances_filtered, \n       aes(x = dist_1, fill = as.factor(label))) +\n    geom_histogram(binwidth = 5, alpha = .5, \n                   position = \"identity\", show.legend = FALSE) + \n    ggtitle(\"Distribution of Euclidean distance 1 vs 0\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#exploring-credit-card-fraud-dataset",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#exploring-credit-card-fraud-dataset",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Exploring credit card fraud dataset",
    "text": "Exploring credit card fraud dataset\nIn this exercise, you will do some data exploration on a sample of the credit card fraud detection dataset from Kaggle. For any problem, starting with some data exploration is a good practice and helps us better understand the characteristics of the data.\nThe credit card fraud dataset is already loaded in the environment as a data table with the name creditcard. As you saw in the video, it consists of 30 numerical variables. The Class column indicates if the transaction is fraudulent. The ggplot2 package has been loaded for you.\n\nload(\"creditcard.RData\") \n\nsetDT(creditcard)\n# Look at the data dimensions\ndim(creditcard)\n\n[1] 28923    31\n\n# Explore the column names\n#names(creditcard)\n\n# Explore the structure\n#str(creditcard)\n\n# Generate a summary\nsummary(creditcard)\n\n      Time              V1                  V2                  V3          \n Min.   :    26   Min.   :-56.40751   Min.   :-72.71573   Min.   :-31.1037  \n 1st Qu.: 54230   1st Qu.: -0.96058   1st Qu.: -0.58847   1st Qu.: -0.9353  \n Median : 84512   Median : -0.02400   Median :  0.08293   Median :  0.1659  \n Mean   : 94493   Mean   : -0.08501   Mean   :  0.05955   Mean   : -0.1021  \n 3rd Qu.:139052   3rd Qu.:  1.30262   3rd Qu.:  0.84003   3rd Qu.:  1.0119  \n Max.   :172788   Max.   :  2.41150   Max.   : 22.05773   Max.   :  3.8771  \n       V4                  V5                  V6           \n Min.   :-5.071241   Min.   :-31.35675   Min.   :-26.16051  \n 1st Qu.:-0.824978   1st Qu.: -0.70869   1st Qu.: -0.78792  \n Median : 0.007618   Median : -0.06071   Median : -0.28396  \n Mean   : 0.073391   Mean   : -0.04367   Mean   : -0.02722  \n 3rd Qu.: 0.789293   3rd Qu.:  0.61625   3rd Qu.:  0.37911  \n Max.   :16.491217   Max.   : 34.80167   Max.   : 20.37952  \n       V7                  V8                  V9           \n Min.   :-43.55724   Min.   :-50.42009   Min.   :-13.43407  \n 1st Qu.: -0.57404   1st Qu.: -0.21025   1st Qu.: -0.66974  \n Median :  0.02951   Median :  0.01960   Median : -0.06343  \n Mean   : -0.08873   Mean   : -0.00589   Mean   : -0.04295  \n 3rd Qu.:  0.57364   3rd Qu.:  0.33457   3rd Qu.:  0.58734  \n Max.   : 29.20587   Max.   : 20.00721   Max.   :  8.95567  \n      V10                 V11                V12                V13           \n Min.   :-24.58826   Min.   :-4.11026   Min.   :-18.6837   Min.   :-3.844974  \n 1st Qu.: -0.54827   1st Qu.:-0.75404   1st Qu.: -0.4365   1st Qu.:-0.661168  \n Median : -0.09843   Median :-0.01036   Median :  0.1223   Median :-0.009685  \n Mean   : -0.08468   Mean   : 0.06093   Mean   : -0.0943   Mean   :-0.002110  \n 3rd Qu.:  0.44762   3rd Qu.: 0.77394   3rd Qu.:  0.6172   3rd Qu.: 0.664794  \n Max.   : 15.33174   Max.   :12.01891   Max.   :  4.8465   Max.   : 4.569009  \n      V14                 V15                 V16                 V17          \n Min.   :-19.21432   Min.   :-4.498945   Min.   :-14.12985   Min.   :-25.1628  \n 1st Qu.: -0.44507   1st Qu.:-0.595272   1st Qu.: -0.48770   1st Qu.: -0.4951  \n Median :  0.04865   Median : 0.045992   Median :  0.05736   Median : -0.0742  \n Mean   : -0.09653   Mean   :-0.007251   Mean   : -0.06186   Mean   : -0.1046  \n 3rd Qu.:  0.48765   3rd Qu.: 0.646584   3rd Qu.:  0.52147   3rd Qu.:  0.3956  \n Max.   :  7.75460   Max.   : 5.784514   Max.   :  5.99826   Max.   :  7.2150  \n      V18                V19                 V20            \n Min.   :-9.49875   Min.   :-4.395283   Min.   :-20.097918  \n 1st Qu.:-0.51916   1st Qu.:-0.462158   1st Qu.: -0.211663  \n Median :-0.01595   Median : 0.010494   Median : -0.059160  \n Mean   :-0.04344   Mean   : 0.009424   Mean   :  0.006943  \n 3rd Qu.: 0.48634   3rd Qu.: 0.471172   3rd Qu.:  0.141272  \n Max.   : 3.88618   Max.   : 5.228342   Max.   : 24.133894  \n      V21                  V22                 V23           \n Min.   :-22.889347   Min.   :-8.887017   Min.   :-36.66600  \n 1st Qu.: -0.230393   1st Qu.:-0.550210   1st Qu.: -0.16093  \n Median : -0.028097   Median :-0.000187   Median : -0.00756  \n Mean   :  0.004995   Mean   :-0.006271   Mean   :  0.00418  \n 3rd Qu.:  0.190465   3rd Qu.: 0.516596   3rd Qu.:  0.15509  \n Max.   : 27.202839   Max.   : 8.361985   Max.   : 13.65946  \n      V24                 V25                 V26           \n Min.   :-2.822684   Min.   :-6.712624   Min.   :-1.658162  \n 1st Qu.:-0.354367   1st Qu.:-0.319410   1st Qu.:-0.328496  \n Median : 0.038722   Median : 0.011815   Median :-0.054131  \n Mean   : 0.000741   Mean   :-0.002847   Mean   :-0.002546  \n 3rd Qu.: 0.440797   3rd Qu.: 0.351797   3rd Qu.: 0.237782  \n Max.   : 3.962197   Max.   : 5.376595   Max.   : 3.119295  \n      V27                 V28               Amount            Class          \n Min.   :-8.358317   Min.   :-8.46461   Min.   :    0.00   Length:28923      \n 1st Qu.:-0.071275   1st Qu.:-0.05424   1st Qu.:    5.49   Class :character  \n Median : 0.002727   Median : 0.01148   Median :   22.19   Mode  :character  \n Mean   :-0.000501   Mean   : 0.00087   Mean   :   87.90                     \n 3rd Qu.: 0.095974   3rd Qu.: 0.08238   3rd Qu.:   78.73                     \n Max.   : 7.994762   Max.   :33.84781   Max.   :11898.09                     \n\n# Plot a histogram of the transaction time\nggplot(creditcard, aes(x = Time)) + \n    geom_histogram()"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#generating-training-and-test-sets",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#generating-training-and-test-sets",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Generating training and test sets",
    "text": "Generating training and test sets\nBefore we can apply the t-SNE algorithm to perform a dimensionality reduction, we need to split the original data into a training and test set. Next, we will perform an under-sampling of the majority class and generate a balanced training set. Generating a balanced dataset is a good practice when we are using tree-based models. In this exercise you already have the creditcard dataset loaded in the environment. The ggplot2 and data.table packages are already loaded.\n\n# Extract positive and negative instances of fraud\ncreditcard_pos <- creditcard[Class == 1]\ncreditcard_neg <- creditcard[Class == 0]\n\n# Fix the seed\nset.seed(1234)\n\n# Create a new negative balanced dataset by undersampling\ncreditcard_neg_bal <- creditcard_neg[sample(1:nrow(creditcard_neg), nrow(creditcard_pos))]\n\n# Generate a balanced train set\ncreditcard_train <- rbind(creditcard_pos, creditcard_neg_bal)"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-original-features",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-original-features",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Training a random forest with original features",
    "text": "Training a random forest with original features\nIn this exercise, we are going to train a random forest model using the original features from the credit card dataset. The goal is to detect new fraud instances in the future and we are doing that by learning the patterns of fraud instances in the balanced training set. Remember that a random forest can be trained with the following piece of code: randomForest(x = features, y = label, ntree = 100) The only pre-processing that has been done to the original features was to scale the Time and Amount variables. You have the balanced training dataset available in the environment as creditcard_train. The randomForest package has been loaded.\n\n# Fix the seed\nset.seed(1234)\nlibrary(randomForest)\n# Separate x and y sets\ntrain_x <- creditcard_train[,-31]\ntrain_y <- creditcard_train$Class %>% as.factor()\n\n# Train a random forests\nrf_model <- randomForest(x = train_x, y = train_y, ntree = 100)\n\n# Plot the error evolution and variable importance\nplot(rf_model, main = \"Error evolution vs number of trees\")\n# Fix the seed\nset.seed(1234)\n\n# Separate x and y sets\ntrain_x <- creditcard_train[,-31]\ntrain_y <- creditcard_train$Class %>%as.factor()\n\n# Train a random forests\nrf_model <- randomForest(x = train_x, y = train_y, ntree = 100)\n\n# Plot the error evolution and variable importance\nplot(rf_model, main = \"Error evolution vs number of trees\")\nlegend(\"topright\", colnames(rf_model$err.rate),col=1:3,cex=0.8,fill=1:3)\n\n\n\nvarImpPlot(rf_model, main = \"Variable importance\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-and-visualising-the-t-sne-embedding",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#computing-and-visualising-the-t-sne-embedding",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Computing and visualising the t-SNE embedding",
    "text": "Computing and visualising the t-SNE embedding\nIn this exercise, we are going to generate a t-SNE embedding using only the balanced training set creditcard_train. The idea is to train a random forest using the two coordinates of the generated embedding instead of the original 30 dimensions. Due to computational restrictions, we are going to compute the embedding of the training data only, but note that in order to generate predictions from the test set we should compute the embedding of the test set together with the train set. Then, we will visualize the obtained embedding highlighting the two classes in order to clarify if we can differentiate between fraud and non-fraud transactions. The creditcard_train data, as well as the Rtsne and ggplot2 packages, have been loaded.\n\n# Set the seed\n#set.seed(1234)\n\n# Generate the t-SNE embedding \ncreditcard_train[, Time := scale(Time)]\nnms <- names(creditcard_train)\npred_nms <- nms[nms != \"Class\"]\nrange01 <- function(x){(x-min(x))/(max(x)-min(x))}\ncreditcard_train[, (pred_nms) := lapply(.SD ,range01), .SDcols = pred_nms]\n\ntsne_output <- Rtsne(as.matrix(creditcard_train[, -31]), check_duplicates = FALSE, PCA = FALSE)\n\n# Generate a data frame to plot the result\ntsne_plot <- data.frame(tsne_x = tsne_output$Y[,1],\n                        tsne_y = tsne_output$Y[,2],\n                        Class = creditcard_train$Class)\n\n# Plot the embedding usign ggplot and the label\nggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = factor(Class))) + \n  ggtitle(\"t-SNE of credit card fraud train set\") + \n  geom_text(aes(label = Class)) + theme(legend.position = \"none\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-embedding-features",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-embedding-features",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Training a random forest with embedding features",
    "text": "Training a random forest with embedding features\nIn this exercise, we are going to train a random forest model using the embedding features from the previous t-SNE embedding. So, in this case, we are going to use a two-dimensional dataset that has been generated from the original input features. In the rest of the chapter, we are going to verify if we have a worse, similar, or better performance for this model in comparison to the random forest trained with the original features. In the environment two objects named train_tsne_x and train_tsne_y that contain the features and the Class variable are available. The randomForest package has been loaded as well.\n\n# Fix the seed\nset.seed(1234)\ntrain_tsne_x <- tsne_output$Y\n# Train a random forest\nrf_model_tsne <- randomForest(x = train_tsne_x, y = train_y, ntree = 100)\n\n# Plot the error evolution\n\nplot(rf_model_tsne)\n\n\n\n# Plot the variable importance\nvarImpPlot(rf_model_tsne)"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#predicting-data-using-original-features",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#predicting-data-using-original-features",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Predicting data using original features",
    "text": "Predicting data using original features\nIn this exercise, we are using the random forest trained with the original features and generate predictions using the test set. These predictions will be plotted to see the distribution and will be evaluated using the ROCR package by considering the area under the curve.\nThe random forest model, named rf_model, and the test set, named creditcard_test, are available in the environment. The randomForest and ROCR packages have been loaded for you\n\n# Predict on the test set using the random forest \ncreditcard_test <- creditcard\npred_rf <- predict(rf_model, creditcard_test, type = \"prob\")\n\n# Plot a probability distibution of the target class\nhist(pred_rf[,2])\n\n\n\nlibrary(ROCR)\n# Compute the area under the curve\npred <-  prediction(pred_rf[,2], creditcard_test$Class)\nperf <- performance(pred, measure = \"auc\") \nperf@y.values\n\n[[1]]\n[1] 0.9995958"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#predicting-data-using-embedding-random-forest",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#predicting-data-using-embedding-random-forest",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Predicting data using embedding random forest",
    "text": "Predicting data using embedding random forest\nNow, we are going to do the same analysis, but instead of using the random forest trained with the original features, we will make predictions using the random forest trained with the t-SNE embedding coordinates. The random forest model is pre-loaded in an object named rf_model_tsne and the t-SNE embedding features from the original test set are stored in the object test_x. Finally, the test set labels are stored in creditcard_test. The randomForest and ROCR packages have been loaded for you.\n\ncreditcard_test[, (pred_nms) := lapply(.SD ,range01), .SDcols = pred_nms]\n\ntsne_output <- Rtsne(as.matrix(creditcard_test[, -31]), check_duplicates = FALSE, PCA = FALSE)\n\ntest_x <- tsne_output$Y\n# Predict on the test set using the random forest generated with t-SNE features\npred_rf <- predict(rf_model_tsne, test_x, type = \"prob\")\n\n# Plot a probability distibution of the target class\nhist(pred_rf[, 2])\n\n\n\n# Compute the area under the curve\npred <- prediction(pred_rf[, 2] , creditcard_test$Class)\nperf <- performance(pred, measure = \"auc\") \nperf@y.values\n\n[[1]]\n[1] 0.3765589"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#exploring-neural-network-layer-output",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#exploring-neural-network-layer-output",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Exploring neural network layer output",
    "text": "Exploring neural network layer output\nIn this exercise, we will have a look at the data that is being generated in a specific layer of a neural network. In particular, this data corresponds to the third layer, composed of 128 neurons, of a neural network trained with the balanced credit card fraud dataset generated before. The goal of the exercise is to perform an exploratory data analysis.\n\n# Observe the dimensions\n#dim(layer_128_train)\n\n# Show the first six records of the last ten columns\n#head(layer_128_train[, 119:128])\n\n# Generate a summary of all columns\n#summary(layer_128_train)"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#using-t-sne-to-visualise-a-neural-network-layer",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#using-t-sne-to-visualise-a-neural-network-layer",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Using t-SNE to visualise a neural network layer",
    "text": "Using t-SNE to visualise a neural network layer\nNow, we would like to visualize the patterns obtained from the neural network model, in particular from the last layer of the neural network. As we mentioned before this last layer has 128 neurons and we have pre-loaded the weights of these neurons in an object named layer_128_train. The goal is to compute a t-SNE embedding using the output of the neurons from this last layer and visualize the embedding colored according to the class. The Rtsne and ggplot2 packages as well as the layer_128_train and creditcard_train have been loaded for you\n\n# Set the seed\nset.seed(1234)\n\n# Generate the t-SNE\n#tsne_output <- Rtsne(as.matrix(layer_128_train), check_duplicates = FALSE, max_iter = 400, perplexity = 50)\n\n# Prepare data.frame\n#tsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], \n#                        Class = creditcard_train$Class)\n\n# Plot the data \n# ggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = Class)) + \n#   geom_point() + \n#   ggtitle(\"Credit card embedding of Last Neural Network Layer\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#using-t-sne-to-visualise-a-neural-network-layer-1",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#using-t-sne-to-visualise-a-neural-network-layer-1",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Using t-SNE to visualise a neural network layer",
    "text": "Using t-SNE to visualise a neural network layer\nNow, we would like to visualize the patterns obtained from the neural network model, in particular from the last layer of the neural network. As we mentioned before this last layer has 128 neurons and we have pre-loaded the weights of these neurons in an object named layer_128_train. The goal is to compute a t-SNE embedding using the output of the neurons from this last layer and visualize the embedding colored according to the class. The Rtsne and ggplot2 packages as well as the layer_128_train and creditcard_train have been loaded for you\n\n# Set the seed\n# set.seed(1234)\n# \n# # Generate the t-SNE\n# tsne_output <- Rtsne(as.matrix(layer_128_train), check_duplicates = FALSE, max_iter = 400, perplexity = 50)\n# \n# # Prepare data.frame\n# tsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], \n#                         Class = creditcard_train$Class)\n# \n# # Plot the data \n# ggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = Class)) + \n#   geom_point() + \n#   ggtitle(\"Credit card embedding of Last Neural Network Layer\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#exploring-fashion-mnist",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#exploring-fashion-mnist",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Exploring fashion MNIST",
    "text": "Exploring fashion MNIST\nThe Fashion MNIST dataset contains grayscale images of 10 clothing categories. The first thing to do when you are analyzing a new dataset is to perform an exploratory data analysis in order to understand the data. A sample of the fashion MNIST dataset fashion_mnist, with only 500 records, is pre-loaded for you.\n\nlibrary(data.table)\n#load(\"fashion_mnist_500.RData\")\nload(\"fashion_mnist.rda\")\nset.seed(100)\n\nind <- sample(1:nrow(fashion_mnist), 1000)\n\nfashion_mnist <- fashion_mnist[ind, ]\n# Show the dimensions\ndim(fashion_mnist)\n\n[1] 1000  785\n\n# Create a summary of the last six columns \nsummary(fashion_mnist[, 780:785])\n\n    pixel779         pixel780         pixel781        pixel782      \n Min.   :  0.00   Min.   :  0.00   Min.   :  0.0   Min.   :  0.000  \n 1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:  0.0   1st Qu.:  0.000  \n Median :  0.00   Median :  0.00   Median :  0.0   Median :  0.000  \n Mean   : 22.16   Mean   : 18.64   Mean   : 10.6   Mean   :  3.781  \n 3rd Qu.:  1.00   3rd Qu.:  0.00   3rd Qu.:  0.0   3rd Qu.:  0.000  \n Max.   :236.00   Max.   :255.00   Max.   :231.0   Max.   :188.000  \n    pixel783          pixel784     \n Min.   :  0.000   Min.   : 0.000  \n 1st Qu.:  0.000   1st Qu.: 0.000  \n Median :  0.000   Median : 0.000  \n Mean   :  0.934   Mean   : 0.043  \n 3rd Qu.:  0.000   3rd Qu.: 0.000  \n Max.   :147.000   Max.   :39.000  \n\n# Table with the class distribution\ntable(fashion_mnist$label)\n\n\n  0   1   2   3   4   5   6   7   8   9 \n100  94 115  90  97  98  97 105 102 102"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#visualizing-fashion-mnist",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#visualizing-fashion-mnist",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Visualizing fashion MNIST",
    "text": "Visualizing fashion MNIST\nIn this exercise, we are going to visualize an example image of the fashion MNIST dataset. Basically, we are going to plot the 28x28 pixels values. To do this we use:\nA custom ggplot theme named plot_theme. A data structure named xy_axis where the pixels values are stored. A character vector named class_names with the names of each class. The fashion_mnist dataset with 500 examples is available in the workspace. The `ggplot2 package is loaded. Note that you can access the definition of the custom theme by typing plot_theme in the console.\n\nlibrary(tidyverse)\nplot_theme <- list(\n    raster = geom_raster(hjust = 0, vjust = 0),\n    gradient_fill = scale_fill_gradient(low = \"white\",\n                                        high = \"black\", guide = FALSE),\n    theme = theme(axis.line = element_blank(),\n                  axis.text = element_blank(),\n                  axis.ticks = element_blank(),\n                  axis.title = element_blank(),\n                  panel.background = element_blank(),\n                  panel.border = element_blank(),\n                  panel.grid.major = element_blank(),\n                  panel.grid.minor = element_blank(),\n                  plot.background = element_blank()))\n\n\nclass_names <-  c(\"T-shirt/top\", \"Trouser\", \"Pullover\", \n                  \"Dress\", \"Coat\", \"Sandal\", \"Shirt\",\n                  \"Sneaker\", \"Bag\", \"Ankle\", \"boot\")\n\n\nxy_axis <- data.frame(x = expand.grid(1:28, 28:1)[,1],\n                      y = expand.grid(1:28, 28:1)[,2])\n\n# Get the data from the last image\nplot_data <- cbind(xy_axis, fill = as.data.frame(t(fashion_mnist[500, -1]))[,1])\n\n# Observe the first records\nhead(plot_data)\n\n  x  y fill\n1 1 28    0\n2 2 28    0\n3 3 28    0\n4 4 28    0\n5 5 28    0\n6 6 28    0\n\n# Plot the image using ggplot()\nggplot(plot_data, aes(x, y, fill = fill)) + \n  ggtitle(class_names[as.integer(fashion_mnist[500, 1])]) + \n  plot_theme"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#reducing-data-with-glrm",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#reducing-data-with-glrm",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Reducing data with GLRM",
    "text": "Reducing data with GLRM\nWe are going to reduce the dimensionality of the fashion MNIST sample data using the GLRM implementation of h2o. In order to do this, in the next steps we are going to: Start a connection to a h2o cluster by invoking the method h2o.init(). Store the fashion_mnist data into the h2o cluster with as.h2o(). Launch a GLRM model with K=2 (rank-2 model) using the h2o.glrm() function. As we have discussed in the video session, it is important to check the convergence of the objective function. Note that here we are also fixing the seed to ensure the same results. The h2o package and fashion_mnist data are pre-loaded in the environment.\n\nlibrary(h2o)\n# Start a connection with the h2o cluster\nh2o.init()\n\n\nH2O is not running yet, starting it now...\n\nNote:  In case of errors look at the following log files:\n    /tmp/RtmpErxcaM/file201c21cffa700/h2o_mburu_started_from_r.out\n    /tmp/RtmpErxcaM/file201c25e7c6a13/h2o_mburu_started_from_r.err\n\n\nStarting H2O JVM and connecting: .. Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         1 seconds 183 milliseconds \n    H2O cluster timezone:       Africa/Nairobi \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.40.0.1 \n    H2O cluster version age:    3 months and 11 days \n    H2O cluster name:           H2O_started_from_R_mburu_cvk380 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   3.84 GB \n    H2O cluster total cores:    8 \n    H2O cluster allowed cores:  8 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.3.0 (2023-04-21) \n\n# Store the data into h2o cluster\nfashion_mnist.hex <- as.h2o(fashion_mnist, \"fashion_mnist.hex\")\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Launch a GLRM model over fashion_mnist data\nmodel_glrm <- h2o.glrm(training_frame = fashion_mnist.hex,\n                       cols = 2:ncol(fashion_mnist), \n                       k = 2,\n                       seed = 123,\n                       max_iterations = 2100)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |======================================================================| 100%\n\n# Plotting the convergence\nplot(model_glrm)"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#improving-model-convergence",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#improving-model-convergence",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Improving model convergence",
    "text": "Improving model convergence\nIn the previous exercise, we didn’t get good convergence values for the GLRM model. Improving convergence values can sometimes be achieved by applying a transformation to the input data. In this exercise, we are going to normalize the input data before we start building the GLRM model. This can be achieved by setting the transform parameter of h2o.glrm() equal to “NORMALIZE”. The h2o package and fashion_mnist dataset are pre-loaded.\n\n# Start a connection with the h2o cluster\n#h2o.init()\n\n# Store the data into h2o cluster\n#fashion_mnist.hex <- as.h2o(fashion_mnist, \"fashion_mnist.hex\")\n\n# Launch a GLRM model with normalized fashion_mnist data  \nmodel_glrm <- h2o.glrm(training_frame = fashion_mnist.hex, \n                       transform = \"NORMALIZE\",\n                       cols = 2:ncol(fashion_mnist), \n                       k = 2, \n                       seed = 123,\n                       max_iterations = 2100)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |======================================================================| 100%\n\n# Plotting the convergence\nplot(model_glrm)"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#visualizing-the-output-of-glrm",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#visualizing-the-output-of-glrm",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Visualizing the output of GLRM",
    "text": "Visualizing the output of GLRM\nA GLRM model generates the X and Y matrixes. In this exercise, we are going to visualize the obtained low-dimensional representation of the input records in the new K-dimensional space. The output of the X matrix from the previous GLRM model has been loaded with the name X_matrix. This matrix has been obtained by calling:\n\nX_matrix <- as.data.table(h2o.getFrame(model_glrm@model$representation_name))\n# Dimension of X_matrix\ndim(X_matrix)\n\n[1] 1000    2\n\n# First records of X_matrix\nhead(X_matrix)\n\n        Arch1       Arch2\n1: -1.4562898  0.28620951\n2:  0.7132191  1.18922464\n3:  0.5600450 -1.29628758\n4:  0.9997013 -0.51894405\n5:  1.3377989 -0.05616662\n6:  1.0687898  0.07447071\n\n# Plot the records in the new two dimensional space\nggplot(as.data.table(X_matrix), aes(x= Arch1, y = Arch2, color =  fashion_mnist$label)) + \n    ggtitle(\"Fashion Mnist GLRM Archetypes\") + \n    geom_text(aes(label =  fashion_mnist$label)) + \n    theme(legend.position=\"none\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#visualizing-the-prototypes",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#visualizing-the-prototypes",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Visualizing the prototypes",
    "text": "Visualizing the prototypes\nNow, we are going to compute the centroids of the coordinates for each of the two archetypes for each label. We did something similar before for the t-SNE embedding. The goal is to have a representation or prototype of each label in this new two-dimensional space.\nThe ggplot2 and data.table packages are pre-loaded, as well as the X_matrix object and the fashion_mnist dataset.\n\n# Store the label of each record and compute the centroids\nX_matrix[, label := as.numeric(fashion_mnist$label)]\nX_matrix[, mean_x := mean(Arch1), by = label]\nX_matrix[, mean_y := mean(Arch2), by = label]\n\n# Get one record per label and create a vector with class names\nX_mean <- unique(X_matrix, by = \"label\")\n\nlabel_names <- c(\"T-shirt/top\", \"Trouser\", \"Pullover\",\n                 \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \n                 \"Sneaker\", \"Bag\", \"Ankle boot\")\n\n# Plot the centroids\nX_mean[, label := factor(label, levels = 0:9, labels = label_names)]\nggplot(X_mean, aes(x = mean_x, y = mean_y, color = label_names)) + \n    ggtitle(\"Fashion Mnist GLRM class centroids\") + \n    geom_text(aes(label = label_names)) +\n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#imputing-missing-data",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#imputing-missing-data",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Imputing missing data",
    "text": "Imputing missing data\nIn this exercise, we will use GLRM to impute missing data. We are going to build a GLRM model from a dataset named fashion_mnist_miss, where 20% of values are missing. The goal is to fill these values by making a prediction using h2o.predict() with the GLRM model. In this exercise an h2o instance is already running, so it is not necessary to call h2o.init(). The h2o package and fashion_mnist_miss have been loaded\n\nfashion_mnist_miss <- h2o.insertMissingValues(fashion_mnist.hex, \n                                              fraction = 0.2, seed = 1234)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Store the input data in h2o\nfashion_mnist_miss.hex <- as.h2o(fashion_mnist_miss, \"fashion_mnist_miss.hex\")\n\n# Build a GLRM model\nmodel_glrm <- h2o.glrm(training_frame = fashion_mnist_miss.hex,\n                       k = 2,\n                       transform = \"NORMALIZE\",\n                       max_iterations = 100)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |======================================================================| 100%\n\n# Impute missing values\nfashion_pred <- predict(model_glrm, fashion_mnist_miss.hex)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Observe the statistics of the first 5 pixels\nsummary(fashion_pred[, 1:5])\n\n reconstr_label       reconstr_pixel2      reconstr_pixel3     \n Min.   :-0.4524689   Min.   :-1.585e-03   Min.   :-4.765e-03  \n 1st Qu.:-0.2072610   1st Qu.:-7.036e-04   1st Qu.:-2.107e-03  \n Median :-0.0042092   Median :-5.921e-05   Median :-1.580e-04  \n Mean   : 0.0001835   Mean   : 1.242e-06   Mean   : 2.147e-06  \n 3rd Qu.: 0.1988427   3rd Qu.: 6.722e-04   3rd Qu.: 2.014e-03  \n Max.   : 0.4843330   Max.   : 2.462e-03   Max.   : 6.151e-03  \n reconstr_pixel4      reconstr_pixel5     \n Min.   :-5.518e-03   Min.   :-3.674e-03  \n 1st Qu.:-2.279e-03   1st Qu.:-2.035e-03  \n Median :-2.118e-04   Median : 1.700e-04  \n Mean   : 4.250e-06   Mean   : 6.488e-07  \n 3rd Qu.: 2.396e-03   3rd Qu.: 1.781e-03  \n Max.   : 8.336e-03   Max.   : 4.301e-03"
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-original-data",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-original-data",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Training a random forest with original data",
    "text": "Training a random forest with original data\nIn this exercise, we are going to train a random forest using the original fashion MNIST dataset with 500 examples. This dataset is preloaded in the environment with the name fashion_mnist. We are going to train a random forest with 20 trees and we will look at the time it takes to compute the model and the out-of-bag error in the 20th tree. The randomForest package is loaded.\n\n# Get the starting timestamp\nlibrary(randomForest)\n\ntime_start <- proc.time()\n\n# Train the random forest\nfashion_mnist[, label := factor(label)]\nrf_model <- randomForest(label~., ntree = 20,\n                         data = fashion_mnist)\n\n# Get the end timestamp\ntime_end <- timetaken(time_start)\n\n# Show the error and the time\nrf_model$err.rate[20]\n\n[1] 0.2512513\n\ntime_end\n\n[1] \"0.700s elapsed (0.699s cpu)\""
  },
  {
    "objectID": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-compressed-data",
    "href": "datacamp/introduction-to-advanced-dimensionality-reduction/introduction_dimensionality_reduction.html#training-a-random-forest-with-compressed-data",
    "title": "Introduction to advanced dimensionality reduction",
    "section": "Training a random forest with compressed data",
    "text": "Training a random forest with compressed data\nNow, we are going to train a random forest using a compressed representation of the previous 500 input records, using only 8 dimensions!\nIn this exercise, you a dataset named train_x that contains the compressed training data and another one named train_y that contains the labels are pre-loaded. We are going to calculate computation time and accuracy, similar to what was done in the previous exercise. Since the dimensionality of this dataset is much smaller, we can train a random forest using 500 trees in less time. The randomForest package is already loaded.\n\nmodel_glrm <- h2o.glrm(training_frame = fashion_mnist.hex, \n                       transform = \"NORMALIZE\",\n                       cols = 2:ncol(fashion_mnist), \n                       k = 8, \n                       seed = 123,\n                       max_iterations = 1000)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |======================================================================| 100%\n\ntrain_x <- as.data.table(h2o.getFrame(model_glrm@model$representation_name))\ntrain_y <- fashion_mnist$label %>% as.factor()\n\n\nlibrary(randomForest)\n# Get the starting timestamp\ntime_start <- proc.time()\n\n# Train the random forest\nrf_model <- randomForest(x = train_x, y = train_y, ntree = 500)\n\n# Get the end timestamp\ntime_end <- timetaken(time_start)\n\n\n# Show the error and the time\nrf_model$err.rate[500]\n\n[1] 0.259\n\ntime_end\n\n[1] \"0.326s elapsed (0.326s cpu)\""
  },
  {
    "objectID": "datacamp/survival_analysis_R/survival_analysis_R.html",
    "href": "datacamp/survival_analysis_R/survival_analysis_R.html",
    "title": "Survival Analysis R",
    "section": "",
    "text": "In this course, we will frequently use the GBSG2 dataset. This dataset contains information on breast cancer patients and their survival. In this exercise, we will take a first look at it in R.\nThe TH.data package is loaded for you in this exercise.\n\n# Check out the help page for this dataset\nhelp(GBSG2, package = \"TH.data\")\n# Load the data\ndata(GBSG2, package = \"TH.data\")\n\n# Look at the summary of the dataset\nsummary(GBSG2)\n\n horTh          age        menostat       tsize        tgrade   \n no :440   Min.   :21.00   Pre :290   Min.   :  3.00   I  : 81  \n yes:246   1st Qu.:46.00   Post:396   1st Qu.: 20.00   II :444  \n           Median :53.00              Median : 25.00   III:161  \n           Mean   :53.05              Mean   : 29.33            \n           3rd Qu.:61.00              3rd Qu.: 35.00            \n           Max.   :80.00              Max.   :120.00            \n     pnodes         progrec           estrec             time       \n Min.   : 1.00   Min.   :   0.0   Min.   :   0.00   Min.   :   8.0  \n 1st Qu.: 1.00   1st Qu.:   7.0   1st Qu.:   8.00   1st Qu.: 567.8  \n Median : 3.00   Median :  32.5   Median :  36.00   Median :1084.0  \n Mean   : 5.01   Mean   : 110.0   Mean   :  96.25   Mean   :1124.5  \n 3rd Qu.: 7.00   3rd Qu.: 131.8   3rd Qu.: 114.00   3rd Qu.:1684.8  \n Max.   :51.00   Max.   :2380.0   Max.   :1144.00   Max.   :2659.0  \n      cens       \n Min.   :0.0000  \n 1st Qu.:0.0000  \n Median :0.0000  \n Mean   :0.4359  \n 3rd Qu.:1.0000  \n Max.   :1.0000  \n\n\n\n\n\nIn the previous exercise, we learned about the GBSG2 dataset. Let’s dig a bit deeper into it to understand the variables we will use in the following.\nThe cens variable contains values that indicate whether or not a person in the study has died. In this exercise, you’ll explore these censored values.\n\n# Count censored and uncensored data\nnum_cens <- table(GBSG2$cens)\nnum_cens\n\n\n  0   1 \n387 299 \n\n# Create barplot of censored and uncensored data\nbarplot(num_cens)\n\n\n\n\n\n\n\nIn the video, we learned about the Surv() function, which generates a Surv object. Let’s look a little deeper into what a Surv object actually is. We will use the GBSG2 data again.\nThe survival package and GBSG2 data are loaded for you in this exercise.\n\n# Create Surv-Object\nsobj <- Surv( GBSG2$time, GBSG2$cens)\n\n# Look at 10 first elements\nsobj[1:10]\n\n [1] 1814  2018   712  1807   772   448  2172+ 2161+  471  2014+\n\n# Look at summary\nsummary(sobj)\n\n      time            status      \n Min.   :   8.0   Min.   :0.0000  \n 1st Qu.: 567.8   1st Qu.:0.0000  \n Median :1084.0   Median :0.0000  \n Mean   :1124.5   Mean   :0.4359  \n 3rd Qu.:1684.8   3rd Qu.:1.0000  \n Max.   :2659.0   Max.   :1.0000  \n\n# Look at structure\nstr(sobj)\n\n 'Surv' num [1:686, 1:2] 1814  2018   712  1807   772   448  2172+ 2161+  471  2014+ ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr [1:2] \"time\" \"status\"\n - attr(*, \"type\")= chr \"right\"\n\n\n\n\n\nThe UnempDur dataset contains information on how long people stay unemployed. In this case, the event (finding a job) is something positive. This information is stored in the censor1 variable, which has a value of 1 if an individual was re-employed at a full-time job. The spell variable indicates the length of time an individual was unemployed in number of two-week intervals.\nIn this exercise, you’ll explore these censored values and create a Surv object, just as you did in the previous exercises with the GBSG2 dataset.\n\n# Load the UnempDur data\ndata(UnempDur, package = \"Ecdat\")\n\n# Count censored and uncensored data\ncens_employ_ft <- table(UnempDur$censor1)\ncens_employ_ft\n\n\n   0    1 \n2270 1073 \n\n# Create barplot of censored and uncensored data\nbarplot(cens_employ_ft)\n\n\n\n# Create Surv-Object\nsobj <- Surv(UnempDur$spell, UnempDur$censor1)\n\n# Look at 10 first elements\nhead(sobj, 10)\n\n [1]  5  13  21   3   9+ 11+  1+  3   7   5+"
  },
  {
    "objectID": "datacamp/survival_analysis_R/survival_analysis_R.html#estimation-of-survival-curves",
    "href": "datacamp/survival_analysis_R/survival_analysis_R.html#estimation-of-survival-curves",
    "title": "Survival Analysis R",
    "section": "Estimation of survival curves",
    "text": "Estimation of survival curves\n\nFirst Kaplan-Meier estimate\nIn this exercise, we will use the same data shown in the video. We will take a look at the survfit() function and the object it generates. This exercise will help you explore the survfit object.\n\nKaplan-Meier estimate Calculation\n\n\n\nKaplan-Meier estimate Calculation\n\n\nThe survival package is loaded for you in this exercise.\n\n# Create time and event data\ntime <- c(5, 6, 2, 4, 4)\nevent <- c(1, 0, 0, 1, 1)\n\n# Compute Kaplan-Meier estimate\nkm <- survfit(Surv(time, event) ~ 1)\nkm\n\nCall: survfit(formula = Surv(time, event) ~ 1)\n\n     n events median 0.95LCL 0.95UCL\n[1,] 5      3    4.5       4      NA\n\n# Take a look at the structure\nstr(km)\n\nList of 16\n $ n        : int 5\n $ time     : num [1:4] 2 4 5 6\n $ n.risk   : num [1:4] 5 4 2 1\n $ n.event  : num [1:4] 0 2 1 0\n $ n.censor : num [1:4] 1 0 0 1\n $ surv     : num [1:4] 1 0.5 0.25 0.25\n $ std.err  : num [1:4] 0 0.5 0.866 0.866\n $ cumhaz   : num [1:4] 0 0.5 1 1\n $ std.chaz : num [1:4] 0 0.354 0.612 0.612\n $ type     : chr \"right\"\n $ logse    : logi TRUE\n $ conf.int : num 0.95\n $ conf.type: chr \"log\"\n $ lower    : num [1:4] 1 0.1877 0.0458 0.0458\n $ upper    : num [1:4] 1 1 1 1\n $ call     : language survfit(formula = Surv(time, event) ~ 1)\n - attr(*, \"class\")= chr \"survfit\"\n\n# Create data.frame\ndata.frame(time = km$time, n.risk = km$n.risk, n.event = km$n.event,\n           n.censor = km$n.censor, surv = km$surv) %>%\n    data_table()\n\n\n\n\n\n\n\n\n\nExercise ignoring censoring\nYou throw a party and at 1 a.m. guests suddenly start dancing. You are curious to analyze how long your guests will dance for and start collecting data. The problem is that you get tired and go to bed after a while.\nYou obtain the following right censored dancing times data given in dancedat:\nname is the name of your friend. time is the right-censored dancing time. obs_end indicates if you observed the end of your friends dance (1) or if you went to sleep before they stopped dancing (0). You start analyzing the data in the morning, but you are tired and, at first, ignore the fact that you have censored observations. Then you remember this course on DataCamp and do it correctly.\nThe survival package is loaded for you in this exercise.\n\n# Create dancedat data\ndancedat <- data.frame(\n    name = c(\"Chris\", \"Martin\", \"Conny\", \"Desi\", \"Reni\", \"Phil\", \n             \"Flo\", \"Andrea\", \"Isaac\", \"Dayra\", \"Caspar\"),\n    time = c(20, 2, 14, 22, 3, 7, 4, 15, 25, 17, 12),\n    obs_end = c(1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0))\n\n# Estimate the survivor function pretending that all censored observations are actual observations.\nkm_wrong <- survfit(Surv(time) ~ 1, data = dancedat)\n\n# Estimate the survivor function from this dataset via kaplan-meier.\nkm <- survfit(Surv(time, obs_end) ~ 1, data = dancedat)\n\n# Plot the two and compare\nggsurvplot_combine(list(correct = km, \n                        wrong = km_wrong))\n\n\n\n\n\n\nEstimating and visualizing a survival curve\nLet’s take a look at the survival of breast cancer patients.\nIn this exercise, we work with the GBSG2 dataset again.\nThe survival and survminer packages and the GBSG2 data are loaded for you in this exercise.\n\n# Kaplan-Meier estimate\nkm <- survfit(Surv(time, cens)~1, data = GBSG2)\n\n# plot of the Kaplan-Meier estimate\nggsurvplot(km)\n\n\n\n# add the risk table to plot\nggsurvplot(km, risk.table = TRUE)\n\n\n\n# add a line showing the median survival time\nggsurvplot(km, risk.table = TRUE, surv.median.line = \"hv\")\n\n\n\n\n\n\nEstimating median survival from a Weibull model\nWe can now estimate the survival of the breast cancer patients in the GBSG2 data using a Weibull model (function survreg()). Remember, the Weibull model estimates a smooth survival function instead of a step function, which is what the Kaplan-Meier method estimates.\nThe predict() function with type = “quantile” allows us to compute the quantiles of the distribution function. We will use this to compute the median survival.\nThe survival package and the GBSG2 data are loaded for you in this exercise.\n\nEstimate a Weibull model for the breast cancer patients.\nCompute the median survival from this model using the predict() function with type = “quantile”\n\n\n# Weibull model\nwb <- survreg(Surv(time, cens) ~ 1, data = GBSG2)\n\n# Compute the median survival from the model\npredict(wb, type = \"quantile\", p = 0.5, newdata = data.frame(1))\n\n      1 \n1693.93 \n\n\nSurvival curve quantiles from a Weibull model We can now estimate the survival of the breast cancer patients in the GBSG2 data using a Weibull model.\nThe predict() function with type = “quantile” allows us to compute the quantiles of the distribution function. As we learned in this course so far, the survival function is 1 - the distribution function ( S = 1 -D), so we can easily compute the quantiles of the survival function using the predict() function.\nThe survival package and GBSG2 data are loaded for you in this exercise.\n\nEstimate a Weibull model for the breast cancer patients.\nGet the time point at which the probability of surviving longer than that time point is 70 Percent (using the predict() function with type = “quantile”).\n\n\n# Weibull model\nwb <- survreg(Surv(time, cens) ~ 1, data = GBSG2)\n\n# 70 Percent of patients survive beyond time point...\npredict(wb, type = \"quantile\", p = 1-.7, newdata = data.frame(1))\n\n       1 \n1004.524 \n\n\n\n\nEstimating the survival curve with survreg()\nWe can now estimate the survival of the breast cancer patients in the GBSG2 data using a Weibull model.\nThe Weibull distribution has two parameters, which determine the form of the survival curve.\nThe survival package and the GBSG2 data are loaded for you in this exercise.\n\nEstimate a Weibull model for the breast cancer patients.\nCompute the estimated survival curve from the model using the predict() function with type = “quantile”.\nCreate a data.frame with the time points and corresponding survival probabilities.\nLook at the first few lines of the result using the function head().\n\n\n# Weibull model\nwb <- survreg(Surv(time, cens) ~ 1, GBSG2)\n\n# Retrieve survival curve from model probabilities \nsurv <- seq(.99, .01, by = -.01)\n\n# Get time for each probability\nt <- predict(wb, type = \"quantile\", p = 1 - surv, newdata = data.frame(1))\n\n# Create data frame with the information\nsurv_wb <- data.frame(time = t, surv = surv)\n\n# Look at first few lines of the result\nhead(surv_wb ) %>%\n    data_table()\n\n\n\n\n\n\n\n\nComparing Weibull model and Kaplan-Meier estimate\nLet’s plot the survival curve we get from the Weibull model for the GBSG2 data!\nThe survival and survminer packages and the GBSG2 data are loaded for you in this exercise.\n\nCompute the Weibull model for the GBSG2 data.\nCompute the survival curve from the model.\nPlot the survival curves you get from the two estimates.\n\n\n# Weibull model\nwb <- survreg(Surv(time, cens) ~ 1, GBSG2)\n\n# Retrieve survival curve from model\nsurv <- seq(.99, .01, by = -.01)\n\n# Get time for each probability\nt <- predict(wb, type = \"quantile\", p = 1-surv, newdata = data.frame(1))\n\n# Create data frame with the information needed for ggsurvplot_df\nsurv_wb <- data.frame(time = t, \n                      surv = surv, \n                      upper = NA, \n                      lower = NA, \n                      std.err = NA)\n\n# Plot\nggsurvplot_df(fit = surv_wb, \n              surv.geom = geom_line)"
  },
  {
    "objectID": "datacamp/survival_analysis_R/survival_analysis_R.html#the-weibull-model",
    "href": "datacamp/survival_analysis_R/survival_analysis_R.html#the-weibull-model",
    "title": "Survival Analysis R",
    "section": "The Weibull model",
    "text": "The Weibull model\n\nInterpreting coefficients\nWe have a dataset of lung cancer patients. In this exercise, we want to know if the sex of the patients is associated with their survival time.\nThe survival package and the dataset are already loaded for you.\n\nUse the survreg() function to estimate a Weibull model.\nWomen survive longer from\n\n\n# Look at the data set\nload(\"dat.rda\")\nstr(dat)\n\n'data.frame':   228 obs. of  3 variables:\n $ time  : num  306 455 1010 210 883 ...\n $ status: num  2 2 1 2 2 1 2 2 2 2 ...\n $ sex   : Factor w/ 2 levels \"male\",\"female\": 1 1 1 1 1 1 2 2 1 1 ...\n\n# Estimate a Weibull model\nwbmod <- survreg(Surv(time, status) ~ sex, data = dat)\nbroom::tidy(wbmod) %>%\n    data_table()\n\n\n\n\n\n\n\n\nCompute Weibull model\nFor a Weibull model with covariates, we can compute the survival curve just as we did for the Weibull model without covariates. The only thing we need to do is specify the covariate values for a given survival curve in the predict() function. This can be done with the argument newdata.\n\nCompute a Weibull model for the GBSG2 dataset with covariate horTh to analyze the effect of hormonal therapy on the survival of patients.\nCompute the survival curve for patients who receive hormonal therapy.\nTake a look at the survival curve with str().\n\n\n# Weibull model\nwbmod <- survreg(Surv(time, cens) ~ horTh, data = GBSG2)\n\n# Retrieve survival curve from model\nsurv <- seq(.99, .01, by = -.01)\nt_yes <- predict(wbmod, type = \"quantile\", p = 1 - surv,\n                 newdata = data.frame(horTh = \"yes\"))\n\n# Take a look at survival curve\nstr(t_yes)\n\n num [1:99] 76.4 131.4 180.9 227.2 271.4 ...\n\n\nComputing a Weibull model and the survival curves In this exercise we will reproduce the example from the video using the following steps:\n\nCompute Weibull model\nDecide on “imaginary patients”\nCompute survival curves\nCreate data.frame with survival curve information\nPlot In this exercise, we will focus on the first three steps. The next exercise will cover the remaining steps.\n\nThe survival, survminer, and reshape2 packages and the GBSG2 data are loaded for you in this exercise.\n\nCompute the Weibull model for the GBSG2 data with covariates horTh and tsize.\nDecide on “imaginary patients”: the two levels of horTh and the 25%, 50%, and 75% quantiles of tsize.\n\n\n# Weibull model\nwbmod <- survreg(Surv(time, cens) ~horTh  + tsize, data = GBSG2)\n\n# Imaginary patients\nnewdat <- expand.grid(\n    horTh = levels(GBSG2$horTh),\n    tsize = quantile(GBSG2$tsize, probs = c(.25, 0.5, 0.75)))\ndata_table(newdat)\n\n\n\n\n\n# Compute survival curves\nsurv <- seq(.99, .01, by = -.01)\nt <- predict(wbmod, type = \"quantile\", p = 1 - surv,\n             newdata = newdat)\n\n# How many rows and columns does t have?\ndim(t)\n\n[1]  6 99\n\n\n\n\nVisualising a Weibull model\nIn this exercise we will reproduce the example from the video following the steps:\n\n# Use cbind() to combine the information in newdat with t\nsurv_wbmod_wide <- cbind(newdat, t)\n\n# Use melt() to bring the data.frame to long format\nlibrary(reshape2)\nsurv_wbmod <- melt(surv_wbmod_wide, \n                   id.vars = c(\"horTh\", \"tsize\"), \n                   variable.name = \"surv_id\", \n                   value.name = \"time\")\n\n# Use surv_wbmod$surv_id to add the correct survival probabilities surv\nsurv_wbmod$surv <- surv[as.numeric(surv_wbmod$surv_id)]\n\n# Add columns upper, lower, std.err, and strata to the data.frame\nsurv_wbmod[, c(\"upper\", \"lower\", \"std.err\", \"strata\")] <- NA\n\n# Plot the survival curves\nggsurvplot_df(surv_wbmod, surv.geom = geom_line,\n              linetype = \"horTh\", color = \"tsize\", legend.title = NULL)\n\n\n\n\n\n\nComputing a Weibull and a log-normal model\nIn this exercise, we want to compute a Weibull model and a log-normal model for the GBSG2 data. You will see that the process of computing the survival curve is the same. In the upcoming exercise, we will compare the results from the two models and see the differences.\nThe survival, survminer, and reshape2 packages and the GBSG2 data are loaded for you in this exercise.\n\nCompute the Weibull model\nCompute the log-normal model\nThe Weibull distribution is the the default for survreg(). To compute a log-normal model you need to adjust the distribution argument.\nDefine the new dataset as a data.frame with the two levels of hormonal Therapy.\nUse predict() to compute the survival functions.\n\n\n# Weibull model\nwbmod <- survreg(Surv(time, cens) ~ horTh, \n                 data = GBSG2)\n\n# Log-Normal model\nlnmod <- survreg(Surv(time, cens) ~ horTh, \n                 data = GBSG2, dist= \"lognormal\")\n\n# Newdata\nnewdat <- data.frame(horTh = levels(GBSG2$horTh))\n\n# Surv\nsurv <- seq(.99, .01, by = -.01)\n\n# Survival curve from Weibull model and log-normal model\nwbt <- predict(wbmod, type= \"quantile\", p = 1- surv, newdata = newdat)\nlnt <- predict(lnmod, type = \"quantile\", p = 1 - surv, newdata = newdat)\nsurv_wide <- cbind(newdat, wbt) %>%\n    mutate(dist = \"weibull\") %>%\n    bind_rows(\n        cbind(newdat, lnt) %>%\n            mutate(dist = \"lognormal\")  \n    )\n\n\n\nComparing Weibull and Log-Normal Model I\nIn this exercise, we want to add the correct survival probabilities to a data frame. This data frame will be used to plot the survival curves. surv_wide is a wide data frame containing hormonal therapy information and the survival curves for the Weibull and log-normal models.\n\n# Melt the data.frame into long format.\nsurv_long <- melt(surv_wide, id.vars = c(\"horTh\", \"dist\"), \n                  variable = \"surv_id\", value.name = \"time\")\n\n# Add column for the survival probabilities\nsurv_long$surv <- surv[as.numeric(surv_long$surv_id)]\n\n# Add columns upper, lower, std.err, and strata contianing NA values\nsurv_long[, c(\"upper\", \"lower\", \"std.err\", \"strata\")] <- NA\n\n\nComparing Weibull and Log-Normal Model II\nIn this exercise, we want to compare the survival curves estimated by a Weibull model and by a log-normal model for the GBSG2 data. This exercise shows how the estimates change if you use a different distribution.\nThe survival, survminer, and reshape2 packages and the GBSG2 data are loaded for you in this exercise.\n\n# Plot the survival curves\nggsurvplot_df(surv_long, surv.geom = geom_line,\n              linetype =\"horTh\" , color = \"dist\", legend.title = NULL)"
  },
  {
    "objectID": "datacamp/survival_analysis_R/survival_analysis_R.html#the-cox-model",
    "href": "datacamp/survival_analysis_R/survival_analysis_R.html#the-cox-model",
    "title": "Survival Analysis R",
    "section": "The Cox model",
    "text": "The Cox model\n\nComputing a Cox model\nWe have a dataset of lung cancer patients. We want to know if their performance score (variable performance) is associated with their survival time. The performance score measures how well a patient can perform usual daily activities (bad=0, good=100).\nThe survival package and the dat dataset are already loaded for you.\n\nCompute a Cox model for the lung cancer data dat using coxph() to estimate the effect of the performance score (variable performance) on the survival of lung cancer patients.\nUse the coef() function to show the model coefficient.\n\n\n# Compute Cox model\nload(\"dat_cox.rda\")\ncxmod <- coxph(Surv(time, status) ~ performance, data = dat_cox)\n\n# Show model coefficient\nlibrary(broom)\ntidy(cxmod) %>% data_table()\n\n\n\n\n\n\n\n\nComputing the survival curve from a Cox model\nIn this exercise, we will reproduce the example from the video following the steps:\nCompute Cox model Decide on “imaginary patients” Compute survival curves Create data.frame with survival curve information Plot We will focus now on the first three steps in this exercise and do the next two steps in the upcoming exercise.\nThe survival and survminer packages and the GBSG2 data are loaded for you in this exercise.\n\nCompute the Cox model for the GBSG2 data with covariates horTh and tsize.\nDecide on “imaginary patients”: the two levels of horTh and the 25%, 50%, and 75% quantile.\nAdd rownames letters “a” - “f” to the imaginary patients data.frame.\n\n\n# Cox model\ncxmod <- coxph(Surv(time, cens) ~ horTh + tsize, data = GBSG2)\n\n# Imaginary patients\nnewdat <- expand.grid(\n  horTh = levels(GBSG2$horTh),\n  tsize = quantile(GBSG2$tsize, probs = c(0.25, 0.5, 0.75)))\n\nrownames(newdat) <- letters[1:6]\n\n# Compute survival curves\ncxsf <- survfit(cxmod, \n                data = GBSG2, \n                newdata = newdat, \n                conf.type = \"none\")\n\n# Look at first 6 rows of cxsf$surv and time points\nhead(cxsf$surv)\n\n     a b c d e f\n[1,] 1 1 1 1 1 1\n[2,] 1 1 1 1 1 1\n[3,] 1 1 1 1 1 1\n[4,] 1 1 1 1 1 1\n[5,] 1 1 1 1 1 1\n[6,] 1 1 1 1 1 1\n\nhead(cxsf$time)\n\n[1]  8 15 16 17 18 29\n\n\n\n\nVisualizing Cox\n\n# Compute data.frame needed for plotting\nsurv_cxmod0 <- surv_summary(cxsf)\n\n# Look at the first few lines\nhead(surv_cxmod0) %>% data_table()\n\n\n\n\n\n# Get a character vector of patient letters (patient IDs)\npid <- as.character(surv_cxmod0$strata)\n\n# Multiple of the rows in newdat so that it fits with surv_cxmod0\nm_newdat <- newdat[pid, ]\n\n# Add patient info to data.frame\nsurv_cxmod <- cbind(surv_cxmod0, m_newdat)\nhead(surv_cxmod) %>% data_table()\n\n\n\n\n\n\n\nPlot\n\n# Plot\nggsurvplot_df(surv_cxmod, linetype = \"horTh\", color = \"tsize\",\n  legend.title = NULL, censor = FALSE)\n\n\n\n\n\n\n\nCapstone: The Cox model\nTo conclude the course, let’s take a look back at the lung cancer dataset we utilized briefly in these last 2 chapters. To recap, this dataset contains information on the survival of patients with advanced lung cancer from the North Central Cancer Treatment Group. The event is stored in the status variable, which has a value of 2 if an individual did not survive. The performance score (variable performance) measures how well a patient can perform usual daily activities (bad=0, good=100), rated by a physician. We want to know the association between specific performance scores and survival time.\n\n# Compute Cox model and survival curves\nlung <-  dat_cox\ncxmod <- coxph(Surv(time, status) ~ performance, data = lung)\nnew_lung <- data.frame(performance = c(60, 70, 80, 90))\ncxsf <- survfit(cxmod, data = lung, newdata = new_lung, conf.type = \"none\")\n\n# Use the summary of cxsf to take a vector of patient IDs\nsurv_cxmod0 <- surv_summary(cxsf)\npid <- as.character(surv_cxmod0$strata)\n\n# Duplicate rows in newdat to fit with surv_cxmod0 and add them in\nm_newdat <- new_lung[pid, , drop = FALSE]\nsurv_cxmod <- cbind(surv_cxmod0, m_newdat)\n\n# Plot\nggsurvplot_df(surv_cxmod, \n              color = \"performance\", \n              legend.title = NULL,\n              censor = FALSE)\n\n\n\n\n\n\nCapstone: Comparing survival curves\nWe saw from the last exercise that performance scores do have an effect on the survival probability. Now, let’s take a look at the survival curve of all individuals using the Kaplan-Meier estimate and compare it to the curve of a Cox model that takes performance into account. Note that for Cox models, you can just enter the survfit() output into ggsurvplot() instead of creating the needed data frame yourself and plugging it into ggsurvplot_df().\n\n# Compute Kaplan-Meier curve\nkm <- survfit(Surv(time, status) ~ 1, data = lung)\n\n# Compute Cox model\ncxmod <- coxph(Surv(time, status) ~ performance, \n               data = lung)\n\n# Compute Cox model survival curves\nnew_lung <- data.frame(performance = c(60, 70, 80, 90))\n\ncxsf <- survfit(cxmod, data = lung,\n                newdata = new_lung, \n                conf.type = \"none\")\n\n# Plot Kaplan-Meier curve\nggsurvplot(km, conf.int = FALSE)\n\n\n\n# Plot Cox model survival curves\nggsurvplot(cxsf, censor = FALSE)"
  },
  {
    "objectID": "notes/iml/intro.html",
    "href": "notes/iml/intro.html",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "",
    "text": "Code is from here\nMachine learning models usually perform really well for predictions, but are not interpretable. The iml package provides tools for analysing any black box machine learning model:\nThis document shows you how to use the iml package to analyse machine learning models.\nIf you want to learn more about the technical details of all the methods, read chapters from: https://christophm.github.io/interpretable-ml-book/agnostic.html"
  },
  {
    "objectID": "notes/iml/intro.html#data-boston-housing",
    "href": "notes/iml/intro.html#data-boston-housing",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Data: Boston Housing",
    "text": "Data: Boston Housing\nWe’ll use the MASS::Boston dataset to demonstrate the abilities of the iml package. This dataset contains median house values from Boston neighbourhoods.\n\ndata(\"Boston\", package = \"MASS\")\nhead(Boston)\n\n#>      crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n#> 1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n#> 2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n#> 3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n#> 4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n#> 5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n#> 6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n#>   medv\n#> 1 24.0\n#> 2 21.6\n#> 3 34.7\n#> 4 33.4\n#> 5 36.2\n#> 6 28.7"
  },
  {
    "objectID": "notes/iml/intro.html#fitting-the-machine-learning-model",
    "href": "notes/iml/intro.html#fitting-the-machine-learning-model",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Fitting the machine learning model",
    "text": "Fitting the machine learning model\nFirst we train a randomForest to predict the Boston median housing value:\n\nset.seed(42)\nlibrary(\"iml\")\nlibrary(\"randomForest\")\ndata(\"Boston\", package = \"MASS\")\nrf <- randomForest(medv ~ ., data = Boston, ntree = 50)"
  },
  {
    "objectID": "notes/iml/intro.html#using-the-iml-predictor-container",
    "href": "notes/iml/intro.html#using-the-iml-predictor-container",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Using the iml Predictor() container",
    "text": "Using the iml Predictor() container\nWe create a Predictor object, that holds the model and the data. The iml package uses R6 classes: New objects can be created by calling Predictor$new().\n\nX <- Boston[which(names(Boston) != \"medv\")]\npredictor <- Predictor$new(rf, data = X, y = Boston$medv)"
  },
  {
    "objectID": "notes/iml/intro.html#feature-importance",
    "href": "notes/iml/intro.html#feature-importance",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Feature importance",
    "text": "Feature importance\nWe can measure how important each feature was for the predictions with FeatureImp. The feature importance measure works by shuffling each feature and measuring how much the performance drops. For this regression task we choose to measure the loss in performance with the mean absolute error (‘mae’), another choice would be the mean squared error (‘mse’).\nOnce we create a new object of FeatureImp, the importance is automatically computed. We can call the plot() function of the object or look at the results in a data.frame.\n\nimp <- FeatureImp$new(predictor, loss = \"mae\")\nlibrary(\"ggplot2\")\nplot(imp)\n\n\n\n\n\n\n\nimp$results\n\n#>    feature importance.05 importance importance.95 permutation.error\n#> 1    lstat      4.394480   4.459282      4.740041          4.394662\n#> 2       rm      3.349928   3.620192      3.715445          3.567732\n#> 3      nox      1.772047   1.796058      1.833039          1.770032\n#> 4     crim      1.661024   1.703701      1.738660          1.679013\n#> 5      dis      1.670726   1.690742      1.694655          1.666242\n#> 6  ptratio      1.423169   1.426636      1.443533          1.405963\n#> 7    indus      1.399432   1.416183      1.434735          1.395661\n#> 8      age      1.346887   1.387961      1.423804          1.367848\n#> 9      tax      1.352382   1.376506      1.384641          1.356559\n#> 10   black      1.227135   1.234735      1.235719          1.216842\n#> 11     rad      1.097253   1.109357      1.131675          1.093281\n#> 12      zn      1.035018   1.042360      1.043935          1.027256\n#> 13    chas      1.032211   1.035745      1.047224          1.020736"
  },
  {
    "objectID": "notes/iml/intro.html#feature-effects",
    "href": "notes/iml/intro.html#feature-effects",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Feature effects",
    "text": "Feature effects\nBesides knowing which features were important, we are interested in how the features influence the predicted outcome. The FeatureEffect class implements accumulated local effect plots, partial dependence plots and individual conditional expectation curves. The following plot shows the accumulated local effects (ALE) for the feature ‘lstat’. ALE shows how the prediction changes locally, when the feature is varied. The marks on the x-axis indicates the distribution of the ‘lstat’ feature, showing how relevant a region is for interpretation (little or no points mean that we should not over-interpret this region).\n\nale <- FeatureEffect$new(predictor, feature = \"lstat\")\nale$plot()\n\n\n\n\n\n\n\n\nIf we want to compute the partial dependence curves on another feature, we can simply reset the feature:\n\nale$set.feature(\"rm\")\nale$plot()"
  },
  {
    "objectID": "notes/iml/intro.html#measure-interactions",
    "href": "notes/iml/intro.html#measure-interactions",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Measure interactions",
    "text": "Measure interactions\nWe can also measure how strongly features interact with each other. The interaction measure regards how much of the variance of \\(f(x)\\) is explained by the interaction. The measure is between 0 (no interaction) and 1 (= 100% of variance of \\(f(x)\\) due to interactions). For each feature, we measure how much they interact with any other feature:\n\ninteract <- Interaction$new(predictor)\n\n#> \n#> Attaching package: 'withr'\n\n\n#> The following objects are masked from 'package:rlang':\n#> \n#>     local_options, with_options\n\n\n#> The following object is masked from 'package:tools':\n#> \n#>     makevars_user\n\nplot(interact)\n\n\n\n\n\n\n\n\nWe can also specify a feature and measure all it’s 2-way interactions with all other features:\n\ninteract <- Interaction$new(predictor, feature = \"crim\")\nplot(interact)\n\n\n\n\n\n\n\n\nYou can also plot the feature effects for all features at once:\n\neffs <- FeatureEffects$new(predictor)\nplot(effs)"
  },
  {
    "objectID": "notes/iml/intro.html#surrogate-model",
    "href": "notes/iml/intro.html#surrogate-model",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Surrogate model",
    "text": "Surrogate model\nAnother way to make the models more interpretable is to replace the black box with a simpler model - a decision tree. We take the predictions of the black box model (in our case the random forest) and train a decision tree on the original features and the predicted outcome. The plot shows the terminal nodes of the fitted tree. The maxdepth parameter controls how deep the tree can grow and therefore how interpretable it is.\n\ntree <- TreeSurrogate$new(predictor, maxdepth = 2)\n\n#> Loading required package: partykit\n\n\n#> Loading required package: libcoin\n\n\n#> Loading required package: mvtnorm\n\nplot(tree)\n\n\n\n\n\n\n\n\nWe can use the tree to make predictions:\n\nhead(tree$predict(Boston))\n\n#> Warning in self$predictor$data$match_cols(data.frame(newdata)): Dropping\n#> additional columns: medv\n\n\n#>     .y.hat\n#> 1 27.09989\n#> 2 27.09989\n#> 3 27.09989\n#> 4 27.09989\n#> 5 27.09989\n#> 6 27.09989"
  },
  {
    "objectID": "notes/iml/intro.html#explain-single-predictions-with-a-local-model",
    "href": "notes/iml/intro.html#explain-single-predictions-with-a-local-model",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Explain single predictions with a local model",
    "text": "Explain single predictions with a local model\nGlobal surrogate model can improve the understanding of the global model behaviour. We can also fit a model locally to understand an individual prediction better. The local model fitted by LocalModel is a linear regression model and the data points are weighted by how close they are to the data point for wich we want to explain the prediction.\n\nlime.explain <- LocalModel$new(predictor, x.interest = X[1, ])\n\n#> Loading required package: glmnet\n\n\n#> Loading required package: Matrix\n\n\n#> Loaded glmnet 4.1-6\n\n\n#> Loading required package: gower\n\nlime.explain$results\n\n#>               beta x.recoded    effect x.original feature feature.value\n#> rm       4.4836483     6.575 29.479987      6.575      rm      rm=6.575\n#> ptratio -0.5244767    15.300 -8.024493       15.3 ptratio  ptratio=15.3\n#> lstat   -0.4348698     4.980 -2.165652       4.98   lstat    lstat=4.98\n\nplot(lime.explain)\n\n\n\n\n\n\n\n\n\nlime.explain$explain(X[2, ])\nplot(lime.explain)"
  },
  {
    "objectID": "notes/iml/intro.html#explain-single-predictions-with-game-theory",
    "href": "notes/iml/intro.html#explain-single-predictions-with-game-theory",
    "title": "Introduction to iml: Interpretable Machine Learning in R",
    "section": "Explain single predictions with game theory",
    "text": "Explain single predictions with game theory\nAn alternative for explaining individual predictions is a method from coalitional game theory named Shapley value. Assume that for one data point, the feature values play a game together, in which they get the prediction as a payout. The Shapley value tells us how to fairly distribute the payout among the feature values.\n\nshapley <- Shapley$new(predictor, x.interest = X[1, ])\nshapley$plot()\n\n\n\n\n\n\n\n\nWe can reuse the object to explain other data points:\n\nshapley$explain(x.interest = X[2, ])\nshapley$plot()\n\n\n\n\n\n\n\n\nThe results in data.frame form can be extracted like this:\n\nresults <- shapley$results\nhead(results)\n\n#>   feature          phi    phi.var feature.value\n#> 1    crim -0.030772464 0.88726982  crim=0.02731\n#> 2      zn -0.009163333 0.01266404          zn=0\n#> 3   indus -0.412309000 0.65608174    indus=7.07\n#> 4    chas -0.065604772 0.04865024        chas=0\n#> 5     nox  0.067133048 0.37635155     nox=0.469\n#> 6      rm -0.354769984 7.26420349      rm=6.421"
  },
  {
    "objectID": "notes/hdx_data/hdx_data.html",
    "href": "notes/hdx_data/hdx_data.html",
    "title": "World Bank kenya data",
    "section": "",
    "text": "library(rhdx)\nlibrary(tidyverse)\nlibrary(data.table)\nset_rhdx_config(hdx_site = \"prod\")\nkenya_growth <- search_datasets(\"Kenya - Economy and Growth\", rows = 1) %>%\n    nth(1) %>%\n  get_resource(1) %>%\n  read_resource(filename = \"world-bank-economy-and-growth-indicators-for-kenya.csv\", hxl = TRUE)"
  },
  {
    "objectID": "notes/busara_task/busara data analysis.html",
    "href": "notes/busara_task/busara data analysis.html",
    "title": "Exploratory Data Analysis for Busara task",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\nlibrary(data.table)\nlibrary(knitr)\nlibrary(ggthemes)\nlibrary(broom)\n#library(data_tableExtra)\n\n\nTask 1\n\nUnderstanding the demographics of company xyz.\nAt least half are youth average age = 33.5\nAt least half earn 5557\n\n\nxyz <- setDT(read_csv(\"XYZ.csv\"))\n\nxyz_sub <- xyz[, .(Gender, Age, Income)]\n\nxyz_subm <- melt(xyz_sub, id.vars = \"Gender\")\n\n\n\nSummary Statistics Age and Income\n\nxyz_subm %>% group_by(variable) %>%\n    summarise(Average = mean(value), Median = median(value),\n              Min = min(value), Max = max(value)) %>%\n    \n    data_table() #%>% data_table_styling() %>%\n\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\n% gender Gap\n\nMale/Female % al most equal\nThere are 5.2% more men than women\n\n\ngender <- xyz %>% group_by(Gender) %>%\n    summarise(freq = n()) %>%\n    mutate(Perc = round(freq/sum(freq) * 100, 2))\n\ngender %>% data_table() #%>% data_table_styling() %>%\n\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\n% gender Gap\n\ngender[, -2] %>% spread(Gender, Perc) %>% \n    mutate(Percentage_Gender_Gap = Male - Female) %>% \n    data_table() #%>% data_table_styling() %>%\n\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n#Single Ladies Nyeri\n\n12(2.4% of the company employees) single ladies from Nyeri county\n\n\nsingle_nyeri <- xyz[Gender == \"Female\" & Marital_Status == \"Single\" & County == \"Nyeri\",]\nnrow(xyz)\n\n[1] 500\n\ncat(\"The Number of single ladies in Nyeri is \", nrow(single_nyeri))\n\nThe Number of single ladies in Nyeri is  12\n\n\n\n\nSummary Statistics Single Ladies Nyeri\n\nAverage age 36 and medium income is about $50\n\n\nsingle_nyeri %>%\n    summarise(Average_Age = mean(Age), Median_Income = median(Income)) %>%\n    data_table() #%>%  data_table_styling() %>%\n\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nNumber of Juniors\n\n28 juniors\n\n\njuniors_26 <-xyz[!grepl(\"Operartions|Data\",Department)  & xyz$Age < 26 & grepl(\"Junior\", Role),]\n\ncat(\"The Number of juniors \", nrow(juniors_26))\n\nThe Number of juniors  28\n\njuniors_26 %>% group_by(Department) %>%\n    summarise(freq = n()) %>%\n    mutate(Perc = round(freq/sum(freq) * 100, 2)) %>%\n    data_table() #%>% data_table_styling() %>%\n\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"40%\")\n\n\n\nDifference in mean income between male and female\n\nThe Operations has the biggest difference in mean income\nFemale/Males average earnings in different departments\n\n\nincome_gender <- xyz %>% group_by(Gender, Department) %>%\n    summarise(Average = mean(Income))\n\n\nincome_gender_dcast <- dcast(Department ~ Gender, data = income_gender) \n\nincome_gender_dcast %>% mutate( Difference = Male - Female) %>%\n    data_table()#%>% data_table_styling() %>%\n\n\n\n\n\n    #scroll_box(width = \"100%\", height = \"45%\")\n\n\n\nFunction to plot categorical variables\n\nbar_plot <- function(data, title,...) {\n    #load ggplot2\n    #function takes a data frame\n    #and other arguments that ggplot\n    #function from ggplot2 takes\n    # the other arguments are aesthetic mappings\n    require(ggplot2)\n    ggplot(data) + geom_bar(aes(...))+\n        ggtitle(title)+\n        ggthemes::theme_hc()+\n        ggthemes::scale_fill_hc()+\n        theme(legend.position = \"none\")\n        \n}\n\n\n\nFunction to plot categorical variables test 1\n\nbar_plot(xyz, Department, title = \"Department Distribution\", fill = Department)\n\n\n\n\n\n\nFunction to plot categorical variables test 2\n\nbar_plot(xyz, Gender, title = \"Gender Distibution\", fill = Gender)\n\n\n\n\n\n\nTask 2\nRead Files\n\nRead files using the patterns\n\n\nmy_files <- dir(path = \"Education\",pattern = \"^Chi|^Sch|^Persi|Secon|^Progr|Pri\")\n\nmy_files <- paste0(\"Education/\", my_files)\n\nlibrary(readxl)\n\nlist_files <- list()\n\nfor (i in 1:length(my_files)) {\n    \n    \n    x = read_excel(my_files[i]) \n    id = grep(\"Country Name\", x$`Data Source`)\n    nms <- x[id,]\n    names(x) <- nms %>% as.character()\n    list_files[[i]] <- x[-c(1:id),] \n    cat(\"...\")\n    \n}\n\n......................................................\n\n\n\n\nCombine Files\n\nSince files are stored in a list combine them\n\n\ndf_world <- rbindlist(list_files) %>% setDT()\n\ndf_world_melt <- melt(df_world, id.vars = names(df_world)[1:4])\n\nnms2 <- Hmisc::Cs(Country_Name, Country_Code,   \n          Indicator_Name,   Indicator_Code, Year,   Indicator_value)\n\nnames(df_world_melt) <- nms2\n\ndf_world_melt[,  Year := as.numeric(as.character(df_world_melt$Year))]\n\n\n\nHead output data frame\n\nhead(df_world_melt) %>% data_table() #%>% \n\n\n\n\n\n    #data_table_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nHead output kenya data\n\nkenya_2011 <- df_world_melt[Country_Name == \"Kenya\" & Year >= 2011]\n\nhead(kenya_2011) #%>% data_table() %>%\n\n   Country_Name Country_Code\n1:        Kenya          KEN\n2:        Kenya          KEN\n3:        Kenya          KEN\n4:        Kenya          KEN\n5:        Kenya          KEN\n6:        Kenya          KEN\n                                               Indicator_Name    Indicator_Code\n1:                    Children out of school, primary, female    SE.PRM.UNER.FE\n2: Persistence to last grade of primary, female (% of cohort) SE.PRM.PRSL.FE.ZS\n3:   Persistence to last grade of primary, male (% of cohort) SE.PRM.PRSL.MA.ZS\n4:  Primary completion rate, female (% of relevant age group) SE.PRM.CMPT.FE.ZS\n5:    Primary completion rate, male (% of relevant age group) SE.PRM.CMPT.MA.ZS\n6:                Progression to secondary school, female (%) SE.SEC.PROG.FE.ZS\n   Year Indicator_value\n1: 2011            <NA>\n2: 2011            <NA>\n3: 2011            <NA>\n4: 2011            <NA>\n5: 2011            <NA>\n6: 2011            <NA>\n\n    #data_table_styling() %>%\n    #scroll_box(width = \"100%\", height = \"70%\")\n\n\n\nHead output kenya data and saving files\n\nwrite.csv(head(kenya_2011, 15), file = \"kenya data.csv\", row.names = F)\n\nkenya_2011_na <- kenya_2011[!is.na(kenya_2011$Indicator_value),]\nwrite.csv(head(kenya_2011_na, 15), file = \"kenya data without na.csv\", row.names = F)\n\nhead(kenya_2011_na) %>% data_table() #%>%\n\n\n\n\n\n   # data_table_styling() %>%\n    #scroll_box(width = \"100%\", height = \"70%\")\n\n#Task 3\n\nfigari_sheet1 <- read_excel(\"Figari Bank.xlsx\" ) %>% setDT()\n\nfigari_sheet2 <- read_excel(\"Figari Bank.xlsx\", sheet = 2 ) %>% setDT()\n\nfigari_sheet2[,  Dates := as.Date(Dates, origin = \"1900-01-01\")]\n\nfigari_sheet2[,  year := year(Dates)]\n\n\nfigari_sheet2[,  month := month(Dates)]\nfigari_sheet2[, month := ifelse(nchar(month) == 1 ,paste(0, month), month)]\n\nfigari_sheet2[,  week_day := as.POSIXlt(Dates)$wday+1]\nfigari_sheet2[, week_day := ifelse(nchar(week_day) == 1 ,paste(0, week_day), week_day)]\nfigari_sheet2[,  week_no := week(Dates)]\n\nfigari_sheet2[, week_no := ifelse(nchar(week_no) == 1 ,paste(0, week_no), week_no)]\n\nfigari_sheet2[,  day_month := format(Dates, \"%d\")]\n\nfigari_sheet2_m <- melt(figari_sheet2[, c(3:9), with = F], id.vars = c(\"Amount\", \"Saving Mode\"))\n\n\n\nTask 3 Plots\n\nTime series will enable us too see if there is seasonal/cyclic effects/trend\nweek number after every two weeks, maybe end month\nSmoothing/decoposing often needed to see trend\n\n\nfigari_dat <- figari_sheet2_m %>% group_by(`Saving Mode`,variable, value) %>%\n    summarise(Average = mean(Amount)) \ntitles <- levels(as.factor(figari_dat$`Saving Mode`))\ntitles <- paste(\"Average Savings for\", titles)\nfigari_dat_split <- split(figari_dat, figari_dat$`Saving Mode`)\nplots_figari <- list()\nfor ( i in 1:length(figari_dat_split)) {\n    this = figari_dat_split[[i]]\n    #write.csv(this, file = \"this.csv\", row.names = F)\n   plots_figari[[i]] <- ggplot(this, aes(value, Average)) +\n       facet_wrap(~variable, scales = \"free_x\", ncol = 1)+\n       geom_line(data = this, aes(value, Average, group = 1)) +\n       ggthemes::theme_hc()+\n       labs(x = \"\", y = \"Average amount saved (KES)\", title = titles[i])# +\n       #theme(axis.text.x = element_text(angle = 30, vjust = 1, hjust = 1))\n    \n}\n\n\n\nAverage Amount saved at the Bank\n\nYear no visible trend/ few years\nfirst two months higher savings\nlowest between month 3 to month 8\nSave more from day 2 - day 4\nWeek 1- 3 more savings drops to week 10\nmore save less in around day 10 of the month\ndecreasing trend trend from day 4 -10\n\n\nplots_figari[[2]]\n\n\n\n\n\n\nAverage Amount saved at the Agent\n\nSave more from March to June\nIncreasing trend from week 1 to 23 then decreasing\nSave less towards end of a month\n\n\nplots_figari[[1]]\n\n\n\n\n\n\nAverage Amount saved Mobile money\n\non average\nsave less from month 3 to 6\nsave less from week 9 to 22\n\n\nplots_figari[[3]]\n\n\n\n\n\n\nEnd Month Savings Favourite tool\n\nI’m thinking about the number of times someone saves. Average maybe skewed.\nWomen prefer to save using agent\nIn regions no Nyeri\n\n\nnames(figari_sheet2)[1] = names(figari_sheet1)[1]\n\nfigari_comb <- merge(figari_sheet2, figari_sheet1, by = \"CustomerID\")\n\nend_month <- figari_comb %>% \n    group_by(day_month, `Saving Mode`) %>%\n    summarise(Freq = n()) %>%\n    mutate(perc = round(100 * Freq/sum(Freq), 2)) %>% ungroup()\n#The number of times one deposits\nggplot(end_month, aes(day_month, Freq )) +\n    geom_line(aes(color =`Saving Mode`, group =`Saving Mode` ), size = 1)+\n    theme_hc()+\n    scale_color_hc(name = \"\") +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\nHistogram Deposits\n\nWhat you would expect.\n\n\ndeposits <- figari_sheet2[, .(freq = .N), by = CustomerID] \n\n#approximmately poison\nhist(deposits$freq, col = \"black\",\n     main = \"Deposits\", \n     xlab = \"Deposits\")\n\n\n\n\n\n\nSubset People who have made one deposit\n\nfigari_deposits <- merge(deposits, figari_sheet1, by = \"CustomerID\")\nfigari_deposits_one <- figari_deposits[freq == 1] \n\n\n\nDemographic characteristics of those who have only made one deposit\n\n\nGender\n\nfigari_deposits_one %>% group_by(Gender) %>%\n    summarise(freq= n()) %>%\n    mutate(Perc =round(freq/sum(freq) *100, 2) ) %>%\n    data_table()# %>%\n\n\n\n\n\n    #data_table_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nRegion\n\nfigari_deposits_one %>% group_by(Region) %>%\n    summarise(freq= n()) %>% \n    mutate(Perc =round(freq/sum(freq) *100, 2) ) %>%\n    data_table() #%>%\n\n\n\n\n\n    #data_table_styling() %>%\n    #scroll_box(width = \"100%\", height = \"100%\")\n\n\n\nAge\n\nfigari_deposits_one %>% \n    summarise(Mean= round(mean(Age), 2), Median  = median(Age)) %>%\n    data_table() #%>%\n\n\n\n\n\n    #data_table_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nTask 4\n\n\nProject Motivation\nData has the potential to transform business and drive the creation of business value. It can be used for a range of tasks such visualization relationships between variables to predicting if an event will occur. The later is one of the heavily reaserched areas in recent times. The reason for this is that data has grown exponentially and so does the computing power. Banks and financial institutions used data analytics for a range of value such as fraud detetction customer segment, recruiting, credit scoring and so on.\nIn this study I will use Bogoza data set to build a credit model where an applicat will be avaluated on whether they will default or not.\nHigh accuracy for this model will be required because predicting false positives will eventually cause a business to make a loss and false negatives means that the financial instituion looses business.\n\n\nData Cleaning\nFirst step is data cleaning. This ensures that columns are consistent. For instance the target variable had values such as Y y yes where all of them represent yes.\n\n#some algorithms like xgboost take numeric data\n#you can convert binary vars to 1,0\n# and form dummie variables using library dummies\n#for variables with more than 2 categories\nborogoza <- setDT(read_csv( \"Bagorogoza Loan.csv\"))\n\nborogoza[, Target := ifelse(grepl(\"y|Y\", Target), 1, 0)]\n\nborogoza[, Gender := ifelse(grepl(\"^m$|^male$\", tolower(Gender)), 0, 1)]\n\nborogoza[, Married := ifelse(grepl(\"Yes\",Married), 1, 0)]\n\nborogoza[, Education := ifelse(grepl(\"not\", tolower(Education)), 0, 1)]\n\nborogoza[, Self_Employed := ifelse(grepl(\"Yes\",Self_Employed), 1, 0)]\n\nborogoza[, Property_Area := ifelse(grepl(\"rural\",tolower(Property_Area)), \"Rural\", Property_Area)]\n\nborogoza[, Property_Area := ifelse(grepl(\"semi\",tolower(Property_Area)), \"Semi-urban\", Property_Area)]\n\nborogoza[, Property_Area := ifelse(grepl(\"^urban$\",tolower(Property_Area)), \"Urban\", Property_Area)]\n\n\n\nVariable Selection\n\nWhere we run descripte statistics\n\n\n\nVisualize Categorical variables\nVisualization and summary statistics is an impostant step before fitting any model as this will give you a glimpse of how the variables are associated with target variable. In this case I will use stacked barplot as from them you can see if the prorpotions of defaulters and non defaulters is equal in defferent categories of a variable. From the graphs we can see that the prorpotion of defaulters and non defaulters is defferent for the different credit history categories. This is aslo seen in the prorpety area. From the categorical variables we can therefore conclude that one of the best predictors is credit history.\n\nnumeric_vars <- Hmisc::Cs(ApplicantIncome,CoapplicantIncome, LoanAmount )\n\nnms_bo <- names(borogoza)[-1]\n\ncat_vars <- nms_bo[!nms_bo %in% numeric_vars]\n\n\nborogoza_catm <- melt(borogoza[, cat_vars, with = F], id.vars = \"Target\")\n\nborogoza_catm_perc <-borogoza_catm  %>%  group_by(variable, value, Target) %>%\n    summarise(freq= n()) %>% mutate(perc =round(freq/sum(freq) *100, 2) )\n\nlibrary(ggthemes)\nggplot(borogoza_catm_perc, aes(value, perc, fill = factor(Target) )) +\n    geom_bar(stat = \"identity\") +facet_wrap(~variable, scales = \"free_x\")+\n    scale_fill_economist(name = \"\")+\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\nVisualize numeric variables\nFor the numeric variables boxplot help us visualize which distribution is different from the other. Non overlapping boxplot for defaulters and non defaulters may indicate that the mean/median values in the two groups was significantly different. From this we can see that it’s unlikely that education and self employment affect loan repayment and for this we drop this two variables\n\nborogoza_numm <- melt(borogoza[, c(numeric_vars, \"Target\"), with = F], id.vars = \"Target\")\n\nggplot(borogoza_numm, aes(as.factor(Target), scale(value ))) +\n    geom_boxplot() +facet_wrap(~variable, scales = \"free_y\")\n\n\n\n\n\n\nOne-Hot Encoding for categorical variables with more than 2 levels\nIn this step variables with more than two categories are converted to dummies variables. The first column in each category is dropped as it’s linearly dependent with the second column.\n\nchars <- unlist(lapply(borogoza[, -1, with = F], is.character)) \n\nchars <- nms_bo[chars]\n\nlibrary(dummies)\nborogoza_dummy <- dummy.data.frame(borogoza, names = c(chars, \"Loan_Amount_Term\")) %>%\n    setDT()\n\n\nborogoza_dummy[, Loan_ID := NULL]\nborogoza_dummy[, Loan_Amount_Term36 := NULL]\nborogoza_dummy[, `Property_AreaSemi-urban` := NULL]\nborogoza_dummy[, `Dependents1` := NULL]\n\n\n\nScale variables\nIt’s important to scale your variables since it leads to faster convergence and since some algorithm use distances to find decision boundary this means that variables with big values will have a big influence.\n\nxvars <- names(borogoza_dummy)[!names(borogoza_dummy) %in% \"Target\"]\nborogoza_dummy[, (xvars) := lapply(.SD, function(x) scale(x)), .SDcols = xvars ]\n\n\n\nSplit test and train sets\nThis is important as it helps evaluate your model on data it has never seen. The model will be trained on one set(training set) and tested using test set.\n\nset.seed(200) # for reproducibility\ntrain_sample <- sample(1:nrow(borogoza_dummy), round(0.7*nrow(borogoza_dummy)))\n\ntrain <- borogoza_dummy[train_sample,]\ntest <- borogoza_dummy[-train_sample,]\n\n\n\nFit Logistic Regression\nLogistic regression was fit to predict the probability of someone defaulting. The advantages of logistic regression is interpret table, ie you can see the association between a predictor and response value, it also gives a probability. This is very important when you want to have your own cut off point eg you want to label someone as a defaulter if you the predicted probability is more than 0.7. This increases precision but lowers recall. Using step wise selection the model was used to select the variables that best predict loan default.\n\nfit_glm <- glm(Target ~ Married + CoapplicantIncome + Loan_Amount_Term60 + \n    Loan_Amount_Term180 + Loan_Amount_Term300 + \n     Loan_Amount_Term360 + Credit_History + Property_AreaRural + \n     Property_AreaUrban ,data = train, family = binomial)\n\nborogoza_dummy <- borogoza_dummy[, .(Target,Married , CoapplicantIncome , Loan_Amount_Term60 , \n     Loan_Amount_Term180 , Loan_Amount_Term300 , \n     Loan_Amount_Term360 , Credit_History , Property_AreaRural , \n     Property_AreaUrban)]\n\n\ntrain <- borogoza_dummy[train_sample,]\ntest <- borogoza_dummy[-train_sample,]\nfit_glm %>% broom::tidy() %>% data_table()# %>%\n\n\n\n\n\n    # data_table_styling() %>%\n    # scroll_box(width = \"100%\", height = \"100%\")\n\n#MASS::stepAIC(fit_glm)\n\nThe estimate column shows the log odds. Positive values means that the variable makes it more likely for a person to repay their loan negative values means that the person is less likely to repay.\n\n\nConfusion Matrix Logistic regression\nThe confusion matrix evaluate correctly classified cases. A perfect fit will have all values in the main diagnose while the entries of lower/upper triangular should be zeros. In this case we have 14 cases of false positives and 7 cases of false negatives the accuracy of the model is 0.82 with and f1 score of 0.87. F1 score is a very important evaluation metric where there is unbalanced classes.\n\nlibrary(caret)\npred_glm <- predict(fit_glm,newdata = test)\n\npred_glm <- ifelse(pred_glm>0.7, 1 , 0)\n\ntable(test$Target, pred_glm) %>% data_table()# %>%\n\n\n\n\n\n    #data_table_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nAccuracy Logistic regression\n\nlibrary(broom)\nlibrary(pROC)\ntable(test$Target, pred_glm) %>% \n    confusionMatrix(positive = \"1\") %>% \n    tidy() %>% data_table()# %>%\n\n\n\n\n\n    # data_table_styling() %>%\n    # scroll_box(width = \"100%\", height = \"100%\")\n\n\n\nArea under curve\nThis is important as it will help you know if the suffers from high false negatives or false positives. A value greater than 0.8 is normally desired in this case we achieve 0.74.\n\nroc(as.numeric(test$Target), pred_glm, print.auc=T, print.auc.y=0.5, levels =0:1 ) \n\n\nCall:\nroc.default(response = as.numeric(test$Target), predictor = pred_glm,     levels = 0:1, print.auc = T, print.auc.y = 0.5)\n\nData: pred_glm in 39 controls (as.numeric(test$Target) 0) < 76 cases (as.numeric(test$Target) 1).\nArea under the curve: 0.7367\n\n\n\n\nCross Validation SVM\nNext we fit Support vector machine model. We start by finding the best parameters using cross validation. We use 10 fold this where train set is randomly split into 10 sets. In each cases one of the 1 set is used as a validation/test set while the other 9 are used to train the model.\n\nlibrary(e1071)\ntune.out = tune(svm, as.factor(Target)~., data = train, kernel =\"radial\", \n                type =\"C-classification\",\n                ranges =list (cost=c(0.01, 0.1, 1 ,5 ,  10),\n                              gamma = c(0.01,  0.1, 1 ,5 )))\n\nsummary(tune.out)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost gamma\n    5  0.01\n\n- best performance: 0.1595442 \n\n- Detailed performance results:\n    cost gamma     error dispersion\n1   0.01  0.01 0.2747863 0.06706580\n2   0.10  0.01 0.2747863 0.06706580\n3   1.00  0.01 0.1596866 0.06510749\n4   5.00  0.01 0.1595442 0.06956599\n5  10.00  0.01 0.1595442 0.06956599\n6   0.01  0.10 0.2747863 0.06706580\n7   0.10  0.10 0.1967236 0.07767111\n8   1.00  0.10 0.1670940 0.06780670\n9   5.00  0.10 0.1633903 0.06543082\n10 10.00  0.10 0.1633903 0.06543082\n11  0.01  1.00 0.2747863 0.06706580\n12  0.10  1.00 0.2747863 0.06706580\n13  1.00  1.00 0.2041311 0.06754684\n14  5.00  1.00 0.2004274 0.07183672\n15 10.00  1.00 0.2078348 0.07591537\n16  0.01  5.00 0.2747863 0.06706580\n17  0.10  5.00 0.2747863 0.06706580\n18  1.00  5.00 0.2078348 0.06963224\n19  5.00  5.00 0.2152422 0.07506115\n20 10.00  5.00 0.2189459 0.08024003\n\n\n\n\nConfusion Matrix SVM\n\nfit_svm <- svm(as.factor(Target)~., data = train, cost =5  , gamma = .01, \n               kernel = \"radial\", type =\"C-classification\")\n\n\npred_svm <-predict(fit_svm, newdata = test)\ntable(test$Target, pred_svm) %>% data_table() #%>%\n\n\n\n\n\n   # data_table_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n\n\n\nArea under curve\n\nroc(test$Target, as.numeric(pred_svm), print.auc=T, print.auc.y=0.5, levels =0:1 ) \n\n\nCall:\nroc.default(response = test$Target, predictor = as.numeric(pred_svm),     levels = 0:1, print.auc = T, print.auc.y = 0.5)\n\nData: as.numeric(pred_svm) in 39 controls (test$Target 0) < 76 cases (test$Target 1).\nArea under the curve: 0.692\n\n\n\n\nAccuracy SVM\n\ntable(test$Target, pred_svm) %>% \n    confusionMatrix(positive = \"1\") %>% \n    tidy() %>% data_table() #%>%\n\n\n\n\n\n    #data_table_styling() %>%\n    #scroll_box(width = \"100%\", height = \"100%\")\n\n\n\nValidation Curves\nThe two models almost give equal results based on accuracy, f1 score and area under the curve. In this section we will evaluate the models using learning curves to see if they suffer from high variance or bias. In this case the model suffers from high bias. It’s evident that adding more data won’t solve accuracy problems. In this case additional features would help.\n\nsets <- seq(from = 50, to = nrow(train), by = 50)\nsets[length(sets)] <-nrow(train) \ntrain.err <- c()\ntest.err<- c()\nfor (i in 1:length(sets)) {\n    \n    traini = train[1:sets[i],]\n    fit_svm <- svm(as.factor(Target)~., data = traini, cost =5  , gamma = .01, \n               kernel = \"radial\", type =\"C-classification\")\n\n    \n    pred_train = predict(fit_svm, newdata = traini)\n    train.err[i] =1 -  mean(pred_train == traini$Target)\n    pred_test <- predict(fit_svm, newdata = test)\n    test.err[i] = 1 - mean(test$Target == pred_test)\n    \n    cat(i,\" \")\n    \n}\n\n1  2  3  4  5  \n\nmatplot(sets, cbind(test.err, train.err), pch = 19, col = c(\"red\", \"blue\"),\n        type = \"b\", ylab = \"Error\", xlab = \"Train sample size\", main = \"SVM Training and Validation errors\")\nlegend(\"topright\", legend = c(\"Test\", \"Train\"), pch = 19, col = c(\"red\", \"blue\"))\n\n\n\n\n\n\nDeployment\nOther model like Xgboost which uses boosting and bagging could first be used to see if the model performs better on this data. The problem could after this be intergrated with a loan evaluation software where it can help loan officers decide if the will award a loan."
  },
  {
    "objectID": "datacamp.html",
    "href": "datacamp.html",
    "title": "Personal Blog",
    "section": "",
    "text": "Survival Analysis R\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFoundations of Probability in R\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Statistics with R\n\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunctions in R\n\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2022\n\n\n\n\n\n\n  \n\n\n\n\nHandling Missing Data with Imputations in R\n\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnomaly Detection in R\n\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2022\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling in R\n\n\n\n\n\n\n\n\n\n\n\n\nSep 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to advanced dimensionality reduction\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2022\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCensus data in r with tidycensus\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\nModeling with tidymodels in R\n\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2021\n\n\n\n\n\n\n  \n\n\n\n\nThe reduction in weekly working hours in Europe\n\n\nLooking at the development between 1996 and 2006\n\n\n\n\n\n\n\n\n\nNov 8, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunicating with Data in the Tidyverse\n\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWriting Efficient R Code\n\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Linear and Logistic Regression in R\n\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScalable Data Processing in R\n\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2021\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork analysis in R\n\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nString manipulation with stringr in r\n\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2020\n\n\nMburu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML with tree based models in r\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2020\n\n\nMburu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html",
    "href": "kaggle/heart_disease/heart_disease.html",
    "title": "Heart Disease Explainable ML",
    "section": "",
    "text": "correlation network plot ml In health research we are not only interested with prediction level but explanability of the fitted algorithm what people in health research call risk factors. We can use heart disease data set to figure out how we can utilize some of the mc.\nI have stolen your idea. For someone working in health research your notebook is a gold mine. In health especially cohort studies, clinical trials people are largely still using generalized linear models because at least from them you can get odds/risk/rate ratios, p values, AIC and so on. I have implemented the same with R(see here). I have also searched for other materials online see here. Thank you.\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(caret)\nlibrary(DT)\nlibrary(iml)\nlibrary(patchwork)\nlibrary(gridExtra)\nheart <- fread(\"heart.csv\")\n\nheart %>% head %>%\n    datatable(options = list(scrollX = TRUE))"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#some-cleaning",
    "href": "kaggle/heart_disease/heart_disease.html#some-cleaning",
    "title": "Heart Disease Explainable ML",
    "section": "Some Cleaning",
    "text": "Some Cleaning\nYou can can check the variable levels from here\n\nheart[, sex := factor(sex, \n                      levels = 0:1, \n                      labels = c(\"female\", \"male\"))]\n\n\nheart[, cp := factor(cp,\n                     levels = 0:3,\n                     labels = c(\"angina\", \"atypical angina\",\n                                \"non-anginal pain\", \"asymptomatic\") )]\n\nheart[, fbs := factor(fbs, levels = 0:1,\n                      labels = c(\"fasting blood sugar <= 120 mg/dl)\",\n                                 \"fasting blood sugar > 120 mg/dl)\"))]\n\n\nlnls_restg <- c(\"normal\", \n                 \"having ST-T wave abnormality \\n (T wave inversions and/or \\n ST elevation or depression\n                of >0.05 mV)\", \n                 \"showing probable or definite \\n left ventricular hypertrophy \\n by Estes' criteria\")\n \nheart[, restecg := factor(restecg,\n                          levels = 0:2,\n                          labels = lnls_restg)]\n\n\n\nheart[, exang := factor(exang, \n                        levels = 0:1,\n                        labels = c(\"yes\", \"no\"))]\n\n\n \nheart[, slope := factor(slope,\n                         levels = 0:2,\n                         labels = c(\"upsloping\", \"flat\", \"downsloping\"))]\n \nheart[thal == 0, thal := 1 ]\nheart[, thal := factor(thal,\n                        levels =  1:3,\n                        labels = c(\"normal\", \"fixed defect\", \"reversable defect\"))]\n\nheart[, ca := factor(ca)]\n\nheart[, target := factor(target,\n                         levels = 0:1,\n                         labels = c(\"No_heart_disease\", \"Heart_disease\"))]"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#visualize-categorical-variables",
    "href": "kaggle/heart_disease/heart_disease.html#visualize-categorical-variables",
    "title": "Heart Disease Explainable ML",
    "section": "Visualize categorical variables",
    "text": "Visualize categorical variables\n\nnms <- names(heart)\ncateg_nms <- nms[sapply(heart, is.factor)]\n\n\ncateg_nms <- categ_nms[categ_nms != \"target\"]\nplots_categ <- list()\nfor (i in categ_nms) {\n    \n   plots_categ[[i]] = heart[, .(freq = .N), by = c(i,\"target\")] %>%\n        .[, perc := round(freq/sum(freq) * 100, 2), by = i] %>%\n        ggplot(aes_string(i, \"perc\", fill = \"target\")) +\n        geom_bar(stat = \"identity\", width = 0.5)+\n       geom_text(aes_string(i, \"perc\", label = \"perc\"),\n                 size = 3 , position =  position_stack(vjust = 0.5))+\n       theme(legend.position = \"bottom\")+\n       scale_fill_brewer(palette  = \"Dark2\")\n\n}\n\ngrid.arrange(grobs = plots_categ, ncol = 2)"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#visualize-numeric-variable",
    "href": "kaggle/heart_disease/heart_disease.html#visualize-numeric-variable",
    "title": "Heart Disease Explainable ML",
    "section": "Visualize numeric variable",
    "text": "Visualize numeric variable\n\nnum_nms <- nms[sapply(heart, is.numeric)]\n\n# zero_one <- function(x){\n#   minx = min(x, na.rm = T)\n#   maxx = max(x, na.rm = T)\n#   \n#   z = (x - minx)/(maxx - minx)\n# }\n# \n# heart[, (num_nms) := lapply(.SD, zero_one), .SDcols = num_nms]\n\nnum_nms <- num_nms[num_nms != \"target\"]\n\nfor (i in num_nms) {\n    \n        p = ggplot(heart, aes_string(\"target\", i)) +\n            geom_boxplot()\n   print(p)\n}"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#fit-logistic-regression",
    "href": "kaggle/heart_disease/heart_disease.html#fit-logistic-regression",
    "title": "Heart Disease Explainable ML",
    "section": "Fit logistic regression",
    "text": "Fit logistic regression\n\nglm_heart <- glm(target ~., data = heart,\n                 family = binomial())\n\nlibrary(broom)\n\ntidy(glm_heart) %>% datatable() %>%\n  formatRound(columns = c(\"estimate\", \"std.error\", \"statistic\", \"p.value\"), digits = 4)"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#chisq-test",
    "href": "kaggle/heart_disease/heart_disease.html#chisq-test",
    "title": "Heart Disease Explainable ML",
    "section": "Chisq test",
    "text": "Chisq test\n\ndrop1(glm_heart, test = \"Chisq\") %>% tidy %>%\n    datatable()  %>%\n  formatRound(columns = c(\"df\", \"Deviance\", \"AIC\", \"LRT\", \"p.value\"), digits = 4)"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#ml-model",
    "href": "kaggle/heart_disease/heart_disease.html#ml-model",
    "title": "Heart Disease Explainable ML",
    "section": "ML model",
    "text": "ML model\n\ntrain_ctrl <- trainControl(method = \"cv\",\n                        number = 5,\n                        summaryFunction = twoClassSummary,\n                        classProbs = TRUE,\n                        allowParallel=T,\n                        #index = cv_fold,\n                        verboseIter = FALSE,\n                        returnResamp = \"all\", \n                        savePredictions = \"final\", \n                        search = \"grid\")\n\n\n\n\n\nranger_grid <- expand.grid(splitrule = \"extratrees\",\n                        mtry = c(2, 5, 10),\n                        min.node.size = c(2, 5, 7))\n\n\nheart_randomforest <- train(target~ .,\n      data = heart,\n      trControl = train_ctrl,\n      tuneGrid = ranger_grid,\n      method = \"ranger\")\nheart_randomforest \n\nRandom Forest \n\n303 samples\n 13 predictor\n  2 classes: 'No_heart_disease', 'Heart_disease' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 243, 242, 243, 242, 242 \nResampling results across tuning parameters:\n\n  mtry  min.node.size  ROC        Sens       Spec     \n   2    2              0.9104137  0.8187831  0.8484848\n   2    5              0.9121292  0.7896825  0.8424242\n   2    7              0.9114558  0.8113757  0.8424242\n   5    2              0.9067981  0.8116402  0.8303030\n   5    5              0.9077201  0.8187831  0.8303030\n   5    7              0.9077040  0.8116402  0.8242424\n  10    2              0.9018118  0.8190476  0.8181818\n  10    5              0.9037117  0.8261905  0.8181818\n  10    7              0.9034632  0.8116402  0.8303030\n\nTuning parameter 'splitrule' was held constant at a value of extratrees\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were mtry = 2, splitrule = extratrees\n and min.node.size = 5."
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#var-importance",
    "href": "kaggle/heart_disease/heart_disease.html#var-importance",
    "title": "Heart Disease Explainable ML",
    "section": "Var Importance",
    "text": "Var Importance\n\npred <- function(heart_randomforest, heart)  {\n  results <- predict(heart_randomforest, newdata = heart, type = \"prob\")\n  return(results[[2L]])\n}"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#partial-dependence",
    "href": "kaggle/heart_disease/heart_disease.html#partial-dependence",
    "title": "Heart Disease Explainable ML",
    "section": "Partial Dependence",
    "text": "Partial Dependence\n\nX_pred <- heart[, .SD, .SDcols = !\"target\"] %>%\n  as.data.frame()\nmodel <- Predictor$new(model = heart_randomforest, \n                      data =X_pred,\n                      predict.function = pred,\n                      y = heart$target)\neffect <- FeatureEffects$new(model)"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#section",
    "href": "kaggle/heart_disease/heart_disease.html#section",
    "title": "Heart Disease Explainable ML",
    "section": "",
    "text": "effect$plot(features = c( \"trestbps\"))\n\n\n\n#effect"
  },
  {
    "objectID": "kaggle/heart_disease/heart_disease.html#section-1",
    "href": "kaggle/heart_disease/heart_disease.html#section-1",
    "title": "Heart Disease Explainable ML",
    "section": "",
    "text": "imp <- FeatureImp$new(model, loss =\"ce\" )\n#imp\n\nvar_important <-imp$results %>% data.table()\n\nsetorder(var_important, -importance)\nplot(imp)\n\n\n\n\n\ninteract <- Interaction$new(model, feature = \"thal\")\nplot(interact)\n\n\n\n\n\nlibrary(tictoc)\ntic()\n\nshap_list <- vector(\"list\", nrow(X_pred)) \n\nfor (i in 1:nrow(X_pred)) {\n  shap <- Shapley$new(model,  x.interest = X_pred[i, ], sample.size = 30)\n  shap_import <-shap$results %>% data.table()\n  shap_import <- shap_import[class == \"Heart_disease\"]\n  shap_list[[i]] <- shap_import[, record_id := i]\n\n  }\ntoc()\n\n56.451 sec elapsed\n\nshap_values <- rbindlist(shap_list, fill = T)\n\n\nlibrary(ggforce)\n\nshap_values[, feature := factor(feature, levels = rev(var_important$feature) )]\nminx <- shap_values[, min(phi.var)]\nmaxx <- shap_values[, max(phi.var)]\nggplot(shap_values, aes(feature, phi,  color = phi.var))+\n  #geom_point()+\n    ggbeeswarm::geom_quasirandom(groupOnX = FALSE, varwidth = TRUE, size = 0.9, alpha = 0.25) +\n  geom_hline(yintercept = 0) +\n  scale_color_gradient(low=\"#2187E3\", high=\"#F32858\", \n                       breaks=c(minx,maxx), labels=c(\"Low\",\"High\"), limits=c(minx,maxx))+ \n  theme_bw() + \n    theme(axis.line.y = element_blank(), \n          axis.ticks.y = element_blank(), # remove axis line\n          legend.position=\"bottom\") +\n  coord_flip()"
  },
  {
    "objectID": "kaggle/heart_failure/heart_failure.html",
    "href": "kaggle/heart_failure/heart_failure.html",
    "title": "Heart Failure Prediction",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(xgboost)\nlibrary(knitr)\nlibrary(broom)\nlibrary(caret)\nlibrary(e1071)\nlibrary(kableExtra)\nlibrary(ggthemes)\nlibrary(DT)\nlibrary(here)\n\n\nheart_failure<- fread(\"data/heart_failure_clinical_records_dataset.csv\")\nnms <- names(heart_failure)  \nnms_new <- nms %>% tolower()\nsetnames(heart_failure, nms, nms_new)\n\ndatatable(heart_failure, \n          options = list(scrollX = T) )\n\n\n\n\n\n\n\nheart_failure[, death_char := factor(death_event,\n                                     levels = c(0, 1), \n                                     labels = c(\"Alive\", \"Death\"))]\n\n\ndeaths_tab <- heart_failure[, .(freq = .N),\n       by = death_char] %>% \n    .[, perc := round(100 * freq/sum(freq), 2)] \n\ndeaths_tab[, bar_text := paste0(\"N = \",freq, \", \", perc, \"%\")]\n#datatable(deaths_tab)\n\n\nggplot(deaths_tab, aes(death_char, perc))+\n    geom_bar(stat = \"identity\", width = 0.5)+\n    geom_text(aes(death_char, perc, label = bar_text),\n              position = position_dodge(width = 0.5),\n              vjust = 0.05)\n\n\n\n\n\nfind_factors <- function(x){\n    y = unique(x)\n    len_x = length(y)\n    val = len_x < 4\n    return(val)\n}\nnms_dt <- sapply(heart_failure, find_factors)\nnms_factors_all <- nms_dt[nms_dt == T] %>% names()\nnms_factors <- nms_factors_all[!nms_factors_all %in% c(\"death_event\")]\n\ndt_factors <- heart_failure[, ..nms_factors]\ndatatable(dt_factors[1:10], \n          options = list(scrollX = T) )\n\n\n\n\n\n\n\ndt_factors_m <- melt(dt_factors, \n                     id.vars = \"death_char\",\n                     variable.factor = F)\n\ndatatable(dt_factors_m[1:10], \n          options = list(scrollX = T) )\n\n\n\n\n\n\n\nsummary_factors <- dt_factors_m[, .(freq = .N), \n             by = c( \"variable\", \"death_char\", \"value\")] %>%\n    .[, perc := round(100 * freq/sum(freq), 2),\n      by = c( \"variable\",\"value\")] \n\n\nsummary_factors[, value := as.factor(value)]\nggplot(summary_factors, aes(value, perc, fill = death_char))+\n    geom_bar(stat = \"identity\")+\n    geom_text(aes(value, perc, label = perc),\n              position = position_stack(vjust = .5))+\n    facet_wrap(~variable)\n\n\n\n\n\nnms_numeric <- nms_dt[nms_dt == FALSE] %>% names()\n\nheart_failure[, (nms_numeric) := lapply(.SD, \n                                        function(x)scales::rescale(x) ),\n              .SDcols = nms_numeric]\n\n\nnms_numeric2 <- c(\"death_char\", nms_numeric)\ndt_num <- heart_failure[, ..nms_numeric2]\ndt_num_m <- melt(dt_num, id.vars = \"death_char\")\n\n\nggplot(dt_num_m, aes(death_char, value))+\n  geom_violin()+\n  facet_wrap(~variable)\n\n\n\n\n\nnms <- c(\"death_event\", \"time\")\ncv_fold <- createFolds(heart_failure$death_char, k = 10)\n\nheart_failure[, (nms) := NULL]\nxgb_ctrl <- trainControl(method = \"cv\",\n                        number = 10,\n                        summaryFunction = twoClassSummary,\n                        classProbs = TRUE,\n                        allowParallel=T,\n                        index = cv_fold,\n                        verboseIter = FALSE,\n                        savePredictions = TRUE,\n                        search = \"grid\")\n\nxgb_grid <- expand.grid(nrounds = c(10, 50, 100),\n                        eta = seq(0.06, .2, length.out = 3),\n                        max_depth = c(50, 80),\n                        gamma = c(0,.01, 0.1),\n                        colsample_bytree = c(0.6, 0.7,0.8),\n                        min_child_weight = 1,\n                        subsample =  .7\n                        \n    )\n\nxgb_model <-train(death_char~.,\n                 data=heart_failure,\n                 method=\"xgbTree\",\n                 trControl= xgb_ctrl,\n                 tuneGrid=xgb_grid,\n                 verbose=T,\n                 metric=\"ROC\",\n                 nthread =4\n                     \n    )\n\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:33] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:34] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:35] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:36] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:37] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:38] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:39] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:40] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:41] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:42] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:43] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:44] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:45] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:06:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\nxgb_model\n\neXtreme Gradient Boosting \n\n299 samples\n 11 predictor\n  2 classes: 'Alive', 'Death' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 30, 29, 29, 29, 30, 30, ... \nResampling results across tuning parameters:\n\n  eta   max_depth  gamma  colsample_bytree  nrounds  ROC        Sens     \n  0.06  50         0.00   0.6                10      0.6991674  0.9376359\n  0.06  50         0.00   0.6                50      0.7065753  0.8938450\n  0.06  50         0.00   0.6               100      0.6985187  0.8686963\n  0.06  50         0.00   0.7                10      0.7140390  0.9031886\n  0.06  50         0.00   0.7                50      0.7029280  0.8911337\n  0.06  50         0.00   0.7               100      0.6977720  0.8599291\n  0.06  50         0.00   0.8                10      0.6911803  0.9053444\n  0.06  50         0.00   0.8                50      0.6904661  0.8823695\n  0.06  50         0.00   0.8               100      0.6941211  0.8615775\n  0.06  50         0.01   0.6                10      0.6903192  0.9070017\n  0.06  50         0.01   0.6                50      0.6981350  0.8971326\n  0.06  50         0.01   0.6               100      0.6952965  0.8686903\n  0.06  50         0.01   0.7                10      0.6759658  0.9053924\n  0.06  50         0.01   0.7                50      0.7000085  0.8922506\n  0.06  50         0.01   0.7               100      0.7003534  0.8648652\n  0.06  50         0.01   0.8                10      0.6895849  0.8966132\n  0.06  50         0.01   0.8                50      0.6989295  0.8905723\n  0.06  50         0.01   0.8               100      0.7008458  0.8599231\n  0.06  50         0.10   0.6                10      0.6971920  0.9250315\n  0.06  50         0.10   0.6                50      0.7015474  0.8954963\n  0.06  50         0.10   0.6               100      0.7028462  0.8785384\n  0.06  50         0.10   0.7                10      0.6915385  0.9004384\n  0.06  50         0.10   0.7                50      0.7072150  0.8971236\n  0.06  50         0.10   0.7               100      0.7021999  0.8686933\n  0.06  50         0.10   0.8                10      0.7000491  0.9163094\n  0.06  50         0.10   0.8                50      0.7038118  0.8872996\n  0.06  50         0.10   0.8               100      0.6991175  0.8599201\n  0.06  80         0.00   0.6                10      0.6922588  0.9245241\n  0.06  80         0.00   0.6                50      0.6882695  0.8845703\n  0.06  80         0.00   0.6               100      0.6956193  0.8676365\n  0.06  80         0.00   0.7                10      0.6982782  0.9146400\n  0.06  80         0.00   0.7                50      0.7007888  0.8922206\n  0.06  80         0.00   0.7               100      0.7036573  0.8670540\n  0.06  80         0.00   0.8                10      0.6974558  0.9288867\n  0.06  80         0.00   0.8                50      0.7036682  0.8851048\n  0.06  80         0.00   0.8               100      0.7015020  0.8577433\n  0.06  80         0.01   0.6                10      0.6753779  0.8911638\n  0.06  80         0.01   0.6                50      0.7034268  0.8993335\n  0.06  80         0.01   0.6               100      0.6991734  0.8692098\n  0.06  80         0.01   0.7                10      0.6766924  0.8769531\n  0.06  80         0.01   0.7                50      0.6990327  0.8851168\n  0.06  80         0.01   0.7               100      0.6928515  0.8659701\n  0.06  80         0.01   0.8                10      0.6825694  0.9124692\n  0.06  80         0.01   0.8                50      0.6929638  0.8823695\n  0.06  80         0.01   0.8               100      0.6952230  0.8572119\n  0.06  80         0.10   0.6                10      0.7008830  0.9304720\n  0.06  80         0.10   0.6                50      0.7015833  0.9053354\n  0.06  80         0.10   0.6               100      0.6979281  0.8741518\n  0.06  80         0.10   0.7                10      0.6984532  0.8966162\n  0.06  80         0.10   0.7                50      0.7014838  0.8938420\n  0.06  80         0.10   0.7               100      0.7000350  0.8588392\n  0.06  80         0.10   0.8                10      0.6989098  0.9037080\n  0.06  80         0.10   0.8                50      0.6992521  0.8785534\n  0.06  80         0.10   0.8               100      0.6981307  0.8610401\n  0.13  50         0.00   0.6                10      0.6717370  0.9157479\n  0.13  50         0.00   0.6                50      0.6909021  0.8632048\n  0.13  50         0.00   0.6               100      0.6888321  0.8352969\n  0.13  50         0.00   0.7                10      0.6842663  0.9157239\n  0.13  50         0.00   0.7                50      0.6804251  0.8522669\n  0.13  50         0.00   0.7               100      0.6814364  0.8271062\n  0.13  50         0.00   0.8                10      0.6783430  0.8944364\n  0.13  50         0.00   0.8                50      0.6901888  0.8539122\n  0.13  50         0.00   0.8               100      0.6920261  0.8347715\n  0.13  50         0.01   0.6                10      0.6995521  0.9179337\n  0.13  50         0.01   0.6                50      0.6890757  0.8692308\n  0.13  50         0.01   0.6               100      0.6903451  0.8358734\n  0.13  50         0.01   0.7                10      0.6791652  0.8856692\n  0.13  50         0.01   0.7                50      0.6969954  0.8621209\n  0.13  50         0.01   0.7               100      0.6899256  0.8293010\n  0.13  50         0.01   0.8                10      0.6890758  0.9059028\n  0.13  50         0.01   0.8                50      0.6906272  0.8593827\n  0.13  50         0.01   0.8               100      0.6901043  0.8407554\n  0.13  50         0.10   0.6                10      0.6868214  0.9092025\n  0.13  50         0.10   0.6                50      0.6885170  0.8577373\n  0.13  50         0.10   0.6               100      0.6913488  0.8331081\n  0.13  50         0.10   0.7                10      0.6952042  0.8939110\n  0.13  50         0.10   0.7                50      0.6968606  0.8489882\n  0.13  50         0.10   0.7               100      0.6925225  0.8216297\n  0.13  50         0.10   0.8                10      0.6726708  0.8944364\n  0.13  50         0.10   0.8                50      0.6845301  0.8555606\n  0.13  50         0.10   0.8               100      0.6841057  0.8331472\n  0.13  80         0.00   0.6                10      0.6937412  0.8987720\n  0.13  80         0.00   0.6                50      0.7009886  0.8675974\n  0.13  80         0.00   0.6               100      0.6945929  0.8320363\n  0.13  80         0.00   0.7                10      0.6769830  0.8714646\n  0.13  80         0.00   0.7                50      0.6952223  0.8566565\n  0.13  80         0.00   0.7               100      0.6896150  0.8336756\n  0.13  80         0.00   0.8                10      0.6934445  0.8856632\n  0.13  80         0.00   0.8                50      0.6984557  0.8566505\n  0.13  80         0.00   0.8               100      0.6880624  0.8276797\n  0.13  80         0.01   0.6                10      0.6671728  0.8998769\n  0.13  80         0.01   0.6                50      0.6873090  0.8665135\n  0.13  80         0.01   0.6               100      0.6882712  0.8396956\n  0.13  80         0.01   0.7                10      0.6915172  0.8834775\n  0.13  80         0.01   0.7                50      0.6880404  0.8544737\n  0.13  80         0.01   0.7               100      0.6888640  0.8342401\n  0.13  80         0.01   0.8                10      0.6954172  0.9020777\n  0.13  80         0.01   0.8                50      0.6939822  0.8462649\n  0.13  80         0.01   0.8               100      0.6879673  0.8238215\n  0.13  80         0.10   0.6                10      0.6717149  0.8993064\n  0.13  80         0.10   0.6                50      0.6936011  0.8604846\n  0.13  80         0.10   0.6               100      0.6929754  0.8336576\n  0.13  80         0.10   0.7                10      0.6706358  0.8758062\n  0.13  80         0.10   0.7                50      0.7001394  0.8577494\n  0.13  80         0.10   0.7               100      0.6943577  0.8298175\n  0.13  80         0.10   0.8                10      0.6833212  0.8900108\n  0.13  80         0.10   0.8                50      0.6884718  0.8506125\n  0.13  80         0.10   0.8               100      0.6886013  0.8232721\n  0.20  50         0.00   0.6                10      0.6814029  0.9173873\n  0.20  50         0.00   0.6                50      0.6892818  0.8342191\n  0.20  50         0.00   0.6               100      0.6822590  0.8068606\n  0.20  50         0.00   0.7                10      0.6757506  0.8561280\n  0.20  50         0.00   0.7                50      0.6904492  0.8429802\n  0.20  50         0.00   0.7               100      0.6807745  0.8090524\n  0.20  50         0.00   0.8                10      0.6805655  0.8648682\n  0.20  50         0.00   0.8                50      0.6918282  0.8254669\n  0.20  50         0.00   0.8               100      0.6836651  0.8145199\n  0.20  50         0.01   0.6                10      0.6700121  0.8960668\n  0.20  50         0.01   0.6                50      0.6887541  0.8440311\n  0.20  50         0.01   0.6               100      0.6834611  0.8205068\n  0.20  50         0.01   0.7                10      0.6776024  0.8840119\n  0.20  50         0.01   0.7                50      0.6860949  0.8292800\n  0.20  50         0.01   0.7               100      0.6851903  0.8139675\n  0.20  50         0.01   0.8                10      0.7000549  0.8845944\n  0.20  50         0.01   0.8                50      0.6928333  0.8435177\n  0.20  50         0.01   0.8               100      0.6888423  0.8161593\n  0.20  50         0.10   0.6                10      0.6979041  0.8998619\n  0.20  50         0.10   0.6                50      0.6910097  0.8451330\n  0.20  50         0.10   0.6               100      0.6873215  0.8265448\n  0.20  50         0.10   0.7                10      0.6886938  0.8845283\n  0.20  50         0.10   0.7                50      0.6853145  0.8363958\n  0.20  50         0.10   0.7               100      0.6810830  0.8106888\n  0.20  50         0.10   0.8                10      0.7032968  0.8791179\n  0.20  50         0.10   0.8                50      0.6955192  0.8407945\n  0.20  50         0.10   0.8               100      0.6926511  0.8194469\n  0.20  80         0.00   0.6                10      0.6682048  0.8725755\n  0.20  80         0.00   0.6                50      0.6835662  0.8293160\n  0.20  80         0.00   0.6               100      0.6835065  0.8101393\n  0.20  80         0.00   0.7                10      0.6779035  0.8889149\n  0.20  80         0.00   0.7                50      0.6871511  0.8402480\n  0.20  80         0.00   0.7               100      0.6864257  0.8227196\n  0.20  80         0.00   0.8                10      0.6763919  0.8758452\n  0.20  80         0.00   0.8                50      0.6777788  0.8309164\n  0.20  80         0.00   0.8               100      0.6750242  0.8095929\n  0.20  80         0.01   0.6                10      0.6696939  0.8900318\n  0.20  80         0.01   0.6                50      0.6839999  0.8282021\n  0.20  80         0.01   0.6               100      0.6831399  0.8188795\n  0.20  80         0.01   0.7                10      0.6915076  0.8916862\n  0.20  80         0.01   0.7                50      0.6923763  0.8429712\n  0.20  80         0.01   0.7               100      0.6910804  0.8227346\n  0.20  80         0.01   0.8                10      0.6794582  0.8566895\n  0.20  80         0.01   0.8                50      0.6884565  0.8363958\n  0.20  80         0.01   0.8               100      0.6782361  0.8112292\n  0.20  80         0.10   0.6                10      0.6741507  0.8960638\n  0.20  80         0.10   0.6                50      0.6895128  0.8397136\n  0.20  80         0.10   0.6               100      0.6772756  0.8222062\n  0.20  80         0.10   0.7                10      0.6764553  0.8752657\n  0.20  80         0.10   0.7                50      0.6925546  0.8374917\n  0.20  80         0.10   0.7               100      0.6844138  0.8079655\n  0.20  80         0.10   0.8                10      0.6972115  0.8818171\n  0.20  80         0.10   0.8                50      0.6870099  0.8216237\n  0.20  80         0.10   0.8               100      0.6839784  0.8156308\n  Spec     \n  0.2108260\n  0.3276798\n  0.3590083\n  0.3024058\n  0.3207698\n  0.3786688\n  0.2551056\n  0.3289762\n  0.3728949\n  0.2769580\n  0.3057471\n  0.3531542\n  0.2664261\n  0.3196872\n  0.3601043\n  0.2998797\n  0.3243250\n  0.3763833\n  0.2570703\n  0.3230553\n  0.3439455\n  0.2713713\n  0.3197140\n  0.3729484\n  0.2734162\n  0.3473536\n  0.3786955\n  0.2395883\n  0.3161187\n  0.3531943\n  0.2526464\n  0.3231222\n  0.3636594\n  0.2570970\n  0.3299652\n  0.3751938\n  0.2880914\n  0.3172547\n  0.3554397\n  0.2824379\n  0.3150628\n  0.3521117\n  0.2595429\n  0.3174419\n  0.3682839\n  0.2236568\n  0.2849505\n  0.3450147\n  0.2534483\n  0.3161321\n  0.3716653\n  0.2605453\n  0.3139669\n  0.3683240\n  0.2444801\n  0.3498129\n  0.4030206\n  0.2584202\n  0.3613606\n  0.4076851\n  0.2929698\n  0.3820904\n  0.4064020\n  0.2433841\n  0.3509356\n  0.3960572\n  0.3128442\n  0.3577653\n  0.3971264\n  0.2824779\n  0.3775194\n  0.4005480\n  0.2885191\n  0.3589949\n  0.4053863\n  0.3067629\n  0.3716520\n  0.3994520\n  0.2827586\n  0.3682572\n  0.3972334\n  0.2919006\n  0.3636060\n  0.4064555\n  0.3299385\n  0.3752473\n  0.4018845\n  0.3186447\n  0.3764769\n  0.4065223\n  0.2835472\n  0.3658915\n  0.3936915\n  0.3323443\n  0.3659449\n  0.4065624\n  0.3069099\n  0.3671478\n  0.4064154\n  0.2549185\n  0.3438786\n  0.4017241\n  0.3207565\n  0.3785886\n  0.4226143\n  0.2780005\n  0.3670142\n  0.4121893\n  0.2444667\n  0.4052259\n  0.4319433\n  0.3404972\n  0.3856856\n  0.4168805\n  0.3369286\n  0.4168271\n  0.4192462\n  0.2744721\n  0.3809810\n  0.4133654\n  0.3079925\n  0.4075916\n  0.4296712\n  0.3230152\n  0.4006549\n  0.4214916\n  0.3020850\n  0.3820770\n  0.4191794\n  0.3287490\n  0.3950548\n  0.4238840\n  0.3392809\n  0.3937584\n  0.4273323\n  0.2812751\n  0.4030072\n  0.4308474\n  0.2884924\n  0.4019113\n  0.4239107\n  0.3136862\n  0.3844694\n  0.4169206\n  0.3080994\n  0.3914461\n  0.4191927\n  0.2998663\n  0.3798717\n  0.4227747\n  0.3451216\n  0.3890003\n  0.4155573\n  0.2892542\n  0.3833734\n  0.3983427\n  0.3300321\n  0.4005346\n  0.4284683\n  0.3240577\n  0.4064288\n  0.4203823\n\nTuning parameter 'min_child_weight' was held constant at a value of 1\n\nTuning parameter 'subsample' was held constant at a value of 0.7\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were nrounds = 10, max_depth = 50, eta\n = 0.06, gamma = 0, colsample_bytree = 0.7, min_child_weight = 1 and\n subsample = 0.7."
  },
  {
    "objectID": "kaggle/malaysia_tourist/malaysia_tourist.html",
    "href": "kaggle/malaysia_tourist/malaysia_tourist.html",
    "title": "Malaysian Tourist Sites",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(leaflet)\n\ntourist_site <-fread(\"dataset tempat perlancongan Malaysia.csv\")\n\ntourist_site[, site_label := paste0(Negeri, \", \" ,`Nama Tempat`)]"
  },
  {
    "objectID": "kaggle/malaysia_tourist/malaysia_tourist.html#leaflet-map",
    "href": "kaggle/malaysia_tourist/malaysia_tourist.html#leaflet-map",
    "title": "Malaysian Tourist Sites",
    "section": "Leaflet Map",
    "text": "Leaflet Map\n\nleaflet(tourist_site) %>%\n    addTiles() %>%\n    addMarkers(~Longitude, ~Latitude, label = ~site_label )"
  },
  {
    "objectID": "kaggle/microsoft_malware_prediction/malware_prediction.html",
    "href": "kaggle/microsoft_malware_prediction/malware_prediction.html",
    "title": "Malware prediction",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(ggthemes)\nlibrary(caret)"
  },
  {
    "objectID": "kaggle/microsoft_malware_prediction/malware_prediction.html#section-1",
    "href": "kaggle/microsoft_malware_prediction/malware_prediction.html#section-1",
    "title": "Malware prediction",
    "section": "",
    "text": "set.seed(100)\nsample_sub <- sample(nrow(micro_train), 1000000)\nmicro_train_sub <- micro_train[sample_sub,]\n\nnrow_train <- nrow(micro_train_sub)\n\nsample_train <- sample(nrow_train, as.integer(nrow_train * 0.7))\n\nmalware_train <- micro_train_sub[sample_train]\n\nmalware_test <- micro_train_sub[-sample_train]\n\n\nna_tally <- round(colSums(is.na(micro_train_sub))/nrow(micro_train_sub) * 100, 2)\n\nna_tally <- na_tally[na_tally != 0]\n\nnms_na <- names(na_tally)\n\nna_dt <- data.table(var = nms_na, perc = na_tally)\n\nggplot(na_dt, aes(reorder(var, perc), perc)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip()+\n    theme(axis.text.x = element_text(size = 10))\n\n\nna_over25 <- na_dt[perc>25, var]\n\nmicro_train_sub[, (na_over25) := NULL]\n\nna_under25dt <- na_dt[perc <= 25]\nval_del <- na_dt[between(perc,6, 25), var]\n\nggplot(na_under25dt, aes(reorder(var, perc), perc)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip()+\n    theme(axis.text.y = element_text(size = 10))"
  },
  {
    "objectID": "kaggle/mnist_digits/mnist.html",
    "href": "kaggle/mnist_digits/mnist.html",
    "title": "MNIST Digits",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(keras)\nlibrary(caret)\nlibrary(DT)\nlibrary(caretEnsemble)\nlibrary(tictoc)\n\ntrain_data <- fread(\"data/train.csv\")\n\ntest_data <- fread(\"data/test.csv\")\n\n\n\n\n\nggplot(train_data, aes(x = factor(label))) +\n    geom_bar()\n\n\n\n\n\n#  image coordinates\nxy_axis <- data.frame(x = expand.grid(1:28, 28:1)[,1],\n                      y = expand.grid(1:28, 28:1)[,2])\n\n\n# get 12 images\nset.seed(100)\nsample_10 <- train_data[sample(1:.N, 12), -1] %>% as.matrix()\n\ndatatable(sample_10, \n          options = list(scrollX = TRUE))\n\nsample_10 <- t(sample_10)\n\nplot_data <- cbind(xy_axis, sample_10 )\n\nsetDT(plot_data, keep.rownames = \"pixel\")\n\n# Observe the first records\nhead(plot_data) %>% datatable(options = list(scrollX = TRUE))\n\n\n\n\n\nplot_data_m <- melt(plot_data, id.vars = c(\"pixel\", \"x\", \"y\"))\n\n# Plot the image using ggplot()\nggplot(plot_data_m, aes(x, y, fill = value)) +\n    geom_raster()+\n     facet_wrap(~variable)+\n    scale_fill_gradient(low = \"white\",\n                        high = \"black\", guide = FALSE)+\n    theme(axis.line = element_blank(),\n                  axis.text = element_blank(),\n                  axis.ticks = element_blank(),\n                  axis.title = element_blank(),\n                  panel.background = element_blank(),\n                  panel.border = element_blank(),\n                  panel.grid.major = element_blank(),\n                  panel.grid.minor = element_blank(),\n                  plot.background = element_blank())\n\n\n\n\n\nDecided to have a self test set\n\n\nnmst <- names(train_data)\nnmst <- nmst[nmst != \"label\"]\nminmax <- function(x) {\n  top =  (x - min(x))\n  bottom = (max(x) - min(x))\n  if(bottom == 0){ \n    return(0)\n  }else{\n      return(top/bottom)\n    }\n}\n#train_data[, (nmst) := lapply(.SD,  function(x) x/255), .SDcols = nmst]\ntrain_data[, (nmst) := lapply(.SD,  minmax), .SDcols = nmst]\nset.seed(100)\nN1 = nrow(train_data)\nsample_one <- sample(N1, 5000)\ntrain_data <- train_data[sample_one]\nN = nrow(train_data)\nsample_train <- sample(N, size = round(0.75 *N ))\ntest_own <- train_data[-sample_train]\ntrain_data2 <- train_data[sample_train, ]\ntrain_y <-to_categorical(train_data2$label, 10)\n\ntrain_x <- train_data2[, -1]\n#convert to matrix\ntrain_x <- train_x %>%\n    as.matrix()\n\n#train_x <- train_x/255\n\n\n\n\n\nmodel <- keras_model_sequential() \nmodel %>% \n    layer_dense(units = 784, activation = 'relu', input_shape = 784) %>%\n    layer_dropout(rate = 0.3) %>%\n    layer_dense(units = 784, activation = 'relu') %>%\n    layer_dropout(rate = 0.3) %>%\n    layer_dense(units = 392, activation = 'relu') %>%\n    layer_dropout(rate = 0.2) %>%\n    layer_dense(units = 200, activation = 'tanh') %>%\n    #layer_dropout(rate = 0.) %>%\n    layer_dense(units = 10, activation = 'softmax')\n\n\n\n\n\nmodel %>% compile(\n    loss = 'categorical_crossentropy',\n    optimizer = 'adam',\n    metrics = c('accuracy'))\n\n\n\n\n\nhist <- model %>% fit(train_x, train_y, \n                      epochs = 9, batch_size = 1000,\n                      validation_split = .2)\n\n\nplot(hist,type = \"b\")\n\n\n\n\n\ntest_own_x <- test_own[, -1] %>% as.matrix()\n\ntest_own_pred <- model %>% predict_classes(test_own_x) %>% factor()\n\nconfusionMatrix(data = test_own_pred, reference = factor(test_own$label))\n\n\n\n\ntest_x <- as.matrix(test_data)/255\n\ntest_pred <- model %>% predict_classes(test_x)\n#head(test_pred)\n\ndf_pred1 <- data.frame(ImageId = 1:length(test_pred),\n                       Label = test_pred)\n\nwrite.csv(df_pred1, file = \"sample_submission.csv\", row.names = F)"
  },
  {
    "objectID": "kaggle/mnist_digits/mnist.html#tsne",
    "href": "kaggle/mnist_digits/mnist.html#tsne",
    "title": "MNIST Digits",
    "section": "TSNE",
    "text": "TSNE\n\nlibrary(Rtsne)\n\ntsne_output <- Rtsne(train_x, check_duplicates = FALSE)\n\n# Generate a data frame to plot the result\ntsne_train <- data.table(tsne_x = tsne_output$Y[,1],\n                        tsne_y = tsne_output$Y[,2],\n                        label =  train_data2$label)\n\n# Plot the embedding usign ggplot and the label\nggplot(tsne_train,\n       aes(x = tsne_x, y = tsne_y, color = factor(label))) + \n  ggtitle(\"t-SNE of MNIST data set\") + \n  geom_text(aes(label = label)) +\n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "kaggle/mnist_digits/mnist.html#plot-tsne-group-means",
    "href": "kaggle/mnist_digits/mnist.html#plot-tsne-group-means",
    "title": "MNIST Digits",
    "section": "Plot tsne group means",
    "text": "Plot tsne group means\n\ntsne_mean <- tsne_train[, \n                        .(mean_x = mean(tsne_x), mean_y = mean(tsne_y)),\n                        by = label]\n\n\nggplot(tsne_mean,\n       aes(x = mean_x, y = mean_y, color = factor(label))) + \n  ggtitle(\"t-SNE of MNIST data set group means\") + \n  geom_text(aes(label = label)) +\n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "kaggle/mnist_digits/mnist.html#kmeans-to-see-if-tsne-and-kmeans-agree",
    "href": "kaggle/mnist_digits/mnist.html#kmeans-to-see-if-tsne-and-kmeans-agree",
    "title": "MNIST Digits",
    "section": "Kmeans to see if tsne and kmeans agree",
    "text": "Kmeans to see if tsne and kmeans agree\n\nset.seed(123)\nk_means_mnist <- kmeans(train_x, 10)\n\ntsne_train[, cluster := k_means_mnist$cluster]\nggplot(tsne_train,\n       aes(x = tsne_x, y = tsne_y, color = factor(cluster))) + \n  geom_point()+\n  ggtitle(\"t-SNE of MNIST data set\") + \n    theme(legend.position = \"none\")\n\ntsne_train[, cluster := NULL]\n\n\nlibrary(kernlab)\ncol_sum <- colSums(train_data2[, .SD, .SDcols = !\"label\"])\n\nzero_var_cols <- col_sum[col_sum == 0] %>% names()\ntrain_data2 <- train_data2[, .SD, .SDcols = !zero_var_cols]\ndf_nms <-  data.frame(vars = names(train_data2))\nwrite.csv(df_nms, file = \"df_nms.csv\", row.names = F)\nmnist_matrix <- train_data2[, .SD, .SDcols = !\"label\"] %>% na.omit %>% as.matrix()\n\n\nspec_models <- list()\ntot_withinss <- c()\nfor(i in 1:10){\n    \n    spec_fit <- specc(mnist_matrix, centers=i+1)\n    tot_withinss[i] <-withinss(spec_fit) %>% median()\n    spec_models[[i]] <- spec_fit\n}\n\nplot( tot_withinss)\n\n\nspec_fit_final <- spec_models[[4]]"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html",
    "title": "New york Airbnb",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(DT)\nlibrary(ggthemes)\nlibrary(leaflet)\nlibrary(ggmap)\nlibrary(plotly)\nlibrary(lubridate)"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#first-few-variables",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#first-few-variables",
    "title": "New york Airbnb",
    "section": "First few variables",
    "text": "First few variables\n\nairbnb_nyc <- fread(\"AB_NYC_2019.csv\")\n\nhead(airbnb_nyc) %>% \n    datatable(options = list(scrollX = TRUE))"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#neighbourhood-disitribution",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#neighbourhood-disitribution",
    "title": "New york Airbnb",
    "section": "Neighbourhood Disitribution",
    "text": "Neighbourhood Disitribution\n\np1 <- airbnb_nyc[, .(freq = .N), by =  neighbourhood_group] %>%\n    .[, perc := round(freq/sum(freq) *100, 2)] %>%\n    ggplot(aes(neighbourhood_group, perc))+\n    geom_bar(stat = \"identity\", width = 0.5, fill =\"slategray2\"  ) +\n    geom_text(aes(neighbourhood_group, perc, label = paste0(perc, \"%\")),\n              position = position_dodge(width = 0.5),\n              vjust = 0.07)+\n    theme_fivethirtyeight() \np1"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#popular-room-types-in-neighbourhoods-disitribution",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#popular-room-types-in-neighbourhoods-disitribution",
    "title": "New york Airbnb",
    "section": "Popular room types in neighbourhoods Disitribution",
    "text": "Popular room types in neighbourhoods Disitribution\n\nairbnb_nyc[, .(freq =.N), by = .(neighbourhood_group, room_type)] %>%\n  .[, perc := round(freq/sum(freq) *100, 2), by = neighbourhood_group] %>%\n  ggplot(aes(neighbourhood_group, perc, fill = room_type))+\n  geom_bar(stat = \"identity\", width = 0.5 ) +\n    geom_text(aes(neighbourhood_group, perc, label = paste0(perc, \"%\")),\n              position = position_stack(vjust = 0.5),\n              vjust = 0.07)+\n  scale_fill_viridis_d(name = \"\")+\n    theme_fivethirtyeight()"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#summary-stats-for-price-based-on-location",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#summary-stats-for-price-based-on-location",
    "title": "New york Airbnb",
    "section": "Summary Stats for price based on location",
    "text": "Summary Stats for price based on location\n\nairbnb_nyc[, price := as.double(price)]\n\nsummary_function <- function(by_col){\n    \n    summary_stats <- airbnb_nyc[!is.na(price)&price !=0, \n                            .(Mean = round(mean(price), 2), \n                              Median = median(price),\n                              First_quartile = quantile(price, .25),\n                              Third_quartile = quantile(price, .75),\n                              Min = min(price),\n                              Max = max(price)),\n                            by = by_col]\n    return(summary_stats)\n}\n\n\ndatatable(summary_function(by_col = \"neighbourhood_group\"))"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#summary-stats-price-based-on-room-type",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#summary-stats-price-based-on-room-type",
    "title": "New york Airbnb",
    "section": "Summary Stats price based on room type",
    "text": "Summary Stats price based on room type\n\ndatatable(summary_function(by_col = \"room_type\"))"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#median-price-roomtype-in-different-neighbourhoods",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#median-price-roomtype-in-different-neighbourhoods",
    "title": "New york Airbnb",
    "section": "Median price roomtype in different neighbourhoods",
    "text": "Median price roomtype in different neighbourhoods\n\nairbnb_nyc[, .(Median = median(price)), \n           by = .(neighbourhood_group, room_type)] %>%\n  dcast(neighbourhood_group ~room_type, value.var = \"Median\") %>%\n  datatable()"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#get-map-using-ggmap",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#get-map-using-ggmap",
    "title": "New york Airbnb",
    "section": "Get map using ggmap",
    "text": "Get map using ggmap\n\nnewyork_map <- get_map(c(left = min(airbnb_nyc$longitude) - .0001,\n                         bottom = min(airbnb_nyc$latitude) - .0001,\n                         right = max(airbnb_nyc$longitude) + .0001,\n                         top = max(airbnb_nyc$latitude) + .0001),\n                       maptype = \"watercolor\", source = \"stamen\")"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#mapping-function",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#mapping-function",
    "title": "New york Airbnb",
    "section": "Mapping function",
    "text": "Mapping function\n\nmap_plot <- function(df, color_col, continues_color_col = TRUE){\n  \n  if(continues_color_col) {\n    \n    scale_fill <- scale_color_viridis_c()\n    \n    } else{\n      \n      scale_fill <- scale_color_viridis_d()\n      \n    }\n    \n    \n  ggmap(newyork_map) +\n    geom_point(data =df, \n               aes_string(\"longitude\", \"latitude\",\n                          color = color_col), size = 1)+\n    theme(legend.position = \"bottom\")+\n                           \n    scale_fill\n  \n  \n}"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#newyork-price-map",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#newyork-price-map",
    "title": "New york Airbnb",
    "section": "Newyork price map",
    "text": "Newyork price map\n\nper95 <- airbnb_nyc[, quantile(price, 0.95)]\nmap_plot(df = airbnb_nyc[price <=per95  ], \n         color_col = \"price\")"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#categorise-price-variable",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#categorise-price-variable",
    "title": "New york Airbnb",
    "section": "Categorise price variable",
    "text": "Categorise price variable\n\nbreaks <-  quantile(airbnb_nyc$price, seq(0, 1, by = .1))\n\nairbnb_nyc[, price_factor := cut(price, breaks = breaks,\n                                 include.lowest = TRUE)]\n\nmap_plot(df = airbnb_nyc, \n         color_col = \"price_factor\",\n         continues_color_col = FALSE)"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#reviews-per-month",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#reviews-per-month",
    "title": "New york Airbnb",
    "section": "Reviews per month",
    "text": "Reviews per month\n\nWe can use this as a proxy of host receiving a lot of guests\nwe could use this for instance to check if some neighborhoods are more popular\n\n\nper95_rev <- airbnb_nyc[!is.na(last_review) & !is.na(reviews_per_month),\n                        quantile(reviews_per_month, 0.95)]\n\nmap_plot(df = airbnb_nyc[reviews_per_month < per95_rev ], \n         color_col = \"reviews_per_month\")"
  },
  {
    "objectID": "kaggle/newyork_airbnb/newyork_airbnb.html#dates",
    "href": "kaggle/newyork_airbnb/newyork_airbnb.html#dates",
    "title": "New york Airbnb",
    "section": "Dates",
    "text": "Dates\n\nairbnb_nyc[, last_review := ymd(last_review)]\nairbnb_nyc[, summary(last_review)]\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"2011-03-28\" \"2018-07-08\" \"2019-05-19\" \"2018-10-04\" \"2019-06-23\" \"2019-07-08\" \n        NA's \n     \"10052\" \n\n\n\nWork in progress !"
  },
  {
    "objectID": "kaggle/who_stats/who_stats.html",
    "href": "kaggle/who_stats/who_stats.html",
    "title": "World Health 2020 STATS",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(DT)\nlibrary(ggthemes)\nlibrary(leaflet)\nlibrary(ggmap)\nlibrary(plotly)\nlibrary(lubridate)\nadolescentBirthRate <- fread(\"data/adolescentBirthRate.csv\")\n\n\nold_nms <- names(adolescentBirthRate)\nold_nms\n\n[1] \"Location\"      \"Period\"        \"Indicator\"     \"First Tooltip\"\n\nold_nms <- old_nms %>% tolower()\nold_nms <- gsub(\"\\\\s\", \"_\", old_nms)\nold_nms\n\n[1] \"location\"      \"period\"        \"indicator\"     \"first_tooltip\"\n\nnames(adolescentBirthRate) <- old_nms\n\nhead(adolescentBirthRate, 10) %>% datatable()\n\n\n\n\n\n\n\nea_country <- c(\"Kenya\", \"Uganda\",  \"Tanzania\")\n\nea_data <- adolescentBirthRate[location %in% ea_country ]\n\np = ggplot(ea_data, aes(period, first_tooltip, group = location, color = location) ) +\n    geom_line(sizee = 1)+\n    theme_hc()+\n    labs(title = \"\", x = \"Year\", y = \"%\")+\n    scale_color_viridis_d(name=\"\")\np"
  },
  {
    "objectID": "kenya_population/ea_poverty.html",
    "href": "kenya_population/ea_poverty.html",
    "title": "East Africa Poverty Indicators",
    "section": "",
    "text": "kenya poverty data Uganda poverty data Tanzania poverty data Tanzania poverty data\n\n\n\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sf)\nlibrary(DT)\nlibrary(tmap)\nlibrary(ggthemes)"
  },
  {
    "objectID": "kenya_population/ea_poverty.html#read-data",
    "href": "kenya_population/ea_poverty.html#read-data",
    "title": "East Africa Poverty Indicators",
    "section": "Read data",
    "text": "Read data\n\nRead data and row bind\n\n\npoverty_data <- fread(\"poverty_data/9c15861e-aeec-486a-8e4c-8bd7c9a40275_Data.csv\", na.strings = c(\"NA\", \"..\", \" \"))\n\n\npoverty_data[sample(nrow(poverty_data), 10)] %>% datatable(options = list(scrollX = TRUE))"
  },
  {
    "objectID": "kenya_population/ea_poverty.html#minor-cleaning",
    "href": "kenya_population/ea_poverty.html#minor-cleaning",
    "title": "East Africa Poverty Indicators",
    "section": "Minor Cleaning",
    "text": "Minor Cleaning\n\nConvert to long\nConvert column names into lower\nReplace space with underscore\n\n\nnms_old <- names(poverty_data)[1:4]\npoverty_data <- melt(poverty_data,\n                                id.vars = nms_old, \n                           variable.factor = F,\n                           value.factor = F,\n                           variable.name = \"year\")\npoverty_data[, value:= as.numeric(value)]\n\npoverty_data[, year:= gsub(\"\\\\[.*\", \"\", year)]\npoverty_data[, year := str_trim(year)]\npoverty_data[, year := as.numeric(year)]\npoverty_data <-  poverty_data[!is.na(value)]\n\n\nnms_new <- nms_old %>% tolower() \nnms_new <- gsub(\"\\\\s\", \"_\", nms_new)\nnms_new\n\n[1] \"country_name\" \"country_code\" \"series_name\"  \"series_code\" \n\nsetnames(poverty_data, nms_old, nms_new)\npoverty_data[, value:= as.numeric(value)]\npoverty_data <-  poverty_data[!is.na(value)]\n\npoverty_data[sample(nrow(poverty_data), 10)] %>% datatable(options = list(scrollX = TRUE))"
  },
  {
    "objectID": "kenya_population/ea_poverty.html#plots",
    "href": "kenya_population/ea_poverty.html#plots",
    "title": "East Africa Poverty Indicators",
    "section": "Plots",
    "text": "Plots\n\npoverty_data[, year := as.numeric(year)]\nea_country <- c(\"Kenya\", \"Uganda\", \"Ghana\", \"Tanzania\")\n\npoverty_data[, unique(series_name)]\n\n [1] \"Population, total\"                                                                                                                                          \n [2] \"Gini index (World Bank estimate)\"                                                                                                                           \n [3] \"Income share held by fourth 20%\"                                                                                                                            \n [4] \"Income share held by highest 10%\"                                                                                                                           \n [5] \"Income share held by highest 20%\"                                                                                                                           \n [6] \"Income share held by lowest 10%\"                                                                                                                            \n [7] \"Income share held by lowest 20%\"                                                                                                                            \n [8] \"Income share held by second 20%\"                                                                                                                            \n [9] \"Income share held by third 20%\"                                                                                                                             \n[10] \"Number of poor at $1.90 a day (2011 PPP) (millions)\"                                                                                                        \n[11] \"Number of poor at $3.20 a day (2011 PPP) (millions)\"                                                                                                        \n[12] \"Number of poor at $5.50 a day (2011 PPP) (millions)\"                                                                                                        \n[13] \"Poverty gap at $1.90 a day (2011 PPP) (%)\"                                                                                                                  \n[14] \"Poverty gap at $3.20 a day (2011 PPP) (% of population)\"                                                                                                    \n[15] \"Poverty gap at $5.50 a day (2011 PPP) (% of population)\"                                                                                                    \n[16] \"Poverty headcount ratio at $1.90 a day (2011 PPP) (% of population)\"                                                                                        \n[17] \"Poverty headcount ratio at $3.20 a day (2011 PPP) (% of population)\"                                                                                        \n[18] \"Poverty headcount ratio at $5.50 a day (2011 PPP) (% of population)\"                                                                                        \n[19] \"Annualized growth in per capita real survey mean consumption or income, bottom 40% (%)\"                                                                     \n[20] \"Annualized growth in per capita real survey mean consumption or income, top 10% (%)\"                                                                        \n[21] \"Annualized growth in per capita real survey mean consumption or income, top 60% (%)\"                                                                        \n[22] \"Annualized growth in per capita real survey mean consumption or income, total population (%)\"                                                               \n[23] \"Annualized growth in per capita real survey median income or consumption expenditure (%)\"                                                                   \n[24] \"Median daily per capita income or consumption expenditure (2011 PPP)\"                                                                                       \n[25] \"Multidimensional poverty, Drinking water (% of population deprived)\"                                                                                        \n[26] \"Multidimensional poverty, Educational attainment (% of population deprived)\"                                                                                \n[27] \"Multidimensional poverty, Educational enrollment (% of population deprived)\"                                                                                \n[28] \"Multidimensional poverty, Electricity (% of population deprived)\"                                                                                           \n[29] \"Multidimensional poverty, Headcount ratio (% of population)\"                                                                                                \n[30] \"Multidimensional poverty, Monetary poverty (% of population deprived)\"                                                                                      \n[31] \"Multidimensional poverty, Sanitation (% of population deprived)\"                                                                                            \n[32] \"Poverty gap at national poverty lines (%)\"                                                                                                                  \n[33] \"Poverty gap at national poverty lines (%), including noncomparable values\"                                                                                  \n[34] \"Poverty headcount ratio at $1.90 a day, age 0-14  (2011 PPP) (% of population age 0-14)\"                                                                    \n[35] \"Poverty headcount ratio at $1.90 a day, age 15-64 (2011 PPP) (% of population age 15-64)\"                                                                   \n[36] \"Poverty headcount ratio at $1.90 a day, age 65+ (2011 PPP) (% of population age 65+)\"                                                                       \n[37] \"Poverty headcount ratio at $1.90 a day, Female (2011 PPP) (% of female population)\"                                                                         \n[38] \"Poverty headcount ratio at $1.90 a day, Male  (2011 PPP) (% of male population)\"                                                                            \n[39] \"Poverty headcount ratio at $1.90 a day, rural (2011 PPP) (% of rural population)\"                                                                           \n[40] \"Poverty headcount ratio at $1.90 a day, urban (2011 PPP) (% of urban population)\"                                                                           \n[41] \"Poverty headcount ratio at $1.90 a day, with primary education (2011 PPP) (% of population age 16+ with primary education)\"                                 \n[42] \"Poverty headcount ratio at $1.90 a day, with secondary education (2011 PPP) (% of population age 16+ with secondary education)\"                             \n[43] \"Poverty headcount ratio at $1.90 a day, without education (2011 PPP) (% of population age 16+ without education)\"                                           \n[44] \"Poverty headcount ratio at $1.90 a day,  with Tertiary/post-secondary education (2011 PPP) (% of population age 16+ with Tertiary/post-secondary education)\"\n[45] \"Poverty headcount ratio at national poverty lines (% of population)\"                                                                                        \n[46] \"Poverty headcount ratio at national poverty lines (% of population), including noncomparable values\"                                                        \n[47] \"Rural poverty gap at national poverty lines (%)\"                                                                                                            \n[48] \"Rural poverty gap at national poverty lines (%), including noncomparable values\"                                                                            \n[49] \"Rural poverty headcount ratio at national poverty lines (% of rural population)\"                                                                            \n[50] \"Rural poverty headcount ratio at national poverty lines (% of rural population), including noncomparable values\"                                            \n[51] \"Survey mean consumption or income per capita, bottom 40% (2011 PPP $ per day)\"                                                                              \n[52] \"Survey mean consumption or income per capita, top 10% (2011 PPP $ per day)\"                                                                                 \n[53] \"Survey mean consumption or income per capita, top 60% (2011 PPP $ per day)\"                                                                                 \n[54] \"Survey mean consumption or income per capita, total population (2011 PPP $ per day)\"                                                                        \n[55] \"Urban poverty gap at national poverty lines (%)\"                                                                                                            \n[56] \"Urban poverty gap at national poverty lines (%), including noncomparable values\"                                                                            \n[57] \"Urban poverty headcount ratio at national poverty lines (% of urban population)\"                                                                            \n[58] \"Urban poverty headcount ratio at national poverty lines (% of urban population), including noncomparable values\"                                            \n[59] \"\"                                                                                                                                                           \n\nindicator <- c(\"Poverty gap at $1.90 a day (2011 PPP) (%)\", \n               \"Income share held by highest 10%\",\n               \"Income share held by lowest 10%\", \n               \"Income share held by highest 20%\",\n               \"Income share held by lowest 20%\",\n               \"Multidimensional poverty, Drinking water (% of population deprived)\",\n               \"Multidimensional poverty, Educational attainment (% of population deprived)\",\n               \"Poverty gap at $3.20 a day (2011 PPP) (% of population)\")\n\npoverty_data <- poverty_data[country_name %in% ea_country & series_name %in% indicator]\npoverty_data_split <- split(poverty_data, f = poverty_data$series_name)\n\nn <- length(poverty_data_split)\nmy_plots <-htmltools::tagList()\nfor (i in 1:n) {\n    df = poverty_data_split[[i]]\n    my_title = df[, unique(series_name)]\n    mn = df[, min(year)]\n    mx = df[, max(year)]\n    breaks = seq(mn, mx,by = 3)\n    p = ggplot(df, aes(year, value, group = country_name, color = country_name) ) +\n        geom_line()+\n        theme_hc()+\n        labs(title = my_title, x = \"year\", y = \"%\")+\n      scale_color_viridis_d(name=\"\")+\n      scale_x_continuous(breaks = breaks)\n    \n    my_plots[[i]] = ggplotly(p)\n    \n}\n\nmy_plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neconomy_and_growth <- fread(\"API_3_DS2_en_csv_v2_1743760.csv\") \n\nnms_old <- economy_and_growth[1,]  %>% as.character()\nnms_old\n\n [1] \"Country Name\"   \"Country Code\"   \"Indicator Name\" \"Indicator Code\"\n [5] \"1960\"           \"1961\"           \"1962\"           \"1963\"          \n [9] \"1964\"           \"1965\"           \"1966\"           \"1967\"          \n[13] \"1968\"           \"1969\"           \"1970\"           \"1971\"          \n[17] \"1972\"           \"1973\"           \"1974\"           \"1975\"          \n[21] \"1976\"           \"1977\"           \"1978\"           \"1979\"          \n[25] \"1980\"           \"1981\"           \"1982\"           \"1983\"          \n[29] \"1984\"           \"1985\"           \"1986\"           \"1987\"          \n[33] \"1988\"           \"1989\"           \"1990\"           \"1991\"          \n[37] \"1992\"           \"1993\"           \"1994\"           \"1995\"          \n[41] \"1996\"           \"1997\"           \"1998\"           \"1999\"          \n[45] \"2000\"           \"2001\"           \"2002\"           \"2003\"          \n[49] \"2004\"           \"2005\"           \"2006\"           \"2007\"          \n[53] \"2008\"           \"2009\"           \"2010\"           \"2011\"          \n[57] \"2012\"           \"2013\"           \"2014\"           \"2015\"          \n[61] \"2016\"           \"2017\"           \"2018\"           \"2019\"          \n[65] \"2020\"           \"NA\"            \n\neconomy_and_growth <- economy_and_growth[-1,]\nnames(economy_and_growth) <- nms_old\nnms_new <- nms_old %>% tolower() \nnms_new <- gsub(\"\\\\s\", \"_\", nms_new)\nnms_new\n\n [1] \"country_name\"   \"country_code\"   \"indicator_name\" \"indicator_code\"\n [5] \"1960\"           \"1961\"           \"1962\"           \"1963\"          \n [9] \"1964\"           \"1965\"           \"1966\"           \"1967\"          \n[13] \"1968\"           \"1969\"           \"1970\"           \"1971\"          \n[17] \"1972\"           \"1973\"           \"1974\"           \"1975\"          \n[21] \"1976\"           \"1977\"           \"1978\"           \"1979\"          \n[25] \"1980\"           \"1981\"           \"1982\"           \"1983\"          \n[29] \"1984\"           \"1985\"           \"1986\"           \"1987\"          \n[33] \"1988\"           \"1989\"           \"1990\"           \"1991\"          \n[37] \"1992\"           \"1993\"           \"1994\"           \"1995\"          \n[41] \"1996\"           \"1997\"           \"1998\"           \"1999\"          \n[45] \"2000\"           \"2001\"           \"2002\"           \"2003\"          \n[49] \"2004\"           \"2005\"           \"2006\"           \"2007\"          \n[53] \"2008\"           \"2009\"           \"2010\"           \"2011\"          \n[57] \"2012\"           \"2013\"           \"2014\"           \"2015\"          \n[61] \"2016\"           \"2017\"           \"2018\"           \"2019\"          \n[65] \"2020\"           \"na\"            \n\nsetnames(economy_and_growth, nms_old, nms_new)\n\nid_vars <- c(\"country_name\", \"country_code\",\n             \"indicator_name\", \"indicator_code\")\n\neconomy_and_growth <- melt(economy_and_growth,\n                                id.vars = id_vars, \n                           variable.factor = F,\n                           value.factor = F,\n                           variable.name = \"year\")\neconomy_and_growth[, value:= as.numeric(value)]\n\neconomy_and_growth[, year := as.numeric(year)]\neconomy_and_growth <-  economy_and_growth[!is.na(value)|!is.na(year)]\neconomy_and_growth[sample(nrow(economy_and_growth), 10)] %>% datatable(options = list(scrollX = TRUE))\n\n\n\n\n\n\n\ngni_gdp_savings_vec <- c(\"Gross savings (% of GNI)\", \n                         \"Gross savings (% of GDP)\",\n                         \"Total debt service (% of exports of goods, services and primary income)\",\n                         \"Total debt service (% of GNI)\",\n                         \"Trade (% of GDP)\",\n                         \"Current account balance (% of GDP)\",\n                         \"Exports of goods and services (annual % growth)\",\n                         \"Manufacturing, value added (annual % growth)\",\n                         \"Price level ratio of PPP conversion factor (GDP) to market exchange rate\")\nea_country <- c(\"Kenya\", \"Uganda\",  \"Tanzania\")\neconomy_and_growth[, unique(country_name)]\n\n  [1] \"Aruba\"                                               \n  [2] \"Afghanistan\"                                         \n  [3] \"Angola\"                                              \n  [4] \"Albania\"                                             \n  [5] \"Andorra\"                                             \n  [6] \"Arab World\"                                          \n  [7] \"United Arab Emirates\"                                \n  [8] \"Argentina\"                                           \n  [9] \"Armenia\"                                             \n [10] \"American Samoa\"                                      \n [11] \"Antigua and Barbuda\"                                 \n [12] \"Australia\"                                           \n [13] \"Austria\"                                             \n [14] \"Azerbaijan\"                                          \n [15] \"Burundi\"                                             \n [16] \"Belgium\"                                             \n [17] \"Benin\"                                               \n [18] \"Burkina Faso\"                                        \n [19] \"Bangladesh\"                                          \n [20] \"Bulgaria\"                                            \n [21] \"Bahrain\"                                             \n [22] \"Bahamas, The\"                                        \n [23] \"Bosnia and Herzegovina\"                              \n [24] \"Belarus\"                                             \n [25] \"Belize\"                                              \n [26] \"Bermuda\"                                             \n [27] \"Bolivia\"                                             \n [28] \"Brazil\"                                              \n [29] \"Barbados\"                                            \n [30] \"Brunei Darussalam\"                                   \n [31] \"Bhutan\"                                              \n [32] \"Botswana\"                                            \n [33] \"Central African Republic\"                            \n [34] \"Canada\"                                              \n [35] \"Central Europe and the Baltics\"                      \n [36] \"Switzerland\"                                         \n [37] \"Channel Islands\"                                     \n [38] \"Chile\"                                               \n [39] \"China\"                                               \n [40] \"Cote d'Ivoire\"                                       \n [41] \"Cameroon\"                                            \n [42] \"Congo, Dem. Rep.\"                                    \n [43] \"Congo, Rep.\"                                         \n [44] \"Colombia\"                                            \n [45] \"Comoros\"                                             \n [46] \"Cabo Verde\"                                          \n [47] \"Costa Rica\"                                          \n [48] \"Caribbean small states\"                              \n [49] \"Cuba\"                                                \n [50] \"Curacao\"                                             \n [51] \"Cayman Islands\"                                      \n [52] \"Cyprus\"                                              \n [53] \"Czech Republic\"                                      \n [54] \"Germany\"                                             \n [55] \"Djibouti\"                                            \n [56] \"Dominica\"                                            \n [57] \"Denmark\"                                             \n [58] \"Dominican Republic\"                                  \n [59] \"Algeria\"                                             \n [60] \"East Asia & Pacific (excluding high income)\"         \n [61] \"Early-demographic dividend\"                          \n [62] \"East Asia & Pacific\"                                 \n [63] \"Europe & Central Asia (excluding high income)\"       \n [64] \"Europe & Central Asia\"                               \n [65] \"Ecuador\"                                             \n [66] \"Egypt, Arab Rep.\"                                    \n [67] \"Euro area\"                                           \n [68] \"Eritrea\"                                             \n [69] \"Spain\"                                               \n [70] \"Estonia\"                                             \n [71] \"Ethiopia\"                                            \n [72] \"European Union\"                                      \n [73] \"Fragile and conflict affected situations\"            \n [74] \"Finland\"                                             \n [75] \"Fiji\"                                                \n [76] \"France\"                                              \n [77] \"Faroe Islands\"                                       \n [78] \"Micronesia, Fed. Sts.\"                               \n [79] \"Gabon\"                                               \n [80] \"United Kingdom\"                                      \n [81] \"Georgia\"                                             \n [82] \"Ghana\"                                               \n [83] \"Gibraltar\"                                           \n [84] \"Guinea\"                                              \n [85] \"Gambia, The\"                                         \n [86] \"Guinea-Bissau\"                                       \n [87] \"Equatorial Guinea\"                                   \n [88] \"Greece\"                                              \n [89] \"Grenada\"                                             \n [90] \"Greenland\"                                           \n [91] \"Guatemala\"                                           \n [92] \"Guam\"                                                \n [93] \"Guyana\"                                              \n [94] \"High income\"                                         \n [95] \"Hong Kong SAR, China\"                                \n [96] \"Honduras\"                                            \n [97] \"Heavily indebted poor countries (HIPC)\"              \n [98] \"Croatia\"                                             \n [99] \"Haiti\"                                               \n[100] \"Hungary\"                                             \n[101] \"IBRD only\"                                           \n[102] \"IDA & IBRD total\"                                    \n[103] \"IDA total\"                                           \n[104] \"IDA blend\"                                           \n[105] \"Indonesia\"                                           \n[106] \"IDA only\"                                            \n[107] \"Isle of Man\"                                         \n[108] \"India\"                                               \n[109] \"Not classified\"                                      \n[110] \"Ireland\"                                             \n[111] \"Iran, Islamic Rep.\"                                  \n[112] \"Iraq\"                                                \n[113] \"Iceland\"                                             \n[114] \"Israel\"                                              \n[115] \"Italy\"                                               \n[116] \"Jamaica\"                                             \n[117] \"Jordan\"                                              \n[118] \"Japan\"                                               \n[119] \"Kazakhstan\"                                          \n[120] \"Kenya\"                                               \n[121] \"Kyrgyz Republic\"                                     \n[122] \"Cambodia\"                                            \n[123] \"Kiribati\"                                            \n[124] \"St. Kitts and Nevis\"                                 \n[125] \"Korea, Rep.\"                                         \n[126] \"Kuwait\"                                              \n[127] \"Latin America & Caribbean (excluding high income)\"   \n[128] \"Lao PDR\"                                             \n[129] \"Lebanon\"                                             \n[130] \"Liberia\"                                             \n[131] \"Libya\"                                               \n[132] \"St. Lucia\"                                           \n[133] \"Latin America & Caribbean\"                           \n[134] \"Least developed countries: UN classification\"        \n[135] \"Low income\"                                          \n[136] \"Liechtenstein\"                                       \n[137] \"Sri Lanka\"                                           \n[138] \"Lower middle income\"                                 \n[139] \"Low & middle income\"                                 \n[140] \"Lesotho\"                                             \n[141] \"Late-demographic dividend\"                           \n[142] \"Lithuania\"                                           \n[143] \"Luxembourg\"                                          \n[144] \"Latvia\"                                              \n[145] \"Macao SAR, China\"                                    \n[146] \"St. Martin (French part)\"                            \n[147] \"Morocco\"                                             \n[148] \"Monaco\"                                              \n[149] \"Moldova\"                                             \n[150] \"Madagascar\"                                          \n[151] \"Maldives\"                                            \n[152] \"Middle East & North Africa\"                          \n[153] \"Mexico\"                                              \n[154] \"Marshall Islands\"                                    \n[155] \"Middle income\"                                       \n[156] \"North Macedonia\"                                     \n[157] \"Mali\"                                                \n[158] \"Malta\"                                               \n[159] \"Myanmar\"                                             \n[160] \"Middle East & North Africa (excluding high income)\"  \n[161] \"Montenegro\"                                          \n[162] \"Mongolia\"                                            \n[163] \"Northern Mariana Islands\"                            \n[164] \"Mozambique\"                                          \n[165] \"Mauritania\"                                          \n[166] \"Mauritius\"                                           \n[167] \"Malawi\"                                              \n[168] \"Malaysia\"                                            \n[169] \"North America\"                                       \n[170] \"Namibia\"                                             \n[171] \"New Caledonia\"                                       \n[172] \"Niger\"                                               \n[173] \"Nigeria\"                                             \n[174] \"Nicaragua\"                                           \n[175] \"Netherlands\"                                         \n[176] \"Norway\"                                              \n[177] \"Nepal\"                                               \n[178] \"Nauru\"                                               \n[179] \"New Zealand\"                                         \n[180] \"OECD members\"                                        \n[181] \"Oman\"                                                \n[182] \"Other small states\"                                  \n[183] \"Pakistan\"                                            \n[184] \"Panama\"                                              \n[185] \"Peru\"                                                \n[186] \"Philippines\"                                         \n[187] \"Palau\"                                               \n[188] \"Papua New Guinea\"                                    \n[189] \"Poland\"                                              \n[190] \"Pre-demographic dividend\"                            \n[191] \"Puerto Rico\"                                         \n[192] \"Korea, Dem. People’s Rep.\"                           \n[193] \"Portugal\"                                            \n[194] \"Paraguay\"                                            \n[195] \"West Bank and Gaza\"                                  \n[196] \"Pacific island small states\"                         \n[197] \"Post-demographic dividend\"                           \n[198] \"French Polynesia\"                                    \n[199] \"Qatar\"                                               \n[200] \"Romania\"                                             \n[201] \"Russian Federation\"                                  \n[202] \"Rwanda\"                                              \n[203] \"South Asia\"                                          \n[204] \"Saudi Arabia\"                                        \n[205] \"Sudan\"                                               \n[206] \"Senegal\"                                             \n[207] \"Singapore\"                                           \n[208] \"Solomon Islands\"                                     \n[209] \"Sierra Leone\"                                        \n[210] \"El Salvador\"                                         \n[211] \"San Marino\"                                          \n[212] \"Somalia\"                                             \n[213] \"Serbia\"                                              \n[214] \"Sub-Saharan Africa (excluding high income)\"          \n[215] \"South Sudan\"                                         \n[216] \"Sub-Saharan Africa\"                                  \n[217] \"Small states\"                                        \n[218] \"Sao Tome and Principe\"                               \n[219] \"Suriname\"                                            \n[220] \"Slovak Republic\"                                     \n[221] \"Slovenia\"                                            \n[222] \"Sweden\"                                              \n[223] \"Eswatini\"                                            \n[224] \"Sint Maarten (Dutch part)\"                           \n[225] \"Seychelles\"                                          \n[226] \"Syrian Arab Republic\"                                \n[227] \"Turks and Caicos Islands\"                            \n[228] \"Chad\"                                                \n[229] \"East Asia & Pacific (IDA & IBRD countries)\"          \n[230] \"Europe & Central Asia (IDA & IBRD countries)\"        \n[231] \"Togo\"                                                \n[232] \"Thailand\"                                            \n[233] \"Tajikistan\"                                          \n[234] \"Turkmenistan\"                                        \n[235] \"Latin America & the Caribbean (IDA & IBRD countries)\"\n[236] \"Timor-Leste\"                                         \n[237] \"Middle East & North Africa (IDA & IBRD countries)\"   \n[238] \"Tonga\"                                               \n[239] \"South Asia (IDA & IBRD)\"                             \n[240] \"Sub-Saharan Africa (IDA & IBRD countries)\"           \n[241] \"Trinidad and Tobago\"                                 \n[242] \"Tunisia\"                                             \n[243] \"Turkey\"                                              \n[244] \"Tuvalu\"                                              \n[245] \"Tanzania\"                                            \n[246] \"Uganda\"                                              \n[247] \"Ukraine\"                                             \n[248] \"Upper middle income\"                                 \n[249] \"Uruguay\"                                             \n[250] \"United States\"                                       \n[251] \"Uzbekistan\"                                          \n[252] \"St. Vincent and the Grenadines\"                      \n[253] \"Venezuela, RB\"                                       \n[254] \"British Virgin Islands\"                              \n[255] \"Virgin Islands (U.S.)\"                               \n[256] \"Vietnam\"                                             \n[257] \"Vanuatu\"                                             \n[258] \"World\"                                               \n[259] \"Samoa\"                                               \n[260] \"Kosovo\"                                              \n[261] \"Yemen, Rep.\"                                         \n[262] \"South Africa\"                                        \n[263] \"Zambia\"                                              \n[264] \"Zimbabwe\"                                            \n\ngni_gdp_savings_df <- economy_and_growth[indicator_name %in% gni_gdp_savings_vec & country_name %in% ea_country ]\n\n\ngni_gdp_savings_df_split <- split(gni_gdp_savings_df, f = gni_gdp_savings_df$indicator_name)\nn <- length(gni_gdp_savings_df_split)\n#my_plots_econ <-htmltools::tagList()\nmy_plots_econ <-list()\nfor (i in 1:n) {\n    df = gni_gdp_savings_df_split[[i]]\n    my_title = df[, unique(indicator_name)]\n    mn = df[, min(year)]\n    mx = df[, max(year)]\n    breaks = seq(mn, mx,by = 5)\n    p = ggplot(df, aes(year, value, group = country_name, color = country_name) ) +\n        geom_line(size = .3)+\n        theme_hc()+\n        labs(title = my_title, x = \"year\", y = \"%\")+\n      scale_color_colorblind(name=\"\")+\n      scale_x_continuous(breaks = breaks)\n    \n    my_plots_econ[[i]] = ggplotly(p)\n    \n}\n\nmy_plots_econ\n\n[[1]]\n\n[[2]]\n\n[[3]]\n\n[[4]]\n\n[[5]]\n\n[[6]]\n\n[[7]]\n\n[[8]]\n\n[[9]]"
  },
  {
    "objectID": "kenya_population/kenya_debt_and_dollar_price.html",
    "href": "kenya_population/kenya_debt_and_dollar_price.html",
    "title": "USD-KES Hisotrical Data",
    "section": "",
    "text": "library(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(DT)\nlibrary(ggthemes)\nlibrary(patchwork)\nusd_kes_hist <- fread(\"USD_KES Historical Data.csv\")\n\n\nusd_kes_hist[, Date :=  as.Date(Date, format = \"%b %d,%Y\")]\n\n\nx <- as.Date(\"2002-12-01\")\nx_end <- as.Date(\"2003-01-01\")\n\ny <-90\ny_end <- 77.500\n\nx_uhuru <- as.Date(\"2013-03-09\")\nx_end_uhuru <- as.Date(\"2013-04-09\")\n\ny_uhuru <-105\ny_end_uhuru <- 85.600\n\ndollar <- ggplot(usd_kes_hist,aes(Date, Price))+\n    geom_line()+\n    scale_x_date(date_breaks = \"3 year\", date_labels = \"%b-%y\")+\n    labs(y = \"1 USD to KES\", title = \"Historical Prices USD to KES\")+\n  \n    annotate(\n    geom = \"curve\", x = x, y = y, xend = x_end, yend = y_end, \n    curvature = .3, arrow = arrow(length = unit(3, \"mm\"))\n  ) +\n  annotate(geom = \"text\", x = x, y = y,\n           label = \"Pres Kibaki \\n takes Office\", hjust = \"left\",\n           angle = 30)+\n    annotate(\n    geom = \"curve\", x = x_uhuru, y = y_uhuru, \n    xend = x_end_uhuru, yend = y_end_uhuru, \n    curvature = .1, arrow = arrow(length = unit(3, \"mm\"))\n  ) +\n  annotate(geom = \"text\", x = x_uhuru, y = y_uhuru+2,\n           label = \"Pres Uhuru \\n takes Office\", hjust = \"left\",\n           angle = 30)\ndollar\n\n\n\n\n\nkenya_debt <- read_csv( \"kenya_debt/Public Debt (Ksh Million).csv\") %>% setDT()\nlibrary(lubridate)\nkenya_debt[, Date := paste(Year, Month, \"01\", sep = \"-\")]\nkenya_debt[, Date := ymd(Date)]\nkenya_debt[, perc_external := round(`External Debt`/Total* 100, 1)]\nkenya_debt[, Total := Total/1000000]\n\n\nx <- as.Date(\"2002-12-01\")\nx_end <- as.Date(\"2003-01-01\")\n\ny <-2.000000\ny_end <- .6152281\n\nx_uhuru <- as.Date(\"2013-03-09\")\nx_end_uhuru <- as.Date(\"2013-04-09\")\n\ny_uhuru <-4.000000\ny_end_uhuru <- 1.8824059\n\n\ndebt <- ggplot(kenya_debt, aes(Date, Total)) +\n    geom_line()+\n  \n    scale_x_date(date_breaks = \"3 year\", date_labels = \"%b-%y\")+\n  \n    labs(title = \"Kenya Debt from 1999 to 2020 June\",\n         y = \"Total Debt in Trillions(KES)\")+\n  \n    annotate(\n    geom = \"curve\", x = x, y = y, xend = x_end, yend = y_end, \n    curvature = .3, arrow = arrow(length = unit(2, \"mm\"))\n  ) +\n  annotate(geom = \"text\", x = x, y = y,\n           label = \"Pres Kibaki \\n takes Office\", hjust = \"left\",\n           angle = 30)+\n  \n    annotate(\n    geom = \"curve\", x = x_uhuru, y = y_uhuru, \n    xend = x_end_uhuru, yend = y_end_uhuru, \n    curvature = -.1, arrow = arrow(length = unit(3, \"mm\"))\n  ) +\n  \n  annotate(geom = \"text\", x = x_uhuru, y = y_uhuru+.5,\n           label = \"Pres Uhuru \\n takes Office\", hjust = \"right\",\n           angle = 30)\n \n\ndebt\n\n\n\n\n\nmy_breaks <- seq(15, 70, 5)\nexternal <- ggplot(kenya_debt, aes(Date, perc_external)) +\n    geom_line()+\n  \n    scale_x_date(date_breaks = \"2 year\", date_labels = \"%Y\")+\n  \n    labs(title = \"Kenya External Debt % from 1999 to 2020 June\",\n         y = \"External Debt (%)\")+\n    theme_hc()+\n    scale_y_continuous(breaks = my_breaks )\n\nexternal\n\n\n\n\n\ndebt_data <- fread(\"poverty_data/IDS-DRSCountries_WLD_Data.csv\")\n\n\nnms_old <-debt_data[1,]  %>% as.character()\nnms_old\n\n [1] \"Country Name\"          \"Country Code\"          \"Counterpart-Area Name\"\n [4] \"Counterpart-Area Code\" \"Series Name\"           \"Series Code\"          \n [7] \"1970\"                  \"1971\"                  \"1972\"                 \n[10] \"1973\"                  \"1974\"                  \"1975\"                 \n[13] \"1976\"                  \"1977\"                  \"1978\"                 \n[16] \"1979\"                  \"1980\"                  \"1981\"                 \n[19] \"1982\"                  \"1983\"                  \"1984\"                 \n[22] \"1985\"                  \"1986\"                  \"1987\"                 \n[25] \"1988\"                  \"1989\"                  \"1990\"                 \n[28] \"1991\"                  \"1992\"                  \"1993\"                 \n[31] \"1994\"                  \"1995\"                  \"1996\"                 \n[34] \"1997\"                  \"1998\"                  \"1999\"                 \n[37] \"2000\"                  \"2001\"                  \"2002\"                 \n[40] \"2003\"                  \"2004\"                  \"2005\"                 \n[43] \"2006\"                  \"2007\"                  \"2008\"                 \n[46] \"2009\"                  \"2010\"                  \"2011\"                 \n[49] \"2012\"                  \"2013\"                  \"2014\"                 \n[52] \"2015\"                  \"2016\"                  \"2017\"                 \n[55] \"2018\"                  \"2019\"                  \"2020\"                 \n[58] \"2021\"                  \"2022\"                  \"2023\"                 \n[61] \"2024\"                  \"2025\"                  \"2026\"                 \n[64] \"2027\"                 \n\ndebt_data <-debt_data[-1,]\nnames(debt_data) <- nms_old\nnms_new <- nms_old %>% tolower() \nnms_new <- gsub(\"\\\\s|-\", \"_\", nms_new)\nnms_new\n\n [1] \"country_name\"          \"country_code\"          \"counterpart_area_name\"\n [4] \"counterpart_area_code\" \"series_name\"           \"series_code\"          \n [7] \"1970\"                  \"1971\"                  \"1972\"                 \n[10] \"1973\"                  \"1974\"                  \"1975\"                 \n[13] \"1976\"                  \"1977\"                  \"1978\"                 \n[16] \"1979\"                  \"1980\"                  \"1981\"                 \n[19] \"1982\"                  \"1983\"                  \"1984\"                 \n[22] \"1985\"                  \"1986\"                  \"1987\"                 \n[25] \"1988\"                  \"1989\"                  \"1990\"                 \n[28] \"1991\"                  \"1992\"                  \"1993\"                 \n[31] \"1994\"                  \"1995\"                  \"1996\"                 \n[34] \"1997\"                  \"1998\"                  \"1999\"                 \n[37] \"2000\"                  \"2001\"                  \"2002\"                 \n[40] \"2003\"                  \"2004\"                  \"2005\"                 \n[43] \"2006\"                  \"2007\"                  \"2008\"                 \n[46] \"2009\"                  \"2010\"                  \"2011\"                 \n[49] \"2012\"                  \"2013\"                  \"2014\"                 \n[52] \"2015\"                  \"2016\"                  \"2017\"                 \n[55] \"2018\"                  \"2019\"                  \"2020\"                 \n[58] \"2021\"                  \"2022\"                  \"2023\"                 \n[61] \"2024\"                  \"2025\"                  \"2026\"                 \n[64] \"2027\"                 \n\nsetnames(debt_data, nms_old, nms_new)\nid_vars <- c(\"country_name\", \"country_code\", \"counterpart_area_name\", \n             \"counterpart_area_code\", \"series_name\", \"series_code\")\n\ndebt_data <- melt(debt_data,\n                                id.vars = id_vars, \n                           variable.factor = F,\n                           value.factor = F,\n                           variable.name = \"year\")\n\ndebt_data[, year := str_trim(year)]\ndebt_data[, year := as.numeric(year)]\ndebt_data <-  debt_data[!is.na(value)]\n\n\nindicator_name <- c(\"Currency composition of PPG debt, U.S. dollars (%)\",\n                    \"Interest payments on external debt (% of exports of goods, services and primary income)\",\n                    \"Interest payments on external debt (% of GNI)\",\n                     \"Short-term debt (% of total external debt)\",\n                     \"Multilateral debt (% of total external debt)\" )\n\n#debt_data[, unique(country_name)]\n#debt_data[, unique(series_name)]\n#\"Uganda\", \"Tanzania\"\nea_country <- c(\"Kenya\", \"Lower middle income\" )\n\n\ndebt_data <- debt_data[country_name %in% ea_country & series_name %in% indicator_name]\ndebt_data_split <- split(debt_data, f = debt_data$series_name)\n\nn <- length(debt_data_split)\nmy_plots <-htmltools::tagList()\nmy_plots <- list()\nfor (i in 1:n) {\n    df = debt_data_split[[i]]\n    my_title = df[, unique(series_name)]\n    mn = df[, min(year)]\n    mx = df[, max(year)]\n    breaks = seq(mn, mx,by = 3)\n    p = ggplot(df, aes(year, value, group = country_name, color = country_name) ) +\n        geom_line()+\n        theme_hc()+\n        labs(title = my_title, x = \"year\", y = \"%\")+\n      scale_color_viridis_d(name=\"\")+\n      scale_x_continuous(breaks = breaks)\n    \n   # my_plots[[i]] = ggplotly(p)\n    my_plots[[i]] = p\n    \n}\n\nmy_plots\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\nworld_debt_data  <- fread(\"poverty_data/API_DT.TDS.DECT.EX.ZS_DS2_en_csv_v2_1865914.csv\", \n                          skip = 4, header = T)\n\nnms_old <- names(world_debt_data)\n\nnms_new <- nms_old %>% tolower() \nnms_new <- gsub(\"\\\\s|-\", \"_\", nms_new)\nnms_new\n\n [1] \"country_name\"   \"country_code\"   \"indicator_name\" \"indicator_code\"\n [5] \"1960\"           \"1961\"           \"1962\"           \"1963\"          \n [9] \"1964\"           \"1965\"           \"1966\"           \"1967\"          \n[13] \"1968\"           \"1969\"           \"1970\"           \"1971\"          \n[17] \"1972\"           \"1973\"           \"1974\"           \"1975\"          \n[21] \"1976\"           \"1977\"           \"1978\"           \"1979\"          \n[25] \"1980\"           \"1981\"           \"1982\"           \"1983\"          \n[29] \"1984\"           \"1985\"           \"1986\"           \"1987\"          \n[33] \"1988\"           \"1989\"           \"1990\"           \"1991\"          \n[37] \"1992\"           \"1993\"           \"1994\"           \"1995\"          \n[41] \"1996\"           \"1997\"           \"1998\"           \"1999\"          \n[45] \"2000\"           \"2001\"           \"2002\"           \"2003\"          \n[49] \"2004\"           \"2005\"           \"2006\"           \"2007\"          \n[53] \"2008\"           \"2009\"           \"2010\"           \"2011\"          \n[57] \"2012\"           \"2013\"           \"2014\"           \"2015\"          \n[61] \"2016\"           \"2017\"           \"2018\"           \"2019\"          \n[65] \"2020\"           \"v66\"           \n\nsetnames(world_debt_data, nms_old, nms_new)\n\nid_vars_debt <- nms_new[1:4]\nworld_debt_data <- melt(world_debt_data,\n                           id.vars = id_vars_debt,\n                           variable.factor = F,\n                           value.factor = F,\n                           variable.name = \"year\")\nworld_debt_data[, year := as.numeric(year)]\nworld_debt_data[, value := as.numeric(value)]\nworld_debt_data <- world_debt_data[!is.na(year)]\nworld_debt_data <- world_debt_data[!is.na(value)]\nhead(world_debt_data[country_name == \"Kenya\"], 10) %>%\n  datatable(options = list(scrollX= T))\n\n\n\n\n\n\n\nea_country <- c(\"Kenya\", \"Lower middle income\" )\n\n\nworld_debt_data <- world_debt_data[country_name %in% ea_country]\nworld_debt_data_split <- split(world_debt_data, f = world_debt_data$indicator_name)\n\nn <- length(world_debt_data_split)\nmy_plots <-htmltools::tagList()\nmy_plots <- list()\nfor (i in 1:n) {\n    df = world_debt_data_split[[i]]\n    my_title = df[, unique(indicator_name)]\n    mn = df[, min(year)]\n    mx = df[, max(year)]\n    breaks = seq(mn, mx,by = 3)\n    p = ggplot(df, aes(year, value, group = country_name, color = country_name) ) +\n        geom_line()+\n        theme_hc()+\n        labs(title = my_title, x = \"year\", y = \"%\")+\n      scale_color_viridis_d(name=\"\")+\n      scale_x_continuous(breaks = breaks)\n    \n   # my_plots[[i]] = ggplotly(p)\n    my_plots[[i]] = p\n    \n}\n\nmy_plots\n\n[[1]]"
  },
  {
    "objectID": "kenya_population/kenya_maps.html",
    "href": "kenya_population/kenya_maps.html",
    "title": "Kenya Census Data",
    "section": "",
    "text": "library(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sf)\nlibrary(DT)\nlibrary(tmap)\nlibrary(rKenyaCensus)\n\n\n# kenya_5yr_births <- readGoogleSheet(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vSf-yNXNk68cIOH0Rb6alaDk9SxKEMm3h6kb2p8jxT8oMdfj4OqUDbs2Ln9OOGGCI9V7SNiZJCDWm4H/pubhtml\")\n# \n# kenya_5yr_births <- cleanGoogleTable(kenya_5yr_births , table = 1, skip = 1 ) %>%\n#   setDT()\ndata(V4_T2.40)\nkenya_5yr_births = V4_T2.40\nsetDT(kenya_5yr_births)\n\n\nold_nms_births <- names(kenya_5yr_births)\nnew_nms_births <- gsub(\"\\\\s\", \"_\", old_nms_births) %>%\n    tolower()\n\nsetnames(kenya_5yr_births, old_nms_births, new_nms_births)\n\n\nnumerics_nms <- c(\"total\", \"notified\", \"not_notified\",\n                  \"don’t_know\", \"not_stated\", \"percent_notified\")\n\nkenya_5yr_births[, (numerics_nms) := lapply(.SD, function(x) gsub(\",\", \"\", x)), .SDcols = numerics_nms]\nkenya_5yr_births[, (numerics_nms) := lapply(.SD, as.numeric), .SDcols = numerics_nms]\n\n\nkenya_counties <- st_read(\"County\")\n\nReading layer `County' from data source \n  `/media/mburu/mburu_ext/home/personal_projects/github_io_blog/kenya_population/County' \n  using driver `ESRI Shapefile'\nSimple feature collection with 47 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 33.91182 ymin: -4.702271 xmax: 41.90626 ymax: 5.430648\nGeodetic CRS:  WGS 84\n\nggplot(kenya_counties)+\n  geom_sf() +\n  theme_void()\n\n\n\n\n\nsetnames(kenya_counties, \"COUNTY\", \"county\")\n\n\nkenya_5yr_births[, county := tolower(gsub(\"\\\\s\", \"-\", county))]\nkenya_5yr_births[county == \"elgeyo-marakwet\", county := \"keiyo-marakwet\" ]\nkenya_5yr_births[county == \"nairobi-city\" , county := \"nairobi\"]\nkenya_5yr_births[county == \"tharaka-nithi\"  , county := \"tharaka\" ]\nkenya_5yr_births[county == \"homabay\"  , county := \"homa-bay\" ]\nsetDT(kenya_counties)\nkenya_counties[, county := tolower(gsub(\"\\\\s\", \"-\", county))]\n\nkenya_births_counties <- merge(kenya_counties, kenya_5yr_births, by = \"county\" )\nsetnames(kenya_births_counties, \"percent_notified\", \"per_cent_in_health_facility\")\nkenya_births_counties[, county := paste(county, \" \", per_cent_in_health_facility,\"%\", sep = \"\" )]\n\nkenya_births_counties <- st_set_geometry(kenya_births_counties, \"geometry\")\n\n\nmap1 <- tm_shape(kenya_births_counties)+\n  tm_borders(col = \"gold\")+\n  tm_polygons(col = \"per_cent_in_health_facility\")+\n  tm_layout(title = \"% Health facility births\",\n            title.position = c(0.3, \"top\"))\n\n\ntmap_leaflet(map1)\n\n\n\n\n\n\n\nkenyan_pop <- fread(\"distribution-of-population-by-age-and-sex-kenya-2019-census-volume-iii.csv\")\n\n\nnumerics_nms <- c(\"Male\", \"Female\", \"Intersex\")\nkenyan_pop[, (numerics_nms) := lapply(.SD, function(x) gsub(\",\", \"\", x)), .SDcols = numerics_nms]\nkenyan_pop[, (numerics_nms) := lapply(.SD, as.numeric), .SDcols = numerics_nms]\nkenyan_pop[, (numerics_nms) := lapply(.SD, function(x) ifelse(is.na(x), 0, x)), .SDcols = numerics_nms]\nkenyan_pop[, Age := gsub(\"Sep\", \"09\", Age)]\nkenyan_pop[, Age := gsub(\"Oct\", \"10\", Age)]\nkenyan_pop[,Total:= Reduce(`+`, .SD),.SDcols= numerics_nms]\n\n\nage_cat <- kenyan_pop[!grepl(\"-\", Age)]\n\nage_cat[is.na(age_cat)] <- NA\n#age_cat[,Total:= Reduce(`+`, .SD),.SDcols= numerics_nms]\n\n\nage_cat_m <- melt(age_cat, id.vars = c(\"Age\", \"Total\"), variable.name = \"Sex\")\nage_cat_m[, Perc := round(value/Total*100, 2)]\n\n\n\n\n\nlibrary(ggthemes)\nage_cat_m <- age_cat_m[Age != \"Total\"]\nage_cat_m[, Age := as.numeric(Age)]\npop_plot <- ggplot(age_cat_m, aes(Age, Perc, group = Sex))+\n  geom_line(aes(color = Sex)) +\n  labs(y = \"Percentage of Population\")+\n  scale_x_continuous(breaks = seq(0, 100, by = 10))+\n  scale_color_viridis_d()+\n  theme_hc()\n\nggplotly(pop_plot)\n\n\n\n\n\n\nage_cat[is.na(Age), Age := 100]\n\nage_cat[, Age := as.numeric(Age)]\nage_cat[, age_factor :=  cut(Age,\n                             breaks = c(0, 5, 15, 20, 30, 40, 50, 65, 100 ), \n                             include.lowest = T,\n                             labels = c(\"0-5\", \"6-15\", \"16-20\", \n                                        \"20-30\", \"31-40\", \"41,50]\", \n                                        \"51-65\", \"> 65\"))]\n\n\n\nage_cat_sum <- age_cat[, .(sum_total = sum(Total)), by = age_factor] %>%\n  .[, Perc := round(sum_total/sum(sum_total)*100,2)]"
  },
  {
    "objectID": "kenya_population/inflation_kenya.html",
    "href": "kenya_population/inflation_kenya.html",
    "title": "Inflation Kenya",
    "section": "",
    "text": "library(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sf)\nlibrary(DT)\nlibrary(tmap)\nlibrary(ggthemes)\nlibrary(here)"
  },
  {
    "objectID": "kenya_population/household_assets_2019census.html",
    "href": "kenya_population/household_assets_2019census.html",
    "title": "Kenya Household Assets",
    "section": "",
    "text": "Kenya selected households assets 2019 census\nI figured that it will be important for me to do this to show why it is impossible for online learning to be adopted in Kenya.\nI used 2019 census data set which can be found here and the counties shape file can be found here\n\n#Packages used\n\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sf)\nlibrary(DT)\nlibrary(tmap)\nlibrary(ggthemes)\nlibrary(ggiraph)\n\n\n\nRead data set\n\nhousehold_assets <- fread(\"percentage-distribution-of-conventional-households-by-ownership-of-selected-household-assets-201.csv\")\n\n#\nkenya_shapefile <- st_read(\"County\") %>% setDT()\n\nReading layer `County' from data source \n  `/media/mburu/mburu_ext/home/personal_projects/github_io_blog/kenya_population/County' \n  using driver `ESRI Shapefile'\nSimple feature collection with 47 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 33.91182 ymin: -4.702271 xmax: 41.90626 ymax: 5.430648\nGeodetic CRS:  WGS 84\n\nkenya_shapefile[, county := tolower(COUNTY)]\n\n# rename column\nsetnames(household_assets, \n         c(\"County / Sub-County\", \"Conventional Households\"  ), \n         c(\"county\", \"households\"))\n\n\n\nSome minor cleaning\n\n# remove commas \nhousehold_assets[, households := as.numeric(gsub(\",|AR K    \", \"\", households))]\n\nhousehold_assets[,  county := tolower(county)]\n\nsub_county <- household_assets %>% \n    group_by(county) %>%\n    filter(households == max(households)) %>% setDT()\n\n\nsub_county_melt <- melt(sub_county, \n                        id.vars = c(\"county\", \"households\"))\n\n\n\n% of households with various assets 2019 census\n\nThis is overall data set for the whole country computer devices is ownership about 8.8% this just means that about 91% of the students can’t access online learning. This is is just a naive estimation the number could be higher.\n\n\nkenya_dat <- sub_county_melt[county == \"kenya\"]\n\np <- ggplot(kenya_dat, aes(variable, value, tooltip = paste(variable, \" : \", value))) +\n    geom_bar_interactive(stat = \"identity\", width = 0.5, fill =\"slategray2\"  ) +\n    geom_text_interactive(aes(variable, value, label = paste0(value, \"%\")),\n              position = position_dodge(width = 0.5),\n              vjust = 0.001, size = 3)+\n    labs(x = \"Household assets\", y = \"%\",\n         title = \"% of households with various assets 2019 census\",\n         caption = \"twitter\\n@mmburu_w\")+\n    theme_fivethirtyeight()+\n  theme(\n    axis.text = element_text(size = 7, angle = 45, vjust = 1, hjust =1),\n    plot.title = element_text_interactive(size =11)\n  )\np1 <- girafe(ggobj = p, width_svg = 7, height_svg = 4.5, \n  options = list(\n    opts_sizing(rescale = T) )\n  )\n\np1\n\n\n\n\n\n\n\n\nMerge shapefile with asset data sets\n\nsub_county_melt[county == \"elgeyo/marakwet\", county := \"keiyo-marakwet\"]\nsub_county_melt[county == \"tharaka-nithi\", county := \"tharaka\"]\nsub_county_melt[county ==  \"taita/taveta\", county := \"taita taveta\"]\nsub_county_melt[county ==  \"nairobi city\", county := \"nairobi\"]\n\ncounty_shapes <- merge(kenya_shapefile, sub_county_melt, by = \"county\") \n\nsetnames(county_shapes, \"value\", \"Percentage\")\n\n\n\nComputer devices data\n\ncomputer <- county_shapes[variable  %in% c(\"Desk Top\\nComputer/\\nLaptop/ Tablet\")]\n\n# this converts to sf object\ncomputer <- st_set_geometry(computer, \"geometry\")\n\n\n\nPercentage of households with computer devices per county\n\nThat is if a household owns a tablet, laptop or a desktop\n\n\n#ttm()\ntm_shape(computer)+\n    tm_borders(col = \"grey\")+\n    tm_fill(col = \"Percentage\")+\n    tm_layout(title = \"% of households with laptop/tablet/desk top \\n 2019 census\",\n              title.size = 1, title.position = c(0.3, 0.95))\n\n\n\n#ttm()\n\n\n\n% of households that can access internet\n\nThis looks like is internet access through mobile phones\n\n\ninternet <- county_shapes[variable == \"Internet\"]\n\ninternet <- st_set_geometry(internet, \"geometry\")\n\n#ttm()\ntm_shape(internet)+\n    tm_borders(col = \"grey\")+\n    tm_fill(col = \"Percentage\")+\n    tm_layout(title = \"% of households with internet acess\",\n              title.size = 1, title.position = c(0.3, 0.95))"
  },
  {
    "objectID": "kenya_population/kenya_inflation.html",
    "href": "kenya_population/kenya_inflation.html",
    "title": "Kenya Inflation",
    "section": "",
    "text": "inflation_rates <- fread(here(\"kenya_debt/Inflation Rates.csv\"))\nsource(\".Rprofile\")\ninflation_rates <- nms_clean(inflation_rates)\ninflation_rates[, date := ymd(paste(year, month, \"01\", sep = \"-\"))]\nsetnames(inflation_rates, \n         \"12_month_inflation\", \n         \"twelve_month_inflation\")\n\n\nggplot(inflation_rates, aes(date, twelve_month_inflation))+\n  geom_line()\n\n\n\n\n\nexports <- fread(here(\"kenya_debt/Principal Exports Volume, Value and Unit Prices (Ksh Million).csv\"))\nexports_m <- melt(exports,\n                  id.vars = c(\"Year\", \"Month\"),\n                  variable.factor = F)\n\nexports_m[, type := fcase(str_detect(variable, \"^Volume\"), \"Volume\",\n                          str_detect(variable, \"^Value\"), \"Income\",\n                           str_detect(variable, \"^Average\"), \"price_per_tonne\",\n                          default = \"na\")]\nexports_m[, variable := tolower(variable)]\nexports_m[, crop := fcase(str_detect(variable, \"coffee\"), \"Coffee\",\n                          str_detect(variable, \"tea\"), \"Tea\",\n                           str_detect(variable, \"horticulture\"), \"Horticulture\",\n                          default = \"na\")]\nexports_mw <- dcast(Year+ Month + crop ~ type, \n                    value.var = \"value\", \n                    data = exports_m,\n                    fun.aggregate = NULL)\n\n\nexports_mw[, date := ymd(paste(Year, Month, \"01\", sep = \"-\"))]\nggplot(exports_mw, aes(date, Income, color = crop))+\n  geom_line()\n\n\n\nggplot(exports_mw, aes(date, price_per_tonne, color = crop))+\n  geom_line()\n\n\n\nggplot(exports_mw, aes(date, Volume, color = crop))+\n  geom_line()"
  },
  {
    "objectID": "water_pumps_tz/water_pumps.html",
    "href": "water_pumps_tz/water_pumps.html",
    "title": "Tanzanian Water Pumps",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(knitr)\nlibrary(broom)\nlibrary(caret)\nlibrary(kableExtra)\nlibrary(ggthemes)\nlibrary(DT)"
  },
  {
    "objectID": "water_pumps_tz/water_pumps.html#section",
    "href": "water_pumps_tz/water_pumps.html#section",
    "title": "Tanzanian Water Pumps",
    "section": "",
    "text": "train_y <- fread(\"0bf8bc6e-30d0-4c50-956a-603fc693d966.csv\")\ntrain_x <-  fread(\"4910797b-ee55-40a7-8668-10efd5c1b960.csv\")\n\ntest <- fread(\"702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv\")"
  },
  {
    "objectID": "water_pumps_tz/water_pumps.html#section-1",
    "href": "water_pumps_tz/water_pumps.html#section-1",
    "title": "Tanzanian Water Pumps",
    "section": "",
    "text": "train_data <- merge(train_y, train_x, by = \"id\")\ntrain_data[, set := \"train\"]\n\ntest[, set := \"test\"]\n\npump_data <- rbindlist(list(train_data, test), fill = T)\n\n\ntrain_data[, .N, by = status_group] %>%\n    .[, Perc :=  round(N/sum(N) * 100, 2)] %>%\n    datatable()\n\n\n\n\n\n\n\npump_data[amount_tsh != 0, .(Mean = mean(amount_tsh),\n               Median = median(amount_tsh),\n               Min = min(amount_tsh),\n               Max = max(amount_tsh),\n               First_qurtile = quantile(amount_tsh, .25),\n               Third_qurtile = quantile(amount_tsh, .75)),\n           by = status_group] %>%\n  datatable()\n\n\n\n\n\n\n\ncol_class <- sapply(train_data, class)\nchar_cols <- names(col_class[col_class == \"character\"])\n\nchar_cols <- char_cols[char_cols != \"date_recorded\"]\nchar_cols\n\n [1] \"status_group\"          \"funder\"                \"installer\"            \n [4] \"wpt_name\"              \"basin\"                 \"subvillage\"           \n [7] \"region\"                \"lga\"                   \"ward\"                 \n[10] \"recorded_by\"           \"scheme_management\"     \"scheme_name\"          \n[13] \"extraction_type\"       \"extraction_type_group\" \"extraction_type_class\"\n[16] \"management\"            \"management_group\"      \"payment\"              \n[19] \"payment_type\"          \"water_quality\"         \"quality_group\"        \n[22] \"quantity\"              \"quantity_group\"        \"source\"               \n[25] \"source_type\"           \"source_class\"          \"waterpoint_type\"      \n[28] \"waterpoint_type_group\" \"set\""
  },
  {
    "objectID": "water_pumps_tz/water_pumps.html#a-lazy-way-of-collapsing-columns-please-do-not-do-it",
    "href": "water_pumps_tz/water_pumps.html#a-lazy-way-of-collapsing-columns-please-do-not-do-it",
    "title": "Tanzanian Water Pumps",
    "section": "A lazy way of collapsing columns, Please DO NOT do it,",
    "text": "A lazy way of collapsing columns, Please DO NOT do it,\n\nIt’s best to go through all columns one by one to see how well lumping together will be beneficial\n\n\nfactor_cols <- pump_data[, ..char_cols]\nfactor_cols[, (char_cols) := lapply(.SD, str_to_lower), .SDcols = char_cols]\nfactor_cols[, (char_cols) := lapply(.SD,  fct_lump_n, n = 20), .SDcols = char_cols]\n#factor_cols[, (char_cols) := lapply(.SD,  fct_lump_n, n = 5), .SDcols = char_cols]\nfactor_cols[factor_cols == \"\"] = NA\nfactor_cols[factor_cols == \"0\"] = NA\nfactor_cols[, (char_cols) :=lapply(.SD, fct_explicit_na, na_level = \"missing\"), .SDcols = char_cols]\n\n\nchars_dat <- melt(factor_cols, id.vars = \"status_group\")\n\nchars_dat[, .(freq = .N), by = .(variable, value)] %>%\n  .[, perc := round(freq/sum(freq) * 100), by = .(variable)] %>%\n    datatable()\n\n\n\n\n\n\n\nfactor_cols[set == \"train\", .N, by = .(funder,status_group)] %>%\n    .[, perc := round(N/sum(N) * 100, 2), by = .(funder)] %>%\n    \n     ggplot(aes(funder, perc, fill = status_group)) +\n     geom_bar(stat = \"identity\") +\n     # geom_text(aes(funder, perc, label = paste0(perc, \"%\"),\n     #               vjust = .05, hjust = .5),\n     #           size = 3, position = position_stack(vjust = 0.5))+\n     theme_hc()+\n    labs(title = \"Percentage loans_\")+\n     scale_fill_economist(name = \"\")+\n    theme(legend.position = \"bottom\",\n          axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n\n\n\n\nfactor_cols[set == \"train\", .N, by = .(water_quality,status_group)] %>%\n    .[, perc := round(N/sum(N) * 100, 2), by = .(water_quality)] %>%\n    \n     ggplot(aes(water_quality, perc, fill = status_group)) +\n     geom_bar(stat = \"identity\") +\n     geom_text(aes(water_quality, perc, label = paste(perc, \"%\"),\n                   vjust = .05, hjust = .5),\n               size = 3, position = position_stack(vjust = 0.5))+\n     theme_hc()+\n    labs(title = \"\")+\n     scale_fill_economist(name = \"\")+\n    theme(legend.position = \"bottom\")\n\n\n\n\n\nfactor_cols[set == \"train\", .N, by = .(installer,status_group)] %>%\n    .[, perc := round(N/sum(N) * 100, 2), by = .(installer)] %>%\n    \n     ggplot(aes(installer, perc, fill = status_group)) +\n     geom_bar(stat = \"identity\") +\n     theme_hc()+\n    labs(title = \"\")+\n     scale_fill_economist(name = \"\")+\n    theme(legend.position = \"bottom\",\n          axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n\n\n\n\npump_data[construction_year == 0, construction_year := NA]\nsummary(pump_data$construction_year)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   1960    1988    2000    1997    2008    2013   25969 \n\npump_data[, construction_year2  := cut(construction_year, breaks = c(1959, 1988, 2000, 2008, 2013))]\npump_data[, construction_year2 := fct_explicit_na(construction_year2, na_level = \"unknown\")]\nsummary(pump_data$construction_year2)\n\n(1959,1988] (1988,2000] (2000,2008] (2008,2013]     unknown \n      12650       12585       13389        9657       25969 \n\n\n\n# train_set <- pump_data[set == \"train\",\n#                        .(status_group, go_funded, water_quality,\n#                          quantity,  construction_year2,\n#                          management_group, go_installer,\n#                          waterpoint_type_group1, longitude, latitude, basin,\n#                          management_group1)]\n\n#is there biase in recording\ndel_cols <- c( \"recorded_by\")\ndata_clean <- cbind(pump_data[, .(construction_year2, latitude, longitude)], factor_cols)\n\ntrain_set <- data_clean[set == \"train\"]\ntrain_set[, set := NULL]\nchar_cols2 <- char_cols[!char_cols %in% c(\"status_group\" )]\ntrain_set_dmmy <- dummies::dummy.data.frame(train_set, names = char_cols2) %>% setDT()\ntrain_set_dmmy[, status_group := factor(status_group,\n                                   levels = c(\"functional\", \"functional needs repair\", \"non functional\"),\n                                   labels  = c(\"functional\", \"functional_needs_repair\", \"non_functional\"))]\n\n\nset.seed(100)\n#train_set <- train_set[status_group %in% c(\"functional\", \"functional_needs_repair\") ]\ntrain_ind <- sample(1:nrow(train_set_dmmy), round(0.7 * nrow(train_set)))\ntrain_set_dmmy[, status_group := factor(status_group)]\n\ntrain_set1 <- train_set_dmmy[train_ind,]\nset.seed(100)\ncv_fold <- createFolds(train_set1$status_group, k = 3)\n\n\nlibrary(caretEnsemble)\n\ntrain_ctrl <- trainControl(method = \"cv\",\n                        number = 3,\n                        summaryFunction = multiClassSummary,\n                        classProbs = TRUE,\n                        allowParallel=T,\n                        index = cv_fold,\n                        verboseIter = TRUE,\n                        returnResamp = \"all\", \n                        savePredictions = \"final\", \n                        search = \"grid\")\n\n\nxgb_grid <-  expand.grid(nrounds = c(50,100),\n                        eta = 0.06,\n                        max_depth =c(10, 50),\n                        gamma = 6,\n                        colsample_bytree = 0.8,\n                        min_child_weight =0.8,\n                        subsample =  .8)\n\n\n\nranger_grid <- expand.grid(splitrule = \"extratrees\",\n                        mtry = c(20, 50, 100),\n                        min.node.size = 1)\n\n\nglmnet_grid <- expand.grid(alpha = c(0, 1),\n                           lambda = seq(0.0001, 1, length = 3))\n\n\nset.seed(100)\n\nlibrary(tictoc)\n\ntic()\n\nmodel_list <- caretList(\n   status_group~.,\n    data=train_set1,\n    trControl=train_ctrl,\n    tuneList = list(caretModelSpec(method=\"xgbTree\", tuneGrid= xgb_grid),\n                    caretModelSpec(method=\"ranger\", tuneGrid= ranger_grid),\n                    caretModelSpec(method=\"glmnet\", tuneGrid= glmnet_grid)\n                    \n                    )\n)\n\n+ Fold1: eta=0.06, max_depth=10, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n[10:39:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:39:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n- Fold1: eta=0.06, max_depth=10, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n+ Fold1: eta=0.06, max_depth=50, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n[10:40:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:40:04] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n- Fold1: eta=0.06, max_depth=50, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n+ Fold2: eta=0.06, max_depth=10, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n[10:40:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:40:26] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n- Fold2: eta=0.06, max_depth=10, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n+ Fold2: eta=0.06, max_depth=50, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n[10:41:27] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:41:28] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n- Fold2: eta=0.06, max_depth=50, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n+ Fold3: eta=0.06, max_depth=10, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n[10:41:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:41:50] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n- Fold3: eta=0.06, max_depth=10, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n+ Fold3: eta=0.06, max_depth=50, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \n[10:43:00] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n[10:43:00] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n- Fold3: eta=0.06, max_depth=50, gamma=6, colsample_bytree=0.8, min_child_weight=0.8, subsample=0.8, nrounds=100 \nAggregating results\nSelecting tuning parameters\nFitting nrounds = 100, max_depth = 50, eta = 0.06, gamma = 6, colsample_bytree = 0.8, min_child_weight = 0.8, subsample = 0.8 on full training set\n+ Fold1: splitrule=extratrees, mtry= 20, min.node.size=1 \n- Fold1: splitrule=extratrees, mtry= 20, min.node.size=1 \n+ Fold1: splitrule=extratrees, mtry= 50, min.node.size=1 \n- Fold1: splitrule=extratrees, mtry= 50, min.node.size=1 \n+ Fold1: splitrule=extratrees, mtry=100, min.node.size=1 \nGrowing trees.. Progress: 76%. Estimated remaining time: 9 seconds.\n- Fold1: splitrule=extratrees, mtry=100, min.node.size=1 \n+ Fold2: splitrule=extratrees, mtry= 20, min.node.size=1 \n- Fold2: splitrule=extratrees, mtry= 20, min.node.size=1 \n+ Fold2: splitrule=extratrees, mtry= 50, min.node.size=1 \n- Fold2: splitrule=extratrees, mtry= 50, min.node.size=1 \n+ Fold2: splitrule=extratrees, mtry=100, min.node.size=1 \nGrowing trees.. Progress: 75%. Estimated remaining time: 10 seconds.\n- Fold2: splitrule=extratrees, mtry=100, min.node.size=1 \n+ Fold3: splitrule=extratrees, mtry= 20, min.node.size=1 \n- Fold3: splitrule=extratrees, mtry= 20, min.node.size=1 \n+ Fold3: splitrule=extratrees, mtry= 50, min.node.size=1 \n- Fold3: splitrule=extratrees, mtry= 50, min.node.size=1 \n+ Fold3: splitrule=extratrees, mtry=100, min.node.size=1 \nGrowing trees.. Progress: 69%. Estimated remaining time: 13 seconds.\n- Fold3: splitrule=extratrees, mtry=100, min.node.size=1 \nAggregating results\nSelecting tuning parameters\nFitting mtry = 20, splitrule = extratrees, min.node.size = 1 on full training set\n+ Fold1: alpha=0, lambda=1 \n- Fold1: alpha=0, lambda=1 \n+ Fold1: alpha=1, lambda=1 \n- Fold1: alpha=1, lambda=1 \n+ Fold2: alpha=0, lambda=1 \n- Fold2: alpha=0, lambda=1 \n+ Fold2: alpha=1, lambda=1 \n- Fold2: alpha=1, lambda=1 \n+ Fold3: alpha=0, lambda=1 \n- Fold3: alpha=0, lambda=1 \n+ Fold3: alpha=1, lambda=1 \n- Fold3: alpha=1, lambda=1 \nAggregating results\nSelecting tuning parameters\nFitting alpha = 0, lambda = 1e-04 on full training set\n\ntoc()\n\n930.916 sec elapsed\n\n\n\nmodel_list\n\n$xgbTree\neXtreme Gradient Boosting \n\n41580 samples\n  317 predictor\n    3 classes: 'functional', 'functional_needs_repair', 'non_functional' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 13860, 13860, 13860 \nResampling results across tuning parameters:\n\n  max_depth  nrounds  logLoss    AUC        prAUC      Accuracy   Kappa    \n  10          50      0.6184224  0.8460033  0.6801849  0.7550024  0.5167732\n  10         100      0.5926097  0.8526962  0.6886836  0.7600890  0.5293239\n  50          50      0.6072174  0.8521591  0.6874586  0.7623617  0.5339285\n  50         100      0.5776355  0.8593970  0.6983684  0.7678451  0.5481911\n  Mean_F1    Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value\n  0.5739352  0.5559239         0.8305834         0.7524923          \n  0.5829963  0.5638717         0.8353955         0.7565636          \n  0.5850994  0.5658591         0.8369418         0.7603295          \n  0.5990653  0.5775931         0.8426834         0.7566597          \n  Mean_Neg_Pred_Value  Mean_Precision  Mean_Recall  Mean_Detection_Rate\n  0.8719837            0.7524923       0.5559239    0.2516675          \n  0.8717203            0.7565636       0.5638717    0.2533630          \n  0.8732480            0.7603295       0.5658591    0.2541206          \n  0.8729287            0.7566597       0.5775931    0.2559484          \n  Mean_Balanced_Accuracy\n  0.6932536             \n  0.6996336             \n  0.7014004             \n  0.7101382             \n\nTuning parameter 'eta' was held constant at a value of 0.06\nTuning\n\nTuning parameter 'min_child_weight' was held constant at a value of 0.8\n\nTuning parameter 'subsample' was held constant at a value of 0.8\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were nrounds = 100, max_depth = 50, eta\n = 0.06, gamma = 6, colsample_bytree = 0.8, min_child_weight = 0.8\n and subsample = 0.8.\n\n$ranger\nRandom Forest \n\n41580 samples\n  317 predictor\n    3 classes: 'functional', 'functional_needs_repair', 'non_functional' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 13860, 13860, 13860 \nResampling results across tuning parameters:\n\n  mtry  logLoss    AUC        prAUC      Accuracy   Kappa      Mean_F1  \n   20   0.5486824  0.8714188  0.7137107  0.7795334  0.5792095  0.6407851\n   50   0.6116078  0.8671270  0.6760604  0.7760823  0.5782371  0.6481112\n  100   0.7202941  0.8628325  0.6346966  0.7723665  0.5735920  0.6466763\n  Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value  Mean_Neg_Pred_Value\n  0.6147133         0.8551205         0.7255057            0.8752240          \n  0.6261864         0.8567513         0.6981690            0.8704719          \n  0.6275167         0.8560894         0.6854621            0.8673085          \n  Mean_Precision  Mean_Recall  Mean_Detection_Rate  Mean_Balanced_Accuracy\n  0.7255057       0.6147133    0.2598445            0.7349169             \n  0.6981690       0.6261864    0.2586941            0.7414688             \n  0.6854621       0.6275167    0.2574555            0.7418031             \n\nTuning parameter 'splitrule' was held constant at a value of extratrees\n\nTuning parameter 'min.node.size' was held constant at a value of 1\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were mtry = 20, splitrule = extratrees\n and min.node.size = 1.\n\n$glmnet\nglmnet \n\n41580 samples\n  317 predictor\n    3 classes: 'functional', 'functional_needs_repair', 'non_functional' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 13860, 13860, 13860 \nResampling results across tuning parameters:\n\n  alpha  lambda   logLoss    AUC        prAUC      Accuracy   Kappa    \n  0      0.00010  0.6218298  0.8319479  0.6497870  0.7439514  0.5036845\n  0      0.50005  0.6746615  0.8169808  0.6330926  0.7311568  0.4643535\n  0      1.00000  0.7068795  0.8096260  0.6250271  0.7228114  0.4433703\n  1      0.00010  0.6251197  0.8319851  0.6495486  0.7437350  0.5066840\n  1      0.50005  0.8893577  0.5000000  0.0000000  0.5424242  0.0000000\n  1      1.00000  0.8893577  0.5000000  0.0000000  0.5424242  0.0000000\n  Mean_F1    Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value\n  0.5646638  0.5517857         0.8294169         0.6773631          \n        NaN  0.5068225         0.8139668               NaN          \n        NaN  0.4978057         0.8061242               NaN          \n  0.5783964  0.5619689         0.8310915         0.6650360          \n        NaN  0.3333333         0.6666667               NaN          \n        NaN  0.3333333         0.6666667               NaN          \n  Mean_Neg_Pred_Value  Mean_Precision  Mean_Recall  Mean_Detection_Rate\n  0.8538487            0.6773631       0.5517857    0.2479838          \n  0.8556091                  NaN       0.5068225    0.2437189          \n  0.8557185                  NaN       0.4978057    0.2409371          \n  0.8526489            0.6650360       0.5619689    0.2479117          \n        NaN                  NaN       0.3333333    0.1808081          \n        NaN                  NaN       0.3333333    0.1808081          \n  Mean_Balanced_Accuracy\n  0.6906013             \n  0.6603946             \n  0.6519649             \n  0.6965302             \n  0.5000000             \n  0.5000000             \n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were alpha = 0 and lambda = 1e-04.\n\nattr(,\"class\")\n[1] \"caretList\"\n\n\n\ntest_own <- train_set_dmmy[-train_ind]\n\n\naccuracy <- c()\nfor (i in 1:length(model_list)) {\n    \n    rf <- model_list[[i]]\n    \n    pred <- predict(rf,  newdata = test_own)\n    \n    \n   #auc[i] <-  caret::(pred, test_own$status_group)\n   accuracy[i] <-  round(Metrics::accuracy(pred, test_own$status_group) * 100, 2)\n   \n   \n    \n}\n\n\npred_df <- data.frame(models = names(model_list), accuracy)\n\nDT::datatable(pred_df)\n\n\n\n\n\n\n\nresamples_models <- resamples(model_list)\n\ndotplot(resamples_models, metric = \"AUC\")\n\n\n\n\n\n# Alternatively, you can put in dense matrix, i.e. basic R-matrix\n# library(lightgbm)\n# \n# train_x <- as.matrix(train_set1[, !status_group])\n# train_y <- train_set1$status_group\n# \n# \n# params = list('task'= 'train',\n#     'boosting_type'= 'gbdt',\n#     'objective'= 'multiclass',\n#     'num_class'=3,\n#     'metric'= 'multi_logloss',\n#     'learning_rate'= 0.002296,\n#     'max_depth'= 7,\n#     'num_leaves'= 17,\n#     'feature_fraction'= 0.4,\n#     'bagging_fraction'= 0.6,\n#     'bagging_freq'= 17)\n# \n# train_lgb = lgb.Dataset(data = train_x , label = train_y, params = params)\n# \n# print(\"Training lightgbm with Matrix\")\n# \n# bst <- lightgbm(\n#     data = train_lgb\n#     , num_leaves = 4L\n#     , learning_rate = 1.0\n#     , nrounds = 2L\n#     , objective = \"multiclass\"\n# )"
  },
  {
    "objectID": "introduction_deeplearning/final miniproject.html",
    "href": "introduction_deeplearning/final miniproject.html",
    "title": "Personal Blog",
    "section": "",
    "text": "def Option():\n    print(\"\"\"OPTIONS:\\n\n            [R]: Prompts the user to load a file (in this project S.typhimurium fasta file)\\n\n            [S]:  It gets the compliment of a dna\\n\n            [W]:  gets the revervse complement of a dna\\n\n            [I]:  prints rest of ORF in a DNA sequence\\n\n            [A]: gets the rest of ORF in one frame of a sequence\\n\n            [K]  gets the rest of ORF in all frames sequences\\n\n            [J]  gets the rest of ORF in one frame in both strands\\n\n            [M]  gets the longest ORF\\n\n            [N]  Computes the maximum length of the longest ORF over num_trials shuffles of the specfied DNA sequence\\n\n            [B]  Computes the Protein encoded by a sequence of DNA\\n\n            [F]   Returns the amino acid sequences that are likely coded by the specified dna\n            [H]:  Give the use of all the Options\\n\n            [Q]:  To exit the program\\n\"\"\")\nOption()\n\nOPTIONS:\n\n            [R]: Prompts the user to load a file (in this project S.typhimurium fasta file)\n\n            [S]:  It gets the compliment of a dna\n\n            [W]:  gets the revervse complement of a dna\n\n            [I]:  prints rest of ORF in a DNA sequence\n\n            [A]: gets the rest of ORF in one frame of a sequence\n\n            [K]  gets the rest of ORF in all frames sequences\n\n            [J]  gets the rest of ORF in one frame in both strands\n\n            [M]  gets the longest ORF\n\n            [N]  Computes the maximum length of the longest ORF over num_trials shuffles of the specfied DNA sequence\n\n            [B]  Computes the Protein encoded by a sequence of DNA\n\n            [F]   Returns the amino acid sequences that are likely coded by the specified dna\n            [H]:  Give the use of all the Options\n\n            [Q]:  To exit the program\n\n\n\n\n\ndef H():\n    Option()\nH()\n\nOPTIONS:\n\n            [R]: Prompts the user to load a file (in this project S.typhimurium fasta file)\n\n            [S]:  It gets the compliment of a dna\n\n            [W]:  gets the revervse complement of a dna\n\n            [I]:  prints rest of ORF in a DNA sequence\n\n            [A]: gets the rest of ORF in one frame of a sequence\n\n            [K]  gets the rest of ORF in all frames sequences\n\n            [J]  gets the rest of ORF in one frame in both strands\n\n            [M]  gets the longest ORF\n\n            [N]  Computes the maximum length of the longest ORF over num_trials shuffles of the specfied DNA sequence\n\n            [B]  Computes the Protein encoded by a sequence of DNA\n\n            [F]   Returns the amino acid sequences that are likely coded by the specified dna\n            [H]:  Give the use of all the Options\n\n            [Q]:  To exit the program\n\n\n\n\n#provide a nucleotide sequnce\nnucleotide ='ATGGCTAGCGATGCGAGCCCTACCGTGACCGATCCATGAGAGATGCTCTAGGCTATGAATGACGTAGCG'\ndef get_complement(nucleotide):\n    \"\"\" Returns the complementary nucleotide\n\n        nucleotide: a nucleotide (A, C, G, or T) represented as a string\n        returns: the complementary nucleotide\n    >>> get_complement('A')\n    'T'\n    >>> get_complement('C')\n    'G'\n    \"\"\"\n    complement = nucleotide.replace(\"A\",\"t\")   #V#replace with small letters and the make uppercase to prevent confusion\n    complement = complement.replace(\"T\",\"a\")\n    complement = complement.replace(\"C\",\"g\")\n    complement = complement.replace(\"G\",\"c\")\n    complement = complement.upper()  #return to upper case\n    return complement\nget_complement(nucleotide)\n\n'TACCGATCGCTACGCTCGGGATGGCACTGGCTAGGTACTCTCTACGAGATCCGATACTTACTGCATCGC'\n\n\n\nget_reverse_complement(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\n'GGATCCCATGAACGTCTTTTAGCGTGGCGCCTTCACTAACGGCATCCCAAATCATTTCCGCAACACGTTCTTCGCTGACATTATTTTGATAATCCATTACTTACTCCTGTTATCTGTCACCGACTTTGTAGAACTTAACGACTGCGTTTATCTGATGCAGTTATTAAACCCCGACGGTGGTTAGTGAACATTCAAAAAACGCCCAATGAATACATCGCTACTGCCTTACGCGGCTCAATGCCGTACCTCGTTTTCTTGTGGCTGAATAACGTCTTTGCCCGCGTTTTCTACCTCTTCCAGCCAAACCAGAAGACGTAAAACTTCATCAATTTCTTCCAGACTCACCAGATCATAACGGCGATGGGTTTTGAAAAGACTGCGCGCCAGTTTGATATCGACGATCACAGGTACGCCAACCTTCTCCGCATAGGCGCGGACGGCCAGTGCGCGCTGATTCGTTTCATACACCGAGATCATCGGAATCGGCATCAATTCGGGTTTAAAATAAATCCCGATCGTAATATGCGTGGGGTTGGCAACAATCAGGCGTGAGTTTTCAATATCAGATTTCACCTGTTCAGACAGAATTTCCATATGAACTTCACGTCTTTTAGATTTAACCTCTGGGTTCCCTTCCTGCTCCTTCATTTCACGCTTCACTTCTTCCTTATCCATTTTCATATCTTTCATGGTCAGGAAATATTCCGCAATAGCATCCAATAATAAGACAATCAATGCGCAAGCAAGGCAAGTTAATACCAATGCGAGGAGAAGTTCACGCCAAATGACGGCAATACCTACAATATTGCCATTTAGCTGAGAAAAGATTTCAACCTTATATTTCTTCCAGCAAATGATGGCGGCCACCACAAAGGATGAGAGATACAGTAGGGTTTTGACCGTATCTTTAACCGTGCGCATACTAAAAAGTTTTTTTGCCCCTTCTACCGGGTTTAACGCCGATAAATTAGGCTTTAATGCTTCTGTCGCCAGCACAAAACCGGCCTGTAATAACGCCGGTAATGCGGAACACACTAAGCAGAGCAGCATAAATGGAATCAGATATTTTAACCCTATCCCAAAAACGGCCAAACTGTAGTCAGCCATGCTCTGATCAAAATTATCCGCAATAATGATCTTAATTATCCCCATAAACTCATTAAATGAGCCATACGACACCAGATAGGCAATTCCTCCCAGCGTCAGGCAGGCGATAATGAGATCTTTACTTTTAAATGACTGGCCTTTTTTAGCGGAGTCTTCCAGCCGTTTTTTAGTCGGTTTTTCTGTTTTATTCGAGGACATGCGTCGCCCCTCGCTCGTAAAACCAACTGCTTAACCCTGTGGCCTGGAAAGAGAGTCGCAGTACATTGTCCGGTAGTACCGGAGAGAAATAAAGCAGCATAATTAAAACGGCAATACCGCTTTTTACCGTCAGTGAAATCGCAAAAGCGTTCATTTGCGGAGCAAAGCGCGACAATAAACCCAGGAATACTTCTGACAGCAACAGCACTAATACCACCGGACTGGCCAGAACCAAGGCGTTTTGAGCCACCTGATTAATAAACGTTAATAGCGGCGGTAATGAAGGCGTGCACTCGTTCATCGGATCGCATAGCTGATAGCTTTTATTTAACACGTCAACCATCGTGACCAGACCGCCGTTTTGTAAATAAACGACAGCGGCAAACATATTCAGGAAATTAGCCATTTCCGAGGTATCAATACCGTTTGCCGGATCGATACTACTACTTAGCGTTGCCCCTCGCTGGTTATCGATAATACAACCCAGCGCATGCATAACCCAAAAAGGCCATGACAGCAGACAGCCCAGCATGACGCCTACCGCCGCTTCTTGCAGAACTAACGGGATCATCGCCACCGATAAAAACGGCGGCGCCTCGTTCAATGCATGCGGCCATACTCCCAATGCCACCAGGATGATAATGGCGTTTCTCGGCGCACCGCTTAATACCCCGCTATTCAAAAACGGCAGGAAGAAAAAAATCGGCGCCACGCGAGCAAACCCTAGCGCCGCAGACGCAACCAGGTGATGAATTTCAAAGTACAACGCGTAAAACATTTTTTACCCCTTAGCCAACGCCAGGAATATCACCTGACGCCCGTAAGAGAGTAAAACTTCGCCATACCAGCCAGACAGTAAAAACAAGCATAAACACACGCCAAGTAATTTAATGCCAAAAGGCAGCGTCTGTTCCTGTAATTGCGTTACCGTCTGGAATAACCCTACCAGGAGGCCGATAATCGTTGCGACAATCGTCGGCCACCCTGACAGGATCAAAACAAGATAGAGCGCCTTATTACCTGCAAACACTAAATCATCCATTTAACTATCCCGTCTCGTAATGATGTCATGTTGCAATGTCCATATACTGTAATATCAATCCCTTAGACAGTAAGGTCCAGCCATCAAGCGCGACAAAAAGCACCAGCTTAATAGGTGTAGATATCGTCACCGGACTCATCATCATCATCCCCAGCGCCAGTAGCACGCTGGATACCACCAGGTCGACGACGACAAAGGGCAAATAAAGATAAAAACCAATTTTAAACGCGCTTTTTATTTCGCTCAGCGCATAAGCAGGTAATAACGCAAATATTGAAGGTTTTTCAATTTCATCTTTGTCACGCTTTACCGTCTCGGTCTCTTCTCCATACTGACGCTTCAGTTGCGCGTTTTCAAAAAACTGAACTAACTCGCGATCTGAATATTTGATCAGATAATCGCGATAACCATCCAGACCTTCATCAACGTGTTTACTTAATGATGAAATATCATTAAAGGTGACATCTTCGTCCTCAAAATAGACGTAGGCATCATGCATTATGGGCCACATAACAAACATAGAAAGCAGCAATGCGACGCCGTTAAGCGTCATATTTGAAGGTATCTGCTGTAATCCCAGGGCGTTACGCACCATGACAAATACAATAGAAAATTTAACGAAACAGGTTCCTGACGCAATAATAAATGGCAACAGGGTGGAAAATGCCAGTAAGGCAATTAATGAGATATCATTCCCCATTACCAGACTCGCTCAGCCATTCATGGATCTCAACGCCTAAGGTGTCATTCATCTGTACCAGTTCGCCATTACCCAGCAAAACACCATTCGCCATAATTTCAACGTTAAGTTCAGCATTGGTCGGCAGTGATAATAGCTGTTGCTGCCCCATGGCTTCGAGTTCGGCGAGGGTAACGTTCTTACGATACAAAACAAATTCCAGTTTGACGGGCAATTGATTCAAGCCAGGCAGAGTTTCTGCAGTTTCAGTTGTATTATTTTCTTCTTCGATATGTTGAATATCTAACGTTTCCACAATAATTCCCCCTTCAACACGGTTGAAATGACCTAACTTTTTCGCGTAGCAATAAACTTCCGCACGGGAAGTACGAATCAGGAGTACATCTCCGATCCCGATTCGGCCCAGCAACGAACGCTGCGTATCACTGCTACCGATTACAAAGCGCAACGGCCAACGCAGCATTTTCGGCCTGCCGCCCCCGACTGCAGGCAGTTCAGGAAGATGCTCAAACCACAGGCCGCCCCGATCGCTCATAATGTGCAACAATTTCCCTTCCGGCAGCGCGCTTCCCGGTACGGGGTTCTCTACGCATAAACGCCGACAGGACAAATGCGGCACGGGCAACTCAAACGGTCGCTCTGTTGCAGCAAGCCAGGGAACGACCAGGTGCTCAGCGCCAGCAGAAACCGCCGCCCCAGCCAGAGCGGGAGAGACATGCTCAAGCCAGTCCCCAGGTTTAATCCAGGCCGACCACCGTTTTTCTGCATCGCTCAACCGAACCCACATTCCCTGTCGCGTCGGATATTCCAGCGTCGCTTCCCGGCCATGGCGCTGGCATTCTGTCGCGGTTTGCGCCAATAGCCATTCGCGACGATCAATCTGTCTCACACGCAATGACATCAGGCGTCATCCTCCTCGCCAGATTGCTGTCTGTGCTGTTGCTGCTGCGGATTTTGTTGATCGTCTCGCGTCAGGTGCCAGCGCTGGGGATTACCGTTTTGCCATTGATCATGCAAACGATGTTCAACCTGCGTATTTGACGGTATTAACGAAAACTCCCCTGCTTGCCGCGCCTGAATATTGACGGAATAGTCATTTCCCCAGCGCTGAAAACGGTAAGTCAGCGAGCTATCCTCTCCTTTCACGCCATCGGCAGTGGGAAAAATAGTCATCATCGGCTTTGATTGCGCCGCTAAAGGCATTTTTTCATCGCCGCCGGTTAATTGGCTAAGATCGGCGATAGTGGTTGGTTGCAGCGGAAGCTGAGAAACATCTTTAACCTTTTTATGATCTTTATCTTCAGGCTTACCGGTATTGGCTGCGGCCATTCGGGCAGGTGCGACATCCCGCGCCAGCGGCGCGCCCTCTTTACGAACGCCTTCGCCCGCGATGGCTTTATTATCCCCAGGCAATGCCTTGATATTATCGTCAGAGATTCCCGTGGCGTTATCGGTTACTTCACTAACGGATTCCACAGCTTTTAAATCAGCAGATAATTTTTTACCGCTTACGCTTTCTAACGGCCTATTTTTAGACGATAGCAACGCTGCGGATTTATCTACTTTGGCCTCCGCAGAGATCAAACCGACAGATTTTTCAGCAGTGACTTTCAACAGTTTTTCAGCAATCCTGAGTTCGCTTTTTCCGTTATGATGCAGACCAGAAACGTTGCCATTGTGATGTTCTGATTTCGCTGGCGCGCCATGTCGCCATGCCGCCAGTAATAACGGTAAAGCCGTTTCTTTATGCATTACGAAAGCATCGCCATAGTCGCGATCTTTTTTATCACCGGAATATTCTGTCTTATGTTTTTCCACCGCTTTTTTTAATGCTTCTGATAAACCGCCAACCTCATCCTGCTGCGGCAGTAAAATGTTCCCGGATGAACTGACAGCTGACACATCGCCCATTAAATTATCTCCTCTGACTCGGCCTCTTCCTGCTGTATCTCTCGCTGGATATAGAATCTTTTCTGACGGATTATCCAGCGTTGATAGTTCCCTTCTTTGCGCAACCAATATTTACTTTTTTTCTGAAACTCTTCCCTTTTCTTTTCCAGCTCGCTCCGTTTTTCCTGAATTTGTATAATCTGGAGTTCTAAATCTTTTATCTGCCGGCGAACAATAGACTGCTTACGTAATAACGTATAAATTTCCTCACGACTGAGCTGTCTGTTTTCTGCACGCAGCGTATCTAATAACAATTTCAGACCCGCTATTTGTTCAAGGATCGCCTCCTCCTCGGCCTGCAGCCCGCGGTCCTCATCCTGATAGCGAAGTAATATCGACTCACACTGTGAATGAAATACCGTACAGCGCCGCTGCAATACTTTAATTCTGGTCAGCGAATGCATTCATACCGCTCAACGTGTCATCAAAGGATGAATACTGCGCTACCGGCTGGCATAACCAGGCTTTCAGGCTATCCCGCATCTGCATCGCCCGATCGTTATCGATATTTTCGCCAGGACGATATTCTCCCAAGTCAATGAAAAGCTGGAGCTCTTCCAAACGCGTCATTAATTTACGCACGGCAGATGCCTGTTCAGCATGTGTCGGCGTCGTGACTTGTCCAAAAACGCGGCTTACGCTTTTCAGTACATCGATTGCCGGGTAATGTCCCTGCCCGGCCAGCTTTCTGCTCAGATACAGGTGACCGTCAAGGATAGAGCGAATTTCATCCGCCATCGGGTCCGCCTCTTCCTCGCTTTCCAGCAGTACCGTATAAAAGGCAGTAATGCTTCCCTCGCTGGTCGCCCCTGGGCGTTCCAGCAAGCGGGGCAAATTATCGAATACGGAGGCGGGATAACCTCGACGAGCCGGACGCTCTCCCGACGCCAGTGCCACGTCTCGCAAAGCACGCGCATAACGGGTCATGGAATCGATAAAAAGCACGACCCGTTTTCCCTGGTCGCGAAAATATTCCGCTACGGTTGTCGCCAGTTGCGCCGCATTGCAGCGATCGACCGAGGGGAAATCGGAAGTGGCAAAAACCAGCACGCATTTTTCTTTCTTATGCGAAGCGCGCAACATATCCACGAATTCAGTGACCTCACGGCCTCGTTCACCGATAAGACCGATAACAAAGACATCCGCCTCCGTTTGCTCGATCAGCATATGCATCAGCATGGTCTTACCGCATCCTGCGGAGGCAAAAATGCCCATTCGCTGGCCTACGCCACAGGTCAATAACCCGTCAATCGCGCGCACACCGGTAATCAGCGGTTCACGGACGCCAACGCGTGAAGCGTAAGACGGCGGTGCGACATCAATAACGCGTTCTTCGCTAATCGGCGCCACTTCAGGGGTAAAACGCTCAACGATTTTCCCTGTCGGATCC'\n\n\n\ndef load_seq(fasta_file):\n    \"\"\" Reads a FASTA file and returns the DNA sequence as a string.\n\n    fasta_file: the path to the FASTA file containing the DNA sequence\n    returns: the DNA sequence as a string\n    \"\"\"\n    retval = \"\"\n    f = open(fasta_file) # assign f to the opened fata file\n    lines = f.readlines()   #assin lines to the lists created\n    for l in lines[1:]:  #remove the ist line in fa(header)\n        retval += l[0:-1] #append the lista without header to the empty list\n    f.close()\n    return retval\n\n\nload_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa')\n\n'GGATCCGACAGGGAAAATCGTTGAGCGTTTTACCCCTGAAGTGGCGCCGATTAGCGAAGAACGCGTTATTGATGTCGCACCGCCGTCTTACGCTTCACGCGTTGGCGTCCGTGAACCGCTGATTACCGGTGTGCGCGCGATTGACGGGTTATTGACCTGTGGCGTAGGCCAGCGAATGGGCATTTTTGCCTCCGCAGGATGCGGTAAGACCATGCTGATGCATATGCTGATCGAGCAAACGGAGGCGGATGTCTTTGTTATCGGTCTTATCGGTGAACGAGGCCGTGAGGTCACTGAATTCGTGGATATGTTGCGCGCTTCGCATAAGAAAGAAAAATGCGTGCTGGTTTTTGCCACTTCCGATTTCCCCTCGGTCGATCGCTGCAATGCGGCGCAACTGGCGACAACCGTAGCGGAATATTTTCGCGACCAGGGAAAACGGGTCGTGCTTTTTATCGATTCCATGACCCGTTATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGATAATTTGCCCCGCTTGCTGGAACGCCCAGGGGCGACCAGCGAGGGAAGCATTACTGCCTTTTATACGGTACTGCTGGAAAGCGAGGAAGAGGCGGACCCGATGGCGGATGAAATTCGCTCTATCCTTGACGGTCACCTGTATCTGAGCAGAAAGCTGGCCGGGCAGGGACATTACCCGGCAATCGATGTACTGAAAAGCGTAAGCCGCGTTTTTGGACAAGTCACGACGCCGACACATGCTGAACAGGCATCTGCCGTGCGTAAATTAATGACGCGTTTGGAAGAGCTCCAGCTTTTCATTGACTTGGGAGAATATCGTCCTGGCGAAAATATCGATAACGATCGGGCGATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAATTAAAGTATTGCAGCGGCGCTGTACGGTATTTCATTCACAGTGTGAGTCGATATTACTTCGCTATCAGGATGAGGACCGCGGGCTGCAGGCCGAGGAGGAGGCGATCCTTGAACAAATAGCGGGTCTGAAATTGTTATTAGATACGCTGCGTGCAGAAAACAGACAGCTCAGTCGTGAGGAAATTTATACGTTATTACGTAAGCAGTCTATTGTTCGCCGGCAGATAAAAGATTTAGAACTCCAGATTATACAAATTCAGGAAAAACGGAGCGAGCTGGAAAAGAAAAGGGAAGAGTTTCAGAAAAAAAGTAAATATTGGTTGCGCAAAGAAGGGAACTATCAACGCTGGATAATCCGTCAGAAAAGATTCTATATCCAGCGAGAGATACAGCAGGAAGAGGCCGAGTCAGAGGAGATAATTTAATGGGCGATGTGTCAGCTGTCAGTTCATCCGGGAACATTTTACTGCCGCAGCAGGATGAGGTTGGCGGTTTATCAGAAGCATTAAAAAAAGCGGTGGAAAAACATAAGACAGAATATTCCGGTGATAAAAAAGATCGCGACTATGGCGATGCTTTCGTAATGCATAAAGAAACGGCTTTACCGTTATTACTGGCGGCATGGCGACATGGCGCGCCAGCGAAATCAGAACATCACAATGGCAACGTTTCTGGTCTGCATCATAACGGAAAAAGCGAACTCAGGATTGCTGAAAAACTGTTGAAAGTCACTGCTGAAAAATCTGTCGGTTTGATCTCTGCGGAGGCCAAAGTAGATAAATCCGCAGCGTTGCTATCGTCTAAAAATAGGCCGTTAGAAAGCGTAAGCGGTAAAAAATTATCTGCTGATTTAAAAGCTGTGGAATCCGTTAGTGAAGTAACCGATAACGCCACGGGAATCTCTGACGATAATATCAAGGCATTGCCTGGGGATAATAAAGCCATCGCGGGCGAAGGCGTTCGTAAAGAGGGCGCGCCGCTGGCGCGGGATGTCGCACCTGCCCGAATGGCCGCAGCCAATACCGGTAAGCCTGAAGATAAAGATCATAAAAAGGTTAAAGATGTTTCTCAGCTTCCGCTGCAACCAACCACTATCGCCGATCTTAGCCAATTAACCGGCGGCGATGAAAAAATGCCTTTAGCGGCGCAATCAAAGCCGATGATGACTATTTTTCCCACTGCCGATGGCGTGAAAGGAGAGGATAGCTCGCTGACTTACCGTTTTCAGCGCTGGGGAAATGACTATTCCGTCAATATTCAGGCGCGGCAAGCAGGGGAGTTTTCGTTAATACCGTCAAATACGCAGGTTGAACATCGTTTGCATGATCAATGGCAAAACGGTAATCCCCAGCGCTGGCACCTGACGCGAGACGATCAACAAAATCCGCAGCAGCAACAGCACAGACAGCAATCTGGCGAGGAGGATGACGCCTGATGTCATTGCGTGTGAGACAGATTGATCGTCGCGAATGGCTATTGGCGCAAACCGCGACAGAATGCCAGCGCCATGGCCGGGAAGCGACGCTGGAATATCCGACGCGACAGGGAATGTGGGTTCGGTTGAGCGATGCAGAAAAACGGTGGTCGGCCTGGATTAAACCTGGGGACTGGCTTGAGCATGTCTCTCCCGCTCTGGCTGGGGCGGCGGTTTCTGCTGGCGCTGAGCACCTGGTCGTTCCCTGGCTTGCTGCAACAGAGCGACCGTTTGAGTTGCCCGTGCCGCATTTGTCCTGTCGGCGTTTATGCGTAGAGAACCCCGTACCGGGAAGCGCGCTGCCGGAAGGGAAATTGTTGCACATTATGAGCGATCGGGGCGGCCTGTGGTTTGAGCATCTTCCTGAACTGCCTGCAGTCGGGGGCGGCAGGCCGAAAATGCTGCGTTGGCCGTTGCGCTTTGTAATCGGTAGCAGTGATACGCAGCGTTCGTTGCTGGGCCGAATCGGGATCGGAGATGTACTCCTGATTCGTACTTCCCGTGCGGAAGTTTATTGCTACGCGAAAAAGTTAGGTCATTTCAACCGTGTTGAAGGGGGAATTATTGTGGAAACGTTAGATATTCAACATATCGAAGAAGAAAATAATACAACTGAAACTGCAGAAACTCTGCCTGGCTTGAATCAATTGCCCGTCAAACTGGAATTTGTTTTGTATCGTAAGAACGTTACCCTCGCCGAACTCGAAGCCATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAATGATATCTCATTAATTGCCTTACTGGCATTTTCCACCCTGTTGCCATTTATTATTGCGTCAGGAACCTGTTTCGTTAAATTTTCTATTGTATTTGTCATGGTGCGTAACGCCCTGGGATTACAGCAGATACCTTCAAATATGACGCTTAACGGCGTCGCATTGCTGCTTTCTATGTTTGTTATGTGGCCCATAATGCATGATGCCTACGTCTATTTTGAGGACGAAGATGTCACCTTTAATGATATTTCATCATTAAGTAAACACGTTGATGAAGGTCTGGATGGTTATCGCGATTATCTGATCAAATATTCAGATCGCGAGTTAGTTCAGTTTTTTGAAAACGCGCAACTGAAGCGTCAGTATGGAGAAGAGACCGAGACGGTAAAGCGTGACAAAGATGAAATTGAAAAACCTTCAATATTTGCGTTATTACCTGCTTATGCGCTGAGCGAAATAAAAAGCGCGTTTAAAATTGGTTTTTATCTTTATTTGCCCTTTGTCGTCGTCGACCTGGTGGTATCCAGCGTGCTACTGGCGCTGGGGATGATGATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACATGACATCATTACGAGACGGGATAGTTAAATGGATGATTTAGTGTTTGCAGGTAATAAGGCGCTCTATCTTGTTTTGATCCTGTCAGGGTGGCCGACGATTGTCGCAACGATTATCGGCCTCCTGGTAGGGTTATTCCAGACGGTAACGCAATTACAGGAACAGACGCTGCCTTTTGGCATTAAATTACTTGGCGTGTGTTTATGCTTGTTTTTACTGTCTGGCTGGTATGGCGAAGTTTTACTCTCTTACGGGCGTCAGGTGATATTCCTGGCGTTGGCTAAGGGGTAAAAAATGTTTTACGCGTTGTACTTTGAAATTCATCACCTGGTTGCGTCTGCGGCGCTAGGGTTTGCTCGCGTGGCGCCGATTTTTTTCTTCCTGCCGTTTTTGAATAGCGGGGTATTAAGCGGTGCGCCGAGAAACGCCATTATCATCCTGGTGGCATTGGGAGTATGGCCGCATGCATTGAACGAGGCGCCGCCGTTTTTATCGGTGGCGATGATCCCGTTAGTTCTGCAAGAAGCGGCGGTAGGCGTCATGCTGGGCTGTCTGCTGTCATGGCCTTTTTGGGTTATGCATGCGCTGGGTTGTATTATCGATAACCAGCGAGGGGCAACGCTAAGTAGTAGTATCGATCCGGCAAACGGTATTGATACCTCGGAAATGGCTAATTTCCTGAATATGTTTGCCGCTGTCGTTTATTTACAAAACGGCGGTCTGGTCACGATGGTTGACGTGTTAAATAAAAGCTATCAGCTATGCGATCCGATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTATTAATCAGGTGGCTCAAAACGCCTTGGTTCTGGCCAGTCCGGTGGTATTAGTGCTGTTGCTGTCAGAAGTATTCCTGGGTTTATTGTCGCGCTTTGCTCCGCAAATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAATAAAACAGAAAAACCGACTAAAAAACGGCTGGAAGACTCCGCTAAAAAAGGCCAGTCATTTAAAAGTAAAGATCTCATTATCGCCTGCCTGACGCTGGGAGGAATTGCCTATCTGGTGTCGTATGGCTCATTTAATGAGTTTATGGGGATAATTAAGATCATTATTGCGGATAATTTTGATCAGAGCATGGCTGACTACAGTTTGGCCGTTTTTGGGATAGGGTTAAAATATCTGATTCCATTTATGCTGCTCTGCTTAGTGTGTTCCGCATTACCGGCGTTATTACAGGCCGGTTTTGTGCTGGCGACAGAAGCATTAAAGCCTAATTTATCGGCGTTAAACCCGGTAGAAGGGGCAAAAAAACTTTTTAGTATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCATTGAGCCGCGTAAGGCAGTAGCGATGTATTCATTGGGCGTTTTTTGAATGTTCACTAACCACCGTCGGGGTTTAATAACTGCATCAGATAAACGCAGTCGTTAAGTTCTACAAAGTCGGTGACAGATAACAGGAGTAAGTAATGGATTATCAAAATAATGTCAGCGAAGAACGTGTTGCGGAAATGATTTGGGATGCCGTTAGTGAAGGCGCCACGCTAAAAGACGTTCATGGGATCC'\n\n\n\npwd\n\n'/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna'\n\n\n\nget_reverse_complement(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\nNameError: name 'get_reverse_complement' is not defined\n\n\n\ndna='ATGGCTAGCGATGCGAGCCCTACCGTGACCGATCCATGAGAGATGCTCTAGGCTATGAATGACGTAGCG'\ndef get_reverse_complement(dna):\n    complement = dna.replace(\"A\",\"t\")\n    complement = complement.replace(\"T\",\"a\")\n    complement = complement.replace(\"C\",\"g\")#replace nucleotides with small letters and later make them uppercase\n    complement = complement.replace(\"G\",\"c\")\n    complement = complement.upper()\n    reverse = complement[::-1]#reverse using index\n    return reverse\n    \"\"\" Computes the reverse complementary sequence of DNA for the specfied DNA\n        sequence|\n\n        dna: a DNA sequence represented as a string\n        returns: the reverse complementary DNA sequence represented as a string\n    >>> get_reverse_complement(\"ATGCCCGCTTT\")\n    'AAAGCGGGCAT'\n    >>> get_reverse_complement(\"CCGCGTTCA\")\n    'TGAACGCGG'\n   \"\"\"\n#get_reverse_complement(dna)\n\n\nget_reverse_complement(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\n'GGATCCCATGAACGTCTTTTAGCGTGGCGCCTTCACTAACGGCATCCCAAATCATTTCCGCAACACGTTCTTCGCTGACATTATTTTGATAATCCATTACTTACTCCTGTTATCTGTCACCGACTTTGTAGAACTTAACGACTGCGTTTATCTGATGCAGTTATTAAACCCCGACGGTGGTTAGTGAACATTCAAAAAACGCCCAATGAATACATCGCTACTGCCTTACGCGGCTCAATGCCGTACCTCGTTTTCTTGTGGCTGAATAACGTCTTTGCCCGCGTTTTCTACCTCTTCCAGCCAAACCAGAAGACGTAAAACTTCATCAATTTCTTCCAGACTCACCAGATCATAACGGCGATGGGTTTTGAAAAGACTGCGCGCCAGTTTGATATCGACGATCACAGGTACGCCAACCTTCTCCGCATAGGCGCGGACGGCCAGTGCGCGCTGATTCGTTTCATACACCGAGATCATCGGAATCGGCATCAATTCGGGTTTAAAATAAATCCCGATCGTAATATGCGTGGGGTTGGCAACAATCAGGCGTGAGTTTTCAATATCAGATTTCACCTGTTCAGACAGAATTTCCATATGAACTTCACGTCTTTTAGATTTAACCTCTGGGTTCCCTTCCTGCTCCTTCATTTCACGCTTCACTTCTTCCTTATCCATTTTCATATCTTTCATGGTCAGGAAATATTCCGCAATAGCATCCAATAATAAGACAATCAATGCGCAAGCAAGGCAAGTTAATACCAATGCGAGGAGAAGTTCACGCCAAATGACGGCAATACCTACAATATTGCCATTTAGCTGAGAAAAGATTTCAACCTTATATTTCTTCCAGCAAATGATGGCGGCCACCACAAAGGATGAGAGATACAGTAGGGTTTTGACCGTATCTTTAACCGTGCGCATACTAAAAAGTTTTTTTGCCCCTTCTACCGGGTTTAACGCCGATAAATTAGGCTTTAATGCTTCTGTCGCCAGCACAAAACCGGCCTGTAATAACGCCGGTAATGCGGAACACACTAAGCAGAGCAGCATAAATGGAATCAGATATTTTAACCCTATCCCAAAAACGGCCAAACTGTAGTCAGCCATGCTCTGATCAAAATTATCCGCAATAATGATCTTAATTATCCCCATAAACTCATTAAATGAGCCATACGACACCAGATAGGCAATTCCTCCCAGCGTCAGGCAGGCGATAATGAGATCTTTACTTTTAAATGACTGGCCTTTTTTAGCGGAGTCTTCCAGCCGTTTTTTAGTCGGTTTTTCTGTTTTATTCGAGGACATGCGTCGCCCCTCGCTCGTAAAACCAACTGCTTAACCCTGTGGCCTGGAAAGAGAGTCGCAGTACATTGTCCGGTAGTACCGGAGAGAAATAAAGCAGCATAATTAAAACGGCAATACCGCTTTTTACCGTCAGTGAAATCGCAAAAGCGTTCATTTGCGGAGCAAAGCGCGACAATAAACCCAGGAATACTTCTGACAGCAACAGCACTAATACCACCGGACTGGCCAGAACCAAGGCGTTTTGAGCCACCTGATTAATAAACGTTAATAGCGGCGGTAATGAAGGCGTGCACTCGTTCATCGGATCGCATAGCTGATAGCTTTTATTTAACACGTCAACCATCGTGACCAGACCGCCGTTTTGTAAATAAACGACAGCGGCAAACATATTCAGGAAATTAGCCATTTCCGAGGTATCAATACCGTTTGCCGGATCGATACTACTACTTAGCGTTGCCCCTCGCTGGTTATCGATAATACAACCCAGCGCATGCATAACCCAAAAAGGCCATGACAGCAGACAGCCCAGCATGACGCCTACCGCCGCTTCTTGCAGAACTAACGGGATCATCGCCACCGATAAAAACGGCGGCGCCTCGTTCAATGCATGCGGCCATACTCCCAATGCCACCAGGATGATAATGGCGTTTCTCGGCGCACCGCTTAATACCCCGCTATTCAAAAACGGCAGGAAGAAAAAAATCGGCGCCACGCGAGCAAACCCTAGCGCCGCAGACGCAACCAGGTGATGAATTTCAAAGTACAACGCGTAAAACATTTTTTACCCCTTAGCCAACGCCAGGAATATCACCTGACGCCCGTAAGAGAGTAAAACTTCGCCATACCAGCCAGACAGTAAAAACAAGCATAAACACACGCCAAGTAATTTAATGCCAAAAGGCAGCGTCTGTTCCTGTAATTGCGTTACCGTCTGGAATAACCCTACCAGGAGGCCGATAATCGTTGCGACAATCGTCGGCCACCCTGACAGGATCAAAACAAGATAGAGCGCCTTATTACCTGCAAACACTAAATCATCCATTTAACTATCCCGTCTCGTAATGATGTCATGTTGCAATGTCCATATACTGTAATATCAATCCCTTAGACAGTAAGGTCCAGCCATCAAGCGCGACAAAAAGCACCAGCTTAATAGGTGTAGATATCGTCACCGGACTCATCATCATCATCCCCAGCGCCAGTAGCACGCTGGATACCACCAGGTCGACGACGACAAAGGGCAAATAAAGATAAAAACCAATTTTAAACGCGCTTTTTATTTCGCTCAGCGCATAAGCAGGTAATAACGCAAATATTGAAGGTTTTTCAATTTCATCTTTGTCACGCTTTACCGTCTCGGTCTCTTCTCCATACTGACGCTTCAGTTGCGCGTTTTCAAAAAACTGAACTAACTCGCGATCTGAATATTTGATCAGATAATCGCGATAACCATCCAGACCTTCATCAACGTGTTTACTTAATGATGAAATATCATTAAAGGTGACATCTTCGTCCTCAAAATAGACGTAGGCATCATGCATTATGGGCCACATAACAAACATAGAAAGCAGCAATGCGACGCCGTTAAGCGTCATATTTGAAGGTATCTGCTGTAATCCCAGGGCGTTACGCACCATGACAAATACAATAGAAAATTTAACGAAACAGGTTCCTGACGCAATAATAAATGGCAACAGGGTGGAAAATGCCAGTAAGGCAATTAATGAGATATCATTCCCCATTACCAGACTCGCTCAGCCATTCATGGATCTCAACGCCTAAGGTGTCATTCATCTGTACCAGTTCGCCATTACCCAGCAAAACACCATTCGCCATAATTTCAACGTTAAGTTCAGCATTGGTCGGCAGTGATAATAGCTGTTGCTGCCCCATGGCTTCGAGTTCGGCGAGGGTAACGTTCTTACGATACAAAACAAATTCCAGTTTGACGGGCAATTGATTCAAGCCAGGCAGAGTTTCTGCAGTTTCAGTTGTATTATTTTCTTCTTCGATATGTTGAATATCTAACGTTTCCACAATAATTCCCCCTTCAACACGGTTGAAATGACCTAACTTTTTCGCGTAGCAATAAACTTCCGCACGGGAAGTACGAATCAGGAGTACATCTCCGATCCCGATTCGGCCCAGCAACGAACGCTGCGTATCACTGCTACCGATTACAAAGCGCAACGGCCAACGCAGCATTTTCGGCCTGCCGCCCCCGACTGCAGGCAGTTCAGGAAGATGCTCAAACCACAGGCCGCCCCGATCGCTCATAATGTGCAACAATTTCCCTTCCGGCAGCGCGCTTCCCGGTACGGGGTTCTCTACGCATAAACGCCGACAGGACAAATGCGGCACGGGCAACTCAAACGGTCGCTCTGTTGCAGCAAGCCAGGGAACGACCAGGTGCTCAGCGCCAGCAGAAACCGCCGCCCCAGCCAGAGCGGGAGAGACATGCTCAAGCCAGTCCCCAGGTTTAATCCAGGCCGACCACCGTTTTTCTGCATCGCTCAACCGAACCCACATTCCCTGTCGCGTCGGATATTCCAGCGTCGCTTCCCGGCCATGGCGCTGGCATTCTGTCGCGGTTTGCGCCAATAGCCATTCGCGACGATCAATCTGTCTCACACGCAATGACATCAGGCGTCATCCTCCTCGCCAGATTGCTGTCTGTGCTGTTGCTGCTGCGGATTTTGTTGATCGTCTCGCGTCAGGTGCCAGCGCTGGGGATTACCGTTTTGCCATTGATCATGCAAACGATGTTCAACCTGCGTATTTGACGGTATTAACGAAAACTCCCCTGCTTGCCGCGCCTGAATATTGACGGAATAGTCATTTCCCCAGCGCTGAAAACGGTAAGTCAGCGAGCTATCCTCTCCTTTCACGCCATCGGCAGTGGGAAAAATAGTCATCATCGGCTTTGATTGCGCCGCTAAAGGCATTTTTTCATCGCCGCCGGTTAATTGGCTAAGATCGGCGATAGTGGTTGGTTGCAGCGGAAGCTGAGAAACATCTTTAACCTTTTTATGATCTTTATCTTCAGGCTTACCGGTATTGGCTGCGGCCATTCGGGCAGGTGCGACATCCCGCGCCAGCGGCGCGCCCTCTTTACGAACGCCTTCGCCCGCGATGGCTTTATTATCCCCAGGCAATGCCTTGATATTATCGTCAGAGATTCCCGTGGCGTTATCGGTTACTTCACTAACGGATTCCACAGCTTTTAAATCAGCAGATAATTTTTTACCGCTTACGCTTTCTAACGGCCTATTTTTAGACGATAGCAACGCTGCGGATTTATCTACTTTGGCCTCCGCAGAGATCAAACCGACAGATTTTTCAGCAGTGACTTTCAACAGTTTTTCAGCAATCCTGAGTTCGCTTTTTCCGTTATGATGCAGACCAGAAACGTTGCCATTGTGATGTTCTGATTTCGCTGGCGCGCCATGTCGCCATGCCGCCAGTAATAACGGTAAAGCCGTTTCTTTATGCATTACGAAAGCATCGCCATAGTCGCGATCTTTTTTATCACCGGAATATTCTGTCTTATGTTTTTCCACCGCTTTTTTTAATGCTTCTGATAAACCGCCAACCTCATCCTGCTGCGGCAGTAAAATGTTCCCGGATGAACTGACAGCTGACACATCGCCCATTAAATTATCTCCTCTGACTCGGCCTCTTCCTGCTGTATCTCTCGCTGGATATAGAATCTTTTCTGACGGATTATCCAGCGTTGATAGTTCCCTTCTTTGCGCAACCAATATTTACTTTTTTTCTGAAACTCTTCCCTTTTCTTTTCCAGCTCGCTCCGTTTTTCCTGAATTTGTATAATCTGGAGTTCTAAATCTTTTATCTGCCGGCGAACAATAGACTGCTTACGTAATAACGTATAAATTTCCTCACGACTGAGCTGTCTGTTTTCTGCACGCAGCGTATCTAATAACAATTTCAGACCCGCTATTTGTTCAAGGATCGCCTCCTCCTCGGCCTGCAGCCCGCGGTCCTCATCCTGATAGCGAAGTAATATCGACTCACACTGTGAATGAAATACCGTACAGCGCCGCTGCAATACTTTAATTCTGGTCAGCGAATGCATTCATACCGCTCAACGTGTCATCAAAGGATGAATACTGCGCTACCGGCTGGCATAACCAGGCTTTCAGGCTATCCCGCATCTGCATCGCCCGATCGTTATCGATATTTTCGCCAGGACGATATTCTCCCAAGTCAATGAAAAGCTGGAGCTCTTCCAAACGCGTCATTAATTTACGCACGGCAGATGCCTGTTCAGCATGTGTCGGCGTCGTGACTTGTCCAAAAACGCGGCTTACGCTTTTCAGTACATCGATTGCCGGGTAATGTCCCTGCCCGGCCAGCTTTCTGCTCAGATACAGGTGACCGTCAAGGATAGAGCGAATTTCATCCGCCATCGGGTCCGCCTCTTCCTCGCTTTCCAGCAGTACCGTATAAAAGGCAGTAATGCTTCCCTCGCTGGTCGCCCCTGGGCGTTCCAGCAAGCGGGGCAAATTATCGAATACGGAGGCGGGATAACCTCGACGAGCCGGACGCTCTCCCGACGCCAGTGCCACGTCTCGCAAAGCACGCGCATAACGGGTCATGGAATCGATAAAAAGCACGACCCGTTTTCCCTGGTCGCGAAAATATTCCGCTACGGTTGTCGCCAGTTGCGCCGCATTGCAGCGATCGACCGAGGGGAAATCGGAAGTGGCAAAAACCAGCACGCATTTTTCTTTCTTATGCGAAGCGCGCAACATATCCACGAATTCAGTGACCTCACGGCCTCGTTCACCGATAAGACCGATAACAAAGACATCCGCCTCCGTTTGCTCGATCAGCATATGCATCAGCATGGTCTTACCGCATCCTGCGGAGGCAAAAATGCCCATTCGCTGGCCTACGCCACAGGTCAATAACCCGTCAATCGCGCGCACACCGGTAATCAGCGGTTCACGGACGCCAACGCGTGAAGCGTAAGACGGCGGTGCGACATCAATAACGCGTTCTTCGCTAATCGGCGCCACTTCAGGGGTAAAACGCTCAACGATTTTCCCTGTCGGATCC'\n\n\n\ndef rest_of_ORF(dna):\n   \n    \"\"\" Takes a DNA sequence that is assumed to begin with a start\n        codon and returns the sequence up to but not including the\n        first in frame stop codon.  If there is no in frame stop codon,\n        returns the whole string.\n\n        dna: a DNA sequence\n        returns: the open reading frame represented as a string\n    >>> rest_of_ORF(\"ATGTGAA\")\n    'ATG'\n    >>> rest_of_ORF(\"ATGAGATAGG\")\n    'ATGAGA'\n    \"\"\"\n    #dna = 'ATGGCTAGCGATGCGAGCCCTACCGTGACCGATCCATGAGAGATGCTCTAGGCTATGAATGACGTAGCG'\n    Orf1 = []\n    for i in range(0, len(dna), 3 ): \n        Orf1.append( dna[0+int(i):3+int(i)] ) #APPend the 3 nucleotide code to the empty list\n    #print(\"ORF IS ---->\", Orf1, \"\\n\")\n    lis = []\n    stops = ['TAG', 'TAA', 'TGA']\n    for i in stops:\n        if i in Orf1:\n            lis.append(i)#list the stop condons in the orf\n           # print(\"All stop codons in this orf ---->\", lis, \"\\n\")\n    indexs_stops = []\n    for i in lis:\n        indexs_stops.append(int(Orf1.index(i)))     #append the index of stop codes and sort threm\n    sorted_indexs_stops = sorted(indexs_stops)\n    start = (Orf1.index(\"ATG\")) #start index\n    #print(start)\n    if len(sorted_indexs_stops) >= 1: \n        stop = sorted_indexs_stops[0]#stop index\n        return (\"\".join(Orf1[start:stop]))#prints orf\n        pass\n    else:\n        return (\"\".join(Orf1[start:]))\n#rest_of_ORF(dna)\n\n\nrest_of_ORF(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\n''\n\n\n\nrest_of_ORF(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\n''\n\n\n\ndef find_all_ORFs_oneframe(dna):\n    \"\"\" Finds all non-nested open reading frames in the given DNA\n        sequence and returns them as a list.  This function should\n        only find ORFs that are in the default frame of the sequence\n        (i.e. they start on indices that are multiples of 3).\n        By non-nested we mean that if an ORF occurs entirely within\n        another ORF, it should not be included in the returned list of ORFs.\n\n        dna: a DNA sequence\n        returns: a list of non-nested ORFs\n    >>> find_all_ORFs_oneframe(\"ATGCATGAATGTAGATAGATGTGCCC\")\n    ['ATGCATGAATGTAGA', 'ATGTGCCC']\n    \"\"\"\n    listOfOrf = list()\n   # dna='ATGGCTAGCGATGCGAGCCCTACCGTGACCGATCCATGAGAGATGCTCTAGGCTATGAATGACGTAGCG'\n    frames = [] # storing the default reading frame\n    frames.append([dna[i:i + 3] for i in range(0, len(dna), 3)])\n    #print(frames)\n    for i in range(0,len(frames),1): #looping  the dna frame\n        start=0\n        while start <len(frames[i]): #looping the frame for start and stop codons\n            if frames[i][start]==\"ATG\":\n                for stop in range(start+1,len(frames[i]),1):\n                    if frames[i][stop]==\"TAA\" or  frames[i][stop]==\"TAG\" or  frames[i][stop]==\"TGA\" :\n                        listOfOrf.append(' '.join(frames[i][start:stop])) # retrieve the orf\n                        break\n                else:\n                     listOfOrf.append(' '.join(frames[i][start:]))\n            start+=1\n    one_f_orf =(\",\".join(listOfOrf).replace(\" \",\"\"))\n    one_f_orf = one_f_orf.split(\",\")\n    return one_f_orf\n#find_all_ORFs_oneframe(dna)\n    # TODO: implement this\n\n\ndef find_all_ORFs(dna):\n    \"\"\" Finds all non-nested open reading frames in the given DNA sequence in\n        all 3 possible frames and returns them as a list.  By non-nested we\n        mean that if an ORF occurs entirely within another ORF and they are\n        both in the same frame, it should not be included in the returned list\n        of ORFs.\n\n        dna: a DNA sequence\n        returns: a list of non-nested ORFs\n\n    >>> find_all_ORFs(\"ATGCATGAATGTAG\")\n    ['ATGCATGAATGTAG', 'ATGAATGTAG', 'ATG']\n    \"\"\"\n    dna='ATGGCTAGCGATGCGAGCCCTACCGTGACCGATCCATGAGAGATGCTCTAGGCTATGAATGACGTAGCG'\n    listOfOrf = list()\n    frames = [] # storing the three reading frames create the positive frames\n    frames.append([dna[i:i + 3] for i in range(0, len(dna), 3)])\n    frames.append([dna[i:i + 3] for i in range(1, len(dna), 3)])\n    frames.append([dna[i:i + 3] for i in range(2, len(dna), 3)])\n    #print(frames)\n    for i in range(0,len(frames),1): #looping all the frames\n        start=0\n        while start <len(frames[i]): #looping each frame for start and stop codons\n            if frames[i][start]==\"ATG\":\n                for stop in range(start+1,len(frames[i]),1):\n                    if frames[i][stop]==\"TAA\" or  frames[i][stop]==\"TAG\" or  frames[i][stop]==\"TGA\" :\n                        listOfOrf.append(' '.join(frames[i][start:stop])) # retrieve the orf\n                        break\n                else:\n                     listOfOrf.append(' '.join(frames[i][start:]))\n            start+=1\n    all_orf= \",\".join(listOfOrf).replace(\" \",\"\")\n    all_orf = all_orf.split(\",\")\n    return all_orf\nfind_all_ORFs(dna)\n\n['ATGGCTAGCGATGCGAGCCCTACCGTGACCGATCCA',\n 'ATGCTC',\n 'ATGAATGACGTAGCG',\n 'ATGCGAGCCCTACCG',\n 'ATGACG',\n 'ATGAGAGATGCTCTAGGCTATGAA']\n\n\n\nfind_all_ORFs_oneframe(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\n['ATGCGG',\n 'ATGCGTGCTGGTTTTTGCCACTTCCGATTTCCCCTCGGTCGATCGCTGCAATGCGGCGCAACTGGCGACAACCGTAGCGGAATATTTTCGCGACCAGGGAAAACGGGTCGTGCTTTTTATCGATTCCATGACCCGTTATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGA',\n 'ATGCCAGCCGGTAGCGCAGTATTCATCCTT',\n 'ATGAGGACCGCGGGCTGCAGGCCGAGGAGGAGGCGATCCTTGAACAAA',\n 'ATGGCGACATGGCGCGCCAGCGAAATCAGAACATCACAATGGCAACGTTTCTGGTCTGCATCA',\n 'ATGGCAAAACGG',\n 'ATGTCATTGCGTGTGAGACAGATTGATCGTCGCGAATGGCTATTGGCGCAAACCGCGACAGAATGCCAGCGCCATGGCCGGGAAGCGACGCTGGAATATCCGACGCGACAGGGAATGTGGGTTCGGTTGAGCGATGCAGAAAAACGGTGGTCGGCCTGGATTAAACCTGGGGACTGGCTTGAGCATGTCTCTCCCGCTCTGGCTGGGGCGGCGGTTTCTGCTGGCGCTGAGCACCTGGTCGTTCCCTGGCTTGCTGCAACAGAGCGACCGTTTGAGTTGCCCGTGCCGCATTTGTCCTGTCGGCGTTTATGCGTAGAGAACCCCGTACCGGGAAGCGCGCTGCCGGAAGGGAAATTGTTGCACATTATGAGCGATCGGGGCGGCCTGTGGTTTGAGCATCTTCCTGAACTGCCTGCAGTCGGGGGCGGCAGGCCGAAAATGCTGCGTTGGCCGTTGCGCTTTGTAATCGGTAGCAGTGATACGCAGCGTTCGTTGCTGGGCCGAATCGGGATCGGAGATGTACTCCTGATTCGTACTTCCCGTGCGGAAGTTTATTGCTACGCGAAAAAGTTAGGTCATTTCAACCGTGTTGAAGGGGGAATTATTGTGGAAACGTTAGATATTCAACATATCGAAGAAGAAAATAATACAACTGAAACTGCAGAAACTCTGCCTGGCTTGAATCAATTGCCCGTCAAACTGGAATTTGTTTTGTATCGTAAGAACGTTACCCTCGCCGAACTCGAAGCCATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGTGGGTTCGGTTGAGCGATGCAGAAAAACGGTGGTCGGCCTGGATTAAACCTGGGGACTGGCTTGAGCATGTCTCTCCCGCTCTGGCTGGGGCGGCGGTTTCTGCTGGCGCTGAGCACCTGGTCGTTCCCTGGCTTGCTGCAACAGAGCGACCGTTTGAGTTGCCCGTGCCGCATTTGTCCTGTCGGCGTTTATGCGTAGAGAACCCCGTACCGGGAAGCGCGCTGCCGGAAGGGAAATTGTTGCACATTATGAGCGATCGGGGCGGCCTGTGGTTTGAGCATCTTCCTGAACTGCCTGCAGTCGGGGGCGGCAGGCCGAAAATGCTGCGTTGGCCGTTGCGCTTTGTAATCGGTAGCAGTGATACGCAGCGTTCGTTGCTGGGCCGAATCGGGATCGGAGATGTACTCCTGATTCGTACTTCCCGTGCGGAAGTTTATTGCTACGCGAAAAAGTTAGGTCATTTCAACCGTGTTGAAGGGGGAATTATTGTGGAAACGTTAGATATTCAACATATCGAAGAAGAAAATAATACAACTGAAACTGCAGAAACTCTGCCTGGCTTGAATCAATTGCCCGTCAAACTGGAATTTGTTTTGTATCGTAAGAACGTTACCCTCGCCGAACTCGAAGCCATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGAGCGATCGGGGCGGCCTGTGGTTTGAGCATCTTCCTGAACTGCCTGCAGTCGGGGGCGGCAGGCCGAAAATGCTGCGTTGGCCGTTGCGCTTTGTAATCGGTAGCAGTGATACGCAGCGTTCGTTGCTGGGCCGAATCGGGATCGGAGATGTACTCCTGATTCGTACTTCCCGTGCGGAAGTTTATTGCTACGCGAAAAAGTTAGGTCATTTCAACCGTGTTGAAGGGGGAATTATTGTGGAAACGTTAGATATTCAACATATCGAAGAAGAAAATAATACAACTGAAACTGCAGAAACTCTGCCTGGCTTGAATCAATTGCCCGTCAAACTGGAATTTGTTTTGTATCGTAAGAACGTTACCCTCGCCGAACTCGAAGCCATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGCTGCGTTGGCCGTTGCGCTTTGTAATCGGTAGCAGTGATACGCAGCGTTCGTTGCTGGGCCGAATCGGGATCGGAGATGTACTCCTGATTCGTACTTCCCGTGCGGAAGTTTATTGCTACGCGAAAAAGTTAGGTCATTTCAACCGTGTTGAAGGGGGAATTATTGTGGAAACGTTAGATATTCAACATATCGAAGAAGAAAATAATACAACTGAAACTGCAGAAACTCTGCCTGGCTTGAATCAATTGCCCGTCAAACTGGAATTTGTTTTGTATCGTAAGAACGTTACCCTCGCCGAACTCGAAGCCATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGACATCATTACGAGACGGGATAGTTAAATGGATGATT',\n 'ATGATT',\n 'ATGGCGAAGTTTTACTCTCTTACGGGCGTCAGG',\n 'ATGCAT',\n 'ATGCGCTGGGTTGTATTATCGATAACCAGCGAGGGGCAACGC',\n 'ATGTACTGCGACTCTCTTTCCAGGCCACAGGGT',\n 'ATGTCCTCGAATAAAACAGAAAAACCGACTAAAAAACGGCTGGAAGACTCCGCTAAAAAAGGCCAGTCATTTAAAAGTAAAGATCTCATTATCGCCTGCCTGACGCTGGGAGGAATTGCCTATCTGGTGTCGTATGGCTCATTTAATGAGTTTATGGGGATAATTAAGATCATTATTGCGGATAATTTTGATCAGAGCATGGCTGACTACAGTTTGGCCGTTTTTGGGATAGGGTTAAAATATCTGATTCCATTTATGCTGCTCTGCTTAGTGTGTTCCGCATTACCGGCGTTATTACAGGCCGGTTTTGTGCTGGCGACAGAAGCATTAAAGCCTAATTTATCGGCGTTAAACCCGGTAGAAGGGGCAAAAAAACTTTTTAGTATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGGGGATAATTAAGATCATTATTGCGGATAATTTTGATCAGAGCATGGCTGACTACAGTTTGGCCGTTTTTGGGATAGGGTTAAAATATCTGATTCCATTTATGCTGCTCTGCTTAGTGTGTTCCGCATTACCGGCGTTATTACAGGCCGGTTTTGTGCTGGCGACAGAAGCATTAAAGCCTAATTTATCGGCGTTAAACCCGGTAGAAGGGGCAAAAAAACTTTTTAGTATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGGCTGACTACAGTTTGGCCGTTTTTGGGATAGGGTTAAAATATCTGATTCCATTTATGCTGCTCTGCTTAGTGTGTTCCGCATTACCGGCGTTATTACAGGCCGGTTTTGTGCTGGCGACAGAAGCATTAAAGCCTAATTTATCGGCGTTAAACCCGGTAGAAGGGGCAAAAAAACTTTTTAGTATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGCTGCTCTGCTTAGTGTGTTCCGCATTACCGGCGTTATTACAGGCCGGTTTTGTGCTGGCGACAGAAGCATTAAAGCCTAATTTATCGGCGTTAAACCCGGTAGAAGGGGCAAAAAAACTTTTTAGTATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGTCAGCGAAGAACGTGTTGCGGAAA',\n 'ATGCCGTTAGTGAAGGCGCCACGC',\n 'ATGGGATCC']\n\n\n\ndef find_all_ORFs_both_strands(dna):\n    \"\"\" Finds all non-nested open reading frames in the given DNA sequence on both\n        strands.\nATGAGGCTCAGGGATGATCTTGGGTTTTGTAATGGTCGCTGTACGATTATGATCG\n\n\n        dna: a DNA sequence\n        returns: a list of non-nested ORFs\n    >>> find_all_ORFs_both_strands(\"ATGCGAATGTAGCATCAAA\")\n    ['ATGCGAATG', 'ATGCTACATTCGCAT']\n    \"\"\"\n    #dna='ATGGCTAGCGATGCGAGCCCTACCGTGACCGATCCATGAGAGATGCTCTAGGCTATGAATGACGTAGCG'\n    listOfOrf = list()\n    frames = [] # storing the six frames that would be extacted from the fragments\n    reverseCdna = [] # storing the reverse compliments\n    frames.append([dna[i:i + 3] for i in range(0, len(dna), 3)])\n    frames.append([dna[i:i + 3] for i in range(1, len(dna), 3)])\n    frames.append([dna[i:i + 3] for i in range(2, len(dna), 3)])\n    # reverse compliment of the fragment\n    reverse = {\"A\": \"T\", \"C\": \"G\", \"T\": \"A\", \"G\": \"C\"}\n    for i in range(len(dna)):\n        reverseCdna.append(reverse[dna[-i - 1]]) if dna[-i - 1] in reverse.keys() else reverseCdna.append(dna[-i - 1])  # if any  contamination found we keep it for further more check\n    reverseCdna = ''.join(reverseCdna) # joining\n    # create the reverse complement  frames\n    frames.append([reverseCdna[i:i + 3] for i in range(0, len(reverseCdna), 3)])\n    frames.append([reverseCdna[i:i + 3] for i in range(1, len(reverseCdna), 3)])\n    frames.append([reverseCdna[i:i + 3] for i in range(2, len(reverseCdna), 3)])\n    #print(frames)\n    #print(reverseCdna)\n    for i in range(0,len(frames),1): #looping all the frames\n        start=0\n        while start <len(frames[i]): #looping each frame for start and stop codons\n            if frames[i][start]==\"ATG\":\n                for stop in range(start+1,len(frames[i]),1):\n                    if frames[i][stop]==\"TAA\" or  frames[i][stop]==\"TAG\" or  frames[i][stop]==\"TGA\" :\n                        listOfOrf.append(' '.join(frames[i][start:stop])) # retrieve the orf\n                        break\n                else:\n                     listOfOrf.append(' '.join(frames[i][start:]))\n            start+=1\n    my_string =\",\".join(listOfOrf).replace(\" \",\"\")\n    my_list = my_string.split(\",\")\n    return my_list\n#find_all_ORFs_both_strands(dna)\n\n\nfind_all_ORFs_both_strands(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\n['ATGCGG',\n 'ATGCGTGCTGGTTTTTGCCACTTCCGATTTCCCCTCGGTCGATCGCTGCAATGCGGCGCAACTGGCGACAACCGTAGCGGAATATTTTCGCGACCAGGGAAAACGGGTCGTGCTTTTTATCGATTCCATGACCCGTTATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGA',\n 'ATGCCAGCCGGTAGCGCAGTATTCATCCTT',\n 'ATGAGGACCGCGGGCTGCAGGCCGAGGAGGAGGCGATCCTTGAACAAA',\n 'ATGGCGACATGGCGCGCCAGCGAAATCAGAACATCACAATGGCAACGTTTCTGGTCTGCATCA',\n 'ATGGCAAAACGG',\n 'ATGTCATTGCGTGTGAGACAGATTGATCGTCGCGAATGGCTATTGGCGCAAACCGCGACAGAATGCCAGCGCCATGGCCGGGAAGCGACGCTGGAATATCCGACGCGACAGGGAATGTGGGTTCGGTTGAGCGATGCAGAAAAACGGTGGTCGGCCTGGATTAAACCTGGGGACTGGCTTGAGCATGTCTCTCCCGCTCTGGCTGGGGCGGCGGTTTCTGCTGGCGCTGAGCACCTGGTCGTTCCCTGGCTTGCTGCAACAGAGCGACCGTTTGAGTTGCCCGTGCCGCATTTGTCCTGTCGGCGTTTATGCGTAGAGAACCCCGTACCGGGAAGCGCGCTGCCGGAAGGGAAATTGTTGCACATTATGAGCGATCGGGGCGGCCTGTGGTTTGAGCATCTTCCTGAACTGCCTGCAGTCGGGGGCGGCAGGCCGAAAATGCTGCGTTGGCCGTTGCGCTTTGTAATCGGTAGCAGTGATACGCAGCGTTCGTTGCTGGGCCGAATCGGGATCGGAGATGTACTCCTGATTCGTACTTCCCGTGCGGAAGTTTATTGCTACGCGAAAAAGTTAGGTCATTTCAACCGTGTTGAAGGGGGAATTATTGTGGAAACGTTAGATATTCAACATATCGAAGAAGAAAATAATACAACTGAAACTGCAGAAACTCTGCCTGGCTTGAATCAATTGCCCGTCAAACTGGAATTTGTTTTGTATCGTAAGAACGTTACCCTCGCCGAACTCGAAGCCATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGTGGGTTCGGTTGAGCGATGCAGAAAAACGGTGGTCGGCCTGGATTAAACCTGGGGACTGGCTTGAGCATGTCTCTCCCGCTCTGGCTGGGGCGGCGGTTTCTGCTGGCGCTGAGCACCTGGTCGTTCCCTGGCTTGCTGCAACAGAGCGACCGTTTGAGTTGCCCGTGCCGCATTTGTCCTGTCGGCGTTTATGCGTAGAGAACCCCGTACCGGGAAGCGCGCTGCCGGAAGGGAAATTGTTGCACATTATGAGCGATCGGGGCGGCCTGTGGTTTGAGCATCTTCCTGAACTGCCTGCAGTCGGGGGCGGCAGGCCGAAAATGCTGCGTTGGCCGTTGCGCTTTGTAATCGGTAGCAGTGATACGCAGCGTTCGTTGCTGGGCCGAATCGGGATCGGAGATGTACTCCTGATTCGTACTTCCCGTGCGGAAGTTTATTGCTACGCGAAAAAGTTAGGTCATTTCAACCGTGTTGAAGGGGGAATTATTGTGGAAACGTTAGATATTCAACATATCGAAGAAGAAAATAATACAACTGAAACTGCAGAAACTCTGCCTGGCTTGAATCAATTGCCCGTCAAACTGGAATTTGTTTTGTATCGTAAGAACGTTACCCTCGCCGAACTCGAAGCCATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGAGCGATCGGGGCGGCCTGTGGTTTGAGCATCTTCCTGAACTGCCTGCAGTCGGGGGCGGCAGGCCGAAAATGCTGCGTTGGCCGTTGCGCTTTGTAATCGGTAGCAGTGATACGCAGCGTTCGTTGCTGGGCCGAATCGGGATCGGAGATGTACTCCTGATTCGTACTTCCCGTGCGGAAGTTTATTGCTACGCGAAAAAGTTAGGTCATTTCAACCGTGTTGAAGGGGGAATTATTGTGGAAACGTTAGATATTCAACATATCGAAGAAGAAAATAATACAACTGAAACTGCAGAAACTCTGCCTGGCTTGAATCAATTGCCCGTCAAACTGGAATTTGTTTTGTATCGTAAGAACGTTACCCTCGCCGAACTCGAAGCCATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGCTGCGTTGGCCGTTGCGCTTTGTAATCGGTAGCAGTGATACGCAGCGTTCGTTGCTGGGCCGAATCGGGATCGGAGATGTACTCCTGATTCGTACTTCCCGTGCGGAAGTTTATTGCTACGCGAAAAAGTTAGGTCATTTCAACCGTGTTGAAGGGGGAATTATTGTGGAAACGTTAGATATTCAACATATCGAAGAAGAAAATAATACAACTGAAACTGCAGAAACTCTGCCTGGCTTGAATCAATTGCCCGTCAAACTGGAATTTGTTTTGTATCGTAAGAACGTTACCCTCGCCGAACTCGAAGCCATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGGGGCAGCAACAGCTATTATCACTGCCGACCAATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGAATGACACCTTAGGCGTTGAGATCCATGAATGGCTGAGCGAGTCTGGTAATGGGGAA',\n 'ATGACATCATTACGAGACGGGATAGTTAAATGGATGATT',\n 'ATGATT',\n 'ATGGCGAAGTTTTACTCTCTTACGGGCGTCAGG',\n 'ATGCAT',\n 'ATGCGCTGGGTTGTATTATCGATAACCAGCGAGGGGCAACGC',\n 'ATGTACTGCGACTCTCTTTCCAGGCCACAGGGT',\n 'ATGTCCTCGAATAAAACAGAAAAACCGACTAAAAAACGGCTGGAAGACTCCGCTAAAAAAGGCCAGTCATTTAAAAGTAAAGATCTCATTATCGCCTGCCTGACGCTGGGAGGAATTGCCTATCTGGTGTCGTATGGCTCATTTAATGAGTTTATGGGGATAATTAAGATCATTATTGCGGATAATTTTGATCAGAGCATGGCTGACTACAGTTTGGCCGTTTTTGGGATAGGGTTAAAATATCTGATTCCATTTATGCTGCTCTGCTTAGTGTGTTCCGCATTACCGGCGTTATTACAGGCCGGTTTTGTGCTGGCGACAGAAGCATTAAAGCCTAATTTATCGGCGTTAAACCCGGTAGAAGGGGCAAAAAAACTTTTTAGTATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGGGGATAATTAAGATCATTATTGCGGATAATTTTGATCAGAGCATGGCTGACTACAGTTTGGCCGTTTTTGGGATAGGGTTAAAATATCTGATTCCATTTATGCTGCTCTGCTTAGTGTGTTCCGCATTACCGGCGTTATTACAGGCCGGTTTTGTGCTGGCGACAGAAGCATTAAAGCCTAATTTATCGGCGTTAAACCCGGTAGAAGGGGCAAAAAAACTTTTTAGTATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGGCTGACTACAGTTTGGCCGTTTTTGGGATAGGGTTAAAATATCTGATTCCATTTATGCTGCTCTGCTTAGTGTGTTCCGCATTACCGGCGTTATTACAGGCCGGTTTTGTGCTGGCGACAGAAGCATTAAAGCCTAATTTATCGGCGTTAAACCCGGTAGAAGGGGCAAAAAAACTTTTTAGTATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGCTGCTCTGCTTAGTGTGTTCCGCATTACCGGCGTTATTACAGGCCGGTTTTGTGCTGGCGACAGAAGCATTAAAGCCTAATTTATCGGCGTTAAACCCGGTAGAAGGGGCAAAAAAACTTTTTAGTATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGCGCACGGTTAAAGATACGGTCAAAACCCTACTGTATCTCTCATCCTTTGTGGTGGCCGCCATCATTTGCTGGAAGAAATATAAGGTTGAAATCTTTTCTCAGCTAAATGGCAATATTGTAGGTATTGCCGTCATTTGGCGTGAACTTCTCCTCGCATTGGTATTAACTTGCCTTGCTTGCGCATTGATTGTCTTATTATTGGATGCTATTGCGGAATATTTCCTGACCATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGAAAGATATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGAAAATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGGATAAGGAAGAAGTGAAGCGTGAAATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGAAGGAGCAGGAAGGGAACCCAGAGGTTAAATCTAAAAGACGTGAAGTTCATATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGGAAATTCTGTCTGAACAGGTGAAATCTGATATTGAAAACTCACGCCTGATTGTTGCCAACCCCACGCATATTACGATCGGGATTTATTTTAAACCCGAATTGATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGCCGATTCCGATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGATCTCGGTGTATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTGTGATCGTCGATATCAAACTGGCGCGCAGTCTTTTCAAAACCCATCGCCGTTATGATCTGGTGAGTCTGGAAGAAATTGATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGGTAGAAAACGCGGGCAAAGACGTTATTCAGCCACAAGAAAACGAGGTACGGCAT',\n 'ATGTCAGCGAAGAACGTGTTGCGGAAA',\n 'ATGCCGTTAGTGAAGGCGCCACGC',\n 'ATGGGATCC',\n 'ATGGGCATTTTTGCCTCCGCAGGATGCGGTAAGACCATGCTGATGCATATGCTGATCGAGCAAACGGAGGCGGATGTCTTTGTTATCGGTCTTATCGGTGAACGAGGCCGTGAGGTCACTGAATTCGTGGATATGTTGCGCGCTTCGCATAAGAAAGAAAAATGCGTGCTGGTTTTTGCCACTTCCGATTTCCCCTCGGTCGATCGCTGCAATGCGGCGCAACTGGCGACAACCGTAGCGGAATATTTTCGCGACCAGGGAAAACGGGTCGTGCTTTTTATCGATTCCATGACCCGTTATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGATAATTTGCCCCGCTTGCTGGAACGCCCAGGGGCGACCAGCGAGGGAAGCATTACTGCCTTTTATACGGTACTGCTGGAAAGCGAGGAAGAGGCGGACCCGATGGCGGATGAAATTCGCTCTATCCTTGACGGTCACCTGTATCTGAGCAGAAAGCTGGCCGGGCAGGGACATTACCCGGCAATCGATGTACTGAAAAGCGTAAGCCGCGTTTTTGGACAAGTCACGACGCCGACACATGCTGAACAGGCATCTGCCGTGCGTAAATTAATGACGCGTTTGGAAGAGCTCCAGCTTTTCATTGACTTGGGAGAATATCGTCCTGGCGAAAATATCGATAACGATCGGGCGATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGCTGATGCATATGCTGATCGAGCAAACGGAGGCGGATGTCTTTGTTATCGGTCTTATCGGTGAACGAGGCCGTGAGGTCACTGAATTCGTGGATATGTTGCGCGCTTCGCATAAGAAAGAAAAATGCGTGCTGGTTTTTGCCACTTCCGATTTCCCCTCGGTCGATCGCTGCAATGCGGCGCAACTGGCGACAACCGTAGCGGAATATTTTCGCGACCAGGGAAAACGGGTCGTGCTTTTTATCGATTCCATGACCCGTTATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGATAATTTGCCCCGCTTGCTGGAACGCCCAGGGGCGACCAGCGAGGGAAGCATTACTGCCTTTTATACGGTACTGCTGGAAAGCGAGGAAGAGGCGGACCCGATGGCGGATGAAATTCGCTCTATCCTTGACGGTCACCTGTATCTGAGCAGAAAGCTGGCCGGGCAGGGACATTACCCGGCAATCGATGTACTGAAAAGCGTAAGCCGCGTTTTTGGACAAGTCACGACGCCGACACATGCTGAACAGGCATCTGCCGTGCGTAAATTAATGACGCGTTTGGAAGAGCTCCAGCTTTTCATTGACTTGGGAGAATATCGTCCTGGCGAAAATATCGATAACGATCGGGCGATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGCATATGCTGATCGAGCAAACGGAGGCGGATGTCTTTGTTATCGGTCTTATCGGTGAACGAGGCCGTGAGGTCACTGAATTCGTGGATATGTTGCGCGCTTCGCATAAGAAAGAAAAATGCGTGCTGGTTTTTGCCACTTCCGATTTCCCCTCGGTCGATCGCTGCAATGCGGCGCAACTGGCGACAACCGTAGCGGAATATTTTCGCGACCAGGGAAAACGGGTCGTGCTTTTTATCGATTCCATGACCCGTTATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGATAATTTGCCCCGCTTGCTGGAACGCCCAGGGGCGACCAGCGAGGGAAGCATTACTGCCTTTTATACGGTACTGCTGGAAAGCGAGGAAGAGGCGGACCCGATGGCGGATGAAATTCGCTCTATCCTTGACGGTCACCTGTATCTGAGCAGAAAGCTGGCCGGGCAGGGACATTACCCGGCAATCGATGTACTGAAAAGCGTAAGCCGCGTTTTTGGACAAGTCACGACGCCGACACATGCTGAACAGGCATCTGCCGTGCGTAAATTAATGACGCGTTTGGAAGAGCTCCAGCTTTTCATTGACTTGGGAGAATATCGTCCTGGCGAAAATATCGATAACGATCGGGCGATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGCTGATCGAGCAAACGGAGGCGGATGTCTTTGTTATCGGTCTTATCGGTGAACGAGGCCGTGAGGTCACTGAATTCGTGGATATGTTGCGCGCTTCGCATAAGAAAGAAAAATGCGTGCTGGTTTTTGCCACTTCCGATTTCCCCTCGGTCGATCGCTGCAATGCGGCGCAACTGGCGACAACCGTAGCGGAATATTTTCGCGACCAGGGAAAACGGGTCGTGCTTTTTATCGATTCCATGACCCGTTATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGATAATTTGCCCCGCTTGCTGGAACGCCCAGGGGCGACCAGCGAGGGAAGCATTACTGCCTTTTATACGGTACTGCTGGAAAGCGAGGAAGAGGCGGACCCGATGGCGGATGAAATTCGCTCTATCCTTGACGGTCACCTGTATCTGAGCAGAAAGCTGGCCGGGCAGGGACATTACCCGGCAATCGATGTACTGAAAAGCGTAAGCCGCGTTTTTGGACAAGTCACGACGCCGACACATGCTGAACAGGCATCTGCCGTGCGTAAATTAATGACGCGTTTGGAAGAGCTCCAGCTTTTCATTGACTTGGGAGAATATCGTCCTGGCGAAAATATCGATAACGATCGGGCGATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGTTGCGCGCTTCGCATAAGAAAGAAAAATGCGTGCTGGTTTTTGCCACTTCCGATTTCCCCTCGGTCGATCGCTGCAATGCGGCGCAACTGGCGACAACCGTAGCGGAATATTTTCGCGACCAGGGAAAACGGGTCGTGCTTTTTATCGATTCCATGACCCGTTATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGATAATTTGCCCCGCTTGCTGGAACGCCCAGGGGCGACCAGCGAGGGAAGCATTACTGCCTTTTATACGGTACTGCTGGAAAGCGAGGAAGAGGCGGACCCGATGGCGGATGAAATTCGCTCTATCCTTGACGGTCACCTGTATCTGAGCAGAAAGCTGGCCGGGCAGGGACATTACCCGGCAATCGATGTACTGAAAAGCGTAAGCCGCGTTTTTGGACAAGTCACGACGCCGACACATGCTGAACAGGCATCTGCCGTGCGTAAATTAATGACGCGTTTGGAAGAGCTCCAGCTTTTCATTGACTTGGGAGAATATCGTCCTGGCGAAAATATCGATAACGATCGGGCGATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGACCCGTTATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGATAATTTGCCCCGCTTGCTGGAACGCCCAGGGGCGACCAGCGAGGGAAGCATTACTGCCTTTTATACGGTACTGCTGGAAAGCGAGGAAGAGGCGGACCCGATGGCGGATGAAATTCGCTCTATCCTTGACGGTCACCTGTATCTGAGCAGAAAGCTGGCCGGGCAGGGACATTACCCGGCAATCGATGTACTGAAAAGCGTAAGCCGCGTTTTTGGACAAGTCACGACGCCGACACATGCTGAACAGGCATCTGCCGTGCGTAAATTAATGACGCGTTTGGAAGAGCTCCAGCTTTTCATTGACTTGGGAGAATATCGTCCTGGCGAAAATATCGATAACGATCGGGCGATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGGCGGATGAAATTCGCTCTATCCTTGACGGTCACCTGTATCTGAGCAGAAAGCTGGCCGGGCAGGGACATTACCCGGCAATCGATGTACTGAAAAGCGTAAGCCGCGTTTTTGGACAAGTCACGACGCCGACACATGCTGAACAGGCATCTGCCGTGCGTAAATTAATGACGCGTTTGGAAGAGCTCCAGCTTTTCATTGACTTGGGAGAATATCGTCCTGGCGAAAATATCGATAACGATCGGGCGATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGACGCGTTTGGAAGAGCTCCAGCTTTTCATTGACTTGGGAGAATATCGTCCTGGCGAAAATATCGATAACGATCGGGCGATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGCAGATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGCGGGATAGCCTGAAAGCCTGGTTATGCCAGCCGGTAGCGCAGTATTCATCCTTTGATGACACGTTGAGCGGTATGAATGCATTCGCTGACCAGAAT',\n 'ATGAATGCATTCGCTGACCAGAAT',\n 'ATGGGCGATGTGTCAGCTGTCAGTTCATCCGGGAACATTTTACTGCCGCAGCAGGATGAGGTTGGCGGTTTATCAGAAGCATTAAAAAAAGCGGTGGAAAAACATAAGACAGAATATTCCGGTGATAAAAAAGATCGCGACTATGGCGATGCTTTCGTAATGCATAAAGAAACGGCTTTACCGTTATTACTGGCGGCATGGCGACATGGCGCGCCAGCGAAATCAGAACATCACAATGGCAACGTTTCTGGTCTGCATCATAACGGAAAAAGCGAACTCAGGATTGCTGAAAAACTGTTGAAAGTCACTGCTGAAAAATCTGTCGGTTTGATCTCTGCGGAGGCCAAAGTAGATAAATCCGCAGCGTTGCTATCGTCTAAAAATAGGCCGTTAGAAAGCGTAAGCGGTAAAAAATTATCTGCTGATTTAAAAGCTGTGGAATCCGTTAGTGAAGTAACCGATAACGCCACGGGAATCTCTGACGATAATATCAAGGCATTGCCTGGGGATAATAAAGCCATCGCGGGCGAAGGCGTTCGTAAAGAGGGCGCGCCGCTGGCGCGGGATGTCGCACCTGCCCGAATGGCCGCAGCCAATACCGGTAAGCCTGAAGATAAAGATCATAAAAAGGTTAAAGATGTTTCTCAGCTTCCGCTGCAACCAACCACTATCGCCGATCTTAGCCAATTAACCGGCGGCGATGAAAAAATGCCTTTAGCGGCGCAATCAAAGCCGATGATGACTATTTTTCCCACTGCCGATGGCGTGAAAGGAGAGGATAGCTCGCTGACTTACCGTTTTCAGCGCTGGGGAAATGACTATTCCGTCAATATTCAGGCGCGGCAAGCAGGGGAGTTTTCGTTAATACCGTCAAATACGCAGGTTGAACATCGTTTGCATGATCAATGGCAAAACGGTAATCCCCAGCGCTGGCACCTGACGCGAGACGATCAACAAAATCCGCAGCAGCAACAGCACAGACAGCAATCTGGCGAGGAGGATGACGCC',\n 'ATGCATAAAGAAACGGCTTTACCGTTATTACTGGCGGCATGGCGACATGGCGCGCCAGCGAAATCAGAACATCACAATGGCAACGTTTCTGGTCTGCATCATAACGGAAAAAGCGAACTCAGGATTGCTGAAAAACTGTTGAAAGTCACTGCTGAAAAATCTGTCGGTTTGATCTCTGCGGAGGCCAAAGTAGATAAATCCGCAGCGTTGCTATCGTCTAAAAATAGGCCGTTAGAAAGCGTAAGCGGTAAAAAATTATCTGCTGATTTAAAAGCTGTGGAATCCGTTAGTGAAGTAACCGATAACGCCACGGGAATCTCTGACGATAATATCAAGGCATTGCCTGGGGATAATAAAGCCATCGCGGGCGAAGGCGTTCGTAAAGAGGGCGCGCCGCTGGCGCGGGATGTCGCACCTGCCCGAATGGCCGCAGCCAATACCGGTAAGCCTGAAGATAAAGATCATAAAAAGGTTAAAGATGTTTCTCAGCTTCCGCTGCAACCAACCACTATCGCCGATCTTAGCCAATTAACCGGCGGCGATGAAAAAATGCCTTTAGCGGCGCAATCAAAGCCGATGATGACTATTTTTCCCACTGCCGATGGCGTGAAAGGAGAGGATAGCTCGCTGACTTACCGTTTTCAGCGCTGGGGAAATGACTATTCCGTCAATATTCAGGCGCGGCAAGCAGGGGAGTTTTCGTTAATACCGTCAAATACGCAGGTTGAACATCGTTTGCATGATCAATGGCAAAACGGTAATCCCCAGCGCTGGCACCTGACGCGAGACGATCAACAAAATCCGCAGCAGCAACAGCACAGACAGCAATCTGGCGAGGAGGATGACGCC',\n 'ATGGCCGCAGCCAATACCGGTAAGCCTGAAGATAAAGATCATAAAAAGGTTAAAGATGTTTCTCAGCTTCCGCTGCAACCAACCACTATCGCCGATCTTAGCCAATTAACCGGCGGCGATGAAAAAATGCCTTTAGCGGCGCAATCAAAGCCGATGATGACTATTTTTCCCACTGCCGATGGCGTGAAAGGAGAGGATAGCTCGCTGACTTACCGTTTTCAGCGCTGGGGAAATGACTATTCCGTCAATATTCAGGCGCGGCAAGCAGGGGAGTTTTCGTTAATACCGTCAAATACGCAGGTTGAACATCGTTTGCATGATCAATGGCAAAACGGTAATCCCCAGCGCTGGCACCTGACGCGAGACGATCAACAAAATCCGCAGCAGCAACAGCACAGACAGCAATCTGGCGAGGAGGATGACGCC',\n 'ATGCCTTTAGCGGCGCAATCAAAGCCGATGATGACTATTTTTCCCACTGCCGATGGCGTGAAAGGAGAGGATAGCTCGCTGACTTACCGTTTTCAGCGCTGGGGAAATGACTATTCCGTCAATATTCAGGCGCGGCAAGCAGGGGAGTTTTCGTTAATACCGTCAAATACGCAGGTTGAACATCGTTTGCATGATCAATGGCAAAACGGTAATCCCCAGCGCTGGCACCTGACGCGAGACGATCAACAAAATCCGCAGCAGCAACAGCACAGACAGCAATCTGGCGAGGAGGATGACGCC',\n 'ATGATGACTATTTTTCCCACTGCCGATGGCGTGAAAGGAGAGGATAGCTCGCTGACTTACCGTTTTCAGCGCTGGGGAAATGACTATTCCGTCAATATTCAGGCGCGGCAAGCAGGGGAGTTTTCGTTAATACCGTCAAATACGCAGGTTGAACATCGTTTGCATGATCAATGGCAAAACGGTAATCCCCAGCGCTGGCACCTGACGCGAGACGATCAACAAAATCCGCAGCAGCAACAGCACAGACAGCAATCTGGCGAGGAGGATGACGCC',\n 'ATGACTATTTTTCCCACTGCCGATGGCGTGAAAGGAGAGGATAGCTCGCTGACTTACCGTTTTCAGCGCTGGGGAAATGACTATTCCGTCAATATTCAGGCGCGGCAAGCAGGGGAGTTTTCGTTAATACCGTCAAATACGCAGGTTGAACATCGTTTGCATGATCAATGGCAAAACGGTAATCCCCAGCGCTGGCACCTGACGCGAGACGATCAACAAAATCCGCAGCAGCAACAGCACAGACAGCAATCTGGCGAGGAGGATGACGCC',\n 'ATGGCCGGGAAGCGACGCTGGAATATCCGACGCGACAGGGAATGTGGGTTCGGT',\n 'ATGCAGAAAAACGGTGGTCGGCCTGGATTAAACCTGGGGACTGGCTTGAGCATGTCTCTCCCGCTCTGGCTGGGGCGGCGGTTTCTGCTGGCGCTGAGCACCTGGTCGTTCCCTGGCTTGCTGCAACAGAGCGACCGTTTGAGTTGCCCGTGCCGCATTTGTCCTGTCGGCGTTTATGCG',\n 'ATGTCTCTCCCGCTCTGGCTGGGGCGGCGGTTTCTGCTGGCGCTGAGCACCTGGTCGTTCCCTGGCTTGCTGCAACAGAGCGACCGTTTGAGTTGCCCGTGCCGCATTTGTCCTGTCGGCGTTTATGCG',\n 'ATGTACTCC',\n 'ATGCTGAACTTAACGTTGAAATTATGGCGAATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGA',\n 'ATGGTGTTTTGCTGGGTAATGGCGAACTGGTACAGA',\n 'ATGGCGAACTGGTACAGA',\n 'ATGACACCT',\n 'ATGAATGGC',\n 'ATGGGGAATGATATCTCATTAATTGCCTTACTGGCATTTTCCACCCTGTTGCCATTTATTATTGCGTCAGGAACCTGTTTCGTTAAATTTTCTATTGTATTTGTCATGGTGCGTAACGCCCTGGGATTACAGCAGATACCTTCAAATATGACGCTTAACGGCGTCGCATTGCTGCTTTCTATGTTTGTTATGTGGCCCATAATGCATGATGCCTACGTCTATTTTGAGGACGAAGATGTCACCTTTAATGATATTTCATCATTAAGTAAACACGTTGATGAAGGTCTGGATGGTTATCGCGATTATCTGATCAAATATTCAGATCGCGAGTTAGTTCAGTTTTTTGAAAACGCGCAACTGAAGCGTCAGTATGGAGAAGAGACCGAGACGGTAAAGCGTGACAAAGATGAAATTGAAAAACCTTCAATATTTGCGTTATTACCTGCTTATGCGCTGAGCGAAATAAAAAGCGCGTTTAAAATTGGTTTTTATCTTTATTTGCCCTTTGTCGTCGTCGACCTGGTGGTATCCAGCGTGCTACTGGCGCTGGGGATGATGATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGGTGCGTAACGCCCTGGGATTACAGCAGATACCTTCAAATATGACGCTTAACGGCGTCGCATTGCTGCTTTCTATGTTTGTTATGTGGCCCATAATGCATGATGCCTACGTCTATTTTGAGGACGAAGATGTCACCTTTAATGATATTTCATCATTAAGTAAACACGTTGATGAAGGTCTGGATGGTTATCGCGATTATCTGATCAAATATTCAGATCGCGAGTTAGTTCAGTTTTTTGAAAACGCGCAACTGAAGCGTCAGTATGGAGAAGAGACCGAGACGGTAAAGCGTGACAAAGATGAAATTGAAAAACCTTCAATATTTGCGTTATTACCTGCTTATGCGCTGAGCGAAATAAAAAGCGCGTTTAAAATTGGTTTTTATCTTTATTTGCCCTTTGTCGTCGTCGACCTGGTGGTATCCAGCGTGCTACTGGCGCTGGGGATGATGATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGACGCTTAACGGCGTCGCATTGCTGCTTTCTATGTTTGTTATGTGGCCCATAATGCATGATGCCTACGTCTATTTTGAGGACGAAGATGTCACCTTTAATGATATTTCATCATTAAGTAAACACGTTGATGAAGGTCTGGATGGTTATCGCGATTATCTGATCAAATATTCAGATCGCGAGTTAGTTCAGTTTTTTGAAAACGCGCAACTGAAGCGTCAGTATGGAGAAGAGACCGAGACGGTAAAGCGTGACAAAGATGAAATTGAAAAACCTTCAATATTTGCGTTATTACCTGCTTATGCGCTGAGCGAAATAAAAAGCGCGTTTAAAATTGGTTTTTATCTTTATTTGCCCTTTGTCGTCGTCGACCTGGTGGTATCCAGCGTGCTACTGGCGCTGGGGATGATGATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGTTTGTTATGTGGCCCATAATGCATGATGCCTACGTCTATTTTGAGGACGAAGATGTCACCTTTAATGATATTTCATCATTAAGTAAACACGTTGATGAAGGTCTGGATGGTTATCGCGATTATCTGATCAAATATTCAGATCGCGAGTTAGTTCAGTTTTTTGAAAACGCGCAACTGAAGCGTCAGTATGGAGAAGAGACCGAGACGGTAAAGCGTGACAAAGATGAAATTGAAAAACCTTCAATATTTGCGTTATTACCTGCTTATGCGCTGAGCGAAATAAAAAGCGCGTTTAAAATTGGTTTTTATCTTTATTTGCCCTTTGTCGTCGTCGACCTGGTGGTATCCAGCGTGCTACTGGCGCTGGGGATGATGATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGTGGCCCATAATGCATGATGCCTACGTCTATTTTGAGGACGAAGATGTCACCTTTAATGATATTTCATCATTAAGTAAACACGTTGATGAAGGTCTGGATGGTTATCGCGATTATCTGATCAAATATTCAGATCGCGAGTTAGTTCAGTTTTTTGAAAACGCGCAACTGAAGCGTCAGTATGGAGAAGAGACCGAGACGGTAAAGCGTGACAAAGATGAAATTGAAAAACCTTCAATATTTGCGTTATTACCTGCTTATGCGCTGAGCGAAATAAAAAGCGCGTTTAAAATTGGTTTTTATCTTTATTTGCCCTTTGTCGTCGTCGACCTGGTGGTATCCAGCGTGCTACTGGCGCTGGGGATGATGATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGCATGATGCCTACGTCTATTTTGAGGACGAAGATGTCACCTTTAATGATATTTCATCATTAAGTAAACACGTTGATGAAGGTCTGGATGGTTATCGCGATTATCTGATCAAATATTCAGATCGCGAGTTAGTTCAGTTTTTTGAAAACGCGCAACTGAAGCGTCAGTATGGAGAAGAGACCGAGACGGTAAAGCGTGACAAAGATGAAATTGAAAAACCTTCAATATTTGCGTTATTACCTGCTTATGCGCTGAGCGAAATAAAAAGCGCGTTTAAAATTGGTTTTTATCTTTATTTGCCCTTTGTCGTCGTCGACCTGGTGGTATCCAGCGTGCTACTGGCGCTGGGGATGATGATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGATGATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGAGTCCGGTGACGATATCTACACCTATTAAGCTGGTGCTTTTTGTCGCGCTTGATGGCTGGACCTTACTGTCTAAGGGATTGATATTACAGTATATGGACATTGCAACA',\n 'ATGGACATTGCAACA',\n 'ATGCTTGTTTTTACTGTCTGGCTGGTATGGCGAAGTTTTACTCTCTTACGGGCGTCAGGTGATATTCCTGGCGTTGGC',\n 'ATGGCCGCATGCATTGAACGAGGCGCCGCCGTTTTTATCGGTGGCGATGATCCCGTTAGTTCTGCAAGAAGCGGCGGTAGGCGTCATGCTGGGCTGTCTGCTGTCATGGCCTTTTTGGGTTATGCATGCGCTGGGTTGTATTATCGA',\n 'ATGGCCTTTTTGGGTTATGCATGCGCTGGGTTGTATTATCGA',\n 'ATGCGATCCGATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTAT',\n 'ATGGCTCATTTAATGAGTTTATGGGGA',\n 'ATGAGTTTATGGGGA',\n 'ATGGCAATATTG',\n 'ATGCTATTGCGGAATATTTCC',\n 'ATGAAACGAATCAGCGCGCACTGGCCGTCCGCGCCTATGCGGAGAAGGTTGGCGTACCTG',\n 'ATGCGGAGAAGGTTGGCGTACCTG',\n 'ATGATCTGG',\n 'ATGAAGTTTTACGTCTTCTGGTTTGGCTGGAAGAGG',\n 'ATGTATTCATTGGGCGTTTTT',\n 'ATGTTCACTAACCACCGTCGGGGTTTAATAACTGCATCAGATAAACGCAGTCGT',\n 'ATGTCGCACCGCCGTCTTACGCTTCACGCGTTGGCGTCCGTGAACCGC',\n 'ATGTCTTTGTTATCGGTCTTATCGGTGAACGAGGCCGTGAGGTCACTGAATTCGTGGATATGTTGCGCGCTTCGCATAAGAAAGAAAAATGCGTGCTGGTTTTTGCCACTTCCGATTTCCCCTCGGTCGATCGCTGCAATGCGGCGCAACTGGCGACAACCG',\n 'ATGCGGCGCAACTGGCGACAACCG',\n 'ATGCGCGTGCTTTGCGAGACGTGGCACTGGCGTCGGGAGAGCGTCCGGCTCGTCGAGGTTATCCCGCCTCCGTATTCGATAATTTGCCCCGCTTGCTGGAACGCCCAGGGGCGACCAGCGAGGGAAGCATTACTGCCTTTTATACGGTACTGCTGGAAAGCGAGGAAGAGGCGGACCCGATGGCGGATGAAATTCGCTCTATCCTTGACGGTCACCTGTATC',\n 'ATGAAATTCGCTCTATCCTTGACGGTCACCTGTATC',\n 'ATGTAC',\n 'ATGCTGAACAGGCATCTGCCGTGCGTAAAT',\n 'ATGACACGT',\n 'ATGCATTCGCTGACCAGAATTAAAGTATTGCAGCGGCGCTGTACGGTATTTCATTCACAGTGTGAGTCGATATTACTTCGCTATCAGGATGAGGACCGCGGGCTGCAGGCCGAGGAGGAGGCGATCCTTGAACAAATAGCGGGTCTGAAATTGTTATTAGATACGCTGCGTGCAGAAAACAGACAGCTCAGTCGTGAGGAAATTTATACGTTATTACGTAAGCAGTCTATTGTTCGCCGGCAGATAAAAGATTTAGAACTCCAGATTATACAAATTCAGGAAAAACGGAGCGAGCTGGAAAAGAAAAGGGAAGAGTTTCAGAAAAAAAGTAAATATTGGTTGCGCAAAGAAGGGAACTATCAACGCTGGATAATCCGTCAGAAAAGATTCTATATCCAGCGAGAGATACAGCAGGAAGAGGCCGAGTCAGAGGAGATAATT',\n 'ATGTGTCAGCTGTCAGTTCATCCGGGAACATTTTACTGCCGCAGCAGGATGAGGTTGGCGGTTTATCAGAAGCAT',\n 'ATGAGGTTGGCGGTTTATCAGAAGCAT',\n 'ATGGCGATGCTTTCG',\n 'ATGCTTTCG',\n 'ATGGCGCGCCAGCGAAATCAGAACATCACAATGGCAACGTTTCTGGTCTGCATCATAACGGAAAAAGCGAACTCAGGATTGCTGAAAAACTGT',\n 'ATGGCAACGTTTCTGGTCTGCATCATAACGGAAAAAGCGAACTCAGGATTGCTGAAAAACTGT',\n 'ATGTCGCACCTGCCCGAATGGCCGCAGCCAATACCGGTAAGCCTGAAGATAAAGATCATAAAAAGGTTAAAGATGTTTCTCAGCTTCCGCTGCAACCAACCACTATCGCCGATCTTAGCCAAT',\n 'ATGTTTCTCAGCTTCCGCTGCAACCAACCACTATCGCCGATCTTAGCCAAT',\n 'ATGAAAAAATGCCTT',\n 'ATGGCG',\n 'ATGACTATTCCGTCAATATTCAGGCGCGGCAAGCAGGGGAGTTTTCGT',\n 'ATGATCAATGGCAAAACGGTAATCCCCAGCGCTGGCACC',\n 'ATGACGCCTGATGTCATTGCGTGTGAGACAGAT',\n 'ATGGCTATTGGCGCAAACCGCGACAGAATGCCAGCGCCATGGCCGGGAAGCGACGCTGGAATATCCGACGCGACAGGGAATGTGGGTTCGGTTGAGCGATGCAGAAAAACGGTGGTCGGCCTGGAT',\n 'ATGCCAGCGCCATGGCCGGGAAGCGACGCTGGAATATCCGACGCGACAGGGAATGTGGGTTCGGTTGAGCGATGCAGAAAAACGGTGGTCGGCCTGGAT',\n 'ATGCGTAGAGAACCCCGTACCGGGAAGCGCGCTGCCGGAAGGGAAATTGTTGCACATTATGAGCGATCGGGGCGGCCTGTGGTT',\n 'ATGGCTGAGCGAGTCTGG',\n 'ATGATATCTCAT',\n 'ATGATGCCTACGTCTATTTTGAGGACGAAGATGTCACCTTTAATGATATTTCATCAT',\n 'ATGCCTACGTCTATTTTGAGGACGAAGATGTCACCTTTAATGATATTTCATCAT',\n 'ATGTCACCTTTAATGATATTTCATCAT',\n 'ATGATATTTCATCAT',\n 'ATGAAGGTCTGGATGGTTATCGCGATTATC',\n 'ATGGTTATCGCGATTATC',\n 'ATGGAGAAGAGACCGAGACGG',\n 'ATGAAATTGAAAAACCTTCAATATTTGCGTTATTACCTGCTTATGCGC',\n 'ATGCGC',\n 'ATGGCTGGACCTTACTGTCTAAGGGAT',\n 'ATGGATGATTTAGTGTTTGCAGGTAATAAGGCGCTCTATCTTGTTTTGATCCTGTCAGGGTGGCCGACGATTGTCGCAACGATTATCGGCCTCCTGGTAGGGTTATTCCAGACGGTAACGCAATTACAGGAACAGACGCTGCCTTTTGGCATTAAATTACTTGGCGTGTGTTTATGCTTGTTTTTACTGTCTGGCTGGTATGGCGAAGTTTTACTCTCTTACGGGCGTCAGGTGATATTCCTGGCGTTGGCTAAGGGG',\n 'ATGTTTTACGCGTTGTACTTTGAAATTCATCACCTGGTTGCGTCTGCGGCGCTAGGGTTTGCTCGCGTGGCGCCGATTTTTTTCTTCCTGCCGTTTTTGAATAGCGGGGTATTAAGCGGTGCGCCGAGAAACGCCATTATCATCCTGGTGGCATTGGGAGTATGGCCGCATGCATTGAACGAGGCGCCGCCGTTTTTATCGGTGGCGATGATCCCGTTAGTTCTGCAAGAAGCGGCGGTAGGCGTCATGCTGGGCTGTCTGCTGTCATGGCCTTTTTGGGTTATGCATGCGCTGGGTTGTATTATCGATAACCAGCGAGGGGCAACGCTAAGTAGTAGTATCGATCCGGCAAACGGTATTGATACCTCGGAAATGGCTAATTTCCTGAATATGTTTGCCGCTGTCGTTTATTTACAAAACGGCGGTCTGGTCACGATGGTTGACGTGTTAAATAAAAGCTATCAGCTATGCGATCCGATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTATTAATCAGGTGGCTCAAAACGCCTTGGTTCTGGCCAGTCCGGTGGTATTAGTGCTGTTGCTGTCAGAAGTATTCCTGGGTTTATTGTCGCGCTTTGCTCCGCAAATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGATCCCGTTAGTTCTGCAAGAAGCGGCGGTAGGCGTCATGCTGGGCTGTCTGCTGTCATGGCCTTTTTGGGTTATGCATGCGCTGGGTTGTATTATCGATAACCAGCGAGGGGCAACGCTAAGTAGTAGTATCGATCCGGCAAACGGTATTGATACCTCGGAAATGGCTAATTTCCTGAATATGTTTGCCGCTGTCGTTTATTTACAAAACGGCGGTCTGGTCACGATGGTTGACGTGTTAAATAAAAGCTATCAGCTATGCGATCCGATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTATTAATCAGGTGGCTCAAAACGCCTTGGTTCTGGCCAGTCCGGTGGTATTAGTGCTGTTGCTGTCAGAAGTATTCCTGGGTTTATTGTCGCGCTTTGCTCCGCAAATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGCTGGGCTGTCTGCTGTCATGGCCTTTTTGGGTTATGCATGCGCTGGGTTGTATTATCGATAACCAGCGAGGGGCAACGCTAAGTAGTAGTATCGATCCGGCAAACGGTATTGATACCTCGGAAATGGCTAATTTCCTGAATATGTTTGCCGCTGTCGTTTATTTACAAAACGGCGGTCTGGTCACGATGGTTGACGTGTTAAATAAAAGCTATCAGCTATGCGATCCGATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTATTAATCAGGTGGCTCAAAACGCCTTGGTTCTGGCCAGTCCGGTGGTATTAGTGCTGTTGCTGTCAGAAGTATTCCTGGGTTTATTGTCGCGCTTTGCTCCGCAAATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGCATGCGCTGGGTTGTATTATCGATAACCAGCGAGGGGCAACGCTAAGTAGTAGTATCGATCCGGCAAACGGTATTGATACCTCGGAAATGGCTAATTTCCTGAATATGTTTGCCGCTGTCGTTTATTTACAAAACGGCGGTCTGGTCACGATGGTTGACGTGTTAAATAAAAGCTATCAGCTATGCGATCCGATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTATTAATCAGGTGGCTCAAAACGCCTTGGTTCTGGCCAGTCCGGTGGTATTAGTGCTGTTGCTGTCAGAAGTATTCCTGGGTTTATTGTCGCGCTTTGCTCCGCAAATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGGCTAATTTCCTGAATATGTTTGCCGCTGTCGTTTATTTACAAAACGGCGGTCTGGTCACGATGGTTGACGTGTTAAATAAAAGCTATCAGCTATGCGATCCGATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTATTAATCAGGTGGCTCAAAACGCCTTGGTTCTGGCCAGTCCGGTGGTATTAGTGCTGTTGCTGTCAGAAGTATTCCTGGGTTTATTGTCGCGCTTTGCTCCGCAAATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGTTTGCCGCTGTCGTTTATTTACAAAACGGCGGTCTGGTCACGATGGTTGACGTGTTAAATAAAAGCTATCAGCTATGCGATCCGATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTATTAATCAGGTGGCTCAAAACGCCTTGGTTCTGGCCAGTCCGGTGGTATTAGTGCTGTTGCTGTCAGAAGTATTCCTGGGTTTATTGTCGCGCTTTGCTCCGCAAATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGGTTGACGTGTTAAATAAAAGCTATCAGCTATGCGATCCGATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTATTAATCAGGTGGCTCAAAACGCCTTGGTTCTGGCCAGTCCGGTGGTATTAGTGCTGTTGCTGTCAGAAGTATTCCTGGGTTTATTGTCGCGCTTTGCTCCGCAAATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGAACGAGTGCACGCCTTCATTACCGCCGCTATTAACGTTTATTAATCAGGTGGCTCAAAACGCCTTGGTTCTGGCCAGTCCGGTGGTATTAGTGCTGTTGCTGTCAGAAGTATTCCTGGGTTTATTGTCGCGCTTTGCTCCGCAAATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGAACGCTTTTGCGATTTCACTGACGGTAAAAAGCGGTATTGCCGTTTTAATTATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGCTGCTTTATTTCTCTCCGGTACTACCGGACAATGTACTGCGACTCTCTTTCCAGGCCACAGGGTTAAGCAGTTGGTTTTACGAGCGAGGGGCGACGCATGTCCTCGAA',\n 'ATGGATTATCAAAATAATGTCAGCGAAGAACGTGTTGCGGAAATGATTTGGGATGCCGTTAGTGAAGGCGCCACGCTAAAAGACGTTCATGGGATCC',\n 'ATGATTTGGGATGCCGTTAGTGAAGGCGCCACGCTAAAAGACGTTCATGGGATCC',\n 'ATGCCGTACCTCGTTTTCTTGTGGCTGAATAACGTCTTTGCCCGCGTTTTCTACCTCTTCCAGCCAAACCAGAAGACG',\n 'ATGGGTTTTGAAAAGACTGCGCGCCAGTTTGATATCGACGATCACAGGTACGCCAACCTTCTCCGCATAGGCGCGGACGGCCAGTGCGCGCTGATTCGTTTCATACACCGAGATCATCGGAATCGGCATCAATTCGGGTTTAAAATAAATCCCGATCGTAATATGCGTGGGGTTGGCAACAATCAGGCG',\n 'ATGCGTGGGGTTGGCAACAATCAGGCG',\n 'ATGAACTTCACGTCTTTTAGATTTAACCTCTGGGTTCCCTTCCTGCTCCTTCATTTCACGCTTCACTTCTTCCTTATCCATTTTCATATCTTTCATGGTCAGGAAATATTCCGCAATAGCATCCAA',\n 'ATGAAGGCGTGCACTCGTTCATCGGATCGCATAGCTGATAGCTTTTATTTAACACGTCAACCATCG',\n 'ATGACAGCAGACAGCCCAGCA',\n 'ATGCATGCGGCCATACTCCCAATGCCACCAGGA',\n 'ATGCCACCAGGA',\n 'ATGTCCATATACTGTAATATCAATCCCTTAGACAGTAAGGTCCAGCCATCAAGCGCGACAAAAAGCACCAGCTTAATAGGTGTAGATATCGTCACCGGACTCATCATCATCATCCCCAGCGCCAGTAGCACGCTGGATACCACCAGGTCGACGACGACAAAGGGCAAA',\n 'ATGGGCCACATAACAAACATAGAAAGCAGCAATGCGACGCCGTTAAGCGTCATATTTGAAGGTATCTGCTGTAATCCCAGGGCGTTACGCACCATGACAAATACAATAGAAAATTTAACGAAACAGGTTCCTGACGCAATAATAAATGGCAACAGGGTGGAAAATGCCAGTAAGGCAATTAATGAGATATCATTCCCCATTACCAGACTCGCTCAGCCATTCATGGATCTCAACGCC',\n 'ATGACAAATACAATAGAAAATTTAACGAAACAGGTTCCTGACGCAATAATAAATGGCAACAGGGTGGAAAATGCCAGTAAGGCAATTAATGAGATATCATTCCCCATTACCAGACTCGCTCAGCCATTCATGGATCTCAACGCC',\n 'ATGGATCTCAACGCC',\n 'ATGTTGAATATC',\n 'ATGACC',\n 'ATGCTCAAACCACAGGCCGCCCCGATCGCTCATAATGTGCAACAATTTCCCTTCCGGCAGCGCGCTTCCCGGTACGGGGTTCTCTACGCA',\n 'ATGCGGCACGGGCAACTCAAACGGTCGCTCTGTTGCAGCAAGCCAGGGAACGACCAGGTGCTCAGCGCCAGCAGAAACCGCCGCCCCAGCCAGAGCGGGAGAGACATGCTCAAGCCAGTCCCCAGGTTTAATCCAGGCCGACCACCGTTTTTCTGCATCGCTCAACCGAACCCACATTCCCTGTCGCGTCGGATATTCCAGCGTCGCTTCCCGGCCATGGCGCTGGCATTCTGTCGCGGTTTGCGCCAA',\n 'ATGCTCAAGCCAGTCCCCAGGTTTAATCCAGGCCGACCACCGTTTTTCTGCATCGCTCAACCGAACCCACATTCCCTGTCGCGTCGGATATTCCAGCGTCGCTTCCCGGCCATGGCGCTGGCATTCTGTCGCGGTTTGCGCCAA',\n 'ATGGCGCTGGCATTCTGTCGCGGTTTGCGCCAA',\n 'ATGGCTTTATTATCCCCAGGCAATGCCTTGATATTATCGTCAGAGATTCCCGTGGCGTTATCGGTTACTTCACTAACGGATTCCACAGCTTTTAAATCAGCAGATAATTTTTTACCGCTTACGCTTTCTAACGGCCTATTTTTAGACGATAGCAACGCTGCGGATTTATCTACTTTGGCCTCCGCAGAGATCAAACCGACAGATTTTTCAGCAGTGACTTTCAACAGTTTTTCAGCAATCCTGAGTTCGCTTTTTCCGTTA',\n 'ATGTTCCCGGATGAACTGACAGCTGACACATCGCCCATTAAATTATCTCCTCTGACTCGGCCTCTTCCTGCTGTATCTCTCGCTGGATATAGAATCTTTTCTGACGGATTATCCAGCGTTGATAGTTCCCTTCTTTGCGCAACCAATATTTACTTTTTTTCTGAAACTCTTCCCTTTTCTTTTCCAGCTCGCTCCGTTTTTCCTGAATTTGTA',\n 'ATGAAAAGCTGGAGCTCTTCCAAACGCGTCATTAATTTACGCACGGCAGATGCCTGTTCAGCATGTGTCGGCGTCGTGACTTGTCCAAAAACGCGGCTTACGCTTTTCAGTACATCGATTGCCGGG',\n 'ATGCTTCCCTCGCTGGTCGCCCCTGGGCGTTCCAGCAAGCGGGGCAAATTATCGAATACGGAGGCGGGA',\n 'ATGGAATCGATAAAAAGCACGACCCGTTTTCCCTGGTCGCGAAAATATTCCGCTACGGTTGTCGCCAGTTGCGCCGCATTGCAGCGATCGACCGAGGGGAAATCGGAAGTGGCAAAAACCAGCACGCATTTTTCTTTCTTATGCGAAGCGCGCAACATATCCACGAATTCAGTGACCTCACGGCCTCGTTCACCGATAAGACCGATAACAAAGACATCCGCCTCCGTTTGCTCGATCAGCATATGCATCAGCATGGTCTTACCGCATCCTGCGGAGGCAAAAATGCCCATTCGCTGGCCTACGCCACAGGTCAATAACCCGTCAATCGCGCGCACACCGGTAATCAGCGGTTCACGGACGCCAACGCGTGAAGCG',\n 'ATGGTCTTACCGCATCCTGCGGAGGCAAAAATGCCCATTCGCTGGCCTACGCCACAGGTCAATAACCCGTCAATCGCGCGCACACCGGTAATCAGCGGTTCACGGACGCCAACGCGTGAAGCG',\n 'ATGCCCATTCGCTGGCCTACGCCACAGGTCAATAACCCGTCAATCGCGCGCACACCGGTAATCAGCGGTTCACGGACGCCAACGCGTGAAGCG',\n 'ATGAACGTCTTT',\n 'ATGCAGTTATTAAACCCCGACGGTGGT',\n 'ATGAATACATCGCTACTGCCTTACGCGGCTCAATGCCGTACCTCGTTTTCTTGTGGC',\n 'ATGGTCAGGAAATATTCCGCAATAGCATCCAATAATAAGACAATCAATGCGCAAGCAAGGCAAGTTAATACCAATGCGAGGAGAAGTTCACGCCAAATGACGGCAATACCTACAATATTGCCATTTAGC',\n 'ATGACGGCAATACCTACAATATTGCCATTTAGC',\n 'ATGATGGCGGCCACCACAAAGGATGAGAGATACAGTAGGGTTTTGACCGTATCTTTAACCGTGCGCATACTAAAAAGTTTTTTTGCCCCTTCTACCGGGTTTAACGCCGATAAATTAGGCTTTAATGCTTCTGTCGCCAGCACAAAACCGGCCTGTAATAACGCCGGTAATGCGGAACACACTAAGCAGAGCAGCATAAATGGAATCAGATATTTTAACCCTATCCCAAAAACGGCCAAACTG',\n 'ATGGCGGCCACCACAAAGGATGAGAGATACAGTAGGGTTTTGACCGTATCTTTAACCGTGCGCATACTAAAAAGTTTTTTTGCCCCTTCTACCGGGTTTAACGCCGATAAATTAGGCTTTAATGCTTCTGTCGCCAGCACAAAACCGGCCTGTAATAACGCCGGTAATGCGGAACACACTAAGCAGAGCAGCATAAATGGAATCAGATATTTTAACCCTATCCCAAAAACGGCCAAACTG',\n 'ATGCTC',\n 'ATGATCTTAATTATCCCCATAAACTCATTAAATGAGCCATACGACACCAGA',\n 'ATGAGATCTTTACTTTTAAATGACTGGCCTTTTTTAGCGGAGTCTTCCAGCCGTTTTTTAGTCGGTTTTTCTGTTTTATTCGAGGACATGCGTCGCCCCTCGCTCGTAAAACCAACTGCT',\n 'ATGCGTCGCCCCTCGCTCGTAAAACCAACTGCT',\n 'ATGCATAACCCAAAAAGGCCA',\n 'ATGCGGCCATACTCCCAATGCCACCAGGATGATAATGGCGTTTCTCGGCGCACCGCT',\n 'ATGAATTTCAAAGTACAACGCGTAAAACATTTTTTACCCCTTAGCCAACGCCAGGAATATCACCTGACGCCCGTAAGAGAG',\n 'ATGTTGCAATGTCCATATACTGTAATATCAATCCCT',\n 'ATGATGAAATATCAT',\n 'ATGAAATATCAT',\n 'ATGCGACGCCGT',\n 'ATGGCAACAGGGTGGAAAATGCCAGTAAGGCAATTAATGAGATATCATTCCCCATTACCAGACTCGCTCAGCCATTCATGGATCTCAACGCCTAAGGTGTCATTCATCTGTACCAGTTCGCCATTACCCAGCAAAACACCATTCGCCATAATTTCAACGTTAAGTTCAGCATTGGTCGGCAGTGATAATAGCTGTTGCTGCCCCATGGCTTCGAGTTCGGCGAGGGTAACGTTCTTACGATACAAAACAAATTCCAGTTTGACGGGCAAT',\n 'ATGCCAGTAAGGCAATTAATGAGATATCATTCCCCATTACCAGACTCGCTCAGCCATTCATGGATCTCAACGCCTAAGGTGTCATTCATCTGTACCAGTTCGCCATTACCCAGCAAAACACCATTCGCCATAATTTCAACGTTAAGTTCAGCATTGGTCGGCAGTGATAATAGCTGTTGCTGCCCCATGGCTTCGAGTTCGGCGAGGGTAACGTTCTTACGATACAAAACAAATTCCAGTTTGACGGGCAAT',\n 'ATGAGATATCATTCCCCATTACCAGACTCGCTCAGCCATTCATGGATCTCAACGCCTAAGGTGTCATTCATCTGTACCAGTTCGCCATTACCCAGCAAAACACCATTCGCCATAATTTCAACGTTAAGTTCAGCATTGGTCGGCAGTGATAATAGCTGTTGCTGCCCCATGGCTTCGAGTTCGGCGAGGGTAACGTTCTTACGATACAAAACAAATTCCAGTTTGACGGGCAAT',\n 'ATGGCTTCGAGTTCGGCGAGGGTAACGTTCTTACGATACAAAACAAATTCCAGTTTGACGGGCAAT',\n 'ATGTGCAACAATTTCCCTTCCGGCAGCGCGCTTCCCGGTACGGGGTTCTCTACGCATAAACGCCGACAGGACAAATGCGGCACGGGCAACTCAAACGGTCGCTCTGTTGCAGCAAGCCAGGGAACGACCAGGTGCTCAGCGCCAGCAGAAACCGCCGCCCCAGCCAGAGCGGGAGAGACATGCTCAAGCCAGTCCCCAGGTTTAATCCAGGCCGACCACCGTTTTTCTGCATCGCTCAACCGAACCCACATTCCCTGTCGCGTCGGATATTCCAGCGTCGCTTCCCGGCCATGGCGCTGGCATTCTGTCGCGGTTTGCGCCAATAGCCATTCGCGACGATCAATCTGTCTCACACGCAATGACATCAGGCGTCATCCTCCTCGCCAGATTGCTGTCTGTGCTGTTGCTGCTGCGGATTTTGTTGATCGTCTCGCGTCAGGTGCCAGCGCTGGGGATTACCGTTTTGCCATTGATCATGCAAACGATGTTCAACCTGCGTATTTGACGGTATTAACGAAAACTCCCCTGCTTGCCGCGCCTGAATAT',\n 'ATGCCT',\n 'ATGCCGCCAGTAATAACGGTAAAGCCGTTTCTTTATGCATTACGAAAGCATCGCCATAGTCGCGATCTTTTTTATCACCGGAATATTCTGTCTTATGTTTTTCCACCGCTTTTTTTAATGCTTCTGATAAACCGCCAACCTCATCCTGCTGCGGCAGTAAAATGTTCCCGGATGAAC',\n 'ATGCTTCTGATAAACCGCCAACCTCATCCTGCTGCGGCAGTAAAATGTTCCCGGATGAAC',\n 'ATGAAC',\n 'ATGAAATACCGTACAGCGCCGCTGCAATACTTTAATTCTGGTCAGCGAATGCATTCATACCGCTCAACGTGTCATCAAAGGATGAATACTGCGCTACCGGCTGGCATAACCAGGCTTTCAGGCTATCCCGCATCTGCATCGCCCGATCGTTATCGATATTTTCGCCAGGACGATATTCTCCCAAGTCAA',\n 'ATGCATTCATACCGCTCAACGTGTCATCAAAGGATGAATACTGCGCTACCGGCTGGCATAACCAGGCTTTCAGGCTATCCCGCATCTGCATCGCCCGATCGTTATCGATATTTTCGCCAGGACGATATTCTCCCAAGTCAA',\n 'ATGAATACTGCGCTACCGGCTGGCATAACCAGGCTTTCAGGCTATCCCGCATCTGCATCGCCCGATCGTTATCGATATTTTCGCCAGGACGATATTCTCCCAAGTCAA',\n 'ATGCCTGTTCAGCATGTGTCGGCGTCG',\n 'ATGCGCAAGCAAGGCAAGTTAATACCAATGCGAGGAGAAGTTCACGCCAAA',\n 'ATGCGAGGAGAAGTTCACGCCAAA',\n 'ATGAGAGATACAGTAGGGTTT',\n 'ATGCTTCTGTCGCCAGCACAAAACCGGCCTGTAATAACGCCGGTAATGCGGAACACACTAAGCAGAGCAGCA',\n 'ATGCGGAACACACTAAGCAGAGCAGCA',\n 'ATGGAATCAGATATTTTAACCCTATCCCAAAAACGGCCAAACTGTAGTCAGCCATGCTCTGATCAAAATTATCCGCAA',\n 'ATGAGCCATACGACACCAGATAGGCAATTCCTCCCAGCGTCAGGCAGGCGA',\n 'ATGACTGGCCTTTTT',\n 'ATGACGCCTACCGCCGCTTCTTGCAGAACTAACGGGATCATCGCCACCGATAAAAACGGCGGCGCCTCGTTCAATGCATGCGGCCATACTCCCAATGCCACCAGGATGATAATGGCGTTTCTCGGCGCACCGCTTAATACCCCGCTATTCAAAAACGGCAGGAAGAAAAAAATCGGCGCCACGCGAGCAAACCCTAGCGCCGCAGACGCAACCAGG',\n 'ATGATAATGGCGTTTCTCGGCGCACCGCTTAATACCCCGCTATTCAAAAACGGCAGGAAGAAAAAAATCGGCGCCACGCGAGCAAACCCTAGCGCCGCAGACGCAACCAGG',\n 'ATGGCGTTTCTCGGCGCACCGCTTAATACCCCGCTATTCAAAAACGGCAGGAAGAAAAAAATCGGCGCCACGCGAGCAAACCCTAGCGCCGCAGACGCAACCAGG',\n 'ATGCCAAAAGGCAGCGTCTGTTCCTGTAATTGCGTTACCGTCTGGAATAACCCTACCAGGAGGCCGATAATCGTTGCGACAATCGTCGGCCACCCTGACAGGATCAAAACAAGA',\n 'ATGATGTCATGTTGCAATGTCCATATACTG',\n 'ATGTCATGTTGCAATGTCCATATACTG',\n 'ATGCATTATGGGCCACATAACAAACATAGAAAGCAGCAATGCGACGCCGTTAAGCGTCATATT',\n 'ATGACATCAGGCGTCATCCTCCTCGCCAGATTGCTGTCTGTGCTGTTGCTGCTGCGGATTTTGTTGATCGTCTCGCGTCAGGTGCCAGCGCTGGGGATTACCGTTTTGCCATTGATCATGCAAACGATGTTCAACCTGCGTATT',\n 'ATGCAAACGATGTTCAACCTGCGTATT',\n 'ATGTTCAACCTGCGTATT',\n 'ATGATCTTTATCTTCAGGCTTACCGGTATTGGCTGCGGCCATTCGGGCAGGTGCGACATCCCGCGCCAGCGGCGCGCCCTCTTTACGAACGCCTTCGCCCGCGATGGCTTTATTATCCCCAGGCAATGCCTTGATATTATCGTCAGAGATTCCCGTGGCGTTATCGGTTACTTCACTAACGGATTCCACAGCTTT',\n 'ATGATGCAGACCAGAAACGTTGCCATTGTGATGTTC',\n 'ATGCAGACCAGAAACGTTGCCATTGTGATGTTC',\n 'ATGTTC',\n 'ATGTCGCCATGCCGCCAG',\n 'ATGCATTACGAAAGCATCGCCATAGTCGCGATCTTTTTTATCACCGGAATATTCTGTCTTATGTTTTTCCACCGCTTTTTT',\n 'ATGTTTTTCCACCGCTTTTTT',\n 'ATGTGTCGGCGTCGTGACTTGTCCAAAAACGCGGCTTACGCTTTTCAGTACATCGATTGCCGGGTAATGTCCCTGCCCGGCCAGCTTTCTGCTCAGATACAGGTGACCGTCAAGGATAGAGCGAATTTCATCCGCCATCGGGTCCGCCTCTTCCTCGCTTTCCAGCAGTACCGTATAAAAGGCAGTAATGCTTCCCTCGCTGGTCGCCCCTGGGCGTTCCAGCAAGCGGGGCAAATTATCGAATACGGAGGCGGGATAACCTCGACGAGCCGGACGCTCTCCCGACGCCAGTGCCACGTCTCGCAAAGCACGCGCATAACGGGTCATGGAATCGATAAAAAGCACGACCCGTTTTCCCTGGTCGCGAAAATATTCCGCTACGGTTGTCGCCAGTTGCGCCGCATTGCAGCGATCGACCGAGGGGAAATCGGAAGTGGCAAAAACCAGCACGCATTTTTCTTTCTTATGCGAAGCGCGCAACATATCCACGAATTCAGTGACCTCACGGCCTCGTTCACCGATAAGACCGATAACAAAGACATCCGCCTCCGTTTGCTCGATCAGCATATGCATCAGCATGGTCTTACCGCATCCTGCGGAGGCAAAAATGCCCATTCGCTGGCCTACGCCACAGGTCAA',\n 'ATGTCCCTGCCCGGCCAGCTTTCTGCTCAGATACAGGTGACCGTCAAGGATAGAGCGAATTTCATCCGCCATCGGGTCCGCCTCTTCCTCGCTTTCCAGCAGTACCGTATAAAAGGCAGTAATGCTTCCCTCGCTGGTCGCCCCTGGGCGTTCCAGCAAGCGGGGCAAATTATCGAATACGGAGGCGGGATAACCTCGACGAGCCGGACGCTCTCCCGACGCCAGTGCCACGTCTCGCAAAGCACGCGCATAACGGGTCATGGAATCGATAAAAAGCACGACCCGTTTTCCCTGGTCGCGAAAATATTCCGCTACGGTTGTCGCCAGTTGCGCCGCATTGCAGCGATCGACCGAGGGGAAATCGGAAGTGGCAAAAACCAGCACGCATTTTTCTTTCTTATGCGAAGCGCGCAACATATCCACGAATTCAGTGACCTCACGGCCTCGTTCACCGATAAGACCGATAACAAAGACATCCGCCTCCGTTTGCTCGATCAGCATATGCATCAGCATGGTCTTACCGCATCCTGCGGAGGCAAAAATGCCCATTCGCTGGCCTACGCCACAGGTCAA',\n 'ATGCGAAGCGCGCAACATATCCACGAATTCAGTGACCTCACGGCCTCGTTCACCGATAAGACCGATAACAAAGACATCCGCCTCCGTTTGCTCGATCAGCATATGCATCAGCATGGTCTTACCGCATCCTGCGGAGGCAAAAATGCCCATTCGCTGGCCTACGCCACAGGTCAA',\n 'ATGCATCAGCATGGTCTTACCGCATCCTGCGGAGGCAAAAATGCCCATTCGCTGGCCTACGCCACAGGTCAA']\n\n\n\ndef longest_ORF(dna):\n    \"\"\" Finds the longest ORF on both strands of the specified DNA and returns it\n        as a string\n    >>> longest_ORF(\"ATGCGAATGTAGCATCAAA\")\n    'ATGCTACATTCGCAT'\n    \"\"\"\n    #dna='ATGGCTAGCGATGCGAGCCCTACCGTGACCGATCCATGAGAGATGCTCTAGGCTATGAATGACGTAGCG'\n    ourorf2=find_all_ORFs_both_strands(dna)\n    thelength=[]\n    for i in ourorf2:\n        thelength.append(len(i))\n    seqlen=dict((j,i) for j,i in zip(thelength,ourorf2))\n    orderedlength=sorted(thelength)\n    return seqlen[thelength[-1]]\n#longest_ORF(dna)\n\n\n longest_ORF(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\n'ATGCATCAGCATGGTCTTACCGCATCCTGCGGAGGCAAAAATGCCCATTCGCTGGCCTACGCCACAGGTCAA'\n\n\n\nimport random\n\n\ndef shuffle_string(s):\n    \"\"\"Shuffles the characters in the input string\n        NOTE: this is a helper function, you do not\n        have to modify th|s in any way \"\"\"\n    return ''.join(random.sample(s, len(s)))\n\n\nnum_trials=1000\ndef longest_ORF_noncoding(dna, num_trials):\n    \"\"\" Computes the maximum length of the longest ORF over num_trials shuffles\n        of the specfied DNA sequence\n\n        dna: a DNA sequence\n        num_trials: the number of random shuffles\n        returns: the maximum length longest ORF \n    \"\"\"\n    num_trials=1000\n    import random\n    i=0\n    frames3 =[]\n    listofshuffled=[]\n    while i <num_trials:\n        listofshuffled.append(shuffle_string(dna))\n        i+=1\n    for i in listofshuffled:\n        frames3.append(longest_ORF(i))\n    return (max(frames3,key=len))\n#longest_ORF_noncoding(dna, num_trials) \n\n\n\nlongest_ORF_noncoding(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'), 1000)\n\n'ATGGGGTCTTGCCATTTCCTCCTTCTCAGCGTCTGTTTACAAGATGAAGTACACGCCGGTCCGTATCTGTCGGCCGAGCTACTTGTTGGCAAATGCGTGGATTTTGCAGAAGGTTACCAAAGCTCTGTTAGTCCGTCGACTCCACTAATTGCCAATTGCTCAGCGCAACTCACAGTCAGCACTCAAGCTCACGCTGCAGCTGCTCATTCTCTTCCAGGTAGTTGGGAGAGATGCACCTTCACTCAGCCTCTCGCTGAGAAAGCTCCTCCGCAAAGGGCCCATCTGATTAGACATTACTGTCTGGTCCTCCTCTTTTCTGATCAA'\n\n\n\ndef coding_strand_to_AA(dna):\n    \"\"\" Computes the Protein encoded by a sequence of DNA.  This function\n        does not check for start and stop codons (it assumes that the input\n        DNA sequence represents an protein coding region).\n\n        dna: a DNA sequence represented as a string\n        returns: a string containing the sequence of amino acids encoded by the\n                 the input DNA fragment\n\n        >>> coding_strand_to_AA(\"ATGCGA\")\n        'MR'\n        >>> coding_strand_to_AA(\"ATGCCCGCTTT\")\n        'MPA'\n    \"\"\"\n    aa = ['F', 'L', 'I', 'M', 'V', 'S', 'P', 'T', 'A', 'Y',\n      '|', 'H', 'Q', 'N', 'K', 'D', 'E', 'C', 'W', 'R',\n      'G']\n\n    codons = [['TTT', 'TTC'],\n              ['TTA', 'TTG', 'CTT', 'CTC', 'CTA', 'CTG'],\n              ['ATT', 'ATC', 'ATA'],\n              ['ATG'],\n              ['GTT', 'GTC', 'GTA', 'GTG'],\n              ['TCT', 'TCC', 'TCA', 'TCG', 'AGT', 'AGC'],\n              ['CCT', 'CCC', 'CCA', 'CCG'],\n              ['ACT', 'ACC', 'ACA', 'ACG'],\n              ['GCT', 'GCC', 'GCA', 'GCG'],\n              ['TAT', 'TAC'],\n              ['TAA', 'TAG', 'TGA'],\n              ['CAT', 'CAC'],\n              ['CAA', 'CAG'],\n              ['AAT', 'AAC'],\n              ['AAA', 'AAG'],\n              ['GAT', 'GAC'],\n              ['GAA', 'GAG'],\n              ['TGT', 'TGC'],\n              ['TGG'],\n              ['CGT', 'CGC', 'CGA', 'CGG', 'AGA', 'AGG'],\n              ['GGT', 'GGC', 'GGA', 'GGG']]\n\n    # create a dictionary lookup table for mapping codons into amino acids\n    aa_table = {}\n    for i in range(len(aa)):\n        for codon in codons[i]:\n            aa_table[codon] = aa[i]\n    init_pos = 0\n    return''.join([aa_table[dna[pos:pos + 3]] for pos in range (init_pos, len(dna) -2, 3)])\n    pass\n#coding_strand_to_AA(dna)\n\n\ncoding_strand_to_AA(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\n'GSDRENR|AFYP|SGAD|RRTRY|CRTAVLRFTRWRP|TADYRCARD|RVIDLWRRPANGHFCLRRMR|DHADAYADRANGGGCLCYRSYR|TRP|GH|IRGYVARFA|ERKMRAGFCHFRFPLGRSLQCGATGDNRSGIFSRPGKTGRAFYRFHDPLCACFARRGTGVGRASGSSRLSRLRIR|FAPLAGTPRGDQRGKHYCLLYGTAGKRGRGGPDGG|NSLYP|RSPVSEQKAGRAGTLPGNRCTEKRKPRFWTSHDADTC|TGICRA|INDAFGRAPAFH|LGRISSWRKYR|RSGDADAG|PESLVMPAGSAVFIL||HVERYECIR|PELKYCSGAVRYFIHSVSRYYFAIRMRTAGCRPRRRRSLNK|RV|NCY|IRCVQKTDSSVVRKFIRYYVSSLLFAGR|KI|NSRLYKFRKNGASWKRKGKSFRKKVNIGCAKKGTINAG|SVRKDSISSERYSRKRPSQRR|FNGRCVSCQFIREHFTAAAG|GWRFIRSIKKSGGKT|DRIFR||KRSRLWRCFRNA|RNGFTVITGGMATWRASEIRTSQWQRFWSAS|RKKRTQDC|KTVESHC|KICRFDLCGGQSR|IRSVAIV|K|AVRKRKR|KIIC|FKSCGIR||SNR|RHGNL|R|YQGIAWG||SHRGRRRS|RGRAAGAGCRTCPNGRSQYR|A|R|RS|KG|RCFSASAATNHYRRS|PINRRR|KNAFSGAIKADDDYFSHCRWRERRG|LADLPFSALGK|LFRQYSGAASRGVFVNTVKYAG|TSFA|SMAKR|SPALAPDARRSTKSAAATAQTAIWRGG|RLMSLRVRQIDRREWLLAQTATECQRHGREATLEYPTRQGMWVRLSDAEKRWSAWIKPGDWLEHVSPALAGAAVSAGAEHLVVPWLAATERPFELPVPHLSCRRLCVENPVPGSALPEGKLLHIMSDRGGLWFEHLPELPAVGGGRPKMLRWPLRFVIGSSDTQRSLLGRIGIGDVLLIRTSRAEVYCYAKKLGHFNRVEGGIIVETLDIQHIEEENNTTETAETLPGLNQLPVKLEFVLYRKNVTLAELEAMGQQQLLSLPTNAELNVEIMANGVLLGNGELVQMNDTLGVEIHEWLSESGNGE|YLINCLTGIFHPVAIYYCVRNLFR|IFYCICHGA|RPGITADTFKYDA|RRRIAAFYVCYVAHNA|CLRLF|GRRCHL||YFIIK|TR||RSGWLSRLSDQIFRSRVSSVF|KRATEASVWRRDRDGKA|QR|N|KTFNICVITCLCAERNKKRV|NWFLSLFALCRRRPGGIQRATGAGDDDDESGDDIYTY|AGAFCRA|WLDLTV|GIDITVYGHCNMTSLRDGIVKWMI|CLQVIRRSILF|SCQGGRRLSQRLSASW|GYSRR|RNYRNRRCLLALNYLACVYACFYCLAGMAKFYSLTGVR|YSWRWLRGKKCFTRCTLKFITWLRLRR|GLLAWRRFFSSCRF|IAGY|AVRRETPLSSWWHWEYGRMH|TRRRRFYRWR|SR|FCKKRR|ASCWAVCCHGLFGLCMRWVVLSITSEGQR|VVVSIRQTVLIPRKWLIS|ICLPLSFIYKTAVWSRWLTC|IKAISYAIR|TSARLHYRRY|RLLIRWLKTPWFWPVRWY|CCCCQKYSWVYCRALLRK|TLLRFH|R|KAVLPF|LCCFISLRYYRTMYCDSLSRPQG|AVGFTSEGRRMSSNKTEKPTKKRLEDSAKKGQSFKSKDLIIACLTLGGIAYLVSYGSFNEFMGIIKIIIADNFDQSMADYSLAVFGIGLKYLIPFMLLCLVCSALPALLQAGFVLATEALKPNLSALNPVEGAKKLFSMRTVKDTVKTLLYLSSFVVAAIICWKKYKVEIFSQLNGNIVGIAVIWRELLLALVLTCLACALIVLLLDAIAEYFLTMKDMKMDKEEVKREMKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH|AA|GSSDVFIGRFLNVH|PPSGFNNCIR|TQSLSSTKSVTDNRSK|WIIKIMSAKNVLRK|FGMPLVKAPR|KTFMGS'\n\n\n\ndef gene_finder(dna):\n    \"\"\" Returns the amino acid sequences that are likely coded by the specified dna\n\n        dna: a DNA sequence\n        returns: a list of all amino acid sequences coded by the sequence dna.\n    \"\"\"\n    my_genes=[]\n    for i in find_all_ORFs_both_strands(dna):\n        my_genes.append(coding_strand_to_AA(i))\n    return my_genes\n#gene_finder(dna)\n\n\ngene_finder(load_seq('/home/eanbit2/mscbioinfo/introtoprog/python/mini-project-eunicenjuguna/data/X73525.fa'))\n\n['MR',\n 'MRAGFCHFRFPLGRSLQCGATGDNRSGIFSRPGKTGRAFYRFHDPLCACFARRGTGVGRASGSSRLSRLRIR',\n 'MPAGSAVFIL',\n 'MRTAGCRPRRRRSLNK',\n 'MATWRASEIRTSQWQRFWSAS',\n 'MAKR',\n 'MSLRVRQIDRREWLLAQTATECQRHGREATLEYPTRQGMWVRLSDAEKRWSAWIKPGDWLEHVSPALAGAAVSAGAEHLVVPWLAATERPFELPVPHLSCRRLCVENPVPGSALPEGKLLHIMSDRGGLWFEHLPELPAVGGGRPKMLRWPLRFVIGSSDTQRSLLGRIGIGDVLLIRTSRAEVYCYAKKLGHFNRVEGGIIVETLDIQHIEEENNTTETAETLPGLNQLPVKLEFVLYRKNVTLAELEAMGQQQLLSLPTNAELNVEIMANGVLLGNGELVQMNDTLGVEIHEWLSESGNGE',\n 'MWVRLSDAEKRWSAWIKPGDWLEHVSPALAGAAVSAGAEHLVVPWLAATERPFELPVPHLSCRRLCVENPVPGSALPEGKLLHIMSDRGGLWFEHLPELPAVGGGRPKMLRWPLRFVIGSSDTQRSLLGRIGIGDVLLIRTSRAEVYCYAKKLGHFNRVEGGIIVETLDIQHIEEENNTTETAETLPGLNQLPVKLEFVLYRKNVTLAELEAMGQQQLLSLPTNAELNVEIMANGVLLGNGELVQMNDTLGVEIHEWLSESGNGE',\n 'MSDRGGLWFEHLPELPAVGGGRPKMLRWPLRFVIGSSDTQRSLLGRIGIGDVLLIRTSRAEVYCYAKKLGHFNRVEGGIIVETLDIQHIEEENNTTETAETLPGLNQLPVKLEFVLYRKNVTLAELEAMGQQQLLSLPTNAELNVEIMANGVLLGNGELVQMNDTLGVEIHEWLSESGNGE',\n 'MLRWPLRFVIGSSDTQRSLLGRIGIGDVLLIRTSRAEVYCYAKKLGHFNRVEGGIIVETLDIQHIEEENNTTETAETLPGLNQLPVKLEFVLYRKNVTLAELEAMGQQQLLSLPTNAELNVEIMANGVLLGNGELVQMNDTLGVEIHEWLSESGNGE',\n 'MGQQQLLSLPTNAELNVEIMANGVLLGNGELVQMNDTLGVEIHEWLSESGNGE',\n 'MANGVLLGNGELVQMNDTLGVEIHEWLSESGNGE',\n 'MNDTLGVEIHEWLSESGNGE',\n 'MTSLRDGIVKWMI',\n 'MI',\n 'MAKFYSLTGVR',\n 'MH',\n 'MRWVVLSITSEGQR',\n 'MYCDSLSRPQG',\n 'MSSNKTEKPTKKRLEDSAKKGQSFKSKDLIIACLTLGGIAYLVSYGSFNEFMGIIKIIIADNFDQSMADYSLAVFGIGLKYLIPFMLLCLVCSALPALLQAGFVLATEALKPNLSALNPVEGAKKLFSMRTVKDTVKTLLYLSSFVVAAIICWKKYKVEIFSQLNGNIVGIAVIWRELLLALVLTCLACALIVLLLDAIAEYFLTMKDMKMDKEEVKREMKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MGIIKIIIADNFDQSMADYSLAVFGIGLKYLIPFMLLCLVCSALPALLQAGFVLATEALKPNLSALNPVEGAKKLFSMRTVKDTVKTLLYLSSFVVAAIICWKKYKVEIFSQLNGNIVGIAVIWRELLLALVLTCLACALIVLLLDAIAEYFLTMKDMKMDKEEVKREMKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MADYSLAVFGIGLKYLIPFMLLCLVCSALPALLQAGFVLATEALKPNLSALNPVEGAKKLFSMRTVKDTVKTLLYLSSFVVAAIICWKKYKVEIFSQLNGNIVGIAVIWRELLLALVLTCLACALIVLLLDAIAEYFLTMKDMKMDKEEVKREMKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MLLCLVCSALPALLQAGFVLATEALKPNLSALNPVEGAKKLFSMRTVKDTVKTLLYLSSFVVAAIICWKKYKVEIFSQLNGNIVGIAVIWRELLLALVLTCLACALIVLLLDAIAEYFLTMKDMKMDKEEVKREMKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MRTVKDTVKTLLYLSSFVVAAIICWKKYKVEIFSQLNGNIVGIAVIWRELLLALVLTCLACALIVLLLDAIAEYFLTMKDMKMDKEEVKREMKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MKDMKMDKEEVKREMKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MKMDKEEVKREMKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MDKEEVKREMKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MKEQEGNPEVKSKRREVHMEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MEILSEQVKSDIENSRLIVANPTHITIGIYFKPELMPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MPIPMISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MISVYETNQRALAVRAYAEKVGVPVIVDIKLARSLFKTHRRYDLVSLEEIDEVLRLLVWLEEVENAGKDVIQPQENEVRH',\n 'MSAKNVLRK',\n 'MPLVKAPR',\n 'MGS',\n 'MGIFASAGCGKTMLMHMLIEQTEADVFVIGLIGERGREVTEFVDMLRASHKKEKCVLVFATSDFPSVDRCNAAQLATTVAEYFRDQGKRVVLFIDSMTRYARALRDVALASGERPARRGYPASVFDNLPRLLERPGATSEGSITAFYTVLLESEEEADPMADEIRSILDGHLYLSRKLAGQGHYPAIDVLKSVSRVFGQVTTPTHAEQASAVRKLMTRLEELQLFIDLGEYRPGENIDNDRAMQMRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MLMHMLIEQTEADVFVIGLIGERGREVTEFVDMLRASHKKEKCVLVFATSDFPSVDRCNAAQLATTVAEYFRDQGKRVVLFIDSMTRYARALRDVALASGERPARRGYPASVFDNLPRLLERPGATSEGSITAFYTVLLESEEEADPMADEIRSILDGHLYLSRKLAGQGHYPAIDVLKSVSRVFGQVTTPTHAEQASAVRKLMTRLEELQLFIDLGEYRPGENIDNDRAMQMRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MHMLIEQTEADVFVIGLIGERGREVTEFVDMLRASHKKEKCVLVFATSDFPSVDRCNAAQLATTVAEYFRDQGKRVVLFIDSMTRYARALRDVALASGERPARRGYPASVFDNLPRLLERPGATSEGSITAFYTVLLESEEEADPMADEIRSILDGHLYLSRKLAGQGHYPAIDVLKSVSRVFGQVTTPTHAEQASAVRKLMTRLEELQLFIDLGEYRPGENIDNDRAMQMRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MLIEQTEADVFVIGLIGERGREVTEFVDMLRASHKKEKCVLVFATSDFPSVDRCNAAQLATTVAEYFRDQGKRVVLFIDSMTRYARALRDVALASGERPARRGYPASVFDNLPRLLERPGATSEGSITAFYTVLLESEEEADPMADEIRSILDGHLYLSRKLAGQGHYPAIDVLKSVSRVFGQVTTPTHAEQASAVRKLMTRLEELQLFIDLGEYRPGENIDNDRAMQMRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MLRASHKKEKCVLVFATSDFPSVDRCNAAQLATTVAEYFRDQGKRVVLFIDSMTRYARALRDVALASGERPARRGYPASVFDNLPRLLERPGATSEGSITAFYTVLLESEEEADPMADEIRSILDGHLYLSRKLAGQGHYPAIDVLKSVSRVFGQVTTPTHAEQASAVRKLMTRLEELQLFIDLGEYRPGENIDNDRAMQMRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MTRYARALRDVALASGERPARRGYPASVFDNLPRLLERPGATSEGSITAFYTVLLESEEEADPMADEIRSILDGHLYLSRKLAGQGHYPAIDVLKSVSRVFGQVTTPTHAEQASAVRKLMTRLEELQLFIDLGEYRPGENIDNDRAMQMRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MADEIRSILDGHLYLSRKLAGQGHYPAIDVLKSVSRVFGQVTTPTHAEQASAVRKLMTRLEELQLFIDLGEYRPGENIDNDRAMQMRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MTRLEELQLFIDLGEYRPGENIDNDRAMQMRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MQMRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MRDSLKAWLCQPVAQYSSFDDTLSGMNAFADQN',\n 'MNAFADQN',\n 'MGDVSAVSSSGNILLPQQDEVGGLSEALKKAVEKHKTEYSGDKKDRDYGDAFVMHKETALPLLLAAWRHGAPAKSEHHNGNVSGLHHNGKSELRIAEKLLKVTAEKSVGLISAEAKVDKSAALLSSKNRPLESVSGKKLSADLKAVESVSEVTDNATGISDDNIKALPGDNKAIAGEGVRKEGAPLARDVAPARMAAANTGKPEDKDHKKVKDVSQLPLQPTTIADLSQLTGGDEKMPLAAQSKPMMTIFPTADGVKGEDSSLTYRFQRWGNDYSVNIQARQAGEFSLIPSNTQVEHRLHDQWQNGNPQRWHLTRDDQQNPQQQQHRQQSGEEDDA',\n 'MHKETALPLLLAAWRHGAPAKSEHHNGNVSGLHHNGKSELRIAEKLLKVTAEKSVGLISAEAKVDKSAALLSSKNRPLESVSGKKLSADLKAVESVSEVTDNATGISDDNIKALPGDNKAIAGEGVRKEGAPLARDVAPARMAAANTGKPEDKDHKKVKDVSQLPLQPTTIADLSQLTGGDEKMPLAAQSKPMMTIFPTADGVKGEDSSLTYRFQRWGNDYSVNIQARQAGEFSLIPSNTQVEHRLHDQWQNGNPQRWHLTRDDQQNPQQQQHRQQSGEEDDA',\n 'MAAANTGKPEDKDHKKVKDVSQLPLQPTTIADLSQLTGGDEKMPLAAQSKPMMTIFPTADGVKGEDSSLTYRFQRWGNDYSVNIQARQAGEFSLIPSNTQVEHRLHDQWQNGNPQRWHLTRDDQQNPQQQQHRQQSGEEDDA',\n 'MPLAAQSKPMMTIFPTADGVKGEDSSLTYRFQRWGNDYSVNIQARQAGEFSLIPSNTQVEHRLHDQWQNGNPQRWHLTRDDQQNPQQQQHRQQSGEEDDA',\n 'MMTIFPTADGVKGEDSSLTYRFQRWGNDYSVNIQARQAGEFSLIPSNTQVEHRLHDQWQNGNPQRWHLTRDDQQNPQQQQHRQQSGEEDDA',\n 'MTIFPTADGVKGEDSSLTYRFQRWGNDYSVNIQARQAGEFSLIPSNTQVEHRLHDQWQNGNPQRWHLTRDDQQNPQQQQHRQQSGEEDDA',\n 'MAGKRRWNIRRDRECGFG',\n 'MQKNGGRPGLNLGTGLSMSLPLWLGRRFLLALSTWSFPGLLQQSDRLSCPCRICPVGVYA',\n 'MSLPLWLGRRFLLALSTWSFPGLLQQSDRLSCPCRICPVGVYA',\n 'MYS',\n 'MLNLTLKLWRMVFCWVMANWYR',\n 'MVFCWVMANWYR',\n 'MANWYR',\n 'MTP',\n 'MNG',\n 'MGNDISLIALLAFSTLLPFIIASGTCFVKFSIVFVMVRNALGLQQIPSNMTLNGVALLLSMFVMWPIMHDAYVYFEDEDVTFNDISSLSKHVDEGLDGYRDYLIKYSDRELVQFFENAQLKRQYGEETETVKRDKDEIEKPSIFALLPAYALSEIKSAFKIGFYLYLPFVVVDLVVSSVLLALGMMMMSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MVRNALGLQQIPSNMTLNGVALLLSMFVMWPIMHDAYVYFEDEDVTFNDISSLSKHVDEGLDGYRDYLIKYSDRELVQFFENAQLKRQYGEETETVKRDKDEIEKPSIFALLPAYALSEIKSAFKIGFYLYLPFVVVDLVVSSVLLALGMMMMSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MTLNGVALLLSMFVMWPIMHDAYVYFEDEDVTFNDISSLSKHVDEGLDGYRDYLIKYSDRELVQFFENAQLKRQYGEETETVKRDKDEIEKPSIFALLPAYALSEIKSAFKIGFYLYLPFVVVDLVVSSVLLALGMMMMSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MFVMWPIMHDAYVYFEDEDVTFNDISSLSKHVDEGLDGYRDYLIKYSDRELVQFFENAQLKRQYGEETETVKRDKDEIEKPSIFALLPAYALSEIKSAFKIGFYLYLPFVVVDLVVSSVLLALGMMMMSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MWPIMHDAYVYFEDEDVTFNDISSLSKHVDEGLDGYRDYLIKYSDRELVQFFENAQLKRQYGEETETVKRDKDEIEKPSIFALLPAYALSEIKSAFKIGFYLYLPFVVVDLVVSSVLLALGMMMMSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MHDAYVYFEDEDVTFNDISSLSKHVDEGLDGYRDYLIKYSDRELVQFFENAQLKRQYGEETETVKRDKDEIEKPSIFALLPAYALSEIKSAFKIGFYLYLPFVVVDLVVSSVLLALGMMMMSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MMMMSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MMMSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MMSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MSPVTISTPIKLVLFVALDGWTLLSKGLILQYMDIAT',\n 'MDIAT',\n 'MLVFTVWLVWRSFTLLRASGDIPGVG',\n 'MAACIERGAAVFIGGDDPVSSARSGGRRHAGLSAVMAFLGYACAGLYYR',\n 'MAFLGYACAGLYYR',\n 'MRSDERVHAFITAAINVY',\n 'MAHLMSLWG',\n 'MSLWG',\n 'MAIL',\n 'MLLRNIS',\n 'MKRISAHWPSAPMRRRLAYL',\n 'MRRRLAYL',\n 'MIW',\n 'MKFYVFWFGWKR',\n 'MYSLGVF',\n 'MFTNHRRGLITASDKRSR',\n 'MSHRRLTLHALASVNR',\n 'MSLLSVLSVNEAVRSLNSWICCALRIRKKNACWFLPLPISPRSIAAMRRNWRQP',\n 'MRRNWRQP',\n 'MRVLCETWHWRRESVRLVEVIPPPYSIICPACWNAQGRPAREALLPFIRYCWKARKRRTRWRMKFALSLTVTCI',\n 'MKFALSLTVTCI',\n 'MY',\n 'MLNRHLPCVN',\n 'MTR',\n 'MHSLTRIKVLQRRCTVFHSQCESILLRYQDEDRGLQAEEEAILEQIAGLKLLLDTLRAENRQLSREEIYTLLRKQSIVRRQIKDLELQIIQIQEKRSELEKKREEFQKKSKYWLRKEGNYQRWIIRQKRFYIQREIQQEEAESEEII',\n 'MCQLSVHPGTFYCRSRMRLAVYQKH',\n 'MRLAVYQKH',\n 'MAMLS',\n 'MLS',\n 'MARQRNQNITMATFLVCIITEKANSGLLKNC',\n 'MATFLVCIITEKANSGLLKNC',\n 'MSHLPEWPQPIPVSLKIKIIKRLKMFLSFRCNQPLSPILAN',\n 'MFLSFRCNQPLSPILAN',\n 'MKKCL',\n 'MA',\n 'MTIPSIFRRGKQGSFR',\n 'MINGKTVIPSAGT',\n 'MTPDVIACETD',\n 'MAIGANRDRMPAPWPGSDAGISDATGNVGSVERCRKTVVGLD',\n 'MPAPWPGSDAGISDATGNVGSVERCRKTVVGLD',\n 'MRREPRTGKRAAGREIVAHYERSGRPVV',\n 'MAERVW',\n 'MISH',\n 'MMPTSILRTKMSPLMIFHH',\n 'MPTSILRTKMSPLMIFHH',\n 'MSPLMIFHH',\n 'MIFHH',\n 'MKVWMVIAII',\n 'MVIAII',\n 'MEKRPRR',\n 'MKLKNLQYLRYYLLMR',\n 'MR',\n 'MAGPYCLRD',\n 'MDDLVFAGNKALYLVLILSGWPTIVATIIGLLVGLFQTVTQLQEQTLPFGIKLLGVCLCLFLLSGWYGEVLLSYGRQVIFLALAKG',\n 'MFYALYFEIHHLVASAALGFARVAPIFFFLPFLNSGVLSGAPRNAIIILVALGVWPHALNEAPPFLSVAMIPLVLQEAAVGVMLGCLLSWPFWVMHALGCIIDNQRGATLSSSIDPANGIDTSEMANFLNMFAAVVYLQNGGLVTMVDVLNKSYQLCDPMNECTPSLPPLLTFINQVAQNALVLASPVVLVLLLSEVFLGLLSRFAPQMNAFAISLTVKSGIAVLIMLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MIPLVLQEAAVGVMLGCLLSWPFWVMHALGCIIDNQRGATLSSSIDPANGIDTSEMANFLNMFAAVVYLQNGGLVTMVDVLNKSYQLCDPMNECTPSLPPLLTFINQVAQNALVLASPVVLVLLLSEVFLGLLSRFAPQMNAFAISLTVKSGIAVLIMLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MLGCLLSWPFWVMHALGCIIDNQRGATLSSSIDPANGIDTSEMANFLNMFAAVVYLQNGGLVTMVDVLNKSYQLCDPMNECTPSLPPLLTFINQVAQNALVLASPVVLVLLLSEVFLGLLSRFAPQMNAFAISLTVKSGIAVLIMLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MHALGCIIDNQRGATLSSSIDPANGIDTSEMANFLNMFAAVVYLQNGGLVTMVDVLNKSYQLCDPMNECTPSLPPLLTFINQVAQNALVLASPVVLVLLLSEVFLGLLSRFAPQMNAFAISLTVKSGIAVLIMLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MANFLNMFAAVVYLQNGGLVTMVDVLNKSYQLCDPMNECTPSLPPLLTFINQVAQNALVLASPVVLVLLLSEVFLGLLSRFAPQMNAFAISLTVKSGIAVLIMLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MFAAVVYLQNGGLVTMVDVLNKSYQLCDPMNECTPSLPPLLTFINQVAQNALVLASPVVLVLLLSEVFLGLLSRFAPQMNAFAISLTVKSGIAVLIMLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MVDVLNKSYQLCDPMNECTPSLPPLLTFINQVAQNALVLASPVVLVLLLSEVFLGLLSRFAPQMNAFAISLTVKSGIAVLIMLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MNECTPSLPPLLTFINQVAQNALVLASPVVLVLLLSEVFLGLLSRFAPQMNAFAISLTVKSGIAVLIMLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MNAFAISLTVKSGIAVLIMLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MLLYFSPVLPDNVLRLSFQATGLSSWFYERGATHVLE',\n 'MDYQNNVSEERVAEMIWDAVSEGATLKDVHGI',\n 'MIWDAVSEGATLKDVHGI',\n 'MPYLVFLWLNNVFARVFYLFQPNQKT',\n 'MGFEKTARQFDIDDHRYANLLRIGADGQCALIRFIHRDHRNRHQFGFKINPDRNMRGVGNNQA',\n 'MRGVGNNQA',\n 'MNFTSFRFNLWVPFLLLHFTLHFFLIHFHIFHGQEIFRNSIQ',\n 'MKACTRSSDRIADSFYLTRQPS',\n 'MTADSPA',\n 'MHAAILPMPPG',\n 'MPPG',\n 'MSIYCNINPLDSKVQPSSATKSTSLIGVDIVTGLIIIIPSASSTLDTTRSTTTKGK',\n 'MGHITNIESSNATPLSVIFEGICCNPRALRTMTNTIENLTKQVPDAIINGNRVENASKAINEISFPITRLAQPFMDLNA',\n 'MTNTIENLTKQVPDAIINGNRVENASKAINEISFPITRLAQPFMDLNA',\n 'MDLNA',\n 'MLNI',\n 'MT',\n 'MLKPQAAPIAHNVQQFPFRQRASRYGVLYA',\n 'MRHGQLKRSLCCSKPGNDQVLSASRNRRPSQSGRDMLKPVPRFNPGRPPFFCIAQPNPHSLSRRIFQRRFPAMALAFCRGLRQ',\n 'MLKPVPRFNPGRPPFFCIAQPNPHSLSRRIFQRRFPAMALAFCRGLRQ',\n 'MALAFCRGLRQ',\n 'MALLSPGNALILSSEIPVALSVTSLTDSTAFKSADNFLPLTLSNGLFLDDSNAADLSTLASAEIKPTDFSAVTFNSFSAILSSLFPL',\n 'MFPDELTADTSPIKLSPLTRPLPAVSLAGYRIFSDGLSSVDSSLLCATNIYFFSETLPFSFPARSVFPEFV',\n 'MKSWSSSKRVINLRTADACSACVGVVTCPKTRLTLFSTSIAG',\n 'MLPSLVAPGRSSKRGKLSNTEAG',\n 'MESIKSTTRFPWSRKYSATVVASCAALQRSTEGKSEVAKTSTHFSFLCEARNISTNSVTSRPRSPIRPITKTSASVCSISICISMVLPHPAEAKMPIRWPTPQVNNPSIARTPVISGSRTPTREA',\n 'MVLPHPAEAKMPIRWPTPQVNNPSIARTPVISGSRTPTREA',\n 'MPIRWPTPQVNNPSIARTPVISGSRTPTREA',\n 'MNVF',\n 'MQLLNPDGG',\n 'MNTSLLPYAAQCRTSFSCG',\n 'MVRKYSAIASNNKTINAQARQVNTNARRSSRQMTAIPTILPFS',\n 'MTAIPTILPFS',\n 'MMAATTKDERYSRVLTVSLTVRILKSFFAPSTGFNADKLGFNASVASTKPACNNAGNAEHTKQSSINGIRYFNPIPKTAKL',\n 'MAATTKDERYSRVLTVSLTVRILKSFFAPSTGFNADKLGFNASVASTKPACNNAGNAEHTKQSSINGIRYFNPIPKTAKL',\n 'ML',\n 'MILIIPINSLNEPYDTR',\n 'MRSLLLNDWPFLAESSSRFLVGFSVLFEDMRRPSLVKPTA',\n 'MRRPSLVKPTA',\n 'MHNPKRP',\n 'MRPYSQCHQDDNGVSRRTA',\n 'MNFKVQRVKHFLPLSQRQEYHLTPVRE',\n 'MLQCPYTVISIP',\n 'MMKYH',\n 'MKYH',\n 'MRRR',\n 'MATGWKMPVRQLMRYHSPLPDSLSHSWISTPKVSFICTSSPLPSKTPFAIISTLSSALVGSDNSCCCPMASSSARVTFLRYKTNSSLTGN',\n 'MPVRQLMRYHSPLPDSLSHSWISTPKVSFICTSSPLPSKTPFAIISTLSSALVGSDNSCCCPMASSSARVTFLRYKTNSSLTGN',\n 'MRYHSPLPDSLSHSWISTPKVSFICTSSPLPSKTPFAIISTLSSALVGSDNSCCCPMASSSARVTFLRYKTNSSLTGN',\n 'MASSSARVTFLRYKTNSSLTGN',\n 'MCNNFPSGSALPGTGFSTHKRRQDKCGTGNSNGRSVAASQGTTRCSAPAETAAPARAGETCSSQSPGLIQADHRFSASLNRTHIPCRVGYSSVASRPWRWHSVAVCANSHSRRSICLTRNDIRRHPPRQIAVCAVAAADFVDRLASGASAGDYRFAIDHANDVQPAYLTVLTKTPLLAAPEY',\n 'MP',\n 'MPPVITVKPFLYALRKHRHSRDLFYHRNILSYVFPPLFLMLLINRQPHPAAAVKCSRMN',\n 'MLLINRQPHPAAAVKCSRMN',\n 'MN',\n 'MKYRTAPLQYFNSGQRMHSYRSTCHQRMNTALPAGITRLSGYPASASPDRYRYFRQDDILPSQ',\n 'MHSYRSTCHQRMNTALPAGITRLSGYPASASPDRYRYFRQDDILPSQ',\n 'MNTALPAGITRLSGYPASASPDRYRYFRQDDILPSQ',\n 'MPVQHVSAS',\n 'MRKQGKLIPMRGEVHAK',\n 'MRGEVHAK',\n 'MRDTVGF',\n 'MLLSPAQNRPVITPVMRNTLSRAA',\n 'MRNTLSRAA',\n 'MESDILTLSQKRPNCSQPCSDQNYPQ',\n 'MSHTTPDRQFLPASGRR',\n 'MTGLF',\n 'MTPTAASCRTNGIIATDKNGGASFNACGHTPNATRMIMAFLGAPLNTPLFKNGRKKKIGATRANPSAADATR',\n 'MIMAFLGAPLNTPLFKNGRKKKIGATRANPSAADATR',\n 'MAFLGAPLNTPLFKNGRKKKIGATRANPSAADATR',\n 'MPKGSVCSCNCVTVWNNPTRRPIIVATIVGHPDRIKTR',\n 'MMSCCNVHIL',\n 'MSCCNVHIL',\n 'MHYGPHNKHRKQQCDAVKRHI',\n 'MTSGVILLARLLSVLLLLRILLIVSRQVPALGITVLPLIMQTMFNLRI',\n 'MQTMFNLRI',\n 'MFNLRI',\n 'MIFIFRLTGIGCGHSGRCDIPRQRRALFTNAFARDGFIIPRQCLDIIVRDSRGVIGYFTNGFHSF',\n 'MMQTRNVAIVMF',\n 'MQTRNVAIVMF',\n 'MF',\n 'MSPCRQ',\n 'MHYESIAIVAIFFITGIFCLMFFHRFF',\n 'MFFHRFF',\n 'MCRRRDLSKNAAYAFQYIDCRVMSLPGQLSAQIQVTVKDRANFIRHRVRLFLAFQQYRIKGSNASLAGRPWAFQQAGQIIEYGGGITSTSRTLSRRQCHVSQSTRITGHGIDKKHDPFSLVAKIFRYGCRQLRRIAAIDRGEIGSGKNQHAFFFLMRSAQHIHEFSDLTASFTDKTDNKDIRLRLLDQHMHQHGLTASCGGKNAHSLAYATGQ',\n 'MSLPGQLSAQIQVTVKDRANFIRHRVRLFLAFQQYRIKGSNASLAGRPWAFQQAGQIIEYGGGITSTSRTLSRRQCHVSQSTRITGHGIDKKHDPFSLVAKIFRYGCRQLRRIAAIDRGEIGSGKNQHAFFFLMRSAQHIHEFSDLTASFTDKTDNKDIRLRLLDQHMHQHGLTASCGGKNAHSLAYATGQ',\n 'MRSAQHIHEFSDLTASFTDKTDNKDIRLRLLDQHMHQHGLTASCGGKNAHSLAYATGQ',\n 'MHQHGLTASCGGKNAHSLAYATGQ']\n\n\n\nif __name__ == \"__main__\":\n    import doctest\n    doctest.testmod()\n\n\nfrom load import load_nitrogenase_seq\n>>> nitrogenase = load_nitrogenase_seq()\n>>> print(nitrogenase)\n\nATGGGAAAACTCCGGCAGATCGCTTTCTACGGCAAGGGCGGGATCGGCAAGTCGACGACC\nTCGCAGAACACCCTCGCGGCACTGGTCGAGATGGGTCAGAAGATCCTCATCGTCGGCTGC\nGATCCCAAGGCCGACTCGACCCGCCTGATCCTGAACACCAAGCTGCAGGACACCGTGCTT\nCACCTCGCCGCCGAAGCGGGCTCCGTCGAGGATCTCGAACTCGAGGATGTGGTCAAGATC\nGGCTACAAGGGCATCAAATGCACCGAAGCCGGCGGGCCGGAGCCGGGCGTGGGCTGCGCG\nGGCCGCGGCGTCATCACCGCCATCAACTTCCTGGAAGAGAACGGCGCCTATGACGACGTC\nGACTACGTCTCCTACGACGTGCTGGGCGACGTGGTCTGCGGCGGCTTCGCCATGCCGATC\nCGCGAGAACAAGGCGCAGGAAATCTACATCGTCATGTCGGGCGAGATGATGGCGCTCTAT\nGCGGCCAACAACATCGCCAAGGGCATCCTGAAATACGCGAACTCGGGCGGCGTGCGCCTC\nGGCGGCCTGATCTGCAACGAGCGCAAGACCGACCGCGAGCTGGAACTGGCCGAGGCCCTC\nGCCGCGCGTCTGGGCTGCAAGATGATCCACTTCGTTCCGCGCGACAATATCGTGCAGCAC\nGCCGAGCTCCGCCGCGAGACGGTCATCCAGTATGCGCCCGAGAGCAAGCAGGCGCAGGAA\nTATCGCGAACTGGCCCGCAAGATCCACGAGAACTCGGGCAAGGGCGTGATCCCGACCCCG\nATCACCATGGAAGAGCTGGAAGAGATGCTGATGGATTTCGGCATCATGCAGTCCGAGGAA\nGACCGGCTCGCCGCCATCGCCGCCGCCGAGGCCTGA"
  },
  {
    "objectID": "introduction_deeplearning/introduction-to-deep-learning-in-python.html",
    "href": "introduction_deeplearning/introduction-to-deep-learning-in-python.html",
    "title": "Personal Blog",
    "section": "",
    "text": "In this exercise, you’ll write code to do forward propagation (prediction) for your first neural network:\n\n\n\n1_4.png\n\n\nEach data point is a customer. The first input is how many accounts they have, and the second input is how many children they have. The model will predict how many transactions the user makes in the next year. You will use this data throughout the first 2 chapters of this course.\nThe input data has been pre-loaded as input_data, and the weights are available in a dictionary called weights. The array of weights for the first node in the hidden layer are in weights[‘node_0’], and the array of weights for the second node in the hidden layer are in weights[‘node_1’].\nThe weights feeding into the output node are available in weights[‘output’].\nNumPy will be pre-imported for you as np in all exercises.\n\nimport numpy as np\n\nweights = {'node_0': np.array([2, 4]), \n           'node_1': np.array([ 4, -5]), \n           'output': np.array([2, 7])}\n\ninput_data = np.array([3, 5])\n\n# Calculate node 0 value: node_0_value\nnode_0_value = (input_data * weights['node_0']).sum()\n\n# Calculate node 1 value: node_1_value\nnode_1_value = (input_data * weights['node_1']).sum()\n\n# Put node values into array: hidden_layer_outputs\nhidden_layer_outputs = np.array([node_0_value, node_1_value])\n\n# Calculate output: output\noutput = (hidden_layer_outputs * weights['output']).sum()\n\n# Print output\nprint(output)\n\n-39\n\n\n\n\nAs Dan explained to you in the video, an “activation function” is a function applied at each node. It converts the node’s input into some output.\nThe rectified linear activation function (called ReLU) has been shown to lead to very high-performance networks. This function takes a single number as an input, returning 0 if the input is negative, and the input if the input is positive.\nHere are some examples: relu(3) = 3 relu(-3) = 0\n\ndef relu(input):\n    '''Define your relu activation function here'''\n    # Calculate the value for the output of the relu function: output\n    output = max(input,0)\n    \n    # Return the value just calculated\n    return(output)\n\n# Calculate node 0 value: node_0_output\nnode_0_input = (input_data * weights['node_0']).sum()\nnode_0_output = relu(node_0_input)\n\n# Calculate node 1 value: node_1_output\nnode_1_input = (input_data * weights['node_1']).sum()\nnode_1_output = relu(node_1_input)\n\n# Put node values into array: hidden_layer_outputs\nhidden_layer_outputs = np.array([node_0_output, node_1_output])\n\n# Calculate model output (do not apply relu)\nmodel_output = (hidden_layer_outputs * weights['output']).sum()\n\n# Print model output\nprint(model_output)\n\n52\n\n\n\n\n\nYou’ll now define a function called predict_with_network() which will generate predictions for multiple data observations, which are pre-loaded as input_data. As before, weights are also pre-loaded. In addition, the relu() function you defined in the previous exercise has been pre-loaded.\n\ninput_data = [np.array([3, 5]), np.array([ 1, -1]), np.array([0, 0]), np.array([8, 4])]\n# Define predict_with_network()\ndef predict_with_network(input_data_row, weights):\n\n    # Calculate node 0 value\n    node_0_input = (input_data_row * weights['node_0']).sum()\n    node_0_output = relu(node_0_input)\n\n    # Calculate node 1 value\n    node_1_input = (input_data_row * weights['node_1']).sum()\n    node_1_output = relu(node_1_input)\n\n    # Put node values into array: hidden_layer_outputs\n    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n    \n    # Calculate model output\n    input_to_final_layer =  (hidden_layer_outputs * weights['output']).sum()\n    model_output = relu(input_to_final_layer)\n    \n    # Return model output\n    return(model_output)\n\n\n# Create empty list to store prediction results\nresults = []\nfor input_data_row in input_data:\n    # Append prediction to results\n    results.append(predict_with_network(input_data_row,weights))\n\n# Print results\nprint(results)\n        \n\n[52, 63, 0, 148]\n\n\n\n\n\nYou now have a model with 2 hidden layers. The values for an input data point are shown inside the input nodes. The weights are shown on the edges/lines. What prediction would this model make on this data point?\nAssume the activation function at each node is the identity function. That is, each node’s output will be the same as its input. So the value of the bottom node in the first hidden layer is -1, and not 0, as it would be if the ReLU activation function was used.\n\n\n\nch1ex9.png\n\n\nThe answer is 0\n\n\n\nIn this exercise, you’ll write code to do forward propagation for a neural network with 2 hidden layers. Each hidden layer has two nodes. The input data has been preloaded as input_data. The nodes in the first hidden layer are called node_0_0 and node_0_1. Their weights are pre-loaded as weights[‘node_0_0’] and weights[‘node_0_1’] respectively.\nThe nodes in the second hidden layer are called node_1_0 and node_1_1. Their weights are pre-loaded as weights[‘node_1_0’] and weights[‘node_1_1’] respectively.\nWe then create a model output from the hidden nodes using weights pre-loaded as weights[‘output’]. \n\nweights = {'node_0_0': np.array([2, 4]),\n           'node_0_1': np.array([ 4, -5]),\n           'node_1_0': np.array([-1,  2]),\n           'node_1_1': np.array([1, 2]),\n           'output': np.array([2, 7])}\n\ninput_data =  np.array([3, 5])\n\ndef predict_with_network(input_data):\n    # Calculate node 0 in the first hidden layer\n    node_0_0_input = (input_data * weights['node_0_0']).sum()\n    node_0_0_output = relu(node_0_0_input)\n\n    # Calculate node 1 in the first hidden layer\n    node_0_1_input =  (input_data * weights['node_0_1']).sum()\n    node_0_1_output = relu(node_0_1_input)\n\n    # Put node values into array: hidden_0_outputs\n    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n    \n    # Calculate node 0 in the second hidden layer\n    node_1_0_input =  (hidden_0_outputs * weights['node_1_0']).sum()\n    node_1_0_output = relu(node_1_0_input)\n\n    # Calculate node 1 in the second hidden layer\n    node_1_1_input = (hidden_0_outputs * weights['node_1_1']).sum()\n    node_1_1_output = relu(node_1_1_input)\n\n    # Put node values into array: hidden_1_outputs\n    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n\n    # Calculate model output: model_output\n    model_output = ( hidden_1_outputs *weights['output']).sum()\n    \n    # Return model_output\n    return(model_output)\n\noutput = predict_with_network(input_data)\nprint(output)\n\n182\n\n\n\n\n\nHow are the weights that determine the features/interactions in Neural Networks created? - The model training process sets them to optimize predictive accuracy.\n\n\n\nWhich layers of a model capture more complex or “higher level” interactions? - The last layers capture the most complex interactions.\n\n\n\nFor the exercises in this chapter, you’ll continue working with the network to predict transactions for a bank.\nWhat is the error (predicted - actual) for the following network using the ReLU activation function when the input data is [3, 2] and the actual value of the target (what you are trying to predict) is 5? It may be helpful to get out a pen and piece of paper to calculate these values. \n\nThe network generates a prediction of 16, which results in an error of 11.\nIncreasing the weight to 2.01 would increase the resulting error from 9 to 9.08, making the predictions less accurate.\n\nCoding how weight changes affect accuracy Now you’ll get to change weights in a real network and see how they affect model accuracy!\nHave a look at the following neural network:\n Its weights have been pre-loaded as weights_0. Your task in this exercise is to update a single weight in weights_0 to create weights_1, which gives a perfect prediction (in which the predicted value is equal to target_actual: 3).\nUse a pen and paper if necessary to experiment with different combinations. You’ll use the predict_with_network() function, which takes an array of data as the first argument, and weights as the second argument.\n\n# Define predict_with_network()\ndef predict_with_network(input_data_row, weights):\n\n    # Calculate node 0 value\n    node_0_input = (input_data_row * weights['node_0']).sum()\n    node_0_output = node_0_input\n\n    # Calculate node 1 value\n    node_1_input = (input_data_row * weights['node_1']).sum()\n    node_1_output = node_1_input\n\n    # Put node values into array: hidden_layer_outputs\n    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n    \n    # Calculate model output\n    input_to_final_layer =  (hidden_layer_outputs * weights['output']).sum()\n    model_output = input_to_final_layer\n    \n    # Return model output\n    return(model_output)\n# The data point you will make a prediction for\ninput_data = np.array([0, 3])\n\n# Sample weights\nweights_0 = {'node_0': [2, 1],\n             'node_1': [1, 2],\n             'output': [1, 1]\n            }\n\n# The actual target value, used to calculate the error\ntarget_actual = 3\n\n# Make prediction using original weights\nmodel_output_0 = predict_with_network(input_data, weights_0)\n\n# Calculate error: error_0\nerror_0 = model_output_0 - target_actual\n\n# Create weights that cause the network to make perfect prediction (3): weights_1\nweights_1 = {'node_0': [2, 1],\n             'node_1': [1, 0],\n             'output': [1, 1]\n            }\n\n# Make prediction using new weights: model_output_1\nmodel_output_1 = predict_with_network(input_data, weights_1)\n\n# Calculate error: error_1\nerror_1 = model_output_1 - target_actual\n\n# Print error_0 and error_1\nprint(error_0)\nprint(error_1)\n\n6\n0\n\n\n\n\n\nYou’ve seen how different weights will have different accuracies on a single prediction. But usually, you’ll want to measure model accuracy on many points. You’ll now write code to compare model accuracies for two different sets of weights, which have been stored as weights_0 and weights_1.\ninput_data is a list of arrays. Each item in that list contains the data to make a single prediction. target_actuals is a list of numbers. Each item in that list is the actual value we are trying to predict.\nIn this exercise, you’ll use the mean_squared_error() function from sklearn.metrics. It takes the true values and the predicted values as arguments.\nYou’ll also use the preloaded predict_with_network() function, which takes an array of data as the first argument, and weights as the second argument.\n\ninput_data = [np.array([0, 3]), np.array([1, 2]), np.array([-1, -2]), np.array([4, 0])]\ntarget_actuals = [1, 3, 5, 7]\nweights_0 = {'node_0': np.array([2, 1]),\n             'node_1': np.array([1, 2]), \n             'output': np.array([1, 1])}\n\nweights_1 = {'node_0': np.array([2, 1]),\n             'node_1': np.array([1. , 1.5]),\n             'output': np.array([1. , 1.5])}\n\nfrom sklearn.metrics import mean_squared_error\n\n# Create model_output_0 \nmodel_output_0 = []\n# Create model_output_1\nmodel_output_1 = []\n\n# Loop over input_data\nfor row in input_data:\n    # Append prediction to model_output_0\n    model_output_0.append(predict_with_network(row, weights_0))\n    \n    # Append prediction to model_output_1\n    model_output_1.append(predict_with_network(row, weights_1))\n\n# Calculate the mean squared error for model_output_0: mse_0\nmse_0 = mean_squared_error(target_actuals, model_output_0)\n\n# Calculate the mean squared error for model_output_1: mse_1\nmse_1 = mean_squared_error(target_actuals, model_output_1)\n\n# Print mse_0 and mse_1\nprint(\"Mean squared error with weights_0: %f\" %mse_0)\nprint(\"Mean squared error with weights_1: %f\" %mse_1)\n\nMean squared error with weights_0: 80.250000\nMean squared error with weights_1: 99.890625\n\n\n\n\n\nYou’re now going to practice calculating slopes. When plotting the mean-squared error loss function against predictions, the slope is 2 * x * (xb-y), or 2 * input_data * error. Note that x and b may have multiple numbers (x is a vector for each data point, and b is a vector). In this case, the output will also be a vector, which is exactly what you want.\nYou’re ready to write the code to calculate this slope while using a single data point. You’ll use pre-defined weights called weights as well as data for a single point called input_data. The actual value of the target you want to predict is stored in target.\n\ninput_data = np.array([1, 2, 3])\nweights =  np.array([0, 2, 1])\ntarget = [0]\n# Calculate the predictions: preds\npreds = (weights * input_data).sum()\n\n# Calculate the error: error\nerror = preds - target\n\n# Calculate the slope: slope\nslope = input_data * error * 2\n\n# Print the slope\nprint(slope)\n\n[14 28 42]\n\n\n\n\n\nHurray! You’ve just calculated the slopes you need. Now it’s time to use those slopes to improve your model. If you add the slopes to your weights, you will move in the right direction. However, it’s possible to move too far in that direction. So you will want to take a small step in that direction first, using a lower learning rate, and verify that the model is improving.\nThe weights have been pre-loaded as weights, the actual value of the target as target, and the input data as input_data. The predictions from the initial weights are stored as preds.\n\n# Set the learning rate: learning_rate\nlearning_rate = 0.01\n\n# Calculate the predictions: preds\npreds = (weights * input_data).sum()\n\n# Calculate the error: error\nerror = preds - target\n\n# Calculate the slope: slope\nslope = 2 * input_data * error\n\n# Update the weights: weights_updated\nweights_updated = weights - (learning_rate * slope) \n\n# Get updated predictions: preds_updated\npreds_updated = (weights_updated * input_data).sum()\n\n# Calculate updated error: error_updated\nerror_updated = preds_updated - target\n\n# Print the original error\nprint(error)\n\n# Print the updated error\nprint(error_updated)\n\n[7]\n[5.04]\n\n\n\n\n\nYou’re now going to make multiple updates so you can dramatically improve your model weights, and see how the predictions improve with each update.\nTo keep your code clean, there is a pre-loaded get_slope() function that takes input_data, target, and weights as arguments. There is also a get_mse() function that takes the same arguments. The input_data, target, and weights have been pre-loaded.\nThis network does not have any hidden layers, and it goes directly from the input (with 3 nodes) to an output node. Note that weights is a single array.\nWe have also pre-loaded matplotlib.pyplot, and the error history will be plotted after you have done your gradient descent steps.\n\nExercise functions\n\n\ndef get_error(input_data, target, weights):\n    preds = (weights * input_data).sum()\n    error = preds - target\n    return(error)\n\n\n# get slope function\ndef get_slope(input_data, target, weights):\n    error = get_error(input_data, target, weights)   \n    slope = 2 * input_data * error\n    return(slope)\n\ndef get_mse(input_data, target, weights):\n    errors = get_error(input_data, target, weights)\n    mse = np.mean(errors**2)\n    return(mse)\n\n\nfrom matplotlib import pyplot as plt\nn_updates = 20\nmse_hist = []\n\n# Iterate over the number of updates\nfor i in range(n_updates):\n    # Calculate the slope: slope\n    slope = get_slope(input_data, target, weights)\n    \n    # Update the weights: weights\n    weights = weights - 0.01 * slope\n    \n    # Calculate mse with new weights: mse\n    mse = get_mse(input_data, target, weights)\n    \n    # Append the mse to mse_hist\n    mse_hist.append(mse)\n\n# Plot the mse history\nplt.plot(mse_hist)\nplt.xlabel('Iterations')\nplt.ylabel('Mean Squared Error')\nplt.show()\n\n\n\n\n\n\n\nIf your predictions were all exactly right, and your errors were all exactly 0, the slope of the loss function with respect to your predictions would also be 0. In that circumstance, which of the following statements would be correct?\n\nthe updates to all weights in the network would indeed also be 0\n\n\n\n\nIn the network shown below, we have done forward propagation, and node values calculated as part of forward propagation are shown in white. The weights are shown in black. Layers after the question mark show the slopes calculated as part of back-prop, rather than the forward-prop values. Those slope values are shown in purple.\nThis network again uses the ReLU activation function, so the slope of the activation function is 1 for any node receiving a positive value as input. Assume the node being examined had a positive value (so the activation function’s slope is 1).\n\n\n\nch2ex14_1.png\n\n\nWhat is the slope needed to update the weight with the question mark?\n\n\n\nch2ex14_2.png\n\n\n\nThe slope needed to update this weight is indeed 6\n\n\n\n\nYou will soon start building models in Keras to predict wages based on various professional and demographic factors. Before you start building a model, it’s good to understand your data by performing some exploratory analysis.\nThe data is pre-loaded into a pandas DataFrame called df. Use the .head() and .describe() methods in the IPython Shell for a quick overview of the DataFrame.\nThe target variable you’ll be predicting is wage_per_hour. Some of the predictor variables are binary indicators, where a value of 1 represents True, and 0 represents False.\nOf the 9 predictor variables in the DataFrame, how many are binary indicators? The min and max values as shown by .describe() will be informative here. How many binary indicator predictors are there? - 6\n\nimport pandas as pd\n\ndf = pd.read_csv(\"hourly_wages.csv\")\n\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      wage_per_hour\n      union\n      education_yrs\n      experience_yrs\n      age\n      female\n      marr\n      south\n      manufacturing\n      construction\n    \n  \n  \n    \n      0\n      5.10\n      0\n      8\n      21\n      35\n      1\n      1\n      0\n      1\n      0\n    \n    \n      1\n      4.95\n      0\n      9\n      42\n      57\n      1\n      1\n      0\n      1\n      0\n    \n    \n      2\n      6.67\n      0\n      12\n      1\n      19\n      0\n      0\n      0\n      1\n      0\n    \n    \n      3\n      4.00\n      0\n      12\n      4\n      22\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      7.50\n      0\n      12\n      17\n      35\n      0\n      1\n      0\n      0\n      0\n    \n  \n\n\n\n\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      wage_per_hour\n      union\n      education_yrs\n      experience_yrs\n      age\n      female\n      marr\n      south\n      manufacturing\n      construction\n    \n  \n  \n    \n      count\n      534.000000\n      534.000000\n      534.000000\n      534.000000\n      534.000000\n      534.000000\n      534.000000\n      534.000000\n      534.000000\n      534.000000\n    \n    \n      mean\n      9.024064\n      0.179775\n      13.018727\n      17.822097\n      36.833333\n      0.458801\n      0.655431\n      0.292135\n      0.185393\n      0.044944\n    \n    \n      std\n      5.139097\n      0.384360\n      2.615373\n      12.379710\n      11.726573\n      0.498767\n      0.475673\n      0.455170\n      0.388981\n      0.207375\n    \n    \n      min\n      1.000000\n      0.000000\n      2.000000\n      0.000000\n      18.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      5.250000\n      0.000000\n      12.000000\n      8.000000\n      28.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      50%\n      7.780000\n      0.000000\n      12.000000\n      15.000000\n      35.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      75%\n      11.250000\n      0.000000\n      15.000000\n      26.000000\n      44.000000\n      1.000000\n      1.000000\n      1.000000\n      0.000000\n      0.000000\n    \n    \n      max\n      44.500000\n      1.000000\n      18.000000\n      55.000000\n      64.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n    \n  \n\n\n\n\n\n\n\nNow you’ll get to work with your first model in Keras, and will immediately be able to run more complex neural network models on larger datasets compared to the first two chapters.\nTo start, you’ll take the skeleton of a neural network and add a hidden layer and an output layer. You’ll then fit that model and see Keras do the optimization so your model continually gets better.\nAs a start, you’ll predict workers wages based on characteristics like their industry, education and level of experience. You can find the dataset in a pandas dataframe called df. For convenience, everything in df except for the target has been converted to a NumPy matrix called predictors. The target, wage_per_hour, is available as a NumPy matrix called target.\nFor all exercises in this chapter, we’ve imported the Sequential model constructor, the Dense layer constructor, and pandas.\n\ntarget = df['wage_per_hour'].values\n\npredictors  = df.drop('wage_per_hour', axis = 1).values\n\n# Import necessary modules\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\n\n# Save the number of columns in predictors: n_cols\nn_cols = predictors.shape[1]\n\n# Set up the model: model\nmodel = Sequential()\n\n# Add the first layer\nmodel.add(Dense(50, activation = \"relu\", input_shape =(n_cols, )))\n\n# Add the second layer\nmodel.add(Dense(32, activation= \"relu\"))\n\n# Add the output layer\nmodel.add(Dense(1))\n\n\n\n\nYou’re now going to compile the model you specified earlier. To compile the model, you need to specify the optimizer and loss function to use. In the video, Dan mentioned that the Adam optimizer is an excellent choice. You can read more about it as well as other keras optimizers here, and if you are really curious to learn more, you can read the original paper that introduced the Adam optimizer.\nIn this exercise, you’ll use the Adam optimizer and the mean squared error loss function. Go for it!\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n\n\n# Verify that model contains information from compiling\nprint(\"Loss function: \" + model.loss)\n\nLoss function: mean_squared_error\n\n\n\n\n\nYou’re at the most fun part. You’ll now fit the model. Recall that the data to be used as predictive features is loaded in a NumPy matrix called predictors and the data to be predicted is stored in a NumPy matrix called target. Your model is pre-written and it has been compiled with the code from the previous exercise.\n\nmodel.fit(predictors, target, epochs = 10)\n\nEpoch 1/10\n534/534 [==============================] - 0s 50us/step - loss: 33.0082\nEpoch 2/10\n534/534 [==============================] - 0s 39us/step - loss: 26.2000\nEpoch 3/10\n534/534 [==============================] - 0s 43us/step - loss: 24.1919\nEpoch 4/10\n534/534 [==============================] - 0s 37us/step - loss: 23.1762\nEpoch 5/10\n534/534 [==============================] - 0s 32us/step - loss: 22.8131\nEpoch 6/10\n534/534 [==============================] - 0s 41us/step - loss: 22.1156\nEpoch 7/10\n534/534 [==============================] - 0s 43us/step - loss: 21.9082\nEpoch 8/10\n534/534 [==============================] - 0s 32us/step - loss: 21.8637\nEpoch 9/10\n534/534 [==============================] - 0s 41us/step - loss: 21.5395\nEpoch 10/10\n534/534 [==============================] - 0s 41us/step - loss: 21.7754\n\n\n<keras.callbacks.callbacks.History at 0x15038471e10>\n\n\n\n\n\nNow you will start modeling with a new dataset for a classification problem. This data includes information about passengers on the Titanic. You will use predictors such as age, fare and where each passenger embarked from to predict who will survive. This data is from a tutorial on data science competitions. Look here for descriptions of the features.\nThe data is pre-loaded in a pandas DataFrame called df.\nIt’s smart to review the maximum and minimum values of each variable to ensure the data isn’t misformatted or corrupted. What was the maximum age of passengers on the Titanic? Use the .describe() method in the IPython Shell to answer this question.\n\ntitanic = pd.read_csv(\"titanic_all_numeric.csv\")\n\ntitanic.describe()\n\n\n\n\n\n  \n    \n      \n      survived\n      pclass\n      age\n      sibsp\n      parch\n      fare\n      male\n      embarked_from_cherbourg\n      embarked_from_queenstown\n      embarked_from_southampton\n    \n  \n  \n    \n      count\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n    \n    \n      mean\n      0.383838\n      2.308642\n      29.699118\n      0.523008\n      0.381594\n      32.204208\n      0.647587\n      0.188552\n      0.086420\n      0.722783\n    \n    \n      std\n      0.486592\n      0.836071\n      13.002015\n      1.102743\n      0.806057\n      49.693429\n      0.477990\n      0.391372\n      0.281141\n      0.447876\n    \n    \n      min\n      0.000000\n      1.000000\n      0.420000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.000000\n      2.000000\n      22.000000\n      0.000000\n      0.000000\n      7.910400\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      50%\n      0.000000\n      3.000000\n      29.699118\n      0.000000\n      0.000000\n      14.454200\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      75%\n      1.000000\n      3.000000\n      35.000000\n      1.000000\n      0.000000\n      31.000000\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      max\n      1.000000\n      3.000000\n      80.000000\n      8.000000\n      6.000000\n      512.329200\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n    \n  \n\n\n\n\n\n\n\nYou’ll now create a classification model using the titanic dataset, which has been pre-loaded into a DataFrame called df. You’ll take information about the passengers and predict which ones survived.\nThe predictive variables are stored in a NumPy array predictors. The target to predict is in df.survived, though you’ll have to manipulate it for keras. The number of predictive features is stored in n_cols.\nHere, you’ll use the ‘sgd’ optimizer, which stands for Stochastic Gradient Descent. You’ll learn more about this in the next chapter!\n\n# Import necessary modules\n\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n# Convert the target to categorical: target\ntitanic_target = titanic.survived.values\n\n\npredictors_titanic = titanic.drop('survived', axis = 1).values\n\nX_train, X_test, y_train, y_test = train_test_split(predictors_titanic, titanic_target, test_size=0.20, random_state=42)\n\nn_cols_tita = X_train.shape[1]\n\ntitanic_train_target = to_categorical(y_train)\n\n# Set up the model\nmodel = Sequential()\n\n# Add the first layer\nmodel.add(Dense(32, activation=\"relu\", input_shape = (n_cols_tita, )))\n\n# Add the output layer\nmodel.add(Dense(2, activation=\"softmax\"))\n\n# Compile the model\nmodel.compile(optimizer='sgd',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Fit the model\n\nmodel.fit(X_train, titanic_train_target, epochs =10)\n\n\nEpoch 1/10\n712/712 [==============================] - 0s 179us/step - loss: 3.4747 - accuracy: 0.5730\nEpoch 2/10\n712/712 [==============================] - 0s 38us/step - loss: 1.0107 - accuracy: 0.6180\nEpoch 3/10\n712/712 [==============================] - 0s 48us/step - loss: 0.6336 - accuracy: 0.6994\nEpoch 4/10\n712/712 [==============================] - 0s 43us/step - loss: 0.6382 - accuracy: 0.6531\nEpoch 5/10\n712/712 [==============================] - 0s 43us/step - loss: 0.6030 - accuracy: 0.6840\nEpoch 6/10\n712/712 [==============================] - 0s 32us/step - loss: 0.6132 - accuracy: 0.6770\nEpoch 7/10\n712/712 [==============================] - 0s 32us/step - loss: 0.6026 - accuracy: 0.6924\nEpoch 8/10\n712/712 [==============================] - 0s 36us/step - loss: 0.5848 - accuracy: 0.7008\nEpoch 9/10\n712/712 [==============================] - 0s 28us/step - loss: 0.5762 - accuracy: 0.7177\nEpoch 10/10\n712/712 [==============================] - 0s 36us/step - loss: 0.5721 - accuracy: 0.7107\n\n\n<keras.callbacks.callbacks.History at 0x1503f6f42e8>\n\n\n\n\n\nThe trained network from your previous coding exercise is now stored as model. New data to make predictions is stored in a NumPy array as pred_data. Use model to make predictions on your new data.\nIn this exercise, your predictions will be probabilities, which is the most common way for data scientists to communicate their predictions to colleagues.\n\n# Calculate predictions: predictions\npredictions = model.predict(X_test)\n\n# Calculate predicted probability of survival: predicted_prob_true\npredicted_prob_true = predictions[:,1]\n\n# print predicted_prob_true\nprint(predicted_prob_true)\n\n[0.4393523  0.48866495 0.39731222 0.61088556 0.45835057 0.74529445\n 0.55348486 0.4456181  0.4904442  0.6187477  0.716584   0.37187845\n 0.63650054 0.37413827 0.541665   0.61706865 0.78558517 0.5543863\n 0.51765    0.8420926  0.4106096  0.6868524  0.50741607 0.41173482\n 0.520634   0.4718528  0.6947375  0.5317584  0.4826215  0.5232366\n 0.39360932 0.529245   0.6563818  0.5493375  0.39192986 0.44160265\n 0.68162596 0.55348486 0.7492457  0.43249092 0.6784965  0.36272633\n 0.43647745 0.4258504  0.60461956 0.5229629  0.39649883 0.42204925\n 0.39490002 0.8264776  0.53941685 0.812945   0.5041746  0.81333935\n 0.32684872 0.73319507 0.48613372 0.97091943 0.7334454  0.5025388\n 0.39904588 0.57022554 0.707388   0.6702328  0.4258504  0.5491688\n 0.7179668  0.40357038 0.31448063 0.8217575  0.6639794  0.9792342\n 0.61968404 0.75069606 0.4067286  0.527727   0.5332341  0.6472109\n 0.6576547  0.650716   0.4980961  0.61798155 0.8576531  0.42570966\n 0.7872377  0.8979904  0.8258033  0.7168161  0.2451397  0.41589525\n 0.48707423 0.49481165 0.831457   0.42561615 0.432491   0.4054837\n 0.838141   0.36951503 0.662202   0.40898585 0.7925822  0.37044364\n 0.77948564 0.3615601  0.4043448  0.39024127 0.7965787  0.80498594\n 0.44281185 0.71056294 0.70790493 0.39157522 0.7721747  0.6972393\n 0.95430315 0.41867766 0.84590524 0.5125226  0.6374015  0.7336344\n 0.41982025 0.7423645  0.9299448  0.5468816  0.3975107  0.6588786\n 0.83827174 0.7418929  0.5992507  0.4020873  0.5549551  0.4393523\n 0.26635876 0.53534836 0.83954513 0.4868858  0.8362998  0.40104023\n 0.3643543  0.5572469  0.40633902 0.78937674 0.52118057 0.6527681\n 0.59238505 0.74086416 0.5282352  0.40132585 0.88646907 0.3742481\n 0.41920006 0.40615824 0.38914305 0.67594177 0.43647745 0.41079226\n 0.6831359  0.55391175 0.75062567 0.5808794  0.36788794 0.6730173\n 0.53411037 0.753962   0.4338457  0.8958585  0.47474706 0.7077925\n 0.40378138 0.36860514 0.68651134 0.51760703 0.6977267  0.4944778\n 0.37849128 0.40261698 0.5987059  0.5686516  0.5323318 ]\n\n\n\n\n\nWhich of the following could prevent a model from showing an improved loss in its first few epochs? * Learning rate too low.\n\nLearning rate too high.\nPoor choice of activation function.\nAll of the above.\nAll of the above.\n\n\n\n\nIt’s time to get your hands dirty with optimization. You’ll now try optimizing a model at a very low learning rate, a very high learning rate, and a “just right” learning rate. You’ll want to look at the results after running this exercise, remembering that a low value for the loss function is good.\nFor these exercises, we’ve pre-loaded the predictors and target values from your previous classification models (predicting who would survive on the Titanic). You’ll want the optimization to start from scratch every time you change the learning rate, to give a fair comparison of how each learning rate did in your results. So we have created a function get_new_model() that creates an unoptimized model to optimize.\n\ninput_shape = (n_cols_tita,)\ndef get_new_model(input_shape = input_shape):\n    model = Sequential()\n    model.add(Dense(32, activation='relu', input_shape = input_shape))\n    model.add(Dense(4, activation='relu'))\n    model.add(Dense(2, activation='softmax'))\n    return(model)\n\n# Import the SGD optimizer\nfrom keras.optimizers import SGD\n\n# Create list of learning rates: lr_to_test\nlr_to_test = [.000001, 0.01, 1]\n\n# Loop over learning rates\nfor lr in lr_to_test:\n    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n    \n    # Build new model to test, unaffected by previous models\n    model = get_new_model()\n    \n    # Create SGD optimizer with specified learning rate: my_optimizer\n    my_optimizer = SGD(lr = lr)\n    \n    # Compile the model\n    model.compile(optimizer = my_optimizer, loss = 'categorical_crossentropy')   \n    \n    # Fit the model\n    model.fit(X_train, titanic_train_target, epochs =10)\n\n    \n\n\n\nTesting model with learning rate: 0.000001\n\nEpoch 1/10\n712/712 [==============================] - 0s 301us/step - loss: 1.8486\nEpoch 2/10\n712/712 [==============================] - 0s 38us/step - loss: 1.8446\nEpoch 3/10\n712/712 [==============================] - 0s 38us/step - loss: 1.8405\nEpoch 4/10\n712/712 [==============================] - 0s 36us/step - loss: 1.8365\nEpoch 5/10\n712/712 [==============================] - 0s 38us/step - loss: 1.8325\nEpoch 6/10\n712/712 [==============================] - 0s 38us/step - loss: 1.8284\nEpoch 7/10\n712/712 [==============================] - 0s 36us/step - loss: 1.8243\nEpoch 8/10\n712/712 [==============================] - 0s 31us/step - loss: 1.8202\nEpoch 9/10\n712/712 [==============================] - 0s 46us/step - loss: 1.8161\nEpoch 10/10\n712/712 [==============================] - 0s 43us/step - loss: 1.8121\n\n\nTesting model with learning rate: 0.010000\n\nEpoch 1/10\n712/712 [==============================] - 0s 296us/step - loss: 0.9847\nEpoch 2/10\n712/712 [==============================] - 0s 35us/step - loss: 0.6400\nEpoch 3/10\n712/712 [==============================] - 0s 38us/step - loss: 0.6324\nEpoch 4/10\n712/712 [==============================] - 0s 28us/step - loss: 0.6304\nEpoch 5/10\n712/712 [==============================] - 0s 42us/step - loss: 0.6224\nEpoch 6/10\n712/712 [==============================] - 0s 32us/step - loss: 0.6261\nEpoch 7/10\n712/712 [==============================] - 0s 38us/step - loss: 0.6325\nEpoch 8/10\n712/712 [==============================] - 0s 32us/step - loss: 0.6248\nEpoch 9/10\n712/712 [==============================] - 0s 35us/step - loss: 0.6300\nEpoch 10/10\n712/712 [==============================] - 0s 35us/step - loss: 0.6293\n\n\nTesting model with learning rate: 1.000000\n\nEpoch 1/10\n712/712 [==============================] - 0s 305us/step - loss: 1.7169\nEpoch 2/10\n712/712 [==============================] - 0s 41us/step - loss: 0.6688\nEpoch 3/10\n712/712 [==============================] - 0s 38us/step - loss: 0.6729\nEpoch 4/10\n712/712 [==============================] - 0s 36us/step - loss: 0.6676\nEpoch 5/10\n712/712 [==============================] - 0s 32us/step - loss: 0.6665\nEpoch 6/10\n712/712 [==============================] - 0s 42us/step - loss: 0.6681\nEpoch 7/10\n712/712 [==============================] - 0s 36us/step - loss: 0.6702\nEpoch 8/10\n712/712 [==============================] - 0s 39us/step - loss: 0.6648\nEpoch 9/10\n712/712 [==============================] - 0s 42us/step - loss: 0.6669\nEpoch 10/10\n712/712 [==============================] - 0s 41us/step - loss: 0.6684\n\n\n\n\n\nNow it’s your turn to monitor model accuracy with a validation data set. A model definition has been provided as model. Your job is to add the code to compile it and then fit it. You’ll check the validation score in each epoch.\n\n# Save the number of columns in predictors: n_cols\nn_cols = predictors.shape[1]\ninput_shape = (n_cols_tita,)\n\n# Specify the model\nmodel = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape = input_shape))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n\n\n# Fit the model\nhist = model.fit(X_train, titanic_train_target,validation_split = 0.3, epochs = 10)\n\n\nTrain on 498 samples, validate on 214 samples\nEpoch 1/10\n498/498 [==============================] - 0s 830us/step - loss: 0.8264 - accuracy: 0.6265 - val_loss: 0.9846 - val_accuracy: 0.6449\nEpoch 2/10\n498/498 [==============================] - 0s 114us/step - loss: 0.8584 - accuracy: 0.6165 - val_loss: 1.0564 - val_accuracy: 0.6495\nEpoch 3/10\n498/498 [==============================] - 0s 90us/step - loss: 0.7803 - accuracy: 0.6928 - val_loss: 0.7301 - val_accuracy: 0.6729\nEpoch 4/10\n498/498 [==============================] - 0s 79us/step - loss: 0.6068 - accuracy: 0.6968 - val_loss: 0.5833 - val_accuracy: 0.7243\nEpoch 5/10\n498/498 [==============================] - 0s 80us/step - loss: 0.6750 - accuracy: 0.6747 - val_loss: 0.5960 - val_accuracy: 0.6449\nEpoch 6/10\n498/498 [==============================] - 0s 102us/step - loss: 0.5876 - accuracy: 0.6847 - val_loss: 0.7054 - val_accuracy: 0.6869\nEpoch 7/10\n498/498 [==============================] - 0s 72us/step - loss: 0.6315 - accuracy: 0.7149 - val_loss: 0.5770 - val_accuracy: 0.6822\nEpoch 8/10\n498/498 [==============================] - 0s 82us/step - loss: 0.5804 - accuracy: 0.7068 - val_loss: 0.6251 - val_accuracy: 0.6402\nEpoch 9/10\n498/498 [==============================] - 0s 81us/step - loss: 0.6225 - accuracy: 0.6928 - val_loss: 0.5864 - val_accuracy: 0.7336\nEpoch 10/10\n498/498 [==============================] - 0s 90us/step - loss: 0.5744 - accuracy: 0.7329 - val_loss: 0.5157 - val_accuracy: 0.7430\n\n\n\n\n\nNow that you know how to monitor your model performance throughout optimization, you can use early stopping to stop optimization when it isn’t helping any more. Since the optimization stops automatically when it isn’t helping, you can also set a high value for epochs in your call to .fit(), as Dan showed in the video.\nThe model you’ll optimize has been specified as model. As before, the data is pre-loaded as predictors and target.\n\n# Import EarlyStopping\nfrom keras.callbacks import EarlyStopping\n\n# Save the number of columns in predictors: n_cols\nn_cols = predictors.shape[1]\ninput_shape = (n_cols_tita,)\n\n# Specify the model\nmodel = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape = input_shape))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n\n# Define early_stopping_monitor\nearly_stopping_monitor = EarlyStopping(patience=2)\n\n# Fit the model\nmodel.fit(X_train, titanic_train_target, validation_split=0.3, epochs = 30,callbacks = [early_stopping_monitor])\n\n\nTrain on 498 samples, validate on 214 samples\nEpoch 1/30\n498/498 [==============================] - 0s 857us/step - loss: 0.9300 - accuracy: 0.6165 - val_loss: 0.8886 - val_accuracy: 0.6495\nEpoch 2/30\n498/498 [==============================] - 0s 80us/step - loss: 0.8286 - accuracy: 0.6566 - val_loss: 0.8388 - val_accuracy: 0.5888\nEpoch 3/30\n498/498 [==============================] - 0s 78us/step - loss: 0.7250 - accuracy: 0.6426 - val_loss: 0.6632 - val_accuracy: 0.7056\nEpoch 4/30\n498/498 [==============================] - 0s 102us/step - loss: 0.6263 - accuracy: 0.6767 - val_loss: 0.6021 - val_accuracy: 0.6916\nEpoch 5/30\n498/498 [==============================] - 0s 90us/step - loss: 0.6294 - accuracy: 0.6847 - val_loss: 0.6558 - val_accuracy: 0.6869\nEpoch 6/30\n498/498 [==============================] - 0s 86us/step - loss: 0.6445 - accuracy: 0.7269 - val_loss: 0.5836 - val_accuracy: 0.7383\nEpoch 7/30\n498/498 [==============================] - 0s 78us/step - loss: 0.6068 - accuracy: 0.7470 - val_loss: 0.6457 - val_accuracy: 0.6308\nEpoch 8/30\n498/498 [==============================] - 0s 121us/step - loss: 0.5972 - accuracy: 0.7309 - val_loss: 0.6628 - val_accuracy: 0.6682\n\n\n<keras.callbacks.callbacks.History at 0x150464ec550>\n\n\n\n\n\nNow you know everything you need to begin experimenting with different models!\nA model called model_1 has been pre-loaded. You can see a summary of this model printed in the IPython Shell. This is a relatively small network, with only 10 units in each hidden layer.\nIn this exercise you’ll create a new model called model_2 which is similar to model_1, except it has 100 units in each hidden layer.\nAfter you create model_2, both models will be fitted, and a graph showing both models loss score at each epoch will be shown. We added the argument verbose=False in the fitting commands to print out fewer updates, since you will look at these graphically instead of as text.\nBecause you are fitting two models, it will take a moment to see the outputs after you hit run, so be patient.\n\n# Define early_stopping_monitor\nearly_stopping_monitor = EarlyStopping(patience=2)\n\n## model 1\n# Create the new model: model_2\nmodel_1 = Sequential()\n\n# Add the first and second layers\nmodel_1.add(Dense(10, activation = \"relu\", input_shape=input_shape))\nmodel_1.add(Dense(10, activation = \"relu\"))\n\n# Add the output layer\nmodel_1.add(Dense(2, activation = \"softmax\"))\n\n\n# Compile model_2\nmodel_1.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n\n# Create the new model: model_2\nmodel_2 = Sequential()\n\n# Add the first and second layers\nmodel_2.add(Dense(100, activation = \"relu\", input_shape=input_shape))\nmodel_2.add(Dense(100, activation = \"relu\"))\n\n# Add the output layer\nmodel_2.add(Dense(2, activation = \"softmax\"))\n\n\n# Compile model_2\nmodel_2.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n\n# Fit model_1\nmodel_1_training = model_1.fit(X_train, titanic_train_target,\n                               epochs=15, validation_split=0.2, \n                               callbacks=[early_stopping_monitor], verbose=False)\n\n# Fit model_2\nmodel_2_training = model_2.fit(X_train, titanic_train_target,\n                               epochs=15, validation_split=0.2,\n                               callbacks=[early_stopping_monitor], verbose=False)\n\nimport matplotlib.pyplot as plt\n# Create the plot\nplt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\nplt.xlabel('Epochs')\nplt.ylabel('Validation score')\nplt.show()\n\n\n\n\n\n\n\nYou’ve seen how to experiment with wider networks. In this exercise, you’ll try a deeper network (more hidden layers).\nOnce again, you have a baseline model called model_1 as a starting point. It has 1 hidden layer, with 50 units. You can see a summary of that model’s structure printed out. You will create a similar network with 3 hidden layers (still keeping 50 units in each layer).\nThis will again take a moment to fit both models, so you’ll need to wait a few seconds to see the results after you run your code.\n\n# The input shape to use in the first hidden layer\ninput_shape = (n_cols_tita,)\n\n# Create the new model: model_2\nmodel_2 = Sequential()\n\n# Add the first, second, and third hidden layers\nmodel_2.add(Dense(50, activation = \"relu\", input_shape=input_shape))\nmodel_2.add(Dense(50, activation = \"relu\"))\nmodel_2.add(Dense(50, activation = \"relu\"))\n\n\n# Add the output layer\nmodel_2.add(Dense(2, activation = \"softmax\"))\n\n\n# Compile model_2\nmodel_2.compile(optimizer = 'adam',\n                loss = 'categorical_crossentropy', \n                metrics=['accuracy'])\n\n\n\n# Fit model 2\nmodel_2_training = model_2.fit(X_train, titanic_train_target,\n                               epochs=20, validation_split=0.4,\n                               callbacks=[early_stopping_monitor], verbose=False)\n\n# Create the plot\nplt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\nplt.xlabel('Epochs')\nplt.ylabel('Validation score')\nplt.show()\n\n\n\n\n\nThe blue model is the one you made and the red is the original model. The model with the lower loss value is the better model.\n\n\n\n\nYou’ve just run an experiment where you compared two networks that were identical except that the 2nd network had an extra hidden layer. You see that this 2nd network (the deeper network) had better performance. Given that, which of the following would be a good experiment to run next for even better performance?\n\nIncreasing the number of units in each hidden layer would be a good next step to try achieving even better performance.\n\n\n\n\nYou’ve reached the final exercise of the course - you now know everything you need to build an accurate model to recognize handwritten digits!\nWe’ve already done the basic manipulation of the MNIST dataset shown in the video, so you have X and y loaded and ready to model with. Sequential and Dense from keras are also pre-imported.\nTo add an extra challenge, we’ve loaded only 2500 images, rather than 60000 which you will see in some published results. Deep learning models perform better with more data, however, they also take longer to train, especially when they start becoming more complex.\nIf you have a computer with a CUDA compatible GPU, you can take advantage of it to improve computation time. If you don’t have a GPU, no problem! You can set up a deep learning environment in the cloud that can run your models on a GPU. Here is a blog post by Dan that explains how to do this - check it out after completing this exercise! It is a great next step as you continue your deep learning journey.\nReady to take your deep learning to the next level? Check out Advanced Deep Learning with Keras in Python to see how the Keras functional API lets you build domain knowledge to solve new types of problems. Once you know how to use the functional API, take a look at “Convolutional Neural Networks for Image Processing” to learn image-specific applications of Keras.\n\nmnist = pd.read_csv(\"mnist.csv\")\nmnist.head()\n\n\n\n\n\n  \n    \n      \n      label\n      pixel0\n      pixel1\n      pixel2\n      pixel3\n      pixel4\n      pixel5\n      pixel6\n      pixel7\n      pixel8\n      ...\n      pixel774\n      pixel775\n      pixel776\n      pixel777\n      pixel778\n      pixel779\n      pixel780\n      pixel781\n      pixel782\n      pixel783\n    \n  \n  \n    \n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n5 rows × 785 columns\n\n\n\n\n# Convert the target to categorical: target\nmnist_target = mnist.label.values\n\n\nmnist_predictors = mnist.drop('label', axis = 1).values\n\nX_train, X_test, y_train, y_test = train_test_split(mnist_predictors, mnist_target,\n                                                    test_size=0.20, random_state=42)\n\nn_cols_mnist = X_train.shape[1]\n\nmnist_train_target = to_categorical(y_train)\n\n\n# Create the model: model\nmodel = Sequential()\n\n# Add the first hidden layer\nmodel.add(Dense(784, activation= \"relu\", input_shape = (784,)))\n\n# Add the second hidden layer\nmodel.add(Dense(784, activation= \"relu\"))\n\n# Add the output layer\nmodel.add(Dense(10, activation= \"softmax\"))\n\n\n# Compile the model\nmodel.compile(optimizer = 'adam',\n                loss = 'categorical_crossentropy', \n                metrics=['accuracy'])\n\nearly_stopping_monitor = EarlyStopping(patience=5)\n\n# Fit the model\nhist = model.fit(X_train, mnist_train_target,\n          epochs=20, validation_split=0.3,\n         callbacks=[early_stopping_monitor])\n                               \n\nTrain on 23520 samples, validate on 10080 samples\nEpoch 1/20\n23520/23520 [==============================] - 13s 537us/step - loss: 3.0947 - accuracy: 0.8814 - val_loss: 0.4898 - val_accuracy: 0.9318\nEpoch 2/20\n23520/23520 [==============================] - 14s 574us/step - loss: 0.3139 - accuracy: 0.9447 - val_loss: 0.3341 - val_accuracy: 0.9422\nEpoch 3/20\n23520/23520 [==============================] - 12s 518us/step - loss: 0.2072 - accuracy: 0.9548 - val_loss: 0.2999 - val_accuracy: 0.9432\nEpoch 4/20\n23520/23520 [==============================] - 12s 515us/step - loss: 0.1948 - accuracy: 0.9574 - val_loss: 0.3780 - val_accuracy: 0.9422\nEpoch 5/20\n23520/23520 [==============================] - 13s 546us/step - loss: 0.2242 - accuracy: 0.9537 - val_loss: 0.2870 - val_accuracy: 0.9459\nEpoch 6/20\n23520/23520 [==============================] - 12s 526us/step - loss: 0.1955 - accuracy: 0.9591 - val_loss: 0.4099 - val_accuracy: 0.9320\nEpoch 7/20\n23520/23520 [==============================] - 12s 492us/step - loss: 0.2014 - accuracy: 0.9546 - val_loss: 0.3483 - val_accuracy: 0.9427\nEpoch 8/20\n23520/23520 [==============================] - 12s 524us/step - loss: 0.1532 - accuracy: 0.9652 - val_loss: 0.2823 - val_accuracy: 0.9504\nEpoch 9/20\n23520/23520 [==============================] - 17s 705us/step - loss: 0.1633 - accuracy: 0.9633 - val_loss: 0.3615 - val_accuracy: 0.9444\nEpoch 10/20\n23520/23520 [==============================] - 14s 615us/step - loss: 0.1365 - accuracy: 0.9675 - val_loss: 0.3071 - val_accuracy: 0.9497\nEpoch 11/20\n23520/23520 [==============================] - 19s 803us/step - loss: 0.1499 - accuracy: 0.9665 - val_loss: 0.3288 - val_accuracy: 0.9531\nEpoch 12/20\n23520/23520 [==============================] - 15s 637us/step - loss: 0.1298 - accuracy: 0.9704 - val_loss: 0.3034 - val_accuracy: 0.9473\nEpoch 13/20\n23520/23520 [==============================] - 15s 632us/step - loss: 0.1144 - accuracy: 0.9740 - val_loss: 0.2939 - val_accuracy: 0.9519"
  },
  {
    "objectID": "Intro_Python_Data/Intro_datascience.html",
    "href": "Intro_Python_Data/Intro_datascience.html",
    "title": "Personal Blog",
    "section": "",
    "text": "Modules (sometimes called packages or libraries) help group together related sets of tools in Python. In this exercise, we’ll examine two modules that are frequently used by Data Scientists:\nstatsmodels: used in machine learning; usually aliased as sm seaborn: a visualization library; usually aliased as sns Note that each module has a standard alias, which allows you to access the tools inside of the module without typing as many characters. For example, aliasing lets us shorten seaborn.scatterplot() to sns.scatterplot().\n\nimport statsmodels as sm\nimport seaborn as sns\nimport numpy as np\n\n\n\n\nBefore we start looking for Bayes’ kidnapper, we need to fill out a Missing Puppy Report with details of the case. Each piece of information will be stored as a variable.\nWe define a variable using an equals sign (=). For instance, we would define the variable height:\nheight = 24 In this exercise, we’ll be defining bayes_age to be 4.0 months old. The data type for this variable will be float, meaning that it is a number.\n\nbayes_age = 4.0\nbayes_age\n\n4.0\n\n\n\n\n\nLet’s continue to fill out the Missing Puppy Report for Bayes. In the previous exercise, we defined bayes_age, which was a float, which represents a number.\nIn this exercise, we’ll define favorite_toy and owner, which will both be strings. A string represents text. A string is surrounded by quotation marks (’ or “) and can contain letters, numbers, and special characters. It doesn’t matter if you use single (’) or double (”) quotes, but it’s important to be consistent throughout your code.\n\nfavorite_toy = \"Mr. Squeaky\"\nowner = \"DataCamp\"\n# Display variables\nprint(favorite_toy)\nprint(owner)\n\nMr. Squeaky\nDataCamp\n\n\n\n\n\nIt’s easy to make errors when you’re trying to type strings quickly.\nDon’t forget to use quotes! Without quotes, you’ll get a name error. owner = DataCamp Use the same type of quotation mark. If you start with a single quote, and end with a double quote, you’ll get a syntax error. fur_color = “blonde’ Someone at the police station made an error when filling out the final lines of Bayes’ Missing Puppy Report. In this exercise, you will correct the errors.\n\n# One or more of the following lines contains an error\n# Correct it so that it runs without producing syntax errors\nbirthday = '2017-07-14'\ncase_id = 'DATACAMP!123-456?'"
  },
  {
    "objectID": "datacamp/introduction_statistics_R/introduction_stats.html",
    "href": "datacamp/introduction_statistics_R/introduction_stats.html",
    "title": "Introduction to Statistics with R",
    "section": "",
    "text": "In this chapter, you’ll be working with the 2018 Food Carbon Footprint Index from nu3. The food_consumption dataset contains information about the kilograms of food consumed per person per year in each country in each food category (consumption) as well as information about the carbon footprint of that food category (co2_emissions) measured in kilograms of carbon dioxide, or CO , per person per year in each country.\nIn this exercise, you’ll compute measures of center to compare food consumption in the US and Belgium using your dplyr skills.\ndplyr is loaded for you and food_consumption is available.\n\nlibrary(tidyverse)\nfood_consumption <- readRDS(\"food_consumption.rds\")\n\nfood_consumption %>%\n  # Filter for Belgium and USA\n  filter(country %in% c(\"Belgium\", \"USA\")) %>%\n  # Group by country\n  group_by(country) %>%\n  # Get mean_consumption and median_consumption\n  summarise(mean_consumption = mean(consumption),\n            median_consumption = median(consumption))\n\n\n\n\ncountry\nmean_consumption\nmedian_consumption\n\n\n\n\nBelgium\n42.13273\n12.59\n\n\nUSA\n44.65000\n14.58\n\n\n\n\n\n\n\n\nIn the video, you learned that the mean is the sum of all the data points divided by the total number of data points, and the median is the middle value of the dataset where 50% of the data is less than the median, and 50% of the data is greater than the median. In this exercise, you’ll compare these two measures of center.\n\nfood_consumption %>%\n  # Filter for rice food category\n  filter(food_category == \"rice\") %>%\n  # Create histogram of co2_emission\n  ggplot(aes(co2_emission)) +\n  geom_histogram()\n\n\n\nfood_consumption %>%\n  # Filter for rice food category\n  filter(food_category == \"rice\")  %>% \n  # Get mean_co2 and median_co2\n  summarise(mean_co2 = mean(co2_emission),\n            median_co2 = median(co2_emission))\n\n\n\n\nmean_co2\nmedian_co2\n\n\n\n\n37.59162\n15.2\n\n\n\n\n\n\n\n\nQuantiles are a great way of summarizing numerical data since they can be used to measure center and spread, as well as to get a sense of where a data point stands in relation to the rest of the dataset. For example, you might want to give a discount to the 10% most active users on a website.\nIn this exercise, you’ll calculate quartiles, quintiles, and deciles, which split up a dataset into 4, 5, and 10 pieces, respectively.\nThe dplyr package is loaded and food_consumption is available.\n\n# Calculate the deciles of co2_emission\nquantile(food_consumption$co2_emission, probs = seq(0,1,0.1))\n\n      0%      10%      20%      30%      40%      50%      60%      70% \n   0.000    0.668    3.540    7.040   11.026   16.530   25.590   44.271 \n     80%      90%     100% \n  99.978  203.629 1712.000 \n\n\n\n\n\nVariance and standard deviation are two of the most common ways to measure the spread of a variable, and you’ll practice calculating these in this exercise. Spread is important since it can help inform expectations. For example, if a salesperson sells a mean of 20 products a day, but has a standard deviation of 10 products, there will probably be days where they sell 40 products, but also days where they only sell one or two. Information like this is important, especially when making predictions.\nBoth dplyr and ggplot2 are loaded, and food_consumption is available.\n\n# Calculate variance and sd of co2_emission for each food_category\nfood_consumption %>% \n  group_by(food_category) %>% \n  summarise(var_co2 = var(co2_emission),\n            sd_co2 = sd(co2_emission))\n\n\n\n\nfood_category\nvar_co2\nsd_co2\n\n\n\n\nbeef\n88748.4081324\n297.9067105\n\n\neggs\n21.3718192\n4.6229665\n\n\nfish\n921.6373491\n30.3584807\n\n\nlamb_goat\n16475.5183631\n128.3569958\n\n\ndairy\n17671.8919851\n132.9356686\n\n\nnuts\n35.6396522\n5.9698955\n\n\npork\n3094.9635372\n55.6323965\n\n\npoultry\n245.0268013\n15.6533320\n\n\nrice\n2281.3762431\n47.7637545\n\n\nsoybeans\n0.8798818\n0.9380202\n\n\nwheat\n71.0239365\n8.4275700\n\n\n\n\n# Plot food_consumption with co2_emission on x-axis\nggplot(food_consumption, aes(co2_emission)) +\n  # Create a histogram\n  geom_histogram() +\n  # Create a separate sub-graph for each food_category\n  facet_wrap(~food_category)\n\n\n\n\n\n\n\nOutliers can have big effects on statistics like mean, as well as statistics that rely on the mean, such as variance and standard deviation. Interquartile range, or IQR, is another way of measuring spread that’s less influenced by outliers. IQR is also often used to find outliers. If a value is less than or greater than , it’s considered an outlier. In fact, this is how the lengths of the whiskers in a ggplot2 box plot are calculated.\n\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country <- food_consumption %>%\n  group_by(country) %>%\n  summarize(total_emission = sum(co2_emission))\n\n# Compute the first and third quartiles and IQR of total_emission\nq1 <- quantile(emissions_by_country$total_emission, 0.25)\nq3 <- quantile(emissions_by_country$total_emission, 0.75)\niqr <- q3 - q1\n\n# Calculate the lower and upper cutoffs for outliers\nlower <- q1 - 1.5 * iqr\nupper <- q3 + 1.5 * iqr\n\n# Filter emissions_by_country to find outliers\nemissions_by_country %>%\n  filter(total_emission<lower | total_emission > upper)\n\n\n\n\ncountry\ntotal_emission\n\n\n\n\nArgentina\n2172.4"
  },
  {
    "objectID": "datacamp/introduction_statistics_R/introduction_stats.html#more-distributions-and-the-central-limit-theorem",
    "href": "datacamp/introduction_statistics_R/introduction_stats.html#more-distributions-and-the-central-limit-theorem",
    "title": "Introduction to Statistics with R",
    "section": "More Distributions and the Central Limit Theorem",
    "text": "More Distributions and the Central Limit Theorem\n\nCalculating probabilities\nYou’re in charge of the sales team, and it’s time for performance reviews, starting with Amir. As part of the review, you want to randomly select a few of the deals that he’s worked on over the past year so that you can look at them more deeply. Before you start selecting deals, you’ll first figure out what the chances are of selecting certain deals.\n\namir_deals <- readRDS(\"seller_1.rds\")\n\n# Calculate probability of picking a deal with each product\namir_deals %>%\n  count(product) %>%\n  mutate(prob = n/sum(n))\n\n\n\n\nproduct\nn\nprob\n\n\n\n\nProduct A\n23\n0.1292135\n\n\nProduct B\n62\n0.3483146\n\n\nProduct C\n15\n0.0842697\n\n\nProduct D\n40\n0.2247191\n\n\nProduct E\n5\n0.0280899\n\n\nProduct F\n11\n0.0617978\n\n\nProduct G\n2\n0.0112360\n\n\nProduct H\n8\n0.0449438\n\n\nProduct I\n7\n0.0393258\n\n\nProduct J\n2\n0.0112360\n\n\nProduct N\n3\n0.0168539\n\n\n\n\n\n\n\nSampling deals\nIn the previous exercise, you counted the deals Amir worked on. Now it’s time to randomly pick five deals so that you can reach out to each customer and ask if they were satisfied with the service they received. You’ll try doing this both with and without replacement.\nAdditionally, you want to make sure this is done randomly and that it can be reproduced in case you get asked how you chose the deals, so you’ll need to set the random seed before sampling from the deals.\ndplyr is loaded and amir_deals is available.\n\n# Set random seed to 31\nset.seed(31)\n\n# Sample 5 deals with replacement\namir_deals %>%\n  sample_n(5, replace = F)\n\n\n\n\nproduct\nclient\nstatus\namount\nnum_users\n\n\n\n\nProduct D\nCurrent\nLost\n3086.88\n55\n\n\nProduct C\nCurrent\nLost\n3727.66\n19\n\n\nProduct D\nCurrent\nLost\n4274.80\n9\n\n\nProduct B\nCurrent\nWon\n4965.08\n9\n\n\nProduct A\nCurrent\nWon\n5827.35\n50\n\n\n\n\n\n\n\nCreating a probability distribution\nA new restaurant opened a few months ago, and the restaurant’s management wants to optimize its seating space based on the size of the groups that come most often. On one night, there are 10 groups of people waiting to be seated at the restaurant, but instead of being called in the order they arrived, they will be called randomly. In this exercise, you’ll investigate the probability of groups of different sizes getting picked first. Data on each of the ten groups is contained in the restaurant_groups data frame.\nRemember that expected value can be calculated by multiplying each possible outcome with its corresponding probability and taking the sum. The restaurant_groups data is available and dplyr and ggplot2 are loaded.\n\nrestaurant_groups <- data.frame(group_id = c(\"A\", \"B\", \"C\",\n                                             \"D\", \"E\", \"F\", \"G\", \n                                             \"H\", \"I\", \"J\"),\n                                group_size = c(2, 4, 6, 2, 2,\n                                               2, 3, 2, 4, 2))\n\n# Create probability distribution\nsize_distribution <- restaurant_groups %>%\n  count(group_size) %>%\n  mutate(probability = n / sum(n))\n\n# Calculate expected group size\nexpected_val <- sum(size_distribution$group_size *\n                      size_distribution$probability)\nexpected_val\n\n[1] 2.9\n\n# Calculate probability of picking group of 4 or more\nsize_distribution %>%\n  # Filter for groups of 4 or larger\n  filter(group_size >= 4) %>%\n  # Calculate prob_4_or_more by taking sum of probabilities\n  summarise(prob_4_or_more = sum(probability))\n\n\n\n\nprob_4_or_more\n\n\n\n\n0.3\n\n\n\n\n\n\n\nData back-ups\nThe sales software used at your company is set to automatically back itself up, but no one knows exactly what time the back-ups happen. It is known, however, that back-ups happen exactly every 30 minutes. Amir comes back from sales meetings at random times to update the data on the client he just met with. He wants to know how long he’ll have to wait for his newly-entered data to get backed up. Use your new knowledge of continuous uniform distributions to model this situation and answer Amir’s questions.\n\n# Min and max wait times for back-up that happens every 30 min\nmin <- 0\nmax <- 30\n\n# Calculate probability of waiting less than 5 mins\nprob_less_than_5 <- punif(5, min, max)\nprob_less_than_5\n\n[1] 0.1666667\n\n# Calculate probability of waiting more than 5 mins\nprob_greater_than_5 <- punif(5, min, max, lower.tail = F)\nprob_greater_than_5\n\n[1] 0.8333333\n\n# Calculate probability of waiting 10-20 mins\nprob_between_10_and_20 <- punif(20, min, max) -  punif(10, min, max)\nprob_between_10_and_20\n\n[1] 0.3333333\n\n\n\n\nSimulating wait times\nTo give Amir a better idea of how long he’ll have to wait, you’ll simulate Amir waiting 1000 times and create a histogram to show him what he should expect. Recall from the last exercise that his minimum wait time is 0 minutes and his maximum wait time is 30 minutes.\nA data frame called wait_times is available and dplyr and ggplot2 are loaded.\n\nwait_times <- data.frame(simulation_nb = 1:1000)\n# Set random seed to 334\nset.seed(334)\n\n# Generate 1000 wait times between 0 and 30 mins, save in time column\nwait_times %>%\n  mutate(time = runif(1000, min = 0, max = 30)) %>%\n  # Create a histogram of simulated times\n  ggplot(aes(time)) +\n  geom_histogram()\n\n\n\n\n\n\nSimulating sales deals\nAssume that Amir usually works on 3 deals per week, and overall, he wins 30% of deals he works on. Each deal has a binary outcome: it’s either lost, or won, so you can model his sales deals with a binomial distribution. In this exercise, you’ll help Amir simulate a year’s worth of his deals so he can better understand his performance.\n\n# Set random seed to 10\nset.seed(10)\n\n# Simulate a single deal\nrbinom(1, 1, p = 0.3)\n\n[1] 0\n\n# Simulate 1 week of 3 deals\nrbinom(1, 3, p = 0.3)\n\n[1] 0\n\n# Simulate 52 weeks of 3 deals\ndeals <- rbinom(52, 3, p = 0.3)\n\n# Calculate mean deals won per week\nmean(deals)\n\n[1] 0.8076923\n\n\n\n\nCalculating binomial probabilities\nJust as in the last exercise, assume that Amir wins 30% of deals. He wants to get an idea of how likely he is to close a certain number of deals each week. In this exercise, you’ll calculate what the chances are of him closing different numbers of deals using the binomial distribution.\n\n# Probability of closing 3 out of 3 deals\ndbinom(3, 3, p = 0.3)\n\n[1] 0.027\n\n# Probability of closing <= 1 deal out of 3 deals\n\npbinom(1, 3, p = 0.3)\n\n[1] 0.784\n\n# Probability of closing > 1 deal out of 3 deals\npbinom(1, 3, p = 0.3, lower.tail = F)\n\n[1] 0.216\n\n\n\n\nHow many sales will be won?\nNow Amir wants to know how many deals he can expect to close each week if his win rate changes. Luckily, you can use your binomial distribution knowledge to help him calculate the expected value in different situations. Recall from the video that the expected value of a binomial distribution can be calculated by n * p\n\n# Expected number won with 30% win rate\nwon_30pct <- 3 * 0.3\nwon_30pct\n\n[1] 0.9\n\n# Expected number won with 25% win rate\nwon_25pct <- 3 * 0.25\nwon_25pct\n\n[1] 0.75\n\n# Expected number won with 35% win rate\nwon_35pct <- 3 * 0.35\nwon_35pct\n\n[1] 1.05"
  },
  {
    "objectID": "datacamp/introduction_statistics_R/introduction_stats.html#more-distributions-and-the-central-limit-theorem-1",
    "href": "datacamp/introduction_statistics_R/introduction_stats.html#more-distributions-and-the-central-limit-theorem-1",
    "title": "Introduction to Statistics with R",
    "section": "More Distributions and the Central Limit Theorem",
    "text": "More Distributions and the Central Limit Theorem\n\nDistribution of Amir’s sales\nSince each deal Amir worked on (both won and lost) was different, each was worth a different amount of money. These values are stored in the amount column of amir_deals As part of Amir’s performance review, you want to be able to estimate the probability of him selling different amounts, but before you can do this, you’ll need to determine what kind of distribution the amount variable follows.\nBoth dplyr and ggplot2 are loaded and amir_deals is available.\n\nsource(\"data.R\")\n# Histogram of amount with 10 bins\nggplot(amir_deals, aes(amount))+\n  geom_histogram(bins = 10)\n\n\n\n\n\n\nProbabilities from the normal distribution\nSince each deal Amir worked on (both won and lost) was different, each was worth a different amount of money. These values are stored in the amount column of amir_deals and follow a normal distribution with a mean of 5000 dollars and a standard deviation of 2000 dollars. As part of his performance metrics, you want to calculate the probability of Amir closing a deal worth various amounts.\n\n# Probability of deal < 7500\npnorm(7500, mean = 5000, sd = 2000)\n\n[1] 0.8943502\n\n# Probability of deal > 1000\n\npnorm(1000, mean = 5000, sd = 2000, lower.tail = F)\n\n[1] 0.9772499\n\n# Probability of deal between 3000 and 7000\n\npnorm(7000, mean = 5000, sd = 2000) -  pnorm(3000, mean = 5000, sd = 2000)\n\n[1] 0.6826895\n\n# Calculate amount that 75% of deals will be more than\n\nqnorm(.75, mean = 5000, sd = 2000, lower.tail = F) \n\n[1] 3651.02\n\n\n\n\nSimulating sales under new market conditions\nThe company’s financial analyst is predicting that next quarter, the worth of each sale will increase by 20% and the volatility, or standard deviation, of each sale’s worth will increase by 30%. To see what Amir’s sales might look like next quarter under these new market conditions, you’ll simulate new sales amounts using the normal distribution and store these in the new_sales data frame, which has already been created for you.\n\n# Calculate new average amount\nnew_mean <- 5000 * 1.2\n\n# Calculate new standard deviation\nnew_sd <- 2000 * 1.3\n\n\n# Simulate 36 sales\nnew_sales <- new_sales %>% \n  mutate(amount = rnorm(36, new_mean, new_sd))\n\n# Create histogram with 10 bins\nggplot(new_sales, aes(amount))+\n  geom_histogram(bins = 10)\n\n\n\n\nThe CLT in action The central limit theorem states that a sampling distribution of a sample statistic approaches the normal distribution as you take more samples, no matter the original distribution being sampled from.\nIn this exercise, you’ll focus on the sample mean and see the central limit theorem in action while examining the num_users column of amir_deals more closely, which contains the number of people who intend to use the product Amir is selling.\nBoth dplyr and ggplot2 are loaded and amir_deals is available.\n\n# Create a histogram of num_users\nggplot(amir_deals, aes(num_users))+\n  geom_histogram(bins = 10)\n\n\n\n# Set seed to 104\nset.seed(104)\n\n# Sample 20 num_users from amir_deals and take mean\nsample(amir_deals$num_users, size = 20, replace = TRUE) %>%\n  mean()\n\n[1] 30.35\n\n# Repeat the above 100 times\nsample_means <- replicate(1000, sample(amir_deals$num_users,\n                                      size = 20, replace = TRUE) %>%\n                            mean())\n\n# Create data frame for plotting\nsamples <- data.frame(mean = sample_means)\n\n# Histogram of sample means\nggplot(samples, aes(mean))+\n  geom_histogram(bins = 10)\n\n\n\n\n\n\nThe mean of means\nYou want to know what the average number of users (num_users) is per deal, but you want to know this number for the entire company so that you can see if Amir’s deals have more or fewer users than the company’s average deal. The problem is that over the past year, the company has worked on more than ten thousand deals, so it’s not realistic to compile all the data. Instead, you’ll estimate the mean by taking several random samples of deals, since this is much easier than collecting data from everyone in the company.\nThe user data for all the company’s deals is available in all_deals.\n\n# Set seed to 321\nset.seed(321)\n\n# Take 30 samples of 20 values of num_users, take mean of each sample\nsample_means <- replicate(30, (sample(all_deals$num_users, size = 20)) %>%\n  mean())\n\n\n# Calculate mean of sample_means\nmean(sample_means)\n\n[1] 37.02667\n\n# Calculate mean of num_users in amir_deals\nmean(amir_deals$num_users)\n\n[1] 37.65169\n\n\n\n\nTracking lead responses\nYour company uses sales software to keep track of new sales leads. It organizes them into a queue so that anyone can follow up on one when they have a bit of free time. Since the number of lead responses is a countable outcome over a period of time, this scenario corresponds to a Poisson distribution. On average, Amir responds to 4 leads each day. In this exercise, you’ll calculate probabilities of Amir responding to different numbers of leads.\n\n# Probability of 5 responses\n#What's the probability that Amir responds to 5 leads in a day, given that he responds to an average of 4?\ndpois(5,  lambda = 4)\n\n[1] 0.1562935\n\n# Probability of 5 responses from coworker\n#Amir's coworker responds to an average of 5.5 leads per day. What is the probability that she answers 5 leads in a day?\ndpois(5,  lambda = 5.5)\n\n[1] 0.1714007\n\n# Probability of 2 or fewer responses\nppois(2,  lambda = 4)\n\n[1] 0.2381033\n\n# Probability of > 10 responses\nppois(10,  lambda = 4, lower.tail = FALSE)\n\n[1] 0.002839766\n\n\nModeling time between leads To further evaluate Amir’s performance, you want to know how much time it takes him to respond to a lead after he opens it. On average, it takes 2.5 hours for him to respond. In this exercise, you’ll calculate probabilities of different amounts of time passing between Amir receiving a lead and sending a response.\n\n# Probability response takes < 1 hour\npexp(1, rate =  1/2.5)\n\n[1] 0.32968\n\n# Probability response takes > 4 hours\npexp(4, rate =  1/2.5, lower.tail = FALSE)\n\n[1] 0.2018965\n\n# Probability response takes 3-4 hours\npexp(4, rate =  1/2.5) - pexp(3, rate =  1/2.5)\n\n[1] 0.09929769"
  },
  {
    "objectID": "datacamp/introduction_statistics_R/introduction_stats.html#correlation-and-experimental-design",
    "href": "datacamp/introduction_statistics_R/introduction_stats.html#correlation-and-experimental-design",
    "title": "Introduction to Statistics with R",
    "section": "Correlation and Experimental Design",
    "text": "Correlation and Experimental Design\n\nRelationships between variables\nIn this chapter, you’ll be working with a dataset world_happiness containing results from the 2019 World Happiness Report. The report scores various countries based on how happy people in that country are. It also ranks each country on various societal aspects such as social support, freedom, corruption, and others. The dataset also includes the GDP per capita and life expectancy for each country.\nIn this exercise, you’ll examine the relationship between a country’s life expectancy (life_exp) and happiness score (happiness_score) both visually and quantitatively. Both dplyr and ggplot2 are loaded and world_happiness is available.\n\nworld_happiness <- readRDS(\"world_happiness_sugar.rds\")\n# Add a linear trendline to scatterplot\nggplot(world_happiness, aes(life_exp, happiness_score)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",se = F)\n\n\n\n# Correlation between life_exp and happiness_score\ncor(world_happiness$life_exp,world_happiness$happiness_score )\n\n[1] 0.7737615\n\n\n\n\nWhat can’t correlation measure?\nWhile the correlation coefficient is a convenient way to quantify the strength of a relationship between two variables, it’s far from perfect. In this exercise, you’ll explore one of the caveats of the correlation coefficient by examining the relationship between a country’s GDP per capita (gdp_per_cap) and happiness score.\nBoth dplyr and ggplot2 are loaded and world_happiness is available.\n\n# Scatterplot of gdp_per_cap and life_exp\nggplot(world_happiness, aes(gdp_per_cap, life_exp)) +\n  geom_point()\n\n\n\n# Correlation between gdp_per_cap and life_exp\ncor(world_happiness$life_exp,world_happiness$gdp_per_cap )\n\n[1] 0.7235027\n\n\n\nThe correlation coefficient can’t account for any relationships that aren’t linear, regardless of strength.\n\n\n\nTransforming variables\nWhen variables have skewed distributions, they often require a transformation in order to form a linear relationship with another variable so that correlation can be computed. In this exercise, you’ll perform a transformation yourself.\nBoth dplyr and ggplot2 are loaded and world_happiness is available.\n\n# Scatterplot of happiness_score vs. gdp_per_cap\nggplot(world_happiness, aes(gdp_per_cap, happiness_score)) +\n  geom_point()\n\n\n\n# Calculate correlation\ncor(world_happiness$happiness_score,world_happiness$gdp_per_cap )\n\n[1] 0.7601853\n\n# Create log_gdp_per_cap column\nworld_happiness <- world_happiness %>%\n  mutate(log_gdp_per_cap = log(gdp_per_cap))\n\n# Scatterplot of happiness_score vs. log_gdp_per_cap\nggplot(world_happiness, aes(log_gdp_per_cap, happiness_score)) +\n  geom_point()\n\n\n\n# Calculate correlation\ncor(world_happiness$log_gdp_per_cap,\n world_happiness$happiness_score)\n\n[1] 0.7965484\n\n\n\nThe relationship between GDP per capita and happiness became more linear by applying a log transformation. Log transformations are great to use on variables with a skewed distribution, such as GDP.\n\n\n\nDoes sugar improve happiness?\nA new column has been added to world_happiness called grams_sugar_per_day, which contains the average amount of sugar eaten per person per day in each country. In this exercise, you’ll examine the effect of a country’s average sugar consumption on its happiness score.\nBoth dplyr and ggplot2 are loaded and world_happiness is available.\n\n# Scatterplot of grams_sugar_per_day and happiness_score\nggplot(world_happiness, aes(grams_sugar_per_day, happiness_score)) +\n  geom_point()\n\n\n\n# Correlation between grams_sugar_per_day and happiness_score\ncor(world_happiness$grams_sugar_per_day,\n world_happiness$happiness_score)\n\n[1] 0.69391\n\n\n\nIf correlation always implied that one thing causes another, people may do some nonsensical things, like eat more sugar to be happier."
  },
  {
    "objectID": "datacamp/string-manipulation-with-stringr-in-r/stringR.html",
    "href": "datacamp/string-manipulation-with-stringr-in-r/stringR.html",
    "title": "String manipulation with stringr in r",
    "section": "",
    "text": "Let’s get started by entering some strings in R. In t he video you saw that you use quotes to tell R to interpret something as a string. Both double quotes (“) and single (’) quotes work, but there are some guidelines for which to use.\nFirst, you should prefer double quotes (“) to single quotes (’). That means, whenever you are defining a string your first intuition should be to use”.\nUnfortunately if your string has ” inside it, R will interpret the double quote as “this is the end of the string”, not as “this is the character”“. This is one time you can forget the first guideline and use the single quote, ’, to define the string.\nFinally, there are cases where you need both ’ and ” inside the string. In this case, fall back to the first guideline and use ” to define the string, but you’ll have to escape any double quotes inside the string using a backslash (i.e. “).\nTo practice, you are going to enter a few lines from Lewis Carroll’s Alice’s Adventures in Wonderland. Alice has just arrived at the tea party…\n\n# Define line1\nline1 <-\"The table was a large one, but the three were all crowded together at one corner of it:\"\n\n# Define line2\nline2 <- '\"No room! No room!\" they cried out when they saw Alice coming.'\n\n# Define line3\nline3 <- '\"There\\'s plenty of room!\" said Alice indignantly, and she sat down in a large arm-chair at one end of the table.'"
  },
  {
    "objectID": "datacamp/string-manipulation-with-stringr-in-r/stringR.html#what-you-see-isnt-always-what-you-have",
    "href": "datacamp/string-manipulation-with-stringr-in-r/stringR.html#what-you-see-isnt-always-what-you-have",
    "title": "String manipulation with stringr in r",
    "section": "What you see isn’t always what you have",
    "text": "What you see isn’t always what you have\nTake a look at line2, the string you just defined, by printing it:\nline2 Even though you used single quotes so you didn’t have to escape any double quotes, when R prints it, you’ll see escaped double quotes (“)! R doesn’t care how you defined the string, it only knows what the string represents, in this case, a string with double quotes inside.\nWhen you ask R for line2 it is actually calling print(line2) and the print() method for strings displays strings as you might enter them. If you want to see the string it represents you’ll need to use a different function: writeLines().\nYou can pass writeLines() a vector of strings and it will print them to the screen, each on a new line. This is a great way to check the string you entered really does represent the string you wanted.\n\nPerfect! The function cat() is very similar to writeLines(), but by default separates elements with a space, and will attempt to convert non-character objects to a string. We won’t use it in this course, but you might see it in other people’s code.\n\n\n# Putting lines in a vector\nlines <- c(line1, line2, line3)\n\n# Print lines\nlines\n\n[1] \"The table was a large one, but the three were all crowded together at one corner of it:\"                           \n[2] \"\\\"No room! No room!\\\" they cried out when they saw Alice coming.\"                                                  \n[3] \"\\\"There's plenty of room!\\\" said Alice indignantly, and she sat down in a large arm-chair at one end of the table.\"\n\n# Use writeLines() on lines\nwriteLines(lines)\n\nThe table was a large one, but the three were all crowded together at one corner of it:\n\"No room! No room!\" they cried out when they saw Alice coming.\n\"There's plenty of room!\" said Alice indignantly, and she sat down in a large arm-chair at one end of the table.\n\n# Write lines with a space separator\nwriteLines(lines,sep=\"\")\n\nThe table was a large one, but the three were all crowded together at one corner of it:\"No room! No room!\" they cried out when they saw Alice coming.\"There's plenty of room!\" said Alice indignantly, and she sat down in a large arm-chair at one end of the table.\n\n# Use writeLines() on the string \"hello\\n\\U1F30D\"\nwriteLines(\"hello\\n\\U1F30D\")\n\nhello\n🌍"
  },
  {
    "objectID": "datacamp/string-manipulation-with-stringr-in-r/stringR.html#escape-sequences",
    "href": "datacamp/string-manipulation-with-stringr-in-r/stringR.html#escape-sequences",
    "title": "String manipulation with stringr in r",
    "section": "Escape sequences",
    "text": "Escape sequences\nYou might have been surprised at the output from the last part of the last exercise. How did you get two lines from one string, and how did you get that little globe? The key is the\n\nA sequence in a string that starts with a  is called an escape sequence and allows us to include special characters in our strings. You saw one escape sequence in the first exercise: ” is used to denote a double quote.\nIn hello \\n\\U1F30D there are two escape sequences: \\n gives a newline, and \\U followed by up to 8 hex digits sequence denotes a particular Unicode character.\nUnicode is a standard for representing characters that might not be on your keyboard. Each available character has a Unicode code point: a number that uniquely identifies it. These code points are generally written in hex notation, that is, using base 16 and the digits 0-9 and A-F. You can find the code point for a particular character by looking up a code chart. If you only need four digits for the codepoint, an alternative escape sequence is \\u.\nWhen R comes across a  it assumes you are starting an escape, so if you actually need a backslash in your string you’ll need the sequence \\\n\n# Should display: To have a \\ you need \\\\\nwriteLines(\"To have a \\\\ you need \\\\\\\\\")\n\nTo have a \\ you need \\\\\n\n# Should display: \n# This is a really \n# really really \n# long string\nwriteLines(\"This is a really \\n really really \\n long string\")\n\nThis is a really \n really really \n long string\n\n# Use writeLines() with \n# \"\\u0928\\u092e\\u0938\\u094d\\u0924\\u0947 \\u0926\\u0941\\u0928\\u093f\\u092f\\u093e\"\nwriteLines(\"\\u0928\\u092e\\u0938\\u094d\\u0924\\u0947 \\u0926\\u0941\\u0928\\u093f\\u092f\\u093e\")\n\nनमस्ते दुनिया"
  },
  {
    "objectID": "datacamp/string-manipulation-with-stringr-in-r/stringR.html#using-format-with-numbers",
    "href": "datacamp/string-manipulation-with-stringr-in-r/stringR.html#using-format-with-numbers",
    "title": "String manipulation with stringr in r",
    "section": "Using format() with numbers",
    "text": "Using format() with numbers\nThe behavior of format() can be pretty confusing, so you’ll spend most of this exercise exploring how it works.\nRecall from the video, the scientific argument to format() controls whether the numbers are displayed in fixed (scientific = FALSE) or scientific (scientific = TRUE) format.\nWhen the representation is scientific, the digits argument is the number of digits before the exponent. When the representation is fixed, digits controls the significant digits used for the smallest (in magnitude) number. Each other number will be formatted to match the number of decimal places in the smallest number. This means the number of decimal places you get in your output depends on all the values you are formatting!\nFor example, if the smallest number is 0.0011, and digits = 1, then 0.0011 requires 3 places after the decimal to represent it to 1 significant digit, 0.001. Every other number will be formatted to 3 places after the decimal point.\nSo, how many decimal places will you get if 1.0011 is the smallest number? You’ll find out in this exercise.\n\n# Some vectors of numbers\npercent_change  <- c(4, -1.91, 3.00, -5.002)\nincome <-  c(72.19, 1030.18, 10291.93, 1189192.18)\np_values <- c(0.12, 0.98, 0.0000191, 0.00000000002)\n\n# Format c(0.0011, 0.011, 1) with digits = 1\nformat(c(0.0011, 0.011, 1), digits = 1)\n\n[1] \"0.001\" \"0.011\" \"1.000\"\n\n# Format c(1.0011, 2.011, 1) with digits = 1\n\nformat(c(1.0011, 2.011, 1) , digits = 1)\n\n[1] \"1\" \"2\" \"1\"\n\n# Format percent_change to one place after the decimal point\n\nformat(percent_change, digits = 2)\n\n[1] \" 4.0\" \"-1.9\" \" 3.0\" \"-5.0\"\n\n# Format income to whole numbers\nformat(income, digits = 2)\n\n[1] \"     72\" \"   1030\" \"  10292\" \"1189192\"\n\n# Format p_values in fixed format\nformat(p_values, scientific = FALSE)\n\n[1] \"0.12000000000\" \"0.98000000000\" \"0.00001910000\" \"0.00000000002\""
  },
  {
    "objectID": "datacamp/string-manipulation-with-stringr-in-r/stringR.html#controlling-other-aspects-of-the-string",
    "href": "datacamp/string-manipulation-with-stringr-in-r/stringR.html#controlling-other-aspects-of-the-string",
    "title": "String manipulation with stringr in r",
    "section": "Controlling other aspects of the string",
    "text": "Controlling other aspects of the string\nNot only does format() control the way the number is represented, it also controls some of the properties of the resulting string that affect its display.\nFor example, by default format() will pad the start of the strings with spaces so that the decimal points line up, which is really useful if you are presenting the numbers in a vertical column. However, if you are putting the number in the middle of a sentence, you might not want these extra spaces. You can set trim = TRUE to remove them.\nWhen numbers are long it can be helpful to “prettify” them, for example instead of 1000000000 display 1,000,000,000. In this case a , is added every 3 digits. This can be controlled by the big.interval and big.mark arguments, e.g. format(1000000000, big.mark = “,”, big.interval = 3, scientific = FALSE). These arguments are actually passed on to prettyNum() so head there for any further details.\n\nformatted_income <- format(income, digits = 2)\n\n# Print formatted_income\nformatted_income\n\n[1] \"     72\" \"   1030\" \"  10292\" \"1189192\"\n\n# Call writeLines() on the formatted income\nwriteLines(formatted_income)\n\n     72\n   1030\n  10292\n1189192\n\n# Define trimmed_income\ntrimmed_income <- format(income, digits = 2, trim = TRUE)\n\n# Call writeLines() on the trimmed_income\n\nwriteLines(trimmed_income)\n\n72\n1030\n10292\n1189192\n\n# Define pretty_income\npretty_income <- format(income, digits = 2, big.mark = \",\")\n\n# Call writeLines() on the pretty_income\nwriteLines(pretty_income)\n\n       72\n    1,030\n   10,292\n1,189,192"
  },
  {
    "objectID": "datacamp/string-manipulation-with-stringr-in-r/stringR.html#formatc",
    "href": "datacamp/string-manipulation-with-stringr-in-r/stringR.html#formatc",
    "title": "String manipulation with stringr in r",
    "section": "formatC()",
    "text": "formatC()\nThe function formatC() provides an alternative way to format numbers based on C style syntax.\nRather than a scientific argument, formatC() has a format argument that takes a code representing the required format. The most useful are:\n“f” for fixed, “e” for scientific, and “g” for fixed unless scientific saves space When using scientific format, the digits argument behaves like it does in format(); it specifies the number of significant digits. However, unlike format(), when using fixed format, digits is the number of digits after the decimal point. This is more predictable than format(), because the number of places after the decimal is fixed regardless of the values being formatted.\nformatC() also formats numbers individually, which means you always get the same output regardless of other numbers in the vector.\nThe flag argument allows you to provide some modifiers that, for example, force the display of the sign (flag = “+”), left align numbers (flag = “-”) and pad numbers with leading zeros (flag = “0”). You’ll see an example in this exercise\n\n# From the format() exercise\nx <- c(0.0011, 0.011, 1)\ny <- c(1.0011, 2.011, 1)\n\n# formatC() on x with format = \"f\", digits = 1\nformatC(x, format = \"f\", digits = 1)\n\n[1] \"0.0\" \"0.0\" \"1.0\"\n\n# formatC() on y with format = \"f\", digits = 1\nformatC(y, format = \"f\", digits = 1)\n\n[1] \"1.0\" \"2.0\" \"1.0\"\n\n# Format percent_change to one place after the decimal point\nformatC(percent_change, format = \"f\", digits = 1)\n\n[1] \"4.0\"  \"-1.9\" \"3.0\"  \"-5.0\"\n\n# percent_change with flag = \"+\"\nformatC(percent_change, format = \"f\", digits = 1 , flag = \"+\")\n\n[1] \"+4.0\" \"-1.9\" \"+3.0\" \"-5.0\"\n\n# Format p_values using format = \"g\" and digits = 2\n\nformatC( p_values, format = \"g\", digits = 2)\n\n[1] \"0.12\"    \"0.98\"    \"1.9e-05\" \"2e-11\""
  },
  {
    "objectID": "datacamp/string-manipulation-with-stringr-in-r/stringR.html#annotation-of-numbers",
    "href": "datacamp/string-manipulation-with-stringr-in-r/stringR.html#annotation-of-numbers",
    "title": "String manipulation with stringr in r",
    "section": "Annotation of numbers",
    "text": "Annotation of numbers\nTo get a handle on using paste(), you are going to annotate some of your formatted number strings. The key points to remember are: The vectors you pass to paste() are pasted together element by element, using the sep argument to combine them. If the vectors passed to paste() aren’t the same length, the shorter vectors are recycled up to the length of the longest one. Only use collapse if you want a single string as output. collapse specifies the string to place between different elements.\n\npretty_percent <-  c(\"4\", \"-1.9\", \"3\", \"-5\")\nyears <-  c(\"2010\", \"2011\", \"2012\", \"2013\")\n# Add $ to pretty_income\npaste(\"$\", pretty_income, sep = \"\")\n\n[1] \"$       72\" \"$    1,030\" \"$   10,292\" \"$1,189,192\"\n\n# Add % to pretty_percent\npaste( pretty_percent,\"%\", sep = \"\")\n\n[1] \"4%\"    \"-1.9%\" \"3%\"    \"-5%\"  \n\n# Create vector with elements like 2010: +4.0%`\nyear_percent <- paste(years, \": \", pretty_percent, \"%\", sep =\"\")\n\n# Collapse all years into single string\n\npaste(year_percent, collapse = \",\")\n\n[1] \"2010: 4%,2011: -1.9%,2012: 3%,2013: -5%\""
  },
  {
    "objectID": "datacamp/string-manipulation-with-stringr-in-r/stringR.html#some-simple-text-statistics",
    "href": "datacamp/string-manipulation-with-stringr-in-r/stringR.html#some-simple-text-statistics",
    "title": "String manipulation with stringr in r",
    "section": "Some simple text statistics",
    "text": "Some simple text statistics\nGenerally, specifying simplify = TRUE will give you output that is easier to work with, but you’ll always get n pieces (even if some are empty, ““).\nSometimes, you want to know how many pieces a string can be split into, or you want to do something with every piece before moving to a simpler structure. This is a situation where you don’t want to simplify and you’ll have to process the output with something like lapply().\nAs an example, you’ll be performing some simple text statistics on your lines from Alice’s Adventures in Wonderland from Chapter 1. Your goal will be to calculate how many words are in each line, and the average length of words in each line.\nTo do these calculations, you’ll need to split the lines into words. One way to break a sentence into words is to split on an empty space ” “. This is a little naive because, for example, it wouldn’t pick up words separated by a newline escape sequence like in”two\\nwords”, but since this situation doesn’t occur in your lines, it will do.\n\nlines <- c(\"The table was a large one, but the three were all crowded together at one corner of it:\",\n           \"\\\\No room! No room!\\ they cried out when they saw Alice coming.\",\n           \"\\\\There’s plenty of room!\\ said Alice indignantly, and she sat down in a large arm-chair at one end of the table.\")\n\n\n# Split lines into words\nwords <- str_split(lines, patter = fixed(\" \"))\n\n# Number of words per line\nlapply(words, length)\n\n[[1]]\n[1] 18\n\n[[2]]\n[1] 12\n\n[[3]]\n[1] 21\n\n# Number of characters in each word\nword_lengths <- lapply(words, nchar)\n  \n  \n# Average word length per line\nlapply( word_lengths, mean)\n\n[[1]]\n[1] 3.888889\n\n[[2]]\n[1] 4.166667\n\n[[3]]\n[1] 4.333333"
  },
  {
    "objectID": "datacamp/string-manipulation-with-stringr-in-r/stringR.html#matching-any-character",
    "href": "datacamp/string-manipulation-with-stringr-in-r/stringR.html#matching-any-character",
    "title": "String manipulation with stringr in r",
    "section": "Matching any character",
    "text": "Matching any character\nIn a regular expression you can use a wildcard to match a single character, no matter what the character is. In rebus it is specified with ANY_CHAR. Try typing ANY_CHAR in the console. You should see that in the regular expression language this is specified by a dot, ..\nFor example, “c” %R% ANY_CHAR %R% “t” will look for patterns like “c_t” where the blank can be any character. Consider the strings: “cat”, “coat”, “scotland” and “tic toc”. Where would the matches to “c” %R% ANY_CHAR %R% “t” be?\nTest your intuition by running:\nstr_view(c(“cat”, “coat”, “scotland”, “tic toc”), pattern = “c” %R% ANY_CHAR %R% “t”) Notice that ANY_CHAR will match a space character (c t in tic toc). It will also match numbers or punctuation symbols, but ANY_CHAR will only ever match one character, which is why we get no match in coat\n\n# Match two characters, where the second is a \"t\"\nstr_view(x, pattern = \"\" %R% ANY_CHAR %R% \"t\")\n\n[1] │ c<at>\n[2] │ co<at>\n[3] │ sc<ot>land\n[4] │ tic< t>oc\n\n# Match a \"t\" followed by any character\nstr_view(x, pattern =  \"t\" %R% ANY_CHAR %R% \"\")\n\n[3] │ sco<tl>and\n[4] │ <ti>c <to>c\n\n# Match two characters\nstr_view(x, pattern = ANY_CHAR %R% ANY_CHAR)\n\n[1] │ <ca>t\n[2] │ <co><at>\n[3] │ <sc><ot><la><nd>\n[4] │ <ti><c ><to>c\n\n# Match a string with exactly three characters\nstr_view(x, pattern = START %R% ANY_CHAR %R% ANY_CHAR %R% ANY_CHAR %R% END)\n\n[1] │ <cat>"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html",
    "href": "datacamp/functions_in_R/functions_R.html",
    "title": "Functions in R",
    "section": "",
    "text": "One way to make your code more readable is to be careful about the order you pass arguments when you call functions, and whether you pass the arguments by position or by name.\ngold_medals, a numeric vector of the number of gold medals won by each country in the 2016 Summer Olympics, is provided.\nFor convenience, the arguments of median() and rank() are displayed using args(). Setting rank()’s na.last argument to “keep” means “keep the rank of NA values as NA”.\nBest practice for calling functions is to include them in the order shown by args(), and to only name rare arguments.\n\ngd_nms <-  c(\"USA\", \"GBR\", \"CHN\", \"RUS\", \"GER\", \"JPN\", \"FRA\", \"KOR\", \"ITA\", \n            \"AUS\", \"NED\", \"HUN\", \"BRA\", \"ESP\", \"KEN\", \"JAM\", \"CRO\", \"CUB\", \n            \"NZL\", \"CAN\", \"UZB\", \"KAZ\", \"COL\", \"SUI\", \"IRI\", \"GRE\", \"ARG\", \n            \"DEN\", \"SWE\", \"RSA\", \"UKR\", \"SRB\", \"POL\", \"PRK\", \"BEL\", \"THA\", \n            \"SVK\", \"GEO\", \"AZE\", \"BLR\", \"TUR\", \"ARM\", \"CZE\", \"ETH\", \"SLO\", \n            \"INA\", \"ROU\", \"BRN\", \"VIE\", \"TPE\", \"BAH\", \"IOA\", \"CIV\", \"FIJ\", \n            \"JOR\", \"KOS\", \"PUR\", \"SIN\", \"TJK\", \"MAS\", \"MEX\", \"VEN\", \"ALG\", \n            \"IRL\", \"LTU\", \"BUL\", \"IND\", \"MGL\", \"BDI\", \"GRN\", \"NIG\", \"PHI\", \n            \"QAT\", \"NOR\", \"EGY\", \"TUN\", \"ISR\", \"AUT\", \"DOM\", \"EST\", \"FIN\", \n            \"MAR\", \"NGR\", \"POR\", \"TTO\", \"UAE\", \"IOC\")\n\ngold_medals <- c(46L, 27L, 26L, 19L, 17L, 12L, 10L, 9L, 8L, 8L, 8L, 8L, 7L, \n                 7L, 6L, 6L, 5L, 5L, 4L, 4L,4L, 3L, 3L, 3L, 3L, 3L, 3L, 2L, 2L, \n                 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L,\n                 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,\n                 1L, 1L, 1L, 1L, 0L,0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, \n                 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,\n                 0L, 0L, 0L, 0L, 0L, 0L, NA)\n\nnames(gold_medals) <- gd_nms\n\ngold_medals\n\nUSA GBR CHN RUS GER JPN FRA KOR ITA AUS NED HUN BRA ESP KEN JAM CRO CUB NZL CAN \n 46  27  26  19  17  12  10   9   8   8   8   8   7   7   6   6   5   5   4   4 \nUZB KAZ COL SUI IRI GRE ARG DEN SWE RSA UKR SRB POL PRK BEL THA SVK GEO AZE BLR \n  4   3   3   3   3   3   3   2   2   2   2   2   2   2   2   2   2   2   1   1 \nTUR ARM CZE ETH SLO INA ROU BRN VIE TPE BAH IOA CIV FIJ JOR KOS PUR SIN TJK MAS \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   0 \nMEX VEN ALG IRL LTU BUL IND MGL BDI GRN NIG PHI QAT NOR EGY TUN ISR AUT DOM EST \n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \nFIN MAR NGR POR TTO UAE IOC \n  0   0   0   0   0   0  NA \n\n# Note the arguments to median()\nargs(median)\n\nfunction (x, na.rm = FALSE, ...) \nNULL\n\n# Rewrite this function call, following best practices\nmedian(gold_medals, na.rm = TRUE)\n\n[1] 1\n\n# Note the arguments to rank()\nargs(rank)\n\nfunction (x, na.last = TRUE, ties.method = c(\"average\", \"first\", \n    \"last\", \"random\", \"max\", \"min\")) \nNULL\n\n# Rewrite this function call, following best practices\n\nrank(-gold_medals, na.last = \"keep\", ties.method=  \"min\")\n\nUSA GBR CHN RUS GER JPN FRA KOR ITA AUS NED HUN BRA ESP KEN JAM CRO CUB NZL CAN \n  1   2   3   4   5   6   7   8   9   9   9   9  13  13  15  15  17  17  19  19 \nUZB KAZ COL SUI IRI GRE ARG DEN SWE RSA UKR SRB POL PRK BEL THA SVK GEO AZE BLR \n 19  22  22  22  22  22  22  28  28  28  28  28  28  28  28  28  28  28  39  39 \nTUR ARM CZE ETH SLO INA ROU BRN VIE TPE BAH IOA CIV FIJ JOR KOS PUR SIN TJK MAS \n 39  39  39  39  39  39  39  39  39  39  39  39  39  39  39  39  39  39  39  60 \nMEX VEN ALG IRL LTU BUL IND MGL BDI GRN NIG PHI QAT NOR EGY TUN ISR AUT DOM EST \n 60  60  60  60  60  60  60  60  60  60  60  60  60  60  60  60  60  60  60  60 \nFIN MAR NGR POR TTO UAE IOC \n 60  60  60  60  60  60  NA"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#your-first-function-tossing-a-coin",
    "href": "datacamp/functions_in_R/functions_R.html#your-first-function-tossing-a-coin",
    "title": "Functions in R",
    "section": "Your first function: tossing a coin",
    "text": "Your first function: tossing a coin\nTime to write your first function! It’s a really good idea when writing functions to start simple. You can always make a function more complicated later if it’s really necessary, so let’s not worry about arguments for now.\n\ncoin_sides <- c(\"head\", \"tail\")\n\n# Sample from coin_sides once\nsample(coin_sides, 1)\n\n[1] \"tail\"\n\n# Your functions, from previous steps\ntoss_coin <- function() {\n  coin_sides <- c(\"head\", \"tail\")\n  sample(coin_sides, 1)\n}\n\n# Call your function\ntoss_coin()\n\n[1] \"tail\""
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#inputs-to-functions",
    "href": "datacamp/functions_in_R/functions_R.html#inputs-to-functions",
    "title": "Functions in R",
    "section": "Inputs to functions",
    "text": "Inputs to functions\nMost functions require some sort of input to determine what to compute. The inputs to functions are called arguments. You specify them inside the parentheses after the word “function.”\nAs mentioned in the video, the following exercises assume that you are using sample() to do random sampling.\n\n# Update the function to return n coin tosses\ntoss_coin <- function(n_flips) {\n  coin_sides <- c(\"head\", \"tail\")\n  sample(coin_sides, n_flips, replace = TRUE)\n}\n\n# Generate 10 coin tosses\ntoss_coin(1000) %>% table() %>% prop.table()\n\n.\n head  tail \n0.493 0.507"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#multiple-inputs-to-functions",
    "href": "datacamp/functions_in_R/functions_R.html#multiple-inputs-to-functions",
    "title": "Functions in R",
    "section": "Multiple inputs to functions",
    "text": "Multiple inputs to functions\nIf a function should have more than one argument, list them in the function signature, separated by commas. To solve this exercise, you need to know how to specify sampling weights to sample(). Set the prob argument to a numeric vector with the same length as x. Each value of prob is the probability of sampling the corresponding element of x, so their values add up to one. In the following example, each sample has a 20% chance of “bat”, a 30% chance of “cat” and a 50% chance of “rat”.\n\n# Update the function so heads have probability p_head\ntoss_coin <- function(n_flips, p_head) {\n  coin_sides <- c(\"head\", \"tail\")\n  # Define a vector of weights\n  weights <- c(p_head,1- p_head)\n  # Modify the sampling to be weighted\n  sample(coin_sides, n_flips, replace = TRUE, prob = weights)\n}\n\n# Generate 10 coin tosses\ntoss_coin(10,p_head=.8)\n\n [1] \"head\" \"head\" \"head\" \"head\" \"tail\" \"tail\" \"tail\" \"tail\" \"head\" \"head\""
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#data-or-detail",
    "href": "datacamp/functions_in_R/functions_R.html#data-or-detail",
    "title": "Functions in R",
    "section": "Data or detail?",
    "text": "Data or detail?\nRecall that data arguments are what a function computes on, and detail arguments advise on how the computation should be performed. Each of the arguments to t.test() is shown, along with a brief description of it."
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#renaming-glm",
    "href": "datacamp/functions_in_R/functions_R.html#renaming-glm",
    "title": "Functions in R",
    "section": "Renaming GLM",
    "text": "Renaming GLM\nR’s generalized linear regression function, glm(), suffers the same usability problems as lm(): its name is an acronym, and its formula and data arguments are in the wrong order. To solve this exercise, you need to know two things about generalized linear regression: glm() formulas are specified like lm() formulas: response is on the left, and explanatory variables are added on the right. To model count data, set glm()’s family argument to poisson, making it a Poisson regression. Here you’ll use data on the number of yearly visits to Snake River at Jackson Hole, Wyoming, snake_river_visits.\n\nsnake_river_visits <-read_rds(\"snake_river_visits.rds\")\n# From previous step\nrun_poisson_regression <- function(data, formula) {\n  glm(formula, data, family = poisson)\n}\n\n# Re-run the Poisson regression, using your function\nmodel <- snake_river_visits %>%\n  run_poisson_regression(n_visits ~ gender + income + travel)\n\n# Run this to see the predictions\nsnake_river_visits %>%\n  mutate(predicted_n_visits = predict(model, ., type = \"response\"))%>%\n  arrange(desc(predicted_n_visits)) %>%\n    head() %>% kable()\n\n\n\n\nn_visits\ngender\nincome\ntravel\npredicted_n_visits\n\n\n\n\n80\nfemale\n[$0,$25k]\n[0h,0.25h]\n86.5186\n\n\n35\nfemale\n[$0,$25k]\n[0h,0.25h]\n86.5186\n\n\n50\nfemale\n[$0,$25k]\n[0h,0.25h]\n86.5186\n\n\n125\nfemale\n[$0,$25k]\n[0h,0.25h]\n86.5186\n\n\n24\nfemale\n[$0,$25k]\n[0h,0.25h]\n86.5186\n\n\n100\nfemale\n[$0,$25k]\n[0h,0.25h]\n86.5186"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#numeric-defaults",
    "href": "datacamp/functions_in_R/functions_R.html#numeric-defaults",
    "title": "Functions in R",
    "section": "Numeric defaults",
    "text": "Numeric defaults\ncut_by_quantile() converts a numeric vector into a categorical variable where quantiles define the cut points. This is a useful function, but at the moment you have to specify five arguments to make it work. This is too much thinking and typing. By specifying default arguments, you can make it easier to use. Let’s start with n, which specifies how many categories to cut x into.A numeric vector of the number of visits to Snake River is provided as n_visits.\n\n# Set the default for n to 5\nn_visits = snake_river_visits$n_visits\ncut_by_quantile <- function(x, n=5, na.rm, labels, interval_type) {\n  probs <- seq(0, 1, length.out = n + 1)\n  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)\n  right <- switch(interval_type, \"(lo, hi]\" = TRUE, \"[lo, hi)\" = FALSE)\n  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)\n}\n\n# Remove the n argument from the call\ncut_by_quantile(\n  n_visits, \n  na.rm = FALSE, \n  labels = c(\"very low\", \"low\", \"medium\", \"high\", \"very high\"),\n  interval_type = \"(lo, hi]\"\n) %>% head()\n\n[1] very low very low very low very low very low very low\nLevels: very low low medium high very high"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#logical-defaults",
    "href": "datacamp/functions_in_R/functions_R.html#logical-defaults",
    "title": "Functions in R",
    "section": "Logical defaults",
    "text": "Logical defaults\ncut_by_quantile() is now slightly easier to use, but you still always have to specify the na.rm argument. This removes missing values – it behaves the same as the na.rm argument to mean() or sd().\nWhere functions have an argument for removing missing values, the best practice is to not remove them by default (in case you hadn’t spotted that you had missing values). That means that the default for na.rm should be FALSE.\n\n# Set the default for na.rm to FALSE\ncut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels, interval_type) {\n  probs <- seq(0, 1, length.out = n + 1)\n  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)\n  right <- switch(interval_type, \"(lo, hi]\" = TRUE, \"[lo, hi)\" = FALSE)\n  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)\n}\n\n# Remove the na.rm argument from the call\ncut_by_quantile(\n  n_visits, \n  labels = c(\"very low\", \"low\", \"medium\", \"high\", \"very high\"),\n  interval_type = \"(lo, hi]\"\n) %>% head()\n\n[1] very low very low very low very low very low very low\nLevels: very low low medium high very high"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#null-defaults",
    "href": "datacamp/functions_in_R/functions_R.html#null-defaults",
    "title": "Functions in R",
    "section": "NULL defaults",
    "text": "NULL defaults\nThe cut() function used by cut_by_quantile() can automatically provide sensible labels for each category. The code to generate these labels is pretty complicated, so rather than appearing in the function signature directly, its labels argument defaults to NULL, and the calculation details are shown on the ?cut help page.\n\n# Set the default for labels to NULL\ncut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels=NULL, interval_type) {\n  probs <- seq(0, 1, length.out = n + 1)\n  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)\n  right <- switch(interval_type, \"(lo, hi]\" = TRUE, \"[lo, hi)\" = FALSE)\n  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)\n}\n\n# Remove the labels argument from the call\ncut_by_quantile(\n  n_visits,\n  #labels = c(\"very low\", \"low\", \"medium\", \"high\", \"very high\"),\n  interval_type = \"(lo, hi]\"\n) %>% head()\n\n[1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1]\nLevels: [0,1] (1,2] (2,10] (10,35] (35,350]"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#categorical-defaults",
    "href": "datacamp/functions_in_R/functions_R.html#categorical-defaults",
    "title": "Functions in R",
    "section": "Categorical defaults",
    "text": "Categorical defaults\nWhen cutting up a numeric vector, you need to worry about what happens if a value lands exactly on a boundary. You can either put this value into a category of the lower interval or the higher interval. That is, you can choose your intervals to include values at the top boundary but not the bottom (in mathematical terminology, “open on the left, closed on the right”, or (lo, hi]). Or you can choose the opposite (“closed on the left, open on the right”, or [lo, hi)). cut_by_quantile() should allow these two choices.\nThe pattern for categorical defaults is:\nfunction(cat_arg = c(“choice1”, “choice2”)) { cat_arg <- match.arg(cat_arg) }\nFree hint: In the console, type head(rank) to see the start of rank()’s definition, and look at the ties.method argument.\n\n# Set the categories for interval_type to \"(lo, hi]\" and \"[lo, hi)\"\ncut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels = NULL, \n                            interval_type= c(\"(lo, hi]\", \"[lo, hi)\")) {\n  # Match the interval_type argument\n  interval_type <- match.arg(interval_type)\n  probs <- seq(0, 1, length.out = n + 1)\n  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)\n  right <- switch(interval_type, \"(lo, hi]\" = TRUE, \"[lo, hi)\" = FALSE)\n  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)\n}\n\n# Remove the interval_type argument from the call\ncut_by_quantile(n_visits) %>% head()\n\n[1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1]\nLevels: [0,1] (1,2] (2,10] (10,35] (35,350]"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#harmonic-mean",
    "href": "datacamp/functions_in_R/functions_R.html#harmonic-mean",
    "title": "Functions in R",
    "section": "Harmonic mean",
    "text": "Harmonic mean\nThe harmonic mean is the reciprocal of the arithmetic mean of the reciprocal of the data. That is\nThe harmonic mean is often used to average ratio data. You’ll be using it on the price/earnings ratio of stocks in the Standard and Poor’s 500 index, provided as std_and_poor500. Price/earnings ratio is a measure of how expensive a stock is.\nThe dplyr package is loaded.\n\nstd_and_poor500 <- read_rds(\"std_and_poor500_with_pe_2019-06-21.rds\")\n# Look at the Standard and Poor 500 data\nglimpse(std_and_poor500)\n\nRows: 505\nColumns: 5\n$ symbol   <chr> \"MMM\", \"ABT\", \"ABBV\", \"ABMD\", \"ACN\", \"ATVI\", \"ADBE\", \"AMD\", \"…\n$ company  <chr> \"3M Company\", \"Abbott Laboratories\", \"AbbVie Inc.\", \"ABIOMED …\n$ sector   <chr> \"Industrials\", \"Health Care\", \"Health Care\", \"Health Care\", \"…\n$ industry <chr> \"Industrial Conglomerates\", \"Health Care Equipment\", \"Pharmac…\n$ pe_ratio <dbl> 18.31678, 57.66621, 22.43805, 45.63993, 27.00233, 20.13596, 5…\n\n# Write a function to calculate the reciprocal\n# From previous steps\nget_reciprocal <- function(x) {\n  1 / x\n}\ncalc_harmonic_mean <- function(x) {\n  x %>%\n    get_reciprocal() %>%\n    mean() %>%\n    get_reciprocal()\n}\n\nstd_and_poor500 %>% \n  # Group by sector\n  group_by(sector) %>% \n  # Summarize, calculating harmonic mean of P/E ratio\n  summarise(hmean_pe_ratio = calc_harmonic_mean(pe_ratio))\n\n# A tibble: 11 × 2\n   sector                 hmean_pe_ratio\n   <chr>                           <dbl>\n 1 Communication Services           NA  \n 2 Consumer Discretionary           NA  \n 3 Consumer Staples                 NA  \n 4 Energy                           NA  \n 5 Financials                       NA  \n 6 Health Care                      NA  \n 7 Industrials                      NA  \n 8 Information Technology           NA  \n 9 Materials                        NA  \n10 Real Estate                      32.5\n11 Utilities                        NA"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#dealing-with-missing-values",
    "href": "datacamp/functions_in_R/functions_R.html#dealing-with-missing-values",
    "title": "Functions in R",
    "section": "Dealing with missing values",
    "text": "Dealing with missing values\nIn the last exercise, many sectors had an NA value for the harmonic mean. It would be useful for your function to be able to remove missing values before calculating.\nRather than writing your own code for this, you can outsource this functionality to mean().\nThe dplyr package is loaded.\n\n# From previous step\ncalc_harmonic_mean <- function(x, na.rm = FALSE) {\n  x %>%\n    get_reciprocal() %>%\n    mean(na.rm = na.rm) %>%\n    get_reciprocal()\n}\n\nstd_and_poor500 %>% \n  # Group by sector\n  group_by(sector) %>% \n  # Summarize, calculating harmonic mean of P/E ratio\n  summarise(hmean_pe_ratio = calc_harmonic_mean(pe_ratio, na.rm = TRUE))\n\n# A tibble: 11 × 2\n   sector                 hmean_pe_ratio\n   <chr>                           <dbl>\n 1 Communication Services           17.5\n 2 Consumer Discretionary           15.2\n 3 Consumer Staples                 19.8\n 4 Energy                           13.7\n 5 Financials                       12.9\n 6 Health Care                      26.6\n 7 Industrials                      18.2\n 8 Information Technology           21.6\n 9 Materials                        16.3\n10 Real Estate                      32.5\n11 Utilities                        23.9"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#passing-arguments-with",
    "href": "datacamp/functions_in_R/functions_R.html#passing-arguments-with",
    "title": "Functions in R",
    "section": "Passing arguments with …",
    "text": "Passing arguments with …\nRather than explicitly giving calc_harmonic_mean() and na.rm argument, you can use … to simply “pass other arguments” to mean().\nThe dplyr package is loaded.\n\ncalc_harmonic_mean <- function(x, ...) {\n  x %>%\n    get_reciprocal() %>%\n    mean(...) %>%\n    get_reciprocal()\n}\n\n\nstd_and_poor500 %>% \n  # Group by sector\n  group_by(sector) %>% \n  # Summarize, calculating harmonic mean of P/E ratio\n  summarise(hmean_pe_ratio = calc_harmonic_mean(pe_ratio, na.rm = TRUE))\n\n# A tibble: 11 × 2\n   sector                 hmean_pe_ratio\n   <chr>                           <dbl>\n 1 Communication Services           17.5\n 2 Consumer Discretionary           15.2\n 3 Consumer Staples                 19.8\n 4 Energy                           13.7\n 5 Financials                       12.9\n 6 Health Care                      26.6\n 7 Industrials                      18.2\n 8 Information Technology           21.6\n 9 Materials                        16.3\n10 Real Estate                      32.5\n11 Utilities                        23.9"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#throwing-errors-with-bad-arguments",
    "href": "datacamp/functions_in_R/functions_R.html#throwing-errors-with-bad-arguments",
    "title": "Functions in R",
    "section": "Throwing errors with bad arguments",
    "text": "Throwing errors with bad arguments\nIf a user provides a bad input to a function, the best course of action is to throw an error letting them know. The two rules are\nThrow the error message as soon as you realize there is a problem (typically at the start of the function). Make the error message easily understandable. You can use the assert_*() functions from assertive to check inputs and throw errors when they fail.\n\nlibrary(assertive)\ncalc_harmonic_mean <- function(x, na.rm = FALSE) {\n  # Assert that x is numeric\n  assert_is_numeric(x)\n  x %>%\n    get_reciprocal() %>%\n    mean(na.rm = na.rm) %>%\n    get_reciprocal()\n}\n\n# See what happens when you pass it strings\n#calc_harmonic_mean(std_and_poor500$sector)"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#custom-error-logic",
    "href": "datacamp/functions_in_R/functions_R.html#custom-error-logic",
    "title": "Functions in R",
    "section": "Custom error logic",
    "text": "Custom error logic\nSometimes the assert_*() functions in assertive don’t give the most informative error message. For example, the assertions that check if a number is in a numeric range will tell the user that a value is out of range, but the won’t say why that’s a problem. In that case, you can use the is_*() functions in conjunction with messages, warnings, or errors to define custom feedback.\nThe harmonic mean only makes sense when x has all positive values. (Try calculating the harmonic mean of one and minus one to see why.) Make sure your users know this!\n\ncalc_harmonic_mean <- function(x, na.rm = FALSE) {\n  assert_is_numeric(x)\n  # Check if any values of x are non-positive\n  if(any(is_non_positive(x), na.rm = TRUE)) {\n    # Throw an error\n    stop(\"x contains non-positive values, so the harmonic mean makes no sense.\")\n  }\n  x %>%\n    get_reciprocal() %>%\n    mean(na.rm = na.rm) %>%\n    get_reciprocal()\n}\n\n# See what happens when you pass it negative numbers\n#calc_harmonic_mean(std_and_poor500$pe_ratio - 20)"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#fixing-function-arguments",
    "href": "datacamp/functions_in_R/functions_R.html#fixing-function-arguments",
    "title": "Functions in R",
    "section": "Fixing function arguments",
    "text": "Fixing function arguments\nThe harmonic mean function is almost complete. However, you still need to provide some checks on the na.rm argument. This time, rather than throwing errors when the input is in an incorrect form, you are going to try to fix it.\nna.rm should be a logical vector with one element (that is, TRUE, or FALSE).\nThe assertive package is loaded for you.\n\n# Update the function definition to fix the na.rm argument\ncalc_harmonic_mean <- function(x, na.rm = FALSE) {\n  assert_is_numeric(x)\n  if(any(is_non_positive(x), na.rm = TRUE)) {\n    stop(\"x contains non-positive values, so the harmonic mean makes no sense.\")\n  }\n  # Use the first value of na.rm, and coerce to logical\n  na.rm <- coerce_to(use_first(na.rm), target_class = \"logical\")\n  x %>%\n    get_reciprocal() %>%\n    mean(na.rm = na.rm) %>%\n    get_reciprocal()\n}\n\n# See what happens when you pass it malformed na.rm\ncalc_harmonic_mean(std_and_poor500$pe_ratio, na.rm = 1:5)\n\n[1] 18.23871"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#returning-early",
    "href": "datacamp/functions_in_R/functions_R.html#returning-early",
    "title": "Functions in R",
    "section": "Returning early",
    "text": "Returning early\nSometimes, you don’t need to run through the whole body of a function to get the answer. In that case you can return early from that function using return().\nTo check if x is divisible by n, you can use is_divisible_by(x, n) from assertive.\nAlternatively, use the modulo operator, %%. x %% n gives the remainder when dividing x by n, so x %% n == 0 determines whether x is divisible by n. Try 1:10 %% 3 == 0 in the console.\nTo solve this exercise, you need to know that a leap year is every 400th year (like the year 2000) or every 4th year that isn’t a century (like 1904 but not 1900 or 1905).\nassertive is loaded.\n\nis_leap_year <- function(year) {\n  # If year is div. by 400 return TRUE\n  if(is_divisible_by(year, 400)) {\n    return(TRUE)\n  }\n  # If year is div. by 100 return FALSE\n  if(is_divisible_by(year, 100)) {\n    return(FALSE)\n  }  \n  # If year is div. by 4 return TRUE\n   if(is_divisible_by(year, 4)) {\n    return(TRUE)\n  \n  \n  \n  # Otherwise return FALSE\n   } else return(FALSE)\n}\nis_leap_year(year = 1900)\n\n[1] FALSE"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#returning-invisibly",
    "href": "datacamp/functions_in_R/functions_R.html#returning-invisibly",
    "title": "Functions in R",
    "section": "Returning invisibly",
    "text": "Returning invisibly\nWhen the main purpose of a function is to generate output, like drawing a plot or printing something in the console, you may not want a return value to be printed as well. In that case, the value should be invisibly returned.\nThe base R plot function returns NULL, since its main purpose is to draw a plot. This isn’t helpful if you want to use it in piped code: instead it should invisibly return the plot data to be piped on to the next step.\nRecall that plot() has a formula interface: instead of giving it vectors for x and y, you can specify a formula describing which columns of a data frame go on the x and y axes, and a data argument for the data frame. Note that just like lm(), the arguments are the wrong way round because the detail argument, formula, comes before the data argument.\n\n# Using cars, draw a scatter plot of dist vs. speed\nplt_dist_vs_speed <- plot(dist ~ speed, data = cars)\n\n# Oh no! The plot object is NULL\nplt_dist_vs_speed\n\nNULL\n\n# Define a pipeable plot fn with data and formula args\npipeable_plot <- function(data, formula) {\n  # Call plot() with the formula interface\n  plot(formula, data)\n  # Invisibly return the input dataset\n  invisible(head(data))\n}\n\n# Draw the scatter plot of dist vs. speed again\nplt_dist_vs_speed <- cars %>% \n  pipeable_plot(dist ~ speed)\n\n\n\n# Now the plot object has a value\nplt_dist_vs_speed\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#returning-many-things",
    "href": "datacamp/functions_in_R/functions_R.html#returning-many-things",
    "title": "Functions in R",
    "section": "Returning many things",
    "text": "Returning many things\nFunctions can only return one value. If you want to return multiple things, then you can store them all in a list.\nIf users want to have the list items as separate variables, they can assign each list element to its own variable using zeallot’s multi-assignment operator, %<-%.\nglance(), tidy(), and augment() each take the model object as their only argument.\nThe Poisson regression model of Snake River visits is available as model. broom and zeallot are loaded.\n\nlibrary(zeallot)\nlibrary(broom)\nmodel <- glm(n_visits ~ gender + income + travel, \n             data =snake_river_visits )\n# From previous step\ngroom_model <- function(model) {\n  list(\n    model = glance(model),\n    coefficients = tidy(model),\n    observations = augment(model)\n  )\n}\n\n# Call groom_model on model, assigning to 3 variables\n\nc(mdl, cff,  obs) %<-% groom_model(model)\n\n# See these individual variables\nmdl; cff; obs\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1       820296.     345 -1791. 3599. 3630.  636485.         339   346\n\n\n# A tibble: 7 × 5\n  term              estimate std.error statistic  p.value\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)          61.5       7.83     7.86  5.06e-14\n2 genderfemale         10.8       4.94     2.19  2.88e- 2\n3 income($25k,$55k]    -1.95      7.74    -0.251 8.02e- 1\n4 income($55k,$95k]   -19.3       8.01    -2.42  1.63e- 2\n5 income($95k,$Inf)   -18.6       7.47    -2.49  1.32e- 2\n6 travel(0.25h,4h]    -26.6       6.00    -4.44  1.24e- 5\n7 travel(4h,Infh)     -45.1       6.30    -7.16  5.06e-12\n\n\n# A tibble: 346 × 11\n   .rownames n_visits gender income  travel .fitted .resid   .hat .sigma .cooksd\n   <chr>        <dbl> <fct>  <fct>   <fct>    <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n 1 25               2 female ($95k,… (4h,I…    8.67  -6.67 0.0179   43.4 6.27e-5\n 2 26               1 female ($95k,… (4h,I…    8.67  -7.67 0.0179   43.4 8.29e-5\n 3 27               1 male   ($95k,… (0.25…   16.3  -15.3  0.0153   43.4 2.82e-4\n 4 29               1 male   ($95k,… (4h,I…   -2.18   3.18 0.0122   43.4 9.60e-6\n 5 30               1 female ($55k,… (4h,I…    7.95  -6.95 0.0229   43.4 8.81e-5\n 6 31               1 male   [$0,$2… [0h,0…   61.5  -60.5  0.0326   43.3 9.73e-3\n 7 33              80 female [$0,$2… [0h,0…   72.4    7.61 0.0291   43.4 1.36e-4\n 8 34             104 female ($95k,… [0h,0…   53.8   50.2  0.0215   43.3 4.31e-3\n 9 35              55 male   ($25k,… (0.25…   33.0   22.0  0.0165   43.4 6.29e-4\n10 36             350 female ($25k,… [0h,0…   70.4  280.   0.0215   40.6 1.33e-1\n# ℹ 336 more rows\n# ℹ 1 more variable: .std.resid <dbl>"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#returning-metadata",
    "href": "datacamp/functions_in_R/functions_R.html#returning-metadata",
    "title": "Functions in R",
    "section": "Returning metadata",
    "text": "Returning metadata\nSometimes you want the return multiple things from a function, but you want the result to have a particular class (for example, a data frame or a numeric vector), so returning a list isn’t appropriate. This is common when you have a result plus metadata about the result. (Metadata is “data about the data”. For example, it could be the file a dataset was loaded from, or the username of the person who created the variable, or the number of iterations for an algorithm to converge.)\nIn that case, you can store the metadata in attributes. Recall the syntax for assigning attributes is as follows.\nattr(object, “attribute_name”) <- attribute_value\n\npipeable_plot <- function(data, formula) {\n  plot(formula, data)\n  # Add a \"formula\" attribute to data\n  attr(data, \"formula\") <- formula\n  \n  invisible(data)\n}\n\n# From previous exercise\nplt_dist_vs_speed <- cars %>% \n  pipeable_plot(dist ~ speed)\n\n\n\n# Examine the structure of the result\nplt_dist_vs_speed\n\n   speed dist\n1      4    2\n2      4   10\n3      7    4\n4      7   22\n5      8   16\n6      9   10\n7     10   18\n8     10   26\n9     10   34\n10    11   17\n11    11   28\n12    12   14\n13    12   20\n14    12   24\n15    12   28\n16    13   26\n17    13   34\n18    13   34\n19    13   46\n20    14   26\n21    14   36\n22    14   60\n23    14   80\n24    15   20\n25    15   26\n26    15   54\n27    16   32\n28    16   40\n29    17   32\n30    17   40\n31    17   50\n32    18   42\n33    18   56\n34    18   76\n35    18   84\n36    19   36\n37    19   46\n38    19   68\n39    20   32\n40    20   48\n41    20   52\n42    20   56\n43    20   64\n44    22   66\n45    23   54\n46    24   70\n47    24   92\n48    24   93\n49    24  120\n50    25   85"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#creating-and-exploring-environments",
    "href": "datacamp/functions_in_R/functions_R.html#creating-and-exploring-environments",
    "title": "Functions in R",
    "section": "Creating and exploring environments",
    "text": "Creating and exploring environments\nEnvironments are used to store other variables. Mostly, you can think of them as lists, but there’s an important extra property that is relevant to writing functions. Every environment has a parent environment (except the empty environment, at the root of the environment tree). This determines which variables R know about at different places in your code.\nFacts about the Republic of South Africa are contained in capitals, national_parks, and population.\n\n# From previous steps\nrsa_lst <- list(\n  capitals = c(\"J Burg\", \"Capetown\"),\n  national_parks = c(\"Krug\", \"Iddo\"),\n  population = 30000\n)\nrsa_env <- list2env(rsa_lst)\n\n# Find the parent environment of rsa_env\nparent <- parent.env(rsa_env)\n\n# Print its name\nenvironmentName(parent)\n\n[1] \"R_GlobalEnv\"\n\n\nDo variables exist? If R cannot find a variable in the current environment, it will look in the parent environment, then the grandparent environment, and so on until it finds it.\nrsa_env has been modified so it includes capitals and national_parks, but not population.\n\n# Compare the contents of the global environment and rsa_env\nls.str(globalenv())\n\ncalc_harmonic_mean : function (x, na.rm = FALSE)  \ncff : tibble [7 × 5] (S3: tbl_df/tbl/data.frame)\ncoin_sides :  chr [1:2] \"head\" \"tail\"\ncut_by_quantile : function (x, n = 5, na.rm = FALSE, labels = NULL, interval_type = c(\"(lo, hi]\", \n    \"[lo, hi)\"))  \ngd_nms :  chr [1:87] \"USA\" \"GBR\" \"CHN\" \"RUS\" \"GER\" \"JPN\" \"FRA\" \"KOR\" \"ITA\" \"AUS\" ...\nget_reciprocal : function (x)  \ngold_medals :  Named int [1:87] 46 27 26 19 17 12 10 9 8 8 ...\ngroom_model : function (model)  \nis_leap_year : function (year)  \nmdl : tibble [1 × 8] (S3: tbl_df/tbl/data.frame)\nmodel : List of 31\n $ coefficients     : Named num [1:7] 61.54 10.85 -1.95 -19.33 -18.62 ...\n $ residuals        : Named num [1:346] -6.67 -7.67 -15.33 3.18 -6.95 ...\n $ fitted.values    : Named num [1:346] 8.67 8.67 16.33 -2.18 7.95 ...\n $ effects          : Named num [1:346] -509.9 -174.8 167.5 -46.6 162.4 ...\n $ R                : num [1:7, 1:7] -18.6 0 0 0 0 ...\n $ rank             : int 7\n $ qr               :List of 5\n $ family           :List of 12\n $ linear.predictors: Named num [1:346] 8.67 8.67 16.33 -2.18 7.95 ...\n $ deviance         : num 636485\n $ aic              : num 3599\n $ null.deviance    : num 820296\n $ iter             : int 2\n $ weights          : Named num [1:346] 1 1 1 1 1 1 1 1 1 1 ...\n $ prior.weights    : Named num [1:346] 1 1 1 1 1 1 1 1 1 1 ...\n $ df.residual      : int 339\n $ df.null          : int 345\n $ y                : Named num [1:346] 2 1 1 1 1 1 80 104 55 350 ...\n $ converged        : logi TRUE\n $ boundary         : logi FALSE\n $ model            :'data.frame':  346 obs. of  4 variables:\n $ na.action        : 'omit' Named int [1:64] 1 2 3 4 5 6 7 8 9 10 ...\n $ call             : language glm(formula = n_visits ~ gender + income + travel, data = snake_river_visits)\n $ formula          :Class 'formula'  language n_visits ~ gender + income + travel\n $ terms            :Classes 'terms', 'formula'  language n_visits ~ gender + income + travel\n $ data             :'data.frame':  410 obs. of  4 variables:\n $ offset           : NULL\n $ control          :List of 3\n $ method           : chr \"glm.fit\"\n $ contrasts        :List of 3\n $ xlevels          :List of 3\nn_visits :  num [1:410] 0 0 0 0 0 0 0 0 0 0 ...\nobs : tibble [346 × 11] (S3: tbl_df/tbl/data.frame)\nparent : <environment: R_GlobalEnv> \npipeable_plot : function (data, formula)  \nplt_dist_vs_speed : 'data.frame':   50 obs. of  2 variables:\n $ speed: num  4 4 7 7 8 9 10 10 10 11 ...\n $ dist : num  2 10 4 22 16 10 18 26 34 17 ...\nrsa_env : <environment: 0x565000cb9880> \nrsa_lst : List of 3\n $ capitals      : chr [1:2] \"J Burg\" \"Capetown\"\n $ national_parks: chr [1:2] \"Krug\" \"Iddo\"\n $ population    : num 30000\nrun_poisson_regression : function (data, formula)  \nsnake_river_visits : 'data.frame':  410 obs. of  4 variables:\n $ n_visits: num  0 0 0 0 0 0 0 0 0 0 ...\n $ gender  : Factor w/ 2 levels \"male\",\"female\": 1 1 1 2 1 2 2 2 1 1 ...\n $ income  : Factor w/ 4 levels \"[$0,$25k]\",\"($25k,$55k]\",..: 4 2 4 2 4 2 4 4 4 4 ...\n $ travel  : Factor w/ 3 levels \"[0h,0.25h]\",\"(0.25h,4h]\",..: NA NA NA NA NA NA NA NA NA NA ...\nstd_and_poor500 : 'data.frame': 505 obs. of  5 variables:\n $ symbol  : chr  \"MMM\" \"ABT\" \"ABBV\" \"ABMD\" ...\n $ company : chr  \"3M Company\" \"Abbott Laboratories\" \"AbbVie Inc.\" \"ABIOMED Inc\" ...\n $ sector  : chr  \"Industrials\" \"Health Care\" \"Health Care\" \"Health Care\" ...\n $ industry: chr  \"Industrial Conglomerates\" \"Health Care Equipment\" \"Pharmaceuticals\" \"Health Care Equipment\" ...\n $ pe_ratio: num  18.3 57.7 22.4 45.6 27 ...\ntoss_coin : function (n_flips, p_head)  \n\nls.str(rsa_env)\n\ncapitals :  chr [1:2] \"J Burg\" \"Capetown\"\nnational_parks :  chr [1:2] \"Krug\" \"Iddo\"\npopulation :  num 30000\n\n# Does population exist in rsa_env?\nexists(\"population\", envir = rsa_env)\n\n[1] TRUE\n\n# Does population exist in rsa_env, ignoring inheritance?\nexists(\"population\", envir = rsa_env, inherits = FALSE)\n\n[1] TRUE"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#converting-areas-to-metric-1",
    "href": "datacamp/functions_in_R/functions_R.html#converting-areas-to-metric-1",
    "title": "Functions in R",
    "section": "Converting areas to metric 1",
    "text": "Converting areas to metric 1\nIn this chapter, you’ll be working with grain yield data from the United States Department of Agriculture, National Agricultural Statistics Service. Unfortunately, they report all areas in acres. So, the first thing you need to do is write some utility functions to convert areas in acres to areas in hectares.\nTo solve this exercise, you need to know the following:\nThere are 4840 square yards in an acre. There are 36 inches in a yard and one inch is 0.0254 meters. There are 10000 square meters in a hectare.\n\n# Write a function to convert acres to sq. yards\nacres_to_sq_yards <- function(acres) {\n  acres * 4840\n}\n\n# Write a function to convert yards to meters\nyards_to_meters <- function(yards){\n    yards * 36*0.0254\n}\n\n# Write a function to convert sq. meters to hectares\n# Write a function to convert yards to meters\nsq_meters_to_hectares <- function(sq_meters){\n    sq_meters/10000\n}"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#converting-areas-to-metric-2",
    "href": "datacamp/functions_in_R/functions_R.html#converting-areas-to-metric-2",
    "title": "Functions in R",
    "section": "Converting areas to metric 2",
    "text": "Converting areas to metric 2\nYou’re almost there with creating a function to convert acres to hectares. You need another utility function to deal with getting from square yards to square meters. Then, you can bring everything together to write the overall acres-to-hectares conversion function. Finally, in the next exercise you’ll be calculating area conversions in the denominator of a ratio, so you’ll need a harmonic acre-to-hectare conversion function.\nFree hints: magrittr’s raise_to_power() will be useful here. The last step is similar to Chapter 2’s Harmonic Mean.\nThe three utility functions from the last exercise (acres_to_sq_yards(), yards_to_meters(), and sq_meters_to_hectares()) are available, as is your get_reciprocal() from Chapter 2. magrittr is loaded.\n\n# Write a function to convert sq. yards to sq. meters\nsq_yards_to_sq_meters <- function(sq_yards) {\n  sq_yards %>%\n    # Take the square root\n    sqrt() %>%\n    # Convert yards to meters\n    yards_to_meters() %>%\n    # Square it\n    raise_to_power(2)\n}\n\n# Load the function from the previous step\n#load_step2()\n\n# Write a function to convert acres to hectares\nacres_to_hectares <- function(acres) {\n  acres %>%\n    # Convert acres to sq yards\n    acres_to_sq_yards() %>%\n    # Convert sq yards to sq meters\n    sq_yards_to_sq_meters() %>%\n    # Convert sq meters to hectares\n    sq_meters_to_hectares()\n}\n\n# Define a harmonic acres to hectares function\nharmonic_acres_to_hectares <- function(acres) {\n  acres %>% \n    # Get the reciprocal\n    get_reciprocal() %>%\n    # Convert acres to hectares\n    acres_to_hectares %>% \n    # Get the reciprocal again\n    get_reciprocal()\n}"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#converting-yields-to-metric",
    "href": "datacamp/functions_in_R/functions_R.html#converting-yields-to-metric",
    "title": "Functions in R",
    "section": "Converting yields to metric",
    "text": "Converting yields to metric\nThe yields in the NASS corn data are also given in US units, namely bushels per acre. You’ll need to write some more utility functions to convert this unit to the metric unit of kg per hectare.\nBushels historically meant a volume of 8 gallons, but in the context of grain, they are now defined as masses. This mass differs for each grain! To solve this exercise, you need to know these facts.\nOne pound (lb) is 0.45359237 kilograms (kg). One bushel is 48 lbs of barley, 56 lbs of corn, or 60 lbs of wheat. magrittr is loaded.\n\nlibrary(magrittr)\n# Write a function to convert lb to kg\nlbs_to_kgs <- function(lbs){\n    lbs * 0.45359237\n}\n\n# Write a function to convert bushels to lbs\nbushels_to_lbs <- function(bushels, crop) {\n  # Define a lookup table of scale factors\n  c(barley = 48, corn = 56, wheat = 60) %>%\n    # Extract the value for the crop\n    magrittr::extract(crop) %>%\n    # Multiply by the no. of bushels\n    multiply_by(bushels)\n}\n\n\n# Write a function to convert bushels to kg\nbushels_to_kgs <- function(bushels, crop) {\n  bushels %>%\n    # Convert bushels to lbs for this crop\n    bushels_to_lbs(crop) %>%\n    # Convert lbs to kgs\n    lbs_to_kgs()\n}\n\n\n\n# Write a function to convert bushels/acre to kg/ha\nbushels_per_acre_to_kgs_per_hectare <- function(bushels_per_acre, crop = c(\"barley\", \"corn\", \"wheat\")) {\n  # Match the crop argument\n  crop <- match.arg(crop)\n  bushels_per_acre %>%\n    # Convert bushels to kgs for this crop\n    bushels_to_kgs(crop) %>%\n    # Convert harmonic acres to ha\n    harmonic_acres_to_hectares()\n}"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#applying-the-unit-conversion",
    "href": "datacamp/functions_in_R/functions_R.html#applying-the-unit-conversion",
    "title": "Functions in R",
    "section": "Applying the unit conversion",
    "text": "Applying the unit conversion\nNow that you’ve written some functions, it’s time to apply them! The NASS corn dataset is available, and you can fortify it (jargon for “adding new columns”) with metrics areas and yields.\nThis fortification process can also be turned in to a function, so you’ll define a function for this, and test it on the NASS wheat dataset.\n\ncorn <- readRDS(\"nass.corn.rds\")\nwheat <- readRDS(\"nass.wheat.rds\")\nglimpse(corn)\n\nRows: 6,381\nColumns: 4\n$ year                   <int> 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866,…\n$ state                  <chr> \"Alabama\", \"Arkansas\", \"California\", \"Connectic…\n$ farmed_area_acres      <dbl> 1050000, 280000, 42000, 57000, 200000, 125000, …\n$ yield_bushels_per_acre <dbl> 9.0, 18.0, 28.0, 34.0, 23.0, 9.0, 6.0, 29.0, 36…\n\ncorn %>%\n  # Add some columns\n  mutate(\n    # Convert farmed area from acres to ha\n    farmed_area_ha = acres_to_hectares(farmed_area_acres),\n    # Convert yield from bushels/acre to kg/ha\n    yield_kg_per_ha = bushels_per_acre_to_kgs_per_hectare(\n      yield_bushels_per_acre,\n      crop = \"corn\"\n    )\n  ) %>%\n  head()\n\n  year       state farmed_area_acres yield_bushels_per_acre farmed_area_ha\n1 1866     Alabama           1050000                      9      424919.92\n2 1866    Arkansas            280000                     18      113311.98\n3 1866  California             42000                     28       16996.80\n4 1866 Connecticut             57000                     34       23067.08\n5 1866    Delaware            200000                     23       80937.13\n6 1866     Florida            125000                      9       50585.71\n  yield_kg_per_ha\n1         564.909\n2        1129.818\n3        1757.495\n4        2134.101\n5        1443.656\n6         564.909\n\n# Wrap this code into a function\nfortify_with_metric_units <- function(data, crop){\n\n\n  data %>%\n    mutate(\n      farmed_area_ha = acres_to_hectares(farmed_area_acres),\n      yield_kg_per_ha = bushels_per_acre_to_kgs_per_hectare(\n        yield_bushels_per_acre, \n        crop = crop\n      )\n    )\n}\n\n# Try it on the wheat dataset\nwheat <- fortify_with_metric_units(wheat, crop = \"wheat\")"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#plotting-yields-over-time",
    "href": "datacamp/functions_in_R/functions_R.html#plotting-yields-over-time",
    "title": "Functions in R",
    "section": "Plotting yields over time",
    "text": "Plotting yields over time\nNow that the units have been dealt with, it’s time to explore the datasets. An obvious question to ask about each crop is, “how do the yields change over time in each US state?” Let’s draw a line plot to find out! ggplot2 is loaded, and corn and wheat datasets are available with metric units.\n\n# Wrap this plotting code into a function\nplot_yield_vs_year <- function(data){\n  ggplot(data, aes(year, yield_kg_per_ha)) +\n    geom_line(aes(group = state)) +\n    geom_smooth()\n}\n\n# Test it on the wheat dataset\nplot_yield_vs_year(wheat)"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#a-nation-divided",
    "href": "datacamp/functions_in_R/functions_R.html#a-nation-divided",
    "title": "Functions in R",
    "section": "A nation divided",
    "text": "A nation divided\nThe USA has a varied climate, so we might expect yields to differ between states. Rather than trying to reason about 50 states separately, we can use the USA Census Regions to get 9 groups.\nThe “Corn Belt”, where most US corn is grown is in the “West North Central” and “East North Central” regions. The “Wheat Belt” is in the “West South Central” region.\ndplyr is loaded, the corn and wheat datasets are available, as is usa_census_regions.\n\n# Inner join the corn dataset to usa_census_regions by state\nlibrary(data.table)\nusa_census_regions <- read_csv(\"usa_census_regions.csv\")\nsetnames(usa_census_regions, c(\"State\", \"Region\"), c(\"state\", \"census_region\"))\nView(usa_census_regions)\n# corn %>%\n#   inner_join(usa_census_regions, by= \"state\")\n# Wrap this code into a function\nfortify_with_census_region <- function(data){\n  data %>%\n    inner_join(usa_census_regions, by = \"state\")\n}\n\n# Try it on the wheat dataset\nwheat <- fortify_with_census_region(wheat)"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#plotting-yields-over-time-by-region",
    "href": "datacamp/functions_in_R/functions_R.html#plotting-yields-over-time-by-region",
    "title": "Functions in R",
    "section": "Plotting yields over time by region",
    "text": "Plotting yields over time by region\nSo far, you have a function to plot yields over time for each crop, and you’ve added a census_region column to the crop datasets. Now you are ready to look at how the yields change over time in each region of the USA.\nggplot2 is loaded. corn and wheat have been fortified with census regions. plot_yield_vs_year() is available.\n\n# Wrap this code into a function\nplot_yield_vs_year_by_region <- function(data) {\n\n  plot_yield_vs_year(data) +\n    facet_wrap(vars(census_region))\n}\n\n# Try it on the wheat dataset\n\nplot_yield_vs_year_by_region(wheat)"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#running-a-model",
    "href": "datacamp/functions_in_R/functions_R.html#running-a-model",
    "title": "Functions in R",
    "section": "Running a model",
    "text": "Running a model\nThe smooth trend line you saw in the plots of yield over time use a generalized additive model (GAM) to determine where the line should lie. This sort of model is ideal for fitting nonlinear curves. So we can make predictions about future yields, let’s explicitly run the model. The syntax for running this GAM takes the following form.\ngam(response ~ s(explanatory_var1) + explanatory_var2, data = dataset) Here, s() means “make the variable smooth”, where smooth very roughly means nonlinear.\nmgcv and dplyr are loaded; the corn and wheat datasets are available.\n\n# Wrap the model code into a function\nlibrary(mgcv)\nrun_gam_yield_vs_year_by_region <- function(data){\n\n\n  gam(yield_kg_per_ha ~ s(year) + census_region, data = data)\n\n}\n\n# Try it on the wheat dataset\nwheat_model <- run_gam_yield_vs_year_by_region(wheat)\nwheat_model\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nyield_kg_per_ha ~ s(year) + census_region\n\nEstimated degrees of freedom:\n6.93  total = 10.93 \n\nGCV score: 341585.5"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#making-yield-predictions",
    "href": "datacamp/functions_in_R/functions_R.html#making-yield-predictions",
    "title": "Functions in R",
    "section": "Making yield predictions",
    "text": "Making yield predictions\nThe fun part of modeling is using the models to make predictions. You can do this using a call to predict(), in the following form.\npredict(model, cases_to_predict, type = “response”) mgcv and dplyr are loaded; GAMs of the corn and wheat datasets are available as corn_model and wheat_model. A character vector of census regions is stored as census_regions.\n\ncensus_regions <- wheat$census_region %>% unique()\n# Wrap this prediction code into a function\npredict_yields <- function(model, year){\n\n  predict_this <- data.table(\n    year = year,\n    census_region = census_regions\n  ) \n  pred_yield_kg_per_ha <- predict(model, predict_this, type = \"response\")\n  predict_this %>%\n    mutate(pred_yield_kg_per_ha = pred_yield_kg_per_ha)\n}\n\n# Try it on the wheat dataset\npredict_yields(wheat_model, year = 2050)\n\n   year census_region pred_yield_kg_per_ha\n1: 2050         South       <multi-column>\n2: 2050          West       <multi-column>\n3: 2050     Northeast       <multi-column>\n4: 2050       Midwest       <multi-column>"
  },
  {
    "objectID": "datacamp/functions_in_R/functions_R.html#do-it-all-over-again",
    "href": "datacamp/functions_in_R/functions_R.html#do-it-all-over-again",
    "title": "Functions in R",
    "section": "Do it all over again",
    "text": "Do it all over again\nHopefully, by now, you’ve realized that the real benefit to writing functions is that you can reuse your code easily. Now you are going to rerun the whole analysis from this chapter on a new crop, barley. Since all the infrastructure is in place, that’s less effort than it sounds!\nBarley prefers a cooler climate compared to corn and wheat and is commonly grown in the US mountain states of Idaho and Montana.\ndplyr and ggplot2, and mgcv are loaded; fortify_with_metric_units(), fortify_with_census_region(), plot_yield_vs_year_by_region(), run_gam_yield_vs_year_by_region(), and predict_yields() are available.\n\nfortified_barley <- barley %>% \n  # Fortify with metric units\n  fortify_with_metric_units() %>%\n  # Fortify with census regions\n  fortify_with_census_region()\n\n# See the result\nglimpse(fortified_barley)\n# From previous step\nfortified_barley <- barley %>% \n  fortify_with_metric_units() %>%\n  fortify_with_census_region()\n\nfortified_barley %>% \n  # Run a GAM of yield vs. year by region\n  run_gam_yield_vs_year_by_region %>% \n  # Make predictions of yields in 2050\n  predict_yields(year = 2050)"
  }
]