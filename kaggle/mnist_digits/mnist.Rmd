---
title: "MNIST Digits"
author: "Mburu"
date: "7/10/2020"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: false
    theme: united
    highlight: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


# MNIST Digits

## Read data and Load libraries

```{r }

library(tidyverse)
library(data.table)
library(keras)
library(caret)
library(DT)
library(caretEnsemble)
library(tictoc)

train_data <- fread("data/train.csv")
set.seed(100)
N = nrow(train_data)
sample_one <- sample(N, 5000)
train_data <- train_data[sample_one]
test_data <- fread("data/test.csv")

```

## Frequency of digits

```{r, fig.height=4.5}
ggplot(train_data, aes(x = factor(label))) +
    geom_bar()
```


## Randomly sample 12 digits

```{r}
#  image coordinates
xy_axis <- data.frame(x = expand.grid(1:28, 28:1)[,1],
                      y = expand.grid(1:28, 28:1)[,2])


# get 12 images
set.seed(100)
sample_10 <- train_data[sample(1:.N, 12), -1] %>% as.matrix()

datatable(sample_10, 
          options = list(scrollX = TRUE))

sample_10 <- t(sample_10)

plot_data <- cbind(xy_axis, sample_10 )

setDT(plot_data, keep.rownames = "pixel")

# Observe the first records
head(plot_data) %>% datatable(options = list(scrollX = TRUE))
```

## Plot the 12 digits

````{r}
plot_data_m <- melt(plot_data, id.vars = c("pixel", "x", "y"))

# Plot the image using ggplot()
ggplot(plot_data_m, aes(x, y, fill = value)) +
    geom_raster()+
     facet_wrap(~variable)+
    scale_fill_gradient(low = "white",
                        high = "black", guide = FALSE)+
    theme(axis.line = element_blank(),
                  axis.text = element_blank(),
                  axis.ticks = element_blank(),
                  axis.title = element_blank(),
                  panel.background = element_blank(),
                  panel.border = element_blank(),
                  panel.grid.major = element_blank(),
                  panel.grid.minor = element_blank(),
                  plot.background = element_blank())
   
```

## Prepare data for model fitting

- Decided to have a self test set

```{r }
N = nrow(train_data)
sample_train <- sample(N, size = round(0.75 *N ))
test_own <- train_data[-sample_train]
train_data2 <- train_data[sample_train, ]
train_y <-to_categorical(train_data2$label, 10)

train_x <- train_data2[, -1]
#convert to matrix
train_x <- train_x %>%
    as.matrix()

train_x <- train_x/255

```


## Construct model layers

```{r }
model <- keras_model_sequential() 
model %>% 
    layer_dense(units = 784, activation = 'relu', input_shape = 784) %>%
    layer_dropout(rate = 0.3) %>%
    layer_dense(units = 784, activation = 'relu') %>%
    layer_dropout(rate = 0.3) %>%
    layer_dense(units = 392, activation = 'sigmoid') %>%
    layer_dropout(rate = 0.2) %>%
    #layer_dense(units = 200, activation = 'relu') %>%
    #layer_dropout(rate = 0.) %>%
    layer_dense(units = 10, activation = 'softmax')
```


## Compile model

```{r}
model %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = 'adam',
    metrics = c('accuracy'))
```

## Fit model

```{r}
hist <- model %>% fit(train_x, train_y, 
                      epochs = 9, batch_size = 1000,
                      validation_split = .2)


plot(hist,type = "b")
```

## Own test

```{r}
test_own_x <- test_own[, -1] %>% as.matrix()/255

test_own_pred <- model %>% predict_classes(test_own_x) %>% factor()

confusionMatrix(data = test_own_pred, reference = factor(test_own$label))

```

### Save predictions

```{r }
test_x <- as.matrix(test_data)/255

test_pred <- model %>% predict_classes(test_x)
#head(test_pred)

df_pred1 <- data.frame(ImageId = 1:length(test_pred),
                       Label = test_pred)

write.csv(df_pred1, file = "sample_submission.csv", row.names = F)

```


# Compare between tsne amd glmr

## TSNE

```{r, fig.width = 6, fig.height = 4}
library(Rtsne)

tsne_output <- Rtsne(train_x, check_duplicates = FALSE, PCA = FALSE)

# Generate a data frame to plot the result
tsne_train <- data.table(tsne_x = tsne_output$Y[,1],
                        tsne_y = tsne_output$Y[,2],
                        label =  train_data2$label)

# Plot the embedding usign ggplot and the label
ggplot(tsne_train,
       aes(x = tsne_x, y = tsne_y, color = factor(label))) + 
  ggtitle("t-SNE of MNIST data set") + 
  geom_text(aes(label = label)) +
    theme(legend.position = "none")


```

## Plot tsne group means

```{r}
tsne_mean <- tsne_train[, 
                        .(mean_x = mean(tsne_x), mean_y = mean(tsne_y)),
                        by = label]


ggplot(tsne_mean,
       aes(x = mean_x, y = mean_y, color = factor(label))) + 
  ggtitle("t-SNE of MNIST data set group means") + 
  geom_text(aes(label = label)) +
    theme(legend.position = "none")


```
## Kmeans to see if tsne and kmeans agree

```{r}
set.seed(123)
k_means_mnist <- kmeans(train_x, 10)

tsne_train[, cluster := k_means_mnist$cluster]
ggplot(tsne_train,
       aes(x = tsne_x, y = tsne_y, color = factor(cluster))) + 
  geom_point()+
  ggtitle("t-SNE of MNIST data set") + 
    theme(legend.position = "none")

tsne_train[, cluster := NULL]
```

## Model hyper parameters


```{r}
tsne_train[, label:=  factor(label)]

set.seed(100)

cv_fold <- createFolds(tsne_train$label, k = 5)


library(caretEnsemble)

train_ctrl <- trainControl(method = "cv",
                        number = 3,
                        summaryFunction = multiClassSummary,
                        classProbs = TRUE,
                        allowParallel=T,
                        index = cv_fold,
                        verboseIter = TRUE,
                        returnResamp = "all", 
                        savePredictions = "final", 
                        search = "grid")



xgb_grid <-  expand.grid(nrounds = c(100, 150),
                        eta = 0.06,
                        max_depth =c(2, 5, 20),
                        gamma = c(6,0),
                        colsample_bytree = 0.8,
                        min_child_weight =0.8,
                        subsample =  .8)



ranger_grid <- expand.grid(splitrule = "extratrees",
                        mtry =2,
                        min.node.size = c(1, 5))


svm_grid <- expand.grid(C = c(.5, 1, 5, 20),
                           sigma= seq(0.001, 1, length.out = 4))
```

## Train TSNE model

- Caret complains when factor levels are numeric

```{r}
lbls <- c("zero", "one", "two", "three", 
          "four", "five", "six", "seven",
          "eight", "nine")

lvl <- 0:9

tsne_train[, label := factor(label, 
                             levels = lvl, 
                             labels = lbls)]
```

## Train models

```{r, message=TRUE}
set.seed(100)

# 

tic()

model_list <- caretList(
   label~.,
    data= tsne_train,
    trControl=train_ctrl,
    tuneList = list(caretModelSpec(method="xgbTree", tuneGrid= xgb_grid),
                    caretModelSpec(method="ranger", tuneGrid= ranger_grid )
                    
                    )
)

toc()
```

### Model output

```{r}
model_list
```


### Resamples


```{r}
resamples_models <- resamples(model_list)

dotplot(resamples_models, metric = "Accuracy")
```
