{
  "hash": "85ed2647c6dbc7d0ad85ff0b5a399d70",
  "result": {
    "markdown": "---\ntitle: 'Introduction to iml: Interpretable Machine Learning in R'\noutput:\n  html_document:\n    df_print: paged\nvignette: |\n  %\\VignetteIndexEntry{Introduction to iml: Interpretable Machine Learning in R} %\\VignetteEngine{knitr::rmarkdown} \\usepackage[utf8]{inputenc}\n---\n\n::: {.cell}\n\n:::\n\n\nCode is from [here](https://github.com/christophM/iml/blob/master/vignettes/intro.Rmd#partial-dependence)\n\nMachine learning models usually perform really well for predictions, but are not interpretable.\nThe iml package provides tools for analysing any black box machine learning model:\n\n* Feature importance: Which were the most important features?\n\n* Feature effects: How does a feature influence the prediction? (Accumulated local effects, partial dependence plots and individual conditional expectation curves)\n\n* Explanations for single predictions: How did the feature values of a single data point affect its prediction?  (LIME and Shapley value)\n\n* Surrogate trees: Can we approximate the underlying black box model with a short decision tree?\n\n* The iml package works for any classification and regression machine learning model: random forests, linear models, neural networks, xgboost, etc.\n\nThis document shows you how to use the iml package to analyse machine learning models. \n\nIf you want to learn more about the technical details of all the methods, read chapters from: https://christophm.github.io/interpretable-ml-book/agnostic.html\n\n## Data: Boston Housing\n\nWe'll use the `MASS::Boston` dataset to demonstrate the abilities of the iml package. This dataset contains median house values from Boston neighbourhoods. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(\"Boston\", package = \"MASS\")\nhead(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>      crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n#> 1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n#> 2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n#> 3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n#> 4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n#> 5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n#> 6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n#>   medv\n#> 1 24.0\n#> 2 21.6\n#> 3 34.7\n#> 4 33.4\n#> 5 36.2\n#> 6 28.7\n```\n:::\n:::\n\n\n## Fitting the machine learning model\n\nFirst we train a randomForest to predict the Boston median housing value:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(42)\nlibrary(\"iml\")\nlibrary(\"randomForest\")\ndata(\"Boston\", package = \"MASS\")\nrf <- randomForest(medv ~ ., data = Boston, ntree = 50)\n```\n:::\n\n\n## Using the iml Predictor() container\n\nWe create a `Predictor` object, that holds the model and the data. The iml package uses R6 classes: New objects can be created by calling `Predictor$new()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nX <- Boston[which(names(Boston) != \"medv\")]\npredictor <- Predictor$new(rf, data = X, y = Boston$medv)\n```\n:::\n\n\n## Feature importance\n\nWe can measure how important each feature was for the predictions with `FeatureImp`. The feature importance measure works by shuffling each feature and measuring how much the performance drops. For this regression task we choose to measure the loss in performance with the mean absolute error ('mae'), another choice would be the  mean squared error ('mse').\n\nOnce we create a new object of `FeatureImp`, the importance is automatically computed. \nWe can call the `plot()` function of the object or look at the results in a data.frame.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nimp <- FeatureImp$new(predictor, loss = \"mae\")\nlibrary(\"ggplot2\")\nplot(imp)\n```\n\n::: {.cell-output-display}\n![](intro_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\nimp$results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>    feature importance.05 importance importance.95 permutation.error\n#> 1    lstat      4.394480   4.459282      4.740041          4.394662\n#> 2       rm      3.349928   3.620192      3.715445          3.567732\n#> 3      nox      1.772047   1.796058      1.833039          1.770032\n#> 4     crim      1.661024   1.703701      1.738660          1.679013\n#> 5      dis      1.670726   1.690742      1.694655          1.666242\n#> 6  ptratio      1.423169   1.426636      1.443533          1.405963\n#> 7    indus      1.399432   1.416183      1.434735          1.395661\n#> 8      age      1.346887   1.387961      1.423804          1.367848\n#> 9      tax      1.352382   1.376506      1.384641          1.356559\n#> 10   black      1.227135   1.234735      1.235719          1.216842\n#> 11     rad      1.097253   1.109357      1.131675          1.093281\n#> 12      zn      1.035018   1.042360      1.043935          1.027256\n#> 13    chas      1.032211   1.035745      1.047224          1.020736\n```\n:::\n:::\n\n\n## Feature effects\n\nBesides knowing which features were important, we are interested in how the features influence the predicted outcome. \nThe `FeatureEffect` class implements accumulated local effect plots, partial dependence plots and individual conditional expectation curves. \nThe following plot shows the accumulated local effects (ALE) for the feature 'lstat'.\nALE shows how the prediction changes locally, when the feature is varied.\nThe marks on the x-axis indicates the distribution of the 'lstat' feature, showing how relevant a region is for interpretation (little or no points mean that we should not over-interpret this region).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nale <- FeatureEffect$new(predictor, feature = \"lstat\")\nale$plot()\n```\n\n::: {.cell-output-display}\n![](intro_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nIf we want to compute the partial dependence curves on another feature, we can simply reset the feature:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nale$set.feature(\"rm\")\nale$plot()\n```\n\n::: {.cell-output-display}\n![](intro_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## Measure interactions\n\nWe can also measure how strongly features interact with each other.\nThe interaction measure regards how much of the variance of $f(x)$ is explained by the interaction.\nThe measure is between 0 (no interaction) and 1 (= 100% of variance of $f(x)$ due to interactions).\nFor each feature, we measure how much they interact with any other feature:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninteract <- Interaction$new(predictor)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> \n#> Attaching package: 'withr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n#> The following objects are masked from 'package:rlang':\n#> \n#>     local_options, with_options\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n#> The following object is masked from 'package:tools':\n#> \n#>     makevars_user\n```\n:::\n\n```{.r .cell-code}\nplot(interact)\n```\n\n::: {.cell-output-display}\n![](intro_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe can also specify a feature and measure all it's 2-way interactions with all other features:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninteract <- Interaction$new(predictor, feature = \"crim\")\nplot(interact)\n```\n\n::: {.cell-output-display}\n![](intro_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nYou can also plot the feature effects for all features at once:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neffs <- FeatureEffects$new(predictor)\nplot(effs)\n```\n\n::: {.cell-output-display}\n![](intro_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n## Surrogate model\nAnother way to make the models more interpretable is to replace the black box with a simpler model - a decision tree.\nWe take the predictions of the black box model (in our case the random forest) and train a decision tree on the original features and the predicted outcome. \nThe plot shows the terminal nodes of the fitted tree.\nThe maxdepth parameter controls how deep the tree can grow and therefore how interpretable it is.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntree <- TreeSurrogate$new(predictor, maxdepth = 2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Loading required package: partykit\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Loading required package: libcoin\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Loading required package: mvtnorm\n```\n:::\n\n```{.r .cell-code}\nplot(tree)\n```\n\n::: {.cell-output-display}\n![](intro_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe can use the tree to make predictions:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(tree$predict(Boston))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning in self$predictor$data$match_cols(data.frame(newdata)): Dropping\n#> additional columns: medv\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n#>     .y.hat\n#> 1 27.09989\n#> 2 27.09989\n#> 3 27.09989\n#> 4 27.09989\n#> 5 27.09989\n#> 6 27.09989\n```\n:::\n:::\n\n\n## Explain single predictions with a local model\n\nGlobal surrogate model can improve the understanding of the global model behaviour. \nWe can also fit a model locally to understand an individual prediction better.\nThe local model fitted by `LocalModel` is a linear regression model and the data points are weighted by how close they are to the data point for wich we want to explain the prediction.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlime.explain <- LocalModel$new(predictor, x.interest = X[1, ])\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Loading required package: glmnet\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Loading required package: Matrix\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Loaded glmnet 4.1-6\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Loading required package: gower\n```\n:::\n\n```{.r .cell-code}\nlime.explain$results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>               beta x.recoded    effect x.original feature feature.value\n#> rm       4.4836483     6.575 29.479987      6.575      rm      rm=6.575\n#> ptratio -0.5244767    15.300 -8.024493       15.3 ptratio  ptratio=15.3\n#> lstat   -0.4348698     4.980 -2.165652       4.98   lstat    lstat=4.98\n```\n:::\n\n```{.r .cell-code}\nplot(lime.explain)\n```\n\n::: {.cell-output-display}\n![](intro_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlime.explain$explain(X[2, ])\nplot(lime.explain)\n```\n\n::: {.cell-output-display}\n![](intro_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## Explain single predictions with game theory\n\nAn alternative for explaining individual predictions is a method from coalitional game theory named Shapley value.\nAssume that for one data point, the feature values play a game together, in which they get the prediction as a payout.\nThe Shapley value tells us how to fairly distribute the payout among the feature values.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshapley <- Shapley$new(predictor, x.interest = X[1, ])\nshapley$plot()\n```\n\n::: {.cell-output-display}\n![](intro_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe can reuse the object to explain other data points:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshapley$explain(x.interest = X[2, ])\nshapley$plot()\n```\n\n::: {.cell-output-display}\n![](intro_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe results in data.frame form can be extracted like this:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresults <- shapley$results\nhead(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>   feature          phi    phi.var feature.value\n#> 1    crim -0.030772464 0.88726982  crim=0.02731\n#> 2      zn -0.009163333 0.01266404          zn=0\n#> 3   indus -0.412309000 0.65608174    indus=7.07\n#> 4    chas -0.065604772 0.04865024        chas=0\n#> 5     nox  0.067133048 0.37635155     nox=0.469\n#> 6      rm -0.354769984 7.26420349      rm=6.421\n```\n:::\n:::\n",
    "supporting": [
      "intro_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}