{
  "hash": "7a60a32416c9b2c6bf0e11aa62115a08",
  "result": {
    "markdown": "---\ntitle: \"Handling Missing Data with Imputations in R\"\noutput:\n  html_document:\n    css: github.css\n    theme: lumen\n    highlight: pygments\n  github_document: default\n  pdf_document: default\n---\n\n\n# The problem of missing data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(\n\techo = TRUE,\n\tmessage = FALSE,\n\twarning = FALSE\n)\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(janitor)\nlibrary(ggthemes)\nlibrary(here)\nlibrary(lubridate)\nlibrary(knitr)\nlibrary(broom)\n```\n:::\n\n\n## Linear regression with incomplete data\n\nMissing data is a common problem and dealing with it appropriately is extremely important. Ignoring the missing data points or filling them incorrectly may cause the models to work in unexpected ways and cause the predictions and inferences to be biased.\n\nIn this chapter, you will be working with the biopics dataset. It contains information on a number of biographical movies, including their earnings, subject characteristics and some other variables. Some of the data points are, however, missing. The original data comes with the fivethirtyeight R package, but in this course, you will work with a slightly preprocessed version.\n\nIn this exercise, you will get to know the dataset and fit a linear regression model to explain a movie's earnings. Let's begin!\n\n### Print first 10 observations\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbiopics <- read_csv(\"data/biopics.csv\")\n# Print first 10 observations\nhead(biopics, 10) %>%\n    kable()\n```\n\n::: {.cell-output-display}\n|country | year| earnings| sub_num|sub_type |sub_race | non_white|sub_sex |\n|:-------|----:|--------:|-------:|:--------|:--------|---------:|:-------|\n|UK      | 1971|       NA|       1|Criminal |NA       |         0|Male    |\n|US/UK   | 2013|   56.700|       1|Other    |African  |         1|Male    |\n|US/UK   | 2010|   18.300|       1|Athlete  |NA       |         0|Male    |\n|Canada  | 2014|       NA|       1|Other    |White    |         0|Male    |\n|US      | 1998|    0.537|       1|Other    |NA       |         0|Male    |\n|US      | 2008|   81.200|       1|Other    |other    |         1|Male    |\n|UK      | 2002|    1.130|       1|Musician |White    |         0|Male    |\n|US      | 2013|   95.000|       1|Athlete  |African  |         1|Male    |\n|US      | 1994|   19.600|       1|Athlete  |NA       |         0|Male    |\n|US/UK   | 1987|    1.080|       2|Author   |NA       |         0|Male    |\n:::\n:::\n\n\n### Get the number of missing values per variable\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the number of missing values per variable\nbiopics %>%\n\tis.na() %>% \n\tcolSums()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  country      year  earnings   sub_num  sub_type  sub_race non_white   sub_sex \n        0         0       324         0         0       197         0         0 \n```\n:::\n:::\n\n\n### Fit linear regression to predict earnings\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit linear regression to predict earnings\nmodel_1 <- lm(earnings ~ country + year + sub_type, \n              data = biopics)\n\nsummary(model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = earnings ~ country + year + sub_type, data = biopics)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.283 -20.466  -5.251   6.871 285.210 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(>|t|)   \n(Intercept)                 -743.2411   273.2831  -2.720  0.00682 **\ncountryCanada/UK              -6.9648    19.5228  -0.357  0.72146   \ncountryUK                      7.0207    15.4945   0.453  0.65071   \ncountryUS                     30.9079    15.0039   2.060  0.04004 * \ncountryUS/Canada              31.6905    18.8308   1.683  0.09316 . \ncountryUS/UK                  23.7589    15.4580   1.537  0.12508   \ncountryUS/UK/Canada           -4.8187    29.6967  -0.162  0.87118   \nyear                           0.3783     0.1359   2.784  0.00562 **\nsub_typeActivist             -21.7103    13.0520  -1.663  0.09701 . \nsub_typeActor                -41.6236    16.8004  -2.478  0.01364 * \nsub_typeActress              -34.9628    17.5264  -1.995  0.04673 * \nsub_typeActress / activist     7.1816    37.6378   0.191  0.84877   \nsub_typeArtist               -25.2620    13.8543  -1.823  0.06898 . \nsub_typeAthlete              -10.7316    12.1242  -0.885  0.37661   \nsub_typeAthlete / military    66.3717    37.6682   1.762  0.07882 . \nsub_typeAuthor               -25.9330    12.6080  -2.057  0.04034 * \nsub_typeAuthor (poet)        -17.1963    17.1851  -1.001  0.31759   \nsub_typeComedian             -29.3344    18.3419  -1.599  0.11053   \nsub_typeCriminal              -7.3534    12.2475  -0.600  0.54857   \nsub_typeGovernment           -16.9917    23.5048  -0.723  0.47016   \nsub_typeHistorical            -4.0166    12.6665  -0.317  0.75133   \nsub_typeJournalist           -30.6610    28.0016  -1.095  0.27418   \nsub_typeMedia                -15.7588    16.7744  -0.939  0.34806   \nsub_typeMedicine               5.0987    21.0749   0.242  0.80895   \nsub_typeMilitary              15.1616    14.0730   1.077  0.28196   \nsub_typeMilitary / activist   29.8300    37.6688   0.792  0.42888   \nsub_typeMusician             -21.1765    12.1482  -1.743  0.08206 . \nsub_typeOther                -17.5989    11.4405  -1.538  0.12476   \nsub_typePolitician           -21.0700    37.6688  -0.559  0.57623   \nsub_typeSinger                 1.0769    14.9161   0.072  0.94248   \nsub_typeTeacher               42.4600    37.6407   1.128  0.25997   \nsub_typeWorld leader           0.5964    16.2407   0.037  0.97072   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 36 on 405 degrees of freedom\n  (324 observations deleted due to missingness)\nMultiple R-squared:  0.1799,\tAdjusted R-squared:  0.1171 \nF-statistic: 2.865 on 31 and 405 DF,  p-value: 1.189e-06\n```\n:::\n:::\n\n\n### Analyzing regression output\n\n-   You are interested in how well the model you've just built fits the data. To measure this, you want to calculate the median absolute difference between the true and predicted earnings. You run the following line of code:\n-   As some observations were removed from the model, the two vectors inside abs() have different lengths, and so the entries of the shorter one get replicated to enable the subtraction. Consequently, the resulting number has no meaning. Analyzing models fit to incomplete data can be treacherous\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmedian(abs(biopics$earnings - model_1$fitted.values), na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 21.66698\n```\n:::\n:::\n\n\n## Comparing models\n\nChoosing the best of multiple competing models can be tricky if these models are built on incomplete data. In this exercise, you will extend the model you have built previously by adding one more explanatory variable: the race of the movie's subject. Then, you will try to compare it to the previous model.\n\nAs a reminder, this is how you have fitted the first model:\n\n-   model_1 \\<- lm(earnings \\~ country + year + sub_type, data = biopics) Let's see if we can judge whether adding the race variable improves the model!\n\n### Fit linear regression to predict earnings\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit linear regression to predict earnings\nmodel_2 <- lm(earnings ~ country + year + sub_type + sub_race, \n              data = biopics)\n\n# Print summaries of both models\n\nsummary(model_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = earnings ~ country + year + sub_type + sub_race, \n    data = biopics)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-58.323 -16.237  -4.018   5.614 200.234 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                -139.27034  287.97218  -0.484 0.629031    \ncountryCanada/UK              4.00206   18.25641   0.219 0.826643    \ncountryUK                    13.84774   14.91395   0.929 0.353943    \ncountryUS                    31.42015   14.32201   2.194 0.029069 *  \ncountryUS/Canada             18.29811   18.65109   0.981 0.327403    \ncountryUS/UK                 29.40669   14.79424   1.988 0.047817 *  \ncountryUS/UK/Canada           5.28487   34.26999   0.154 0.877553    \nyear                          0.08053    0.14277   0.564 0.573156    \nsub_typeActivist            -22.70696   13.91011  -1.632 0.103718    \nsub_typeActor               -37.18944   16.80696  -2.213 0.027722 *  \nsub_typeActress             -29.08213   17.54697  -1.657 0.098561 .  \nsub_typeActress / activist   22.74806   34.10892   0.667 0.505370    \nsub_typeArtist              -16.16366   14.44232  -1.119 0.264019    \nsub_typeAthlete               1.82705   13.21810   0.138 0.890163    \nsub_typeAthlete / military   81.76200   33.27768   2.457 0.014619 *  \nsub_typeAuthor              -16.89061   13.34913  -1.265 0.206817    \nsub_typeAuthor (poet)       -10.46216   17.81790  -0.587 0.557562    \nsub_typeComedian            -29.04858   19.58703  -1.483 0.139185    \nsub_typeCriminal             -3.63899   13.49577  -0.270 0.787636    \nsub_typeGovernment           -3.98375   21.53144  -0.185 0.853347    \nsub_typeHistorical           -1.84026   13.64400  -0.135 0.892806    \nsub_typeJournalist          -19.52435   25.70076  -0.760 0.448085    \nsub_typeMedia               -23.58188   18.39661  -1.282 0.200952    \nsub_typeMedicine             19.79476   33.28029   0.595 0.552465    \nsub_typeMilitary            -11.90055   15.58559  -0.764 0.445772    \nsub_typeMusician            -11.87866   12.76816  -0.930 0.352999    \nsub_typeOther                -8.26334   12.46291  -0.663 0.507854    \nsub_typePolitician          -13.12470   33.28805  -0.394 0.693677    \nsub_typeSinger               12.59513   15.42311   0.817 0.414829    \nsub_typeTeacher              52.19210   33.25064   1.570 0.117624    \nsub_typeWorld leader          5.70258   15.84955   0.360 0.719272    \nsub_raceAsian               -33.21461   17.04703  -1.948 0.052365 .  \nsub_raceHispanic            -25.63976    9.37824  -2.734 0.006657 ** \nsub_raceMid Eastern          -0.75224   11.54403  -0.065 0.948091    \nsub_raceMulti racial        -26.03619    9.67832  -2.690 0.007571 ** \nsub_raceother               -23.90532   12.36017  -1.934 0.054113 .  \nsub_raceWhite               -20.10327    5.90967  -3.402 0.000767 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31.01 on 280 degrees of freedom\n  (444 observations deleted due to missingness)\nMultiple R-squared:  0.2566,\tAdjusted R-squared:  0.161 \nF-statistic: 2.684 on 36 and 280 DF,  p-value: 3.145e-06\n```\n:::\n:::\n\n\n-   The two models are not comparable, because each of them is based on a different data sample.\n-   With incomplete datasets, changing the model's architecture can impact the set of observations that are actually used by the model. This might prevent us from comparing different models.\n\n## Recognizing missing data mechanisms\n\nIn this exercise, you will face six different scenarios in which some data are missing. Try assigning each of them to the most likely missing data mechanism. As a refresher, here are some general guidelines:\n\n-   If the reason for missingness is purely random, it's MCAR.\n-   If the reason for missingness can be explained by another variable, it's MAR.\n-   If the reason for missingness depends on the missing value itself, it's MNAR.\n\nFurther explanation from [Missing data mechanisms](https://www.ncbi.nlm.nih.gov/books/NBK493614/)\n\n-   Missing completely at random (MCAR). When data are MCAR, the fact that the data are missing is independent of the observed and unobserved data. In other words, no systematic differences exist between participants with missing data and those with complete data. For example, some participants may have missing laboratory values because a batch of lab samples was processed improperly. In these instances, the missing data reduce the analyzable population of the study and consequently, the statistical power, but do not introduce bias: when data are MCAR, the data which remain can be considered a simple random sample of the full data set of interest. MCAR is generally regarded as a strong and often unrealistic assumption.\n-   Missing at random (MAR). When data are MAR, the fact that the data are missing is systematically related to the observed but not the unobserved data.15 For example, a registry examining depression may encounter data that are MAR if male participants are less likely to complete a survey about depression severity than female participants. That is, if probability of completion of the survey is related to their sex (which is fully observed) but not the severity of their depression, then the data may be regarded as MAR. Complete case analyses, which are based on only observations for which all relevant data are present and no fields are missing, of a data set containing MAR data may or may not result in bias. If the complete case analysis is biased, however, proper accounting for the known factors (in the above example, sex) can produce unbiased results in analysis.\n-   Missing not at random (MNAR). When data are MNAR, the fact that the data are missing is systematically related to the unobserved data, that is, the missingness is related to events or factors which are not measured by the researcher. To extend the previous example, the depression registry may encounter data that are MNAR if participants with severe depression are more likely to refuse to complete the survey about depression severity. As with MAR data, complete case analysis of a data set containing MNAR data may or may not result in bias; if the complete case analysis is biased, however, the fact that the sources of missing data are themselves unmeasured means that (in general) this issue cannot be addressed in analysis and the estimate of effect will likely be biased.\n\n![Missing mechanisms examples](data/miss_mecha.png) \\## t-test for MAR: data preparation Great work on classifying the missing data mechanisms in the last exercise! Of all three, MAR is arguably the most important one to detect, as many imputation methods assume the data are MAR. This exercise will, therefore, focus on testing for MAR.\n\nYou will be working with the familiar biopics data. The goal is to test whether the number of missing values in earnings differs per subject's gender. In this exercise, you will only prepare the data for the t-test. First, you will create a dummy variable indicating missingness in earnings. Then, you will split it per gender by first filtering the data to keep one of the genders, and then pulling the dummy variable. For filtering, it might be helpful to print biopics's head() in the console and examine the gender variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a dummy variable for missing earnings\nbiopics <- biopics %>% \n  mutate(missing_earnings = ifelse(is.na(earnings), TRUE, FALSE))\n\n# Pull the missing earnings dummy for males\nmissing_earnings_males <- biopics %>% \n  filter(sub_sex == \"Male\") %>% \n  pull(missing_earnings)\n\n# Pull the missing earnings dummy for females\nmissing_earnings_females <- biopics %>% \n  filter(sub_sex == \"Female\") %>% \n  pull(missing_earnings)\n\n# Run the t-test\nt.test(missing_earnings_males, missing_earnings_females)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  missing_earnings_males and missing_earnings_females\nt = 1.1116, df = 294.39, p-value = 0.2672\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.03606549  0.12969214\nsample estimates:\nmean of x mean of y \n0.4366438 0.3898305 \n```\n:::\n:::\n\n\n-   Notice how the missing earnings percentage is not significantly different for both genders, even though the sample values (at the bottom of the test's output) differ by almost 5 percentage points. Also, keep in mind that the conclusion that the data are not MAR is only valid for the specific variables we have tested.\n\n## Aggregation plot\n\nThe aggregation plot provides the answer to the basic question one may ask about an incomplete dataset: in which combinations of variables the data are missing, and how often? It is very useful for gaining a high-level overview of the missingness patterns. For example, it makes it immediately visible if there is some combination of variables that are often missing together, which might suggest some relation between them.\n\nIn this exercise, you will first draw the aggregation plot for the biopics data and then practice making conclusions based on it. Let's do some plotting!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the VIM package\nlibrary(VIM)\n\n# Draw an aggregation plot of biopics\nbiopics %>% \n\taggr(combined = TRUE, numbers = TRUE)\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n-   10% of the observations have missing values in both earnings and sub_race.\n\n-   There are more missing values in sub_race than in earnings. This is false\n\n-   42% of the observations have no missing entries.\n\n-   There are exactly two variables in the biopics data that have missing values.\n\n-   This one is false! It is actually the other way round, there are more missing values in earnings. You can see it from the bars above the plot. Now that you have a high-level overview of the missingness in the data, let's look more closely at specific variables!\n\n## Spine plot\n\nThe aggregation plot you have drawn in the previous exercise gave you some high-level overview of the missing data. If you are interested in the interaction between specific variables, a spine plot is the way to go. It allows you to study the percentage of missing values in one variable for different values of the other, which is conceptually very similar to the t-tests you have been running in the previous lesson.\n\nIn this exercise, you will draw a spine plot to investigate the percentage of missing data in earnings for different categories of sub_race. Is there more missing data on earnings for some specific races of the movie's main character? Let's find out! The VIM package has already been loaded for you.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Draw a spine plot to analyse missing values in earnings by sub_race\nbiopics %>% \n\tselect(sub_race, earnings) %>% as.data.frame() %>%\n\tspineMiss()\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n### Based on the spine plot you have just created, which of the following statements is false?\n\na)  In the vast majority of movies, the main character is white.\n\nb)  When the main subject is African, we are the most likely to have complete earnings information.\n\nc)  As far as earnings and sub_race are concerned, the data seem to be MAR.\n\nd)  The race that appears most rarely in the data has around 40% of earnings missing.\n\n-   ***This one is false! The scarcest race is Asian, as this bar is the thinnest. The missing earnings, however, amount to around 20%, not 40%. Let's build upon the idea of a spine plot to create one more visualization in the next exercise!***\n\n## Mosaic plot\n\nThe spine plot you have created in the previous exercise allows you to study missing data patterns between two variables at a time. This idea is generalized to more variables in the form of a mosaic plot.\n\nIn this exercise, you will start by creating a dummy variable indicating whether the United States was involved in the production of each movie. To do this, you will use the grepl() function, which checks if the string passed as its first argument is present in the object passed as its second argument. Then, you will draw a mosaic plot to see if the subject's gender correlates with the amount of missing data on earnings for both US and non-US movies.\n\nThe biopics data as well as the VIM package are already loaded for you. Let's do some exploratory plotting!\n\nNote that a propriety display_image()function has been created to return the output from the latest VIMpackage version. Make sure to expand the HTML Viewer section.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prepare data for plotting and draw a mosaic plot\nbiopics %>%\n\t# Create a dummy variable for US-produced movies\n\tmutate(is_US_movie = grepl(\"US\", country)) %>%\n\t# Draw mosaic plot\n\tmosaicMiss(highlight = \"earnings\", \n             plotvars = c(\"is_US_movie\", \"sub_sex\"))\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Return plot from latest VIM package - expand the HTML viewer section\n#display_image()\n```\n:::\n\n\n-   ***Before you expand the output, notice how, for non-US movies, there is less missing data on earnings for movies featuring females. This doesn't look MCAR! You are now done with Chapter 1 and ready to take a deep dive into imputation methods.***\n\n# Donor-based imputation\n\n## Smelling the danger of mean imputation\n\nOne of the most popular imputation methods is the mean imputation, in which missing values in a variable are replaced with the mean of the observed values in this variable. However, in many cases this simple approach is a poor choice. Sometimes a quick look at the data can already alert you to the dangers of mean-imputing.\n\nIn this chapter, you will be working with a subsample of the Tropical Atmosphere Ocean (tao) project data. The dataset consists of atmospheric measurements taken in two different time periods at five different locations. The data comes with the VIM package.\n\nIn this exercise you will familiarize yourself with the data and perform a simple analysis that will indicate what the consequences of mean imputation could be. Let's take a look at the tao data!\n\n### Print first 10 observations\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntao <- read.csv(\"data/tao.csv\")\n# Print first 10 observations\nhead(tao, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   year latitude longitude sea_surface_temp air_temp humidity uwind vwind\n1  1997        0      -110            27.59    27.15     79.6  -6.4   5.4\n2  1997        0      -110            27.55    27.02     75.8  -5.3   5.3\n3  1997        0      -110            27.57    27.00     76.5  -5.1   4.5\n4  1997        0      -110            27.62    26.93     76.2  -4.9   2.5\n5  1997        0      -110            27.65    26.84     76.4  -3.5   4.1\n6  1997        0      -110            27.83    26.94     76.7  -4.4   1.6\n7  1997        0      -110            28.01    27.04     76.5  -2.0   3.5\n8  1997        0      -110            28.04    27.11     78.3  -3.7   4.5\n9  1997        0      -110            28.02    27.21     78.6  -4.2   5.0\n10 1997        0      -110            28.05    27.25     76.9  -3.6   3.5\n```\n:::\n:::\n\n\n### Get the number of missing values per column\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the number of missing values per column\ntao %>%\n  is.na() %>% \n  colSums()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            year         latitude        longitude sea_surface_temp \n               0                0                0                3 \n        air_temp         humidity            uwind            vwind \n              81               93                0                0 \n```\n:::\n:::\n\n\n### Calculate the number of missing values in air_temp per year\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the number of missing values in air_temp per year\ntao %>% \n  group_by(year) %>% \n  summarize(num_miss = sum(is.na(air_temp))) %>%\n    kable()\n```\n\n::: {.cell-output-display}\n| year| num_miss|\n|----:|--------:|\n| 1993|        4|\n| 1997|       77|\n:::\n:::\n\n\n## Mean-imputing the temperature\n\nMean imputation can be a risky business. If the variable you are mean-imputing is correlated with other variables, this correlation might be destroyed by the imputed values. You saw it looming in the previous exercise when you analyzed the air_temp variable.\n\nTo find out whether these concerns are valid, in this exercise you will perform mean imputation on air_temp, while also creating a binary indicator for where the values are imputed. It will come in handy in the next exercise, when you will be assessing your imputation's performance. Let's fill in those missing values!\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntao_imp <- tao %>% \n  # Create a binary indicator for missing values in air_temp\n  mutate(air_temp_imp = ifelse(is.na(air_temp), TRUE, FALSE)) %>% \n  # Impute air_temp with its mean\n  mutate(air_temp = ifelse(is.na(air_temp), mean(air_temp, na.rm = TRUE), air_temp))\n\n# Print the first 10 rows of tao_imp\nhead(tao_imp, 10) %>%\n    head() %>%\n    kable()\n```\n\n::: {.cell-output-display}\n| year| latitude| longitude| sea_surface_temp| air_temp| humidity| uwind| vwind|air_temp_imp |\n|----:|--------:|---------:|----------------:|--------:|--------:|-----:|-----:|:------------|\n| 1997|        0|      -110|            27.59|    27.15|     79.6|  -6.4|   5.4|FALSE        |\n| 1997|        0|      -110|            27.55|    27.02|     75.8|  -5.3|   5.3|FALSE        |\n| 1997|        0|      -110|            27.57|    27.00|     76.5|  -5.1|   4.5|FALSE        |\n| 1997|        0|      -110|            27.62|    26.93|     76.2|  -4.9|   2.5|FALSE        |\n| 1997|        0|      -110|            27.65|    26.84|     76.4|  -3.5|   4.1|FALSE        |\n| 1997|        0|      -110|            27.83|    26.94|     76.7|  -4.4|   1.6|FALSE        |\n:::\n:::\n\n\n### Assessing imputation quality with margin plot\n\nIn the last exercise, you have mean-imputed air_temp and added an indicator variable to denote which values were imputed, called air_temp_imp. Time to see how well this works.\n\nUpon examining the tao data, you might have noticed that it also contains a variable called sea_surface_temp, which could reasonably be expected to be positively correlated with air_temp. If that's the case, you would expect these two temperatures to be both high or both low at the same time. Imputing mean air temperature when the sea temperature is high or low would break this relation.\n\nTo find out, in this exercise you will select the two temperature variables and the indicator variable and use them to draw a margin plot. Let's assess the mean imputation!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Draw a margin plot of air_temp vs sea_surface_temp\ntao_imp %>% \n  select(air_temp, sea_surface_temp, air_temp_imp) %>%\n  marginplot(delimiter = \"imp\")\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n**Question**\n\n-   Judging by the margin plot you have drawn, what's wrong with this mean imputation?\n\n-   *Possible Answers*\n\ni.  All the imputed air_temp values are the same, no matter the sea_surface_temp. This breaks the correlation between these two variables.\n\nii. The imputed values are located in the space where there is no observed data, which makes them outliers.\n\niii. The variance of the imputed data differs from the one of observed data.\n\niv. All three above answers are correct. ***correct***\n\n-   ***Notice how air and sea surface temperatures correlate. Imputing average air temperature in the observations where sea surface temperature is high creates clearly outlying data points and destroys the relation between these two variables. If the sea surface temperature is high, we would like to impute air temperature values that are also high. Head over to the upcoming video to learn a method that is able to do that!***\n\n### The problem of mean imputation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(tao_imp, aes(air_temp, sea_surface_temp, color = air_temp_imp))+\n    geom_point()+\n    scale_color_brewer(name = \"Imputed\", type = \"qual\", palette = \"Dark2\")+\n    theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n### Vanilla hot-deck\n\nHot-deck imputation is a simple method that replaces every missing value in a variable by the last observed value in this variable. It's very fast, as only one pass through the data is needed, but in its simplest form, hot-deck may sometimes break relations between the variables.\n\nIn this exercise, you will try it out on the tao dataset. You will hot-deck-impute missing values in the air temperature column air_temp and then draw a margin plot to analyze the relation between the imputed values with the sea surface temperature column sea_surface_temp. Let's see how it works!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load VIM package\nlibrary(VIM)\n\n# Impute air_temp in tao with hot-deck imputation\ntao_imp <- hotdeck(tao, variable = \"air_temp\")\n\n# Check the number of missing values in each variable\ntao_imp %>% \n\tis.na() %>% \n\tcolSums()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            year         latitude        longitude sea_surface_temp \n               0                0                0                3 \n        air_temp         humidity            uwind            vwind \n               0               93                0                0 \n    air_temp_imp \n               0 \n```\n:::\n\n```{.r .cell-code}\n# Draw a margin plot of air_temp vs sea_surface_temp\ntao_imp %>% \n\tselect(air_temp, sea_surface_temp, air_temp_imp) %>% \n\tmarginplot(delimiter = \"imp\")\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n-   ***Does the imputation look good? Notice the observations in the top left part of the plot with imputed air_temp and high sea_surface_temp. These observations must have been preceded by ones with low air_temp in the data frame, and so after hot-deck imputation, they ended up being outliers with low air_temp and high sea_surface_temp.***\n\n### Hot-deck tricks & tips I: imputing within domains\n\nOne trick that may help when hot-deck imputation breaks the relations between the variables is imputing within domains. What this means is that if the variable to be imputed is correlated with another, categorical variable, one can simply run hot-deck separately for each of its categories.\n\nFor instance, you might expect air temperature to depend on time, as we are seeing the average temperatures rising due to global warming. The time indicator you have available in the tao data is a categorical variable, year. Let's first check if the average air temperature is different in each of the two studied years and then run hot-deck within year domains. Finally, you will draw the margin plot again to assess the imputation performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate mean air_temp per year\ntao %>% \n\tgroup_by(year) %>% \n\tsummarize(average_air_temp = mean(air_temp, na.rm = TRUE)) %>%\n    kable()\n```\n\n::: {.cell-output-display}\n| year| average_air_temp|\n|----:|----------------:|\n| 1993|         23.36596|\n| 1997|         27.10979|\n:::\n\n```{.r .cell-code}\n# Hot-deck-impute air_temp in tao by year domain\ntao_imp <- hotdeck(tao, variable = \"air_temp\", domain_var = \"year\")\n\n# Draw a margin plot of air_temp vs sea_surface_temp\ntao_imp %>% \n\tselect(air_temp, sea_surface_temp, air_temp_imp) %>% \n\tmarginplot(delimiter = \"imp\")\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n-   ***The results look much better this time. However, if you look at the top right corner of the plot, you will see that the variance in the imputed (orange) values is somewhat larger than among the observed (blue) values. Let's see if we can improve even further in the next exercise***\n\n### Hot-deck tricks & tips II: sorting by correlated variables\n\nAnother trick that can boost the performance of hot-deck imputation is sorting the data by variables correlated to the one we want to impute.\n\nFor instance, in all the margin plots you have been drawing recently, you have seen that air temperature is strongly correlated with sea surface temperature, which makes a lot of sense. You can exploit this knowledge to improve your hot-deck imputation. If you first order the data by sea_surface_temp, then every imputed air_temp value will come from a donor with a similar sea_surface_temp. Let's see how this will work!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Hot-deck-impute air_temp in tao ordering by sea_surface_temp\ntao_imp <- hotdeck(tao, \n                   variable = \"air_temp\", \n                   ord_var = \"sea_surface_temp\")\n\n# Draw a margin plot of air_temp vs sea_surface_temp\ntao_imp %>% \n\tselect(air_temp, sea_surface_temp, air_temp_imp) %>% \n\tmarginplot(delimiter = \"imp\")\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n-   ***This time the imputation seems not to impact the relation between air and sea temperatures: if not for the colors, you likely wouldn't know which ones are the imputed values. Hot-deck imputation, possibly enhanced with domain-imputing or sorting, is a fast and simple method that can serve you well in many situations. However, sometimes you may need a more complex approach. Head over to the next video to learn about k-Nearest-Neighbors imputation!***\n\n### Just a little experiment\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Hot-deck-impute air_temp in tao ordering by sea_surface_temp\ntao_imp <- hotdeck(tao, \n                   variable = \"air_temp\", \n                   ord_var = \"sea_surface_temp\",\n                   domain_var = \"year\")\n\n# Draw a margin plot of air_temp vs sea_surface_temp\ntao_imp %>% \n\tselect(air_temp, sea_surface_temp, air_temp_imp) %>% \n\tmarginplot(delimiter = \"imp\")\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n### Choosing the number of neighbors\n\nk-Nearest-Neighbors (or kNN) imputation fills the missing values in an observation based on the values coming from the k other observations that are most similar to it. The number of these similar observations, called neighbors, that are considered is a parameter that has to be chosen beforehand.\n\nHow to choose k? One way is to try different values and see how they impact the relations between the imputed and observed data.\n\nLet's try imputing humidity in the tao data using three different values of k and see how the imputed values fit the relation between humidity and sea_surface_temp.\n\n-   **Impute humidity with kNN imputation using 30 neighbors and draw a marginplot() of sea_surface_temp vs humidity.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Impute humidity using 30 neighbors\ntao_imp <- kNN(tao, k = 30, variable = \"humidity\")\n\n# Draw a margin plot of sea_surface_temp vs humidity\ntao_imp %>% \n\tselect(sea_surface_temp, humidity, humidity_imp) %>% \n\tmarginplot(delimiter = \"imp\", main = \"k = 30\")\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n-   **Impute humidity with kNN imputation using 15 neighbors and draw a margin plot of sea_surface_temp vs humidity.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Impute humidity using 15 neighbors\ntao_imp <- kNN(tao, k = 15, variable = \"humidity\")\n\n# Draw a margin plot of sea_surface_temp vs humidity\ntao_imp %>% \n\tselect(sea_surface_temp, humidity, humidity_imp) %>% \n\tmarginplot(delimiter = \"imp\", main = \"k = 15\")\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n-   **Impute humidity with kNN imputation using 5 neighbors and draw a margin plot of sea_surface_temp vs humidity.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Impute humidity using 5 neighbors\ntao_imp <- kNN(tao, k = 5, variable = \"humidity\")\n\n# Draw a margin plot of sea_surface_temp vs humidity\ntao_imp %>% \n\tselect(sea_surface_temp, humidity, humidity_imp) %>% \n\tmarginplot(delimiter = \"imp\", main = \"k = 5\")\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n-   ***You can browse through the three plots you have just drawn. The last one seems to capture the most variation in the data, so you should be good to use k = 5 in this case. Let's look at how we can improve on this default kNN imputation with some tricks!***\n\n## kNN tricks & tips I: weighting donors\n\nA variation of kNN imputation that is frequently applied uses the so-called distance-weighted aggregation. What this means is that when we aggregate the values from the neighbors to obtain a replacement for a missing value, we do so using the weighted mean and the weights are inverted distances from each neighbor. As a result, closer neighbors have more impact on the imputed value.\n\nIn this exercise, you will apply the distance-weighted aggregation while imputing the tao data. This will only require passing two additional arguments to the kNN() function. Let's try it out!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the VIM package\nlibrary(VIM)\n\n# Impute humidity with kNN using distance-weighted mean\ntao_imp <- kNN(tao, \n               k = 5, \n               variable = \"humidity\", \n               numFun = weighted.mean,\n               weightDist = TRUE)\n\ntao_imp %>% \n\tselect(sea_surface_temp, humidity, humidity_imp) %>% \n\tmarginplot(delimiter = \"imp\")\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n-   ***Distance-weighted aggregation makes the kNN imputation more robust to situations where an observation is unique in some way and doesn't have many very similar neighbors. In such cases, the least similar neighbors get assigned a small weight and contribute less to the imputed values. Head over to the last exercise of this chapter to learn one more trick that makes kNN more robust and accurate!***\n\n## kNN tricks & tips II: sorting variables\n\nAs the k-Nearest Neighbors algorithm loops over the variables in the data to impute them, it computes distances between observations using other variables, some of which have already been imputed in the previous steps. This means that if the variables located earlier in the data have a lot of missing values, then the subsequent distance calculation is based on a lot of imputed values. This introduces noise to the distance calculation.\n\nFor this reason, it is a good practice to sort the variables increasingly by the number of missing values before performing kNN imputation. This way, each distance calculation is based on as much observed data and as little imputed data as possible.\n\nLet's try this out on the tao data!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get tao variable names sorted by number of NAs\nvars_by_NAs <- tao %>%\n  is.na() %>%\n  colSums() %>%\n  sort(decreasing = FALSE) %>% \n  names()\nvars_by_NAs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"year\"             \"latitude\"         \"longitude\"        \"uwind\"           \n[5] \"vwind\"            \"sea_surface_temp\" \"air_temp\"         \"humidity\"        \n```\n:::\n\n```{.r .cell-code}\n# Sort tao variables and feed it to kNN imputation\ntao_imp <- tao %>% \n  select(vars_by_NAs) %>% \n  kNN(k = 5)\n\ntao_imp %>% \n\tselect(sea_surface_temp, humidity, humidity_imp) %>% \n\tmarginplot(delimiter = \"imp\")\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n-   ***The kNN you have just coded should be more accurate and robust against faulty imputations, so remember to sort your variables first before performing kNN imputation! This brings us to the end of this chapter. Keep it up! See you in Chapter 3, where you will learn to use statistical and machine learning models to impute missing values!***\n\n# Model-based imputation\n\n## Linear regression imputation\n\nSometimes, you can use domain knowledge, previous research or simply your common sense to describe the relations between the variables in your data. In such cases, model-based imputation is a great solution, as it allows you to impute each variable according to a statistical model that you can specify yourself, taking into account any assumptions you might have about how the variables impact each other.\n\nFor continuous variables, a popular model choice is linear regression. It doesn't restrict you to linear relations though! You can always include a square or a logarithm of a variable in the predictors. In this exercise, you will work with the simputation package to run a single linear regression imputation on the tao data and analyze the results. Let's give it a try!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the simputation package\nlibrary(simputation)\n\n# Impute air_temp and humidity with linear regression\nformula <- air_temp + humidity ~ year + latitude + sea_surface_temp\ntao_imp <- impute_lm(tao, formula)\n\n# Check the number of missing values per column\ntao_imp %>% \n  is.na() %>% \n  colSums()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            year         latitude        longitude sea_surface_temp \n               0                0                0                3 \n        air_temp         humidity            uwind            vwind \n               3                2                0                0 \n```\n:::\n\n```{.r .cell-code}\n# Print rows of tao_imp in which air_temp or humidity are still missing \ntao_imp %>% \n  filter(is.na(air_temp) | is.na(humidity)) %>%\n    kable()\n```\n\n::: {.cell-output-display}\n| year| latitude| longitude| sea_surface_temp| air_temp| humidity| uwind| vwind|\n|----:|--------:|---------:|----------------:|--------:|--------:|-----:|-----:|\n| 1993|        0|       -95|               NA|       NA|       NA|  -5.6|   3.1|\n| 1993|        0|       -95|               NA|       NA|       NA|  -6.3|   0.5|\n| 1993|       -2|       -95|               NA|       NA|     89.9|  -3.4|   2.4|\n:::\n:::\n\n\n-   ***Linear regression fails when at least one of the predictors is missing. In this case, it was sea_surface_temp. In the next exercise, you will fix it by initializing the missing values before running impute_lm()***\n\n## Initializing missing values & iterating over variables\n\nAs you have just seen, running impute_lm() might not fill-in all the missing values. To ensure you impute all of them, you should initialize the missing values with a simple method, such as the hot-deck imputation you learned about in the previous chapter, which simply feeds forward the last observed value.\n\nMoreover, a single imputation is usually not enough. It is based on the basic initialized values and could be biased. A proper approach is to iterate over the variables, imputing them one at a time in the locations where they were originally missing.\n\nIn this exercise, you will first initialize the missing values with hot-deck imputation and then loop five times over air_temp and humidity from the tao data to impute them with linear regression. Let's get to it!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Initialize missing values with hot-deck\ntao_imp <- hotdeck(tao)\n\n# Create boolean masks for where air_temp and humidity are missing\nmissing_air_temp <- tao_imp$air_temp_imp\nmissing_humidity <-  tao_imp$humidity_imp\n\nfor (i in 1:5) {\n  # Set air_temp to NA in places where it was originally missing and re-impute it\n  tao_imp$air_temp[missing_air_temp] <- NA\n  tao_imp <- impute_lm(tao_imp, air_temp ~ year + latitude + sea_surface_temp + humidity)\n  # Set humidity to NA in places where it was originally missing and re-impute it\n  tao_imp$humidity[missing_humidity] <- NA\n  tao_imp <- impute_lm(tao_imp, humidity ~ year + latitude + sea_surface_temp + air_temp)\n}\n```\n:::\n\n\n-   ***That's a professional approach to model-based imputation you have just coded! But how do we know that 5 is the proper number of iterations to run? Let's look at the convergence in the next exercise!***\n\n## Detecting convergence\n\nGreat job iterating over the variables in the last exercise! But how many iterations are needed? When the imputed values don't change with the new iteration, we can stop.\n\nYou will now extend your code to compute the differences between the imputed variables in subsequent iterations. To do this, you will use the Mean Absolute Percentage Change function, defined for you as follows:\n\nmapc \\<- function(a, b) { mean(abs(b - a) / a, na.rm = TRUE) } mapc() outputs a single number that tells you how much b differs from a. You will use it to check how much the imputed variables change across iterations. Based on this, you will decide how many of them are needed!\n\nThe boolean masks missing_air_temp and missing_humidity are available for you, as is the hotdeck-initialized tao_imp data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmapc <- function(a, b) {\n  mean(abs(b - a) / a, na.rm = TRUE)\n}\n\ndiff_air_temp <- c()\ndiff_humidity <- c()\n\nfor (i in 1:5) {\n  # Assign the outcome of the previous iteration (or initialization) to prev_iter\n  prev_iter <- tao_imp\n  # Impute air_temp and humidity at originally missing locations\n  tao_imp$air_temp[missing_air_temp] <- NA\n  tao_imp <- impute_lm(tao_imp, air_temp ~ year + latitude + sea_surface_temp + humidity)\n  tao_imp$humidity[missing_humidity] <- NA\n  tao_imp <- impute_lm(tao_imp, humidity ~ year + latitude + sea_surface_temp + air_temp)\n  # Calculate MAPC for air_temp and humidity and append them to previous iteration's MAPCs\n  diff_air_temp <- c(diff_air_temp, mapc(prev_iter$air_temp, tao_imp$air_temp))\n  diff_humidity <- c(diff_humidity, mapc(prev_iter$humidity, tao_imp$humidity))\n}\n\ndf_diff  <- data.frame(diff_air_temp, diff_humidity)\nplot_diffs <- function(a, b) {\n  data.frame(\"mapc\" = c(a, b),\n             \"Variable\" = c(rep(\"air_temp\", length(a)),\n                            rep(\"humidity\", length(b))),\n             \"Iterations\" = c(1:length(a), 1:length(b))) %>% \n    ggplot(aes(Iterations, mapc, color = Variable)) +\n    geom_line(size = 1.5) +\n    ylab(\"Mean absolute percentage change\") +\n    ggtitle(\"Changes in imputed variables' values across iterations\") +\n    theme(legend.position = \"bottom\")\n}\n\nplot_diffs(diff_air_temp, diff_humidity)\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n-   ***Two are enough, as the third one brings virtually no change anymore!***\n\n## Logistic regression imputation\n\nA popular choice for imputing binary variables is logistic regression. Unfortunately, there is no function similar to impute_lm() that would do it. That's why you'll write such a function yourself!\n\nLet's call the function impute_logreg(). Its first argument will be a data frame df, whose missing values have been initialized and only containing missing values in the column to be imputed. The second argument will be a formula for the logistic regression model.\n\nThe function will do the following:\n\nKeep the locations of missing values. Build the model. Make predictions. Replace missing values with predictions. Don't worry about the line creating imp_var - this is just a way to extract the name of the column to impute from the formula. Let's do some functional programming!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimpute_logreg <- function(df, formula) {\n  # Extract name of response variable\n  imp_var <- as.character(formula[2])\n  # Save locations where the response is missing\n  missing_imp_var <- is.na(df[imp_var])\n  # Fit logistic regression mode\n  logreg_model <- glm(formula, data = df, family = binomial)\n  # Predict the response and convert it to 0s and 1s\n  preds <- predict(logreg_model, type = \"response\")\n  preds <- ifelse(preds >= 0.5, 1, 0)\n  # Impute missing values with predictions\n  df[missing_imp_var, imp_var] <-preds[missing_imp_var]\n  return(df)\n}\n```\n:::\n\n\n## Drawing from conditional distribution\n\nSimply calling predict() on a model will always return the same value for the same values of the predictors. This results in a small variability in imputed data. In order to increase it, so that the imputation replicates the variability from the original data, we can draw from the conditional distribution. What this means is that instead of always predicting 1 whenever the model outputs a probability larger than 0.5, we can draw the prediction from a binomial distribution described by the probability returned by the model.\n\nYou will work on the code you have written in the previous exercise. The following line was removed:\n\npreds \\<- ifelse(preds \\>= 0.5, 1, 0) Your task is to fill its place with drawing from a binomial distribution. That's just one line of code!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n impute_logreg <- function(df, formula) {\n  # Extract name of response variable\n  imp_var <- as.character(formula[2])\n  # Save locations where the response is missing\n  missing_imp_var <- is.na(df[imp_var])\n  # Fit logistic regression mode\n  logreg_model <- glm(formula, data = df, family = binomial)\n  # Predict the response\n  preds <- predict(logreg_model, type = \"response\")\n  # Sample the predictions from binomial distribution\n  preds <- rbinom(length(preds), size = 1, prob = preds)\n  # Impute missing values with predictions\n  df[missing_imp_var, imp_var] <- preds[missing_imp_var]\n  return(df)\n}\n```\n:::\n\n\n-   ***Drawing from the conditional distribution will make the imputed data's variability more similar to the one of original, observed data. With this powerful function at hand, you are now ready to design a model-based imputation flow that takes care of both continuous and binary variables. Let's do it in the next exercise!***\n\n## Model-based imputation with multiple variable types\n\nGreat job on writing the function to implement logistic regression imputation with drawing from conditional distribution. That's pretty advanced statistics you have coded! In this exercise, you will combine what you learned so far about model-based imputation to impute different types of variables in the tao data.\n\nYour task is to iterate over variables just like you have done in the previous chapter and impute two variables:\n\nis_hot, a new binary variable that was created out of air_temp, which is 1 if air_temp is at or above 26 degrees and is 0 otherwise; humidity, a continuous variable you are already familiar with. You will have to use the linear regression function you have learned before, as well as your own function for logistic regression. Let's get to it!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Initialize missing values with hot-deck\ntao <- tao %>% \n    mutate(is_hot = ifelse(air_temp > 26, 1, 0))\ntao_imp <- hotdeck(tao)\n\n# Create boolean masks for where is_hota and humidity are missing\nmissing_is_hot <- tao_imp$is_hot_imp\nmissing_humidity <- tao_imp$humidity_imp\n\nfor (i in 1:3) {\n  # Set is_hot to NA in places where it was originally missing and re-impute it\n  tao_imp$is_hot[missing_is_hot] <- NA\n  tao_imp <- impute_logreg(tao_imp, is_hot ~ sea_surface_temp)\n  # Set humidity to NA in places where it was originally missing and re-impute it\n  tao_imp$humidity[missing_humidity] <- NA\n  tao_imp <- impute_lm(tao_imp, \n  humidity ~ sea_surface_temp + air_temp)\n}\n```\n:::\n\n\n-   ***You have used the simputation package where possible, filling the gaps with your own programming, in order to run a model-based imputation that takes care of both continuous and binary variables, additionally inreasing variability in imputed data in the latter case. Well done! Let's continue to the final lesson of this chapter, where you will learn how to use tree-based machine learning models for imputation.***\n\n## Imputing with random forests\n\nA machine learning approach to imputation might be both more accurate and easier to implement compared to traditional statistical models. First, it doesn't require you to specify relationships between variables. Moreover, machine learning models such as random forests are able to discover highly complex, non-linear relations and exploit them to predict missing values.\n\nIn this exercise, you will get acquainted with the missForest package, which builds a separate random forest to predict missing values for each variable, one by one. You will call the imputing function on the biographic movies data, biopics, which you have worked with earlier in the course and then extract the filled-in data as well as the estimated imputation errors.\n\nLet's plant some random forests!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the missForest package\nbiopics <- read_csv(\"data/biopics.csv\")\nlibrary(missForest)\n\ncont_lev <- c(\"UK\", \"US/UK\", \"Canada US\", \n           \"Canada/UK\", \"US/Canada\", \"US/UK/Canada\")\n\nbiopics <- biopics %>%\n    mutate(country = factor(country, levels = cont_lev))\nbiopics <- biopics %>%\n    mutate_if(is.character, factor)\n# Impute biopics data using missForest\nbiopics <- as.data.frame(biopics)\nimp_res <- missForest(biopics)\n\n# Extract imputed data and check for missing values\nimp_data <- imp_res$ximpnhanes_imp\nprint(sum(is.na(imp_data)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\n# Extract and print imputation errors\nimp_err <- imp_res$OOBerror\nprint(imp_err)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     NRMSE        PFC \n0.01928484 0.14432903 \n```\n:::\n:::\n\n\n***Note that missForest() outputs a list and you have to manually extract the imputed data - it's a common mistake to overlook it when building a data processing pipeline. Also, take a look at the errors. Can you tell which variables have been imputed particularly well? Let's look at it more closely in the next exercise!***\n\n## Variable-wise imputation errors\n\nIn the previous exercise you have extracted the estimated imputation errors from missForest's output. This gave you two numbers:\n\nthe normalized root mean squared error (NRMSE) for all continuous variables; the proportion of falsely classified entries (PFC) for all categorical variables. However, it could well be that the imputation model performs great for one continuous variable and poor for another! To diagnose such cases, it is enough to tell missForest to produce variable-wise error estimates. This is done by setting the variablewise argument to TRUE.\n\nThe biopics data and missForest package have already been loaded for you, so let's take a closer look at the errors!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Impute biopics data with missForest computing per-variable errors\nimp_res <- missForest(biopics, variablewise = TRUE)\n\n# Extract and print imputation errors\nper_variable_errors <- imp_res$OOBerror\nprint(per_variable_errors)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        PFC         MSE         MSE         MSE         PFC         PFC \n  0.3385827   0.0000000 998.4068755   0.0000000   0.0000000   0.1897163 \n        MSE         PFC \n  0.0000000   0.0000000 \n```\n:::\n\n```{.r .cell-code}\n# Rename errors' columns to include variable names\nnames(per_variable_errors) <- paste(names(biopics), \n                                    names(per_variable_errors),\n                                    sep = \"_\")\n\n# Print the renamed errors\nprint(per_variable_errors)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  country_PFC      year_MSE  earnings_MSE   sub_num_MSE  sub_type_PFC \n    0.3385827     0.0000000   998.4068755     0.0000000     0.0000000 \n sub_race_PFC non_white_MSE   sub_sex_PFC \n    0.1897163     0.0000000     0.0000000 \n```\n:::\n:::\n\n\n## Speed-accuracy trade-off\n\nIn the last video, you have seen there are two knobs you can tune to influence the performance of the random forests:\n\nNumber of decision trees in each forest. Number of variables used for splitting within decision trees. Increasing each of them might improve the accuracy of the imputation model, but it will also require more time to run. In this exercise, you will explore these ideas yourself by fitting missForest() to the biopics data twice with different settings. As you follow the instructions, pay attention to the errors you will be printing, and to the time the code takes to run.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set number of trees to 50 and number of variables used for splitting to 6\nimp_res <- missForest(biopics, ntree = 5, mtry = 2)\n\n# Print the resulting imputation errors\nprint(imp_res$OOBerror)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     NRMSE        PFC \n0.02315201 0.20332849 \n```\n:::\n\n```{.r .cell-code}\n# Set number of trees to 50 and number of variables used for splitting to 6\nimp_res <- missForest(biopics, ntree = 50, mtry = 6)\n\n# Print the resulting imputation errors\nprint(imp_res$OOBerror)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     NRMSE        PFC \n0.02110847 0.14142165 \n```\n:::\n:::\n\n\n-   ***Compare the errors and the run times of the two imputation models. Can you see a relation? There ain't no such thing as a free lunch, they say. To get a more precise imputation, you had to spend more in computing time! Congratulations on finishing the chapter! See you in the final chapter, where you will learn to incorporate uncertainty from imputation into your analyses and predictions.***\n\n# Uncertainty from imputation\n\n## Wrapping imputation & modeling in a function\n\nWhenever you perform any analysis or modeling on imputed data, you should account for the uncertainty from imputation. Running a model on a dataset imputed only once ignores the fact that imputation estimates the missing values with uncertainty. Standard errors from such a model tend to be too small. The solution to this is multiple imputation and one way to implement it is by bootstrapping.\n\nIn the upcoming exercises, you will work with the familiar biopics data. The goal is to use multiple imputation by bootstrapping and linear regression to see if, based on the data at hand, biographical movies featuring females earn less than those about males.\n\nLet's start with writing a function that creates a bootstrap sample, imputes it, and fits a linear regression model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncalc_gender_coef <- function(data, indices) {\n  # Get bootstrap sample\n  data_boot <- data[indices, ]\n  # Impute with kNN imputation\n  data_imp <- kNN(data_boot, k = 5)\n  # Fit linear regression\n  linear_model <- lm(earnings ~ sub_sex + sub_type + year,data = data_imp)\n  # Extract and return gender coefficient\n  gender_coefficient <- coef(linear_model)[2]\n  return(gender_coefficient)\n}\n```\n:::\n\n\n***The calc_gender_coef() function you have just coded takes the data and bootstrap indices as inputs, and outputs our statistic of interest - the impact of gender on earnings from linear regression. You can now feed this function to the bootstrapping algorithm!***\n\n## Running the bootstrap\n\nGood job writing calc_gender_coef() in the last exercise! This function creates a bootstrap sample, imputes it and, outputs the linear regression coefficient describing the impact of movie subject's being a female on the movie's earnings.\n\nIn this exercise, you will use the boot package in order to obtain a bootstrapped distribution of such coefficients. The spread of this distribution will capture the uncertainty from imputation. You will also look at how the bootstrapped distribution differs from a single-time imputation and regression. Let's do some bootstrapping!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the boot library\nlibrary(boot)\n\n# Run bootstrapping on biopics data\nboot_results <- boot(biopics, statistic = calc_gender_coef, R = 50)\n\n# Print and plot bootstrapping results\nprint(boot_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = biopics, statistic = calc_gender_coef, R = 50)\n\n\nBootstrap Statistics :\n    original    bias    std. error\nt1*  2.64305 -1.859911    5.456893\n```\n:::\n\n```{.r .cell-code}\nplot(boot_results)\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Calculate and print confidence interval\nboot_ci <- boot.ci(boot_results, conf = .95, type = \"norm\")\nprint(boot_ci)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 50 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_results, conf = 0.95, type = \"norm\")\n\nIntervals : \nLevel      Normal        \n95%   (-6.192, 15.198 )  \nCalculations and Intervals on Original Scale\n```\n:::\n:::\n\n\n-   ***If you had run the kNN imputation and the regression analysis on biopics data only once, you would have obtained the female-coefficient of -1.45 (called 'original' in the console output), suggesting that movies about females indeed earn less. However, correcting for the uncertainty from imputation, you have obtained the distribution that covers both negative and postive values!***\n\n### Bootstrapping confidence intervals\n\nHaving bootstrapped the distribution of the female-effect coefficient in the last exercise, you can now use it to estimate a confidence interval. It will allow you to make the following assessment about your data: \"Given the uncertainty from imputation, we are 95% sure that the female-effect on earnings is between a and b\", where a and b are the lower and upper bounds of the interval.\n\nIn the last exercise, you have run bootstrapping with R = 50 replicates. In most applications, however, this is not enough. In this exercise, you can use boot_results that were prepared for you using 1000 replicates. First, you will look at the bootstrapped distribution to see if it looks normal. If so, you can then rely on the normal distribution to calculate the confidence interval.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run bootstrapping on biopics data\nboot_results <- boot(biopics, statistic = calc_gender_coef, R = 1000)\n\n# Print and plot bootstrapping results\nprint(boot_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = biopics, statistic = calc_gender_coef, R = 1000)\n\n\nBootstrap Statistics :\n    original     bias    std. error\nt1* 2.172231 -0.7234102    3.785335\n```\n:::\n\n```{.r .cell-code}\nplot(boot_results)\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Calculate and print confidence interval\nboot_ci <- boot.ci(boot_results, conf = .95, type = \"norm\")\nprint(boot_ci)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_results, conf = 0.95, type = \"norm\")\n\nIntervals : \nLevel      Normal        \n95%   (-4.523, 10.315 )  \nCalculations and Intervals on Original Scale\n```\n:::\n:::\n\n\n-   ***Despite the coefficient leaning to be a negative relationship, bootstrap replicates show that some movies with female leads actually earn more! Accounting for the uncertainty from imputation, you cannot be 100% sure about the direction of this relation, even though a single analysis suggests otherwise.***\n\n## The mice flow: mice - with - pool\n\nMultiple imputation by chained equations, or MICE, allows us to estimate the uncertainty from imputation by imputing a data set multiple times with model-based imputation, while drawing from conditional distributions. This way, each imputed data set is slightly different. Then, an analysis is conducted on each of them and the results are pooled together, yielding the quantities of interest, alongside their confidence intervals that reflect the imputation uncertainty.\n\nIn this exercise, you will practice the typical MICE flow: mice() - with() - pool(). You will perform a regression analysis on the biopics data to see which subject occupation, sub_type, is associated with highest movie earnings. Let's play with mice!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load mice package\nlibrary(mice)\n\n# Impute biopics with mice using 5 imputations\nbiopics_multiimp <- mice(biopics, m = 5, seed = 3108)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n iter imp variable\n  1   1  country  earnings  sub_race\n  1   2  country  earnings  sub_race\n  1   3  country  earnings  sub_race\n  1   4  country  earnings  sub_race\n  1   5  country  earnings  sub_race\n  2   1  country  earnings  sub_race\n  2   2  country  earnings  sub_race\n  2   3  country  earnings  sub_race\n  2   4  country  earnings  sub_race\n  2   5  country  earnings  sub_race\n  3   1  country  earnings  sub_race\n  3   2  country  earnings  sub_race\n  3   3  country  earnings  sub_race\n  3   4  country  earnings  sub_race\n  3   5  country  earnings  sub_race\n  4   1  country  earnings  sub_race\n  4   2  country  earnings  sub_race\n  4   3  country  earnings  sub_race\n  4   4  country  earnings  sub_race\n  4   5  country  earnings  sub_race\n  5   1  country  earnings  sub_race\n  5   2  country  earnings  sub_race\n  5   3  country  earnings  sub_race\n  5   4  country  earnings  sub_race\n  5   5  country  earnings  sub_race\n```\n:::\n\n```{.r .cell-code}\n# Fit linear regression to each imputed data set \nlm_multiimp <- with(biopics_multiimp,  lm(earnings~year+sub_type ))\n\n# Pool and summarize regression results\nlm_pooled <- pool(lm_multiimp)\nsummary(lm_pooled, conf.int = TRUE, conf.level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                             term     estimate  std.error   statistic\n1                     (Intercept) -408.5476080 188.003023 -2.17309063\n2                            year    0.2195188   0.091547  2.39788059\n3  sub_typeAcademic (Philosopher)  -12.0393151  39.386794 -0.30566883\n4                sub_typeActivist  -12.5649024  12.493282 -1.00573272\n5                   sub_typeActor  -23.5461708  15.296101 -1.53935771\n6                 sub_typeActress  -17.5662047  12.764488 -1.37617778\n7      sub_typeActress / activist   20.3881286  35.764655  0.57006362\n8                  sub_typeArtist  -22.4452374  13.451761 -1.66857238\n9                 sub_typeAthlete   -0.8963086  13.009378 -0.06889712\n10     sub_typeAthlete / military   82.4367906  35.569407  2.31763188\n11                 sub_typeAuthor  -19.1635360  12.262989 -1.56271336\n12          sub_typeAuthor (poet)  -20.0620382  14.444056 -1.38894771\n13               sub_typeComedian  -18.1686840  16.980795 -1.06995483\n14               sub_typeCriminal   -5.8479945  13.420469 -0.43575187\n15             sub_typeGovernment   16.0450423  46.390726  0.34586746\n16             sub_typeHistorical   -3.7623435  11.294642 -0.33310871\n17             sub_typeJournalist  -22.8565780  26.181388 -0.87300866\n18                  sub_typeMedia   -7.0627461  18.145706 -0.38922410\n19               sub_typeMedicine   19.1339721  20.966662  0.91259031\n20               sub_typeMilitary   27.6733819  17.764776  1.55776701\n21    sub_typeMilitary / activist   41.9247600  35.860752  1.16909873\n22               sub_typeMusician  -14.1096056  11.123075 -1.26849868\n23                  sub_typeOther  -12.5957619  11.127539 -1.13194499\n24             sub_typePolitician   -8.9752400  35.860752 -0.25028030\n25                 sub_typeSinger   -2.3189932  14.863480 -0.15601953\n26                sub_typeTeacher   55.5076474  35.777696  1.55145953\n27           sub_typeWorld leader    1.9363047  14.102239  0.13730477\n           df    p.value         2.5 %      97.5 %\n1    8.441788 0.05977554 -838.16570234  21.0704863\n2    8.986358 0.04007631    0.01237714   0.4266604\n3   78.086760 0.76067020  -90.45102629  66.3723961\n4   23.732194 0.32468917  -38.36517557  13.2353708\n5   23.125611 0.13728938  -55.17905857   8.0867171\n6   63.616800 0.17359176  -43.06915926   7.9367499\n7  533.362518 0.56887459  -49.86873468  90.6449919\n8   17.334211 0.11316311  -50.78434701   5.8938723\n9   11.560021 0.94624889  -29.36142352  27.5688062\n10 610.794724 0.02079891   12.58361683 152.2899644\n11  18.269586 0.13527579  -44.89989577   6.5728238\n12  40.490599 0.17244132  -49.24354989   9.1194734\n13  68.008128 0.28842244  -52.05326005  15.7158921\n14  10.341265 0.67197238  -35.61744245  23.9214534\n15   5.438799 0.74242360 -100.36959617 132.4596809\n16  19.622883 0.74258502  -27.35161361  19.8269266\n17 401.472498 0.38318021  -74.32631797  28.6131621\n18  10.959555 0.70456670  -47.01916501  32.8936729\n19  11.569949 0.38008106  -26.73749557  65.0054398\n20   7.029288 0.16307001  -14.29818398  69.6449479\n21 499.260259 0.24292173  -28.53182460 112.3813447\n22  20.977532 0.21851670  -37.24281435   9.0236032\n23  15.905797 0.27443480  -36.19645010  11.0049264\n24 499.260259 0.80247354  -79.43182460  61.4813447\n25  16.388102 0.87792336  -33.76762529  29.1296390\n26 528.585147 0.12139010  -14.77627920 125.7915739\n27  27.018773 0.89180799  -26.99815808  30.8707674\n```\n:::\n:::\n\n\n***You have followed the mice - with - pool flow to impute, model and pool the results. Now take a look at the console output: a couple of sub_types have a positive impact on earnings. However, accounting for imputation uncertainty with 95% confidence, we are never sure of these effects, as the lower bounds are negative! With one exception: for sub_typeAthlete / military, both upper and lower bounds are positive. What we can say for sure is thus that movies about military athletes are popular!***\n\n## Choosing default models\n\nMICE creates a separate imputation model for each variable in the data. What kind of model it is depends on the type of the variable in question. A popular way to specify the kinds of models we want to use is set a default model for each of the four variable types.\n\nYou can do this by passing the defaultMethod argument to mice(), which should be a vector of length 4 containing the default imputation methods for:\n\nContinuous variables, Binary variables, Categorical variables (unordered factors), Factor variables (ordered factors). In this exercise, you will take advantage of mice's documentation to view the list of available methods and to pick the desired ones for the algorithm to use. Let's do some model selection!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Impute biopics using the methods specified in the instruction\nbiopics_multiimp <- mice(biopics, m = 20, \n                         defaultMethod = c(\"cart\", \"lda\", \"pmm\", \"polr\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n iter imp variable\n  1   1  country  earnings  sub_race\n  1   2  country  earnings  sub_race\n  1   3  country  earnings  sub_race\n  1   4  country  earnings  sub_race\n  1   5  country  earnings  sub_race\n  1   6  country  earnings  sub_race\n  1   7  country  earnings  sub_race\n  1   8  country  earnings  sub_race\n  1   9  country  earnings  sub_race\n  1   10  country  earnings  sub_race\n  1   11  country  earnings  sub_race\n  1   12  country  earnings  sub_race\n  1   13  country  earnings  sub_race\n  1   14  country  earnings  sub_race\n  1   15  country  earnings  sub_race\n  1   16  country  earnings  sub_race\n  1   17  country  earnings  sub_race\n  1   18  country  earnings  sub_race\n  1   19  country  earnings  sub_race\n  1   20  country  earnings  sub_race\n  2   1  country  earnings  sub_race\n  2   2  country  earnings  sub_race\n  2   3  country  earnings  sub_race\n  2   4  country  earnings  sub_race\n  2   5  country  earnings  sub_race\n  2   6  country  earnings  sub_race\n  2   7  country  earnings  sub_race\n  2   8  country  earnings  sub_race\n  2   9  country  earnings  sub_race\n  2   10  country  earnings  sub_race\n  2   11  country  earnings  sub_race\n  2   12  country  earnings  sub_race\n  2   13  country  earnings  sub_race\n  2   14  country  earnings  sub_race\n  2   15  country  earnings  sub_race\n  2   16  country  earnings  sub_race\n  2   17  country  earnings  sub_race\n  2   18  country  earnings  sub_race\n  2   19  country  earnings  sub_race\n  2   20  country  earnings  sub_race\n  3   1  country  earnings  sub_race\n  3   2  country  earnings  sub_race\n  3   3  country  earnings  sub_race\n  3   4  country  earnings  sub_race\n  3   5  country  earnings  sub_race\n  3   6  country  earnings  sub_race\n  3   7  country  earnings  sub_race\n  3   8  country  earnings  sub_race\n  3   9  country  earnings  sub_race\n  3   10  country  earnings  sub_race\n  3   11  country  earnings  sub_race\n  3   12  country  earnings  sub_race\n  3   13  country  earnings  sub_race\n  3   14  country  earnings  sub_race\n  3   15  country  earnings  sub_race\n  3   16  country  earnings  sub_race\n  3   17  country  earnings  sub_race\n  3   18  country  earnings  sub_race\n  3   19  country  earnings  sub_race\n  3   20  country  earnings  sub_race\n  4   1  country  earnings  sub_race\n  4   2  country  earnings  sub_race\n  4   3  country  earnings  sub_race\n  4   4  country  earnings  sub_race\n  4   5  country  earnings  sub_race\n  4   6  country  earnings  sub_race\n  4   7  country  earnings  sub_race\n  4   8  country  earnings  sub_race\n  4   9  country  earnings  sub_race\n  4   10  country  earnings  sub_race\n  4   11  country  earnings  sub_race\n  4   12  country  earnings  sub_race\n  4   13  country  earnings  sub_race\n  4   14  country  earnings  sub_race\n  4   15  country  earnings  sub_race\n  4   16  country  earnings  sub_race\n  4   17  country  earnings  sub_race\n  4   18  country  earnings  sub_race\n  4   19  country  earnings  sub_race\n  4   20  country  earnings  sub_race\n  5   1  country  earnings  sub_race\n  5   2  country  earnings  sub_race\n  5   3  country  earnings  sub_race\n  5   4  country  earnings  sub_race\n  5   5  country  earnings  sub_race\n  5   6  country  earnings  sub_race\n  5   7  country  earnings  sub_race\n  5   8  country  earnings  sub_race\n  5   9  country  earnings  sub_race\n  5   10  country  earnings  sub_race\n  5   11  country  earnings  sub_race\n  5   12  country  earnings  sub_race\n  5   13  country  earnings  sub_race\n  5   14  country  earnings  sub_race\n  5   15  country  earnings  sub_race\n  5   16  country  earnings  sub_race\n  5   17  country  earnings  sub_race\n  5   18  country  earnings  sub_race\n  5   19  country  earnings  sub_race\n  5   20  country  earnings  sub_race\n```\n:::\n\n```{.r .cell-code}\n# Print biopics_multiimp\nprint(biopics_multiimp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nClass: mids\nNumber of multiple imputations:  20 \nImputation methods:\n  country      year  earnings   sub_num  sub_type  sub_race non_white   sub_sex \n    \"pmm\"        \"\"    \"cart\"        \"\"        \"\"     \"pmm\"        \"\"        \"\" \nPredictorMatrix:\n         country year earnings sub_num sub_type sub_race non_white sub_sex\ncountry        0    1        1       1        1        1         1       1\nyear           1    0        1       1        1        1         1       1\nearnings       1    1        0       1        1        1         1       1\nsub_num        1    1        1       0        1        1         1       1\nsub_type       1    1        1       1        0        1         1       1\nsub_race       1    1        1       1        1        0         1       1\nNumber of logged events:  300 \n  it im      dep meth\n1  1  1  country  pmm\n2  1  1 earnings cart\n3  1  1 sub_race  pmm\n4  1  2  country  pmm\n5  1  2 earnings cart\n6  1  2 sub_race  pmm\n                                                                                                                                           out\n1 sub_typeActress / activist, sub_typeAthlete / military, sub_typeGovernment, sub_typeMilitary / activist, sub_typePolitician, sub_typeTeacher\n2                                                                                             countryCanada US, sub_typeAcademic (Philosopher)\n3                                                                                                countryCanada US, sub_typeMilitary / activist\n4 sub_typeActress / activist, sub_typeAthlete / military, sub_typeGovernment, sub_typeMilitary / activist, sub_typePolitician, sub_typeTeacher\n5                                                                                             countryCanada US, sub_typeAcademic (Philosopher)\n6                                                                                                countryCanada US, sub_typeMilitary / activist\n```\n:::\n:::\n\n\n-   ***The ability to specify imputation models might come in handy when you see some specific methods underperforming. Another factor infuencing how the imputation methods work is the set of predictors they use. Let's look at how to set these in the next exercise.***\n\n## Using predictor matrix\n\nAn important decision that needs to be taken when using model-based imputation is which variables should be included as predictors, and in which models. In mice(), this is governed by the predictor matrix and by default, all variables are used to impute all others.\n\nIn case of many variables in the data or little time to do a proper model selection, you can use mice's functionality to create a predictor matrix based on the correlations between the variables. This matrix can then be passed to mice(). In this exercise, you will practice exactly this: you will first build a predictor matrix such that each variable will be imputed using variables most correlated to it; then, you will feed your predictor matrix to the imputing function. Let's try this simple model selection!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create predictor matrix with minimum correlation of 0.1\npred_mat <- quickpred(biopics, mincor = 0.1)\n\n# Impute biopics with mice\nbiopics_multiimp <- mice(biopics, \n                         m = 10, \n                         predictorMatrix = pred_mat,\n                         seed = 3108)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n iter imp variable\n  1   1  country  earnings  sub_race\n  1   2  country  earnings  sub_race\n  1   3  country  earnings  sub_race\n  1   4  country  earnings  sub_race\n  1   5  country  earnings  sub_race\n  1   6  country  earnings  sub_race\n  1   7  country  earnings  sub_race\n  1   8  country  earnings  sub_race\n  1   9  country  earnings  sub_race\n  1   10  country  earnings  sub_race\n  2   1  country  earnings  sub_race\n  2   2  country  earnings  sub_race\n  2   3  country  earnings  sub_race\n  2   4  country  earnings  sub_race\n  2   5  country  earnings  sub_race\n  2   6  country  earnings  sub_race\n  2   7  country  earnings  sub_race\n  2   8  country  earnings  sub_race\n  2   9  country  earnings  sub_race\n  2   10  country  earnings  sub_race\n  3   1  country  earnings  sub_race\n  3   2  country  earnings  sub_race\n  3   3  country  earnings  sub_race\n  3   4  country  earnings  sub_race\n  3   5  country  earnings  sub_race\n  3   6  country  earnings  sub_race\n  3   7  country  earnings  sub_race\n  3   8  country  earnings  sub_race\n  3   9  country  earnings  sub_race\n  3   10  country  earnings  sub_race\n  4   1  country  earnings  sub_race\n  4   2  country  earnings  sub_race\n  4   3  country  earnings  sub_race\n  4   4  country  earnings  sub_race\n  4   5  country  earnings  sub_race\n  4   6  country  earnings  sub_race\n  4   7  country  earnings  sub_race\n  4   8  country  earnings  sub_race\n  4   9  country  earnings  sub_race\n  4   10  country  earnings  sub_race\n  5   1  country  earnings  sub_race\n  5   2  country  earnings  sub_race\n  5   3  country  earnings  sub_race\n  5   4  country  earnings  sub_race\n  5   5  country  earnings  sub_race\n  5   6  country  earnings  sub_race\n  5   7  country  earnings  sub_race\n  5   8  country  earnings  sub_race\n  5   9  country  earnings  sub_race\n  5   10  country  earnings  sub_race\n```\n:::\n\n```{.r .cell-code}\n# Print biopics_multiimp\nprint(biopics_multiimp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nClass: mids\nNumber of multiple imputations:  10 \nImputation methods:\n  country      year  earnings   sub_num  sub_type  sub_race non_white   sub_sex \n\"polyreg\"        \"\"     \"pmm\"        \"\"        \"\" \"polyreg\"        \"\"        \"\" \nPredictorMatrix:\n         country year earnings sub_num sub_type sub_race non_white sub_sex\ncountry        0    1        1       0        0        0         0       0\nyear           0    0        0       0        0        0         0       0\nearnings       1    1        0       0        0        1         1       0\nsub_num        0    0        0       0        0        0         0       0\nsub_type       0    0        0       0        0        0         0       0\nsub_race       0    1        1       1        1        0         1       0\nNumber of logged events:  100 \n  it im      dep    meth                         out\n1  1  1 earnings     pmm            countryCanada US\n2  1  1 sub_race polyreg sub_typeMilitary / activist\n3  1  2 earnings     pmm            countryCanada US\n4  1  2 sub_race polyreg sub_typeMilitary / activist\n5  1  3 earnings     pmm            countryCanada US\n6  1  3 sub_race polyreg sub_typeMilitary / activist\n```\n:::\n:::\n\n\n1.  Look at the predictor matrix you've used that is printed in the console. Which variables have been used as predictors to impute earnings?\n\n-   country, year and non_white\n\n-   ***Provides an easy way to set up predictor matrices, but if you can afford it, you should try to choose the predictors based on the insights from data analytics or on domain knowledge.***\n\n## Analyzing missing data patterns\n\nThe first step in working with incomplete data is to gain some insights into the missingness patterns, and a good way to do it is with visualizations. You will start your analysis of the africa data with employing the VIM package to create two visualizations: the aggregation plot and the spine plot. They will tell you how many data are missing, in which variables and configurations, and whether we can say something about the missing data mechanism. Let's kick off with some plotting!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nafrica <- read.csv( \"data/africa_clean.csv\")\n# Load VIM\nlibrary(VIM)\n\n# Draw a combined aggregation plot of africa\nafrica %>%\n  aggr(combined = TRUE, numbers = TRUE)\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\n### Question\n\n-   Based on the aggregation plot you have just created, which of the following statements is TRUE?\n-   ans Whenever gdp_pc is missing, trade is missing too\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Draw a spine plot of country vs trade\nafrica %>% \n  select(country ,trade) %>%\n  spineMiss()\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\n### Question\n\n-   Based on the spine plot you have just created, which of the following statements is TRUE?\n-   ***Correct, there are not that many missing values! Also, notice from the spine plot that the africa data seem to be MAR - at least with respect to the GDP and country, which means it can be imputed. Let's try multiple imputation to fill-in the missing values in the next exercise!***\n\n## Imputing and inspecting outcomes\n\nGood job on visualizing missing data in the previous exercise! You have discovered there are some missing entries in GDP, gdp_pc, and trade as percentage of GDP, trade. Also, you suspect the data are MAR, and thus imputable. In this exercise, you will make use of multiple imputation from the mice package to impute the africa data. Then, you will draw a strip plot of gdp_pc vs trade to see if the imputed data do not break the relation between these variables. Let mice do the job!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load mice\nlibrary(mice)\n\n# Impute africa with mice\nafrica_multiimp <- mice(africa, m = 5, defaultMethod = \"cart\", seed = 3108)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n iter imp variable\n  1   1  gdp_pc  trade\n  1   2  gdp_pc  trade\n  1   3  gdp_pc  trade\n  1   4  gdp_pc  trade\n  1   5  gdp_pc  trade\n  2   1  gdp_pc  trade\n  2   2  gdp_pc  trade\n  2   3  gdp_pc  trade\n  2   4  gdp_pc  trade\n  2   5  gdp_pc  trade\n  3   1  gdp_pc  trade\n  3   2  gdp_pc  trade\n  3   3  gdp_pc  trade\n  3   4  gdp_pc  trade\n  3   5  gdp_pc  trade\n  4   1  gdp_pc  trade\n  4   2  gdp_pc  trade\n  4   3  gdp_pc  trade\n  4   4  gdp_pc  trade\n  4   5  gdp_pc  trade\n  5   1  gdp_pc  trade\n  5   2  gdp_pc  trade\n  5   3  gdp_pc  trade\n  5   4  gdp_pc  trade\n  5   5  gdp_pc  trade\n```\n:::\n\n```{.r .cell-code}\n# Draw a stripplot of gdp_pc versus trade\nstripplot(africa_multiimp, gdp_pc ~ trade | .imp, pch = 20, cex = 2)\n```\n\n::: {.cell-output-display}\n![](imputations_in_R_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\n-   ***t seems the imputation works well: there are small clusters in the scatter plots, likely corresponding to different countries. Each imputed data point fits into one of the clusters, instead of being an outlier somewhere between the clusters. Having done the imputation, you can now proceed to modeling!***\n\n## Inference with imputed data\n\nIn the last exercise, you have run mice to multiply impute the africa data. In this one, you will implement the other two steps of the mice - with - pool flow you've learned about earlier in the course. The model of interest is a linear regression that explains the GDP, gdp_pc, with other variables. You are particularly interested in the coefficient of civil liberties, civlib. Is more liberty associated with more economic growth once we incorporate the uncertainty from imputation? Let's find out!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit linear regression to each imputed data set\nlm_multiimp <- with(africa_multiimp, lm(gdp_pc ~ country + year + trade + infl + civlib))\n\n# Pool regression results\nlm_pooled <- pool(lm_multiimp)\n\n# Summarize pooled results\nsummary(lm_pooled, conf.int = TRUE, conf.level = 0.90)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              term      estimate    std.error  statistic       df      p.value\n1      (Intercept) -30068.441810 5618.7067367 -5.3514880 107.7805 4.954407e-07\n2   countryBurundi     67.199848   61.9274336  1.0851386 107.8599 2.802797e-01\n3  countryCameroon    650.257144   58.5640793 11.1033444 107.1503 1.591166e-19\n4     countryCongo   1343.288063  113.7299489 11.8112078 106.8653 4.209726e-21\n5   countrySenegal    527.686999   78.7955032  6.6969177 106.7578 1.027035e-09\n6    countryZambia    415.136577   83.9727225  4.9437075 107.0560 2.853378e-06\n7             year     15.335776    2.8263293  5.4260402 107.7640 3.574950e-07\n8            trade      4.941659    1.5893889  3.1091568 105.7726 2.410907e-03\n9             infl     -4.347124    0.9876079 -4.4016696 108.0009 2.533732e-05\n10          civlib    -49.411852  132.1837617 -0.3738118 107.4442 7.092809e-01\n             5 %          95 %\n1  -39390.518971 -20746.364650\n2     -35.544192    169.943888\n3     553.087683    747.426605\n4    1154.583058   1531.993068\n5     396.945387    658.428610\n6     275.808050    554.465103\n7      10.646566     20.024986\n8       2.304247      7.579071\n9      -5.985649     -2.708598\n10   -268.725783    169.902078\n```\n:::\n:::\n\n\n### Question\n\n-   Based on the summary of the pooled regression results that you have just printed to the console, which of the following statements about the civil liberties in Africa is false?\n\n-   ***Correct, this one is false! Since the lower and upper bounds have different signs, we cannot be sure of the direction of the effect. Congratulations, you have come a long way and learned a lot. Well done! Let's sum it all up in the final video of the course.***\n",
    "supporting": [
      "imputations_in_R_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}