{
  "hash": "811b2c3d930cc2ef462785930fdbb325",
  "result": {
    "markdown": "---\ntitle: \"Modeling with tidymodels in R\"\noutput: \n    github_document:\n        df_print: kable\n---\n\n\n\n\n## Creating training and test datasets\n\nThe rsample package is designed to create training and test datasets. Creating a test dataset is important for estimating how a trained model will likely perform on new data. It also guards against overfitting, where a model memorizes patterns that exist only in the training data and performs poorly on new data.\n\nIn this exercise, you will create training and test datasets from the home_sales data. This data contains information on homes sold in the Seattle, Washington area between 2015 and 2016.\n\nThe outcome variable in this data is selling_price.\n\nThe tidymodels package will be pre-loaded in every exercise in the course. The home_sales tibble has also been loaded for you.\n\n![Tidy model packages](tidymodels_paks.png)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhome_sales <- readRDS(\"home_sales.rds\")\n\n# Create a data split object\nhome_split <- initial_split(home_sales, \n                            prop = 0.7, \n                            strata = selling_price)\n\n# Create the training data\nhome_training <- home_split %>%\n  training()\n\n# Create the test data\nhome_test <- home_split %>% \n  testing()\n\n# Check number of rows in each dataset\nnrow(home_training)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1042\n```\n:::\n\n```{.r .cell-code}\nnrow(home_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 450\n```\n:::\n:::\n\n\nDistribution of outcome variable values Stratifying by the outcome variable when generating training and test datasets ensures that the outcome variable values have a similar range in both datasets.\n\nSince the original data is split at random, stratification avoids placing all the expensive homes in home_sales into the test dataset, for example. In this case, your model would most likely perform poorly because it was trained on less expensive homes.\n\nIn this exercise, you will calculate summary statistics for the selling_price variable in the training and test datasets. The home_training and home_test tibbles have been loaded from the previous exercise.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Distribution of selling_price in training data\nlibrary(knitr)\nsummary_func <- function(df){\n      df %>% summarize(min_sell_price = min(selling_price),\n            max_sell_price = max(selling_price),\n            mean_sell_price = mean(selling_price),\n            sd_sell_price = sd(selling_price)) %>%\n    kable()\n\n}\nhome_training %>% \n    summary_func()\n```\n\n::: {.cell-output-display}\n| min_sell_price| max_sell_price| mean_sell_price| sd_sell_price|\n|--------------:|--------------:|---------------:|-------------:|\n|         350000|         650000|        479256.2|       81483.7|\n:::\n\n```{.r .cell-code}\nhome_test %>% \n  summary_func()\n```\n\n::: {.cell-output-display}\n| min_sell_price| max_sell_price| mean_sell_price| sd_sell_price|\n|--------------:|--------------:|---------------:|-------------:|\n|         350000|         650000|          478687|      79886.39|\n:::\n:::\n\n\n## Fitting a linear regression model\n\nThe parsnip package provides a unified syntax for the model fitting process in R.\n\nWith parsnip, it is easy to define models using the various packages, or engines, that exist in the R ecosystem.\n\nIn this exercise, you will define a parsnip linear regression object and train your model to predict selling_price using home_age and sqft_living as predictor variables from the home_sales data.\n\nThe home_training and home_test tibbles that you created in the previous lesson have been loaded into this session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specify a linear regression model, linear_model\nlinear_model <- linear_reg() %>% \n  # Set the model engine\n  set_engine('lm') %>% \n  # Set the model mode\n  set_mode('regression')\n\n# Train the model with the training data\nlm_fit <- linear_model %>% \n  fit(selling_price~ home_age + sqft_living,\n      data = home_training)\n\n# Print lm_fit to view model information\ntidy(lm_fit) %>%\n    kable()\n```\n\n::: {.cell-output-display}\n|term        |    estimate|   std.error| statistic| p.value|\n|:-----------|-----------:|-----------:|---------:|-------:|\n|(Intercept) | 290906.9182| 7426.744972| 39.170177|       0|\n|home_age    |  -1513.4108|  172.353326| -8.780862|       0|\n|sqft_living |    103.1971|    2.677773| 38.538389|       0|\n:::\n:::\n\n\n## Predicting home selling prices\n\nAfter fitting a model using the training data, the next step is to use it to make predictions on the test dataset. The test dataset acts as a new source of data for the model and will allow you to evaluate how well it performs.\n\nBefore you can evaluate model performance, you must add your predictions to the test dataset.\n\nIn this exercise, you will use your trained model, lm_fit, to predict selling_price in the home_test dataset.\n\nYour trained model, lm_fit, as well as the test dataset, home_test have been loaded into your session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict selling_price\nhome_predictions <- predict(lm_fit,\n                        new_data = home_test)\n\n# View predicted selling prices\n#home_predictions\n\n# Combine test data with predictions\nhome_test_results <- home_test %>% \n  select(selling_price, home_age, sqft_living) %>% \n  bind_cols(home_predictions)\n\n# View results\nhome_test_results %>% \n    head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  selling_price home_age sqft_living   .pred\n          <dbl>    <dbl>       <dbl>   <dbl>\n1        487000       10        2540 537893.\n2        355000       19        1430 409724.\n3        535000        3        2360 529912.\n4        525000       16        2100 483406.\n5        559900       20        2930 563006.\n6        552321       29        1960 449284.\n```\n:::\n:::\n\n\n## Model performance metrics\n\nEvaluating model results is an important step in the modeling process. Model evaluation should be done on the test dataset in order to see how well a model will generalize to new datasets.\n\nIn the previous exercise, you trained a linear regression model to predict selling_price using home_age and sqft_living as predictor variables. You then created the home_test_results tibble using your trained model on the home_test data.\n\nIn this exercise, you will calculate the RMSE and R squared metrics using your results in home_test_results.\n\nThe home_test_results tibble has been loaded into your session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Print home_test_results\n#home_test_results\n\n# Caculate the RMSE metric\nhome_test_results %>% \n  rmse(truth = selling_price, estimate =.pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      48337.\n```\n:::\n\n```{.r .cell-code}\n# Calculate the R squared metric\nhome_test_results %>% \n  rsq(truth = selling_price, estimate =.pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.634\n```\n:::\n:::\n\n\n## R squared plot\n\nIn the previous exercise, you got an R squared value of 0.651. The R squared metric ranges from 0 to 1, 0 being the worst and 1 the best.\n\nCalculating the R squared value is only the first step in studying your model's predictions.\n\nMaking an R squared plot is extremely important because it will uncover potential problems with your model, such as non-linear patterns or regions where your model is either over or under-predicting the outcome variable.\n\nIn this exercise, you will create an R squared plot of your model's performance.\n\nThe home_test_results tibble has been loaded into your session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create an R squared plot of model performance\nggplot(home_test_results, aes(x = selling_price, y = .pred)) +\n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred()  +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')\n```\n\n::: {.cell-output-display}\n![](tidymodels_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## Complete model fitting process with last_fit()\n\nIn this exercise, you will train and evaluate the performance of a linear regression model that predicts selling_price using all the predictors available in the home_sales tibble.\n\nThis exercise will give you a chance to perform the entire model fitting process with tidymodels, from defining your model object to evaluating its performance on the test data.\n\nEarlier in the chapter, you created an rsample object called home_split by passing the home_sales tibble into initial_split(). The home_split object contains the instructions for randomly splitting home_sales into training and test sets.\n\nThe home_sales tibble, and home_split object have been loaded into this session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define a linear regression model\nlinear_model <- linear_reg() %>% \n  set_engine('lm') %>% \n  set_mode('regression')\n\n# Train linear_model with last_fit()\nlinear_fit <- linear_model %>% \n  last_fit(selling_price ~ ., split = home_split)\n\n# Collect predictions and view results\npredictions_df <- linear_fit %>% collect_predictions()\npredictions_df %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 5\n  id                 .pred  .row selling_price .config             \n  <chr>              <dbl> <int>         <dbl> <chr>               \n1 train/test split 527769.     1        487000 Preprocessor1_Model1\n2 train/test split 400886.     7        355000 Preprocessor1_Model1\n3 train/test split 479682.    10        535000 Preprocessor1_Model1\n4 train/test split 501382.    13        525000 Preprocessor1_Model1\n5 train/test split 580302.    14        559900 Preprocessor1_Model1\n6 train/test split 468806.    15        552321 Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\n# Make an R squared plot using predictions_df\nggplot(predictions_df, aes(x = selling_price, y = .pred)) + \n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = 2) +\n  coord_obs_pred() +\n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')\n```\n\n::: {.cell-output-display}\n![](tidymodels_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Data resampling\n\nThe first step in a machine learning project is to create training and test datasets for model fitting and evaluation. The test dataset provides an estimate of how your model will perform on new data and helps to guard against overfitting.\n\nYou will be working with the telecom_df dataset which contains information on customers of a telecommunications company. The outcome variable is canceled_service and it records whether a customer canceled their contract with the company. The predictor variables contain information about customers' cell phone and internet usage as well as their contract type and monthly charges.\n\nThe telecom_df tibble has been loaded into your session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntelecom_df <- readRDS(\"telecom_df.rds\")\n# Create data split object\ntelecom_split <- initial_split(telecom_df, prop = 0.75,\n                     strata = canceled_service)\n\n# Create the training data\ntelecom_training <- telecom_split %>% \n  training()\n\n# Create the test data\ntelecom_test <- telecom_split %>% \n  testing()\n\n# Check the number of rows\nnrow(telecom_training)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 731\n```\n:::\n\n```{.r .cell-code}\nnrow(telecom_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 244\n```\n:::\n:::\n\n\n## Fitting a logistic regression model\n\nIn addition to regression models, the parsnip package also provides a general interface to classification models in R.\n\nIn this exercise, you will define a parsnip logistic regression object and train your model to predict canceled_service using avg_call_mins, avg_intl_mins, and monthly_charges as predictor variables from the telecom_df data.\n\nThe telecom_training and telecom_test tibbles that you created in the previous lesson have been loaded into this session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specify a logistic regression model\nlogistic_model <- logistic_reg() %>% \n  # Set the engine\n  set_engine('glm') %>% \n  # Set the mode\n  set_mode('classification')\n\n# Fit to training data\nlogistic_fit <- logistic_model %>% \n  fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n      data = telecom_training)\n\n# Print model fit object\nlogistic_fit %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 5\n  term            estimate std.error statistic  p.value\n  <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)      1.72      0.582        2.96 3.08e- 3\n2 avg_call_mins   -0.0104    0.00127     -8.14 4.02e-16\n3 avg_intl_mins    0.0208    0.00311      6.70 2.12e-11\n4 monthly_charges  0.00565   0.00478      1.18 2.37e- 1\n```\n:::\n:::\n\n\n## Combining test dataset results\n\nEvaluating your model's performance on the test dataset gives insights into how well your model predicts on new data sources. These insights will help you communicate your model's value in solving problems or improving decision making.\n\nBefore you can calculate classification metrics such as sensitivity or specificity, you must create a results tibble with the required columns for yardstick metric functions.\n\nIn this exercise, you will use your trained model to predict the outcome variable in the telecom_test dataset and combine it with the true outcome values in the canceled_service column.\n\nYour trained model, logistic_fit, and test dataset, telecom_test, have been loaded from the previous exercise.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict outcome categories\nclass_preds <- predict(logistic_fit, new_data = telecom_test,\n                       type = 'class')\n\n# Obtain estimated probabilities for each outcome value\nprob_preds <- predict(logistic_fit, new_data = telecom_test, \n                      type = 'prob')\n\n# Combine test set results\ntelecom_results <- telecom_test %>% \n  select(canceled_service) %>% \n  bind_cols(class_preds, prob_preds)\n\n# View results tibble\ntelecom_results %>%\n    head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  canceled_service .pred_class .pred_yes .pred_no\n  <fct>            <fct>           <dbl>    <dbl>\n1 yes              no             0.353     0.647\n2 yes              no             0.155     0.845\n3 no               no             0.108     0.892\n4 yes              no             0.181     0.819\n5 no               no             0.281     0.719\n6 no               no             0.0182    0.982\n```\n:::\n:::\n\n\n## Evaluating performance with yardstick\n\nIn the previous exercise, you calculated classification metrics from a sample confusion matrix. The yardstick package was designed to automate this process.\n\nFor classification models, yardstick functions require a tibble of model results as the first argument. This should include the actual outcome values, predicted outcome values, and estimated probabilities for each value of the outcome variable.\n\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate performance metrics.\n\nThe telecom_results tibble has been loaded into your session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the confusion matrix\nconf_mat(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction yes  no\n       yes  30  21\n       no   52 141\n```\n:::\n\n```{.r .cell-code}\n# Calculate the accuracy\naccuracy(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.701\n```\n:::\n\n```{.r .cell-code}\n# Calculate the sensitivity\n\nsens(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.366\n```\n:::\n\n```{.r .cell-code}\n# Calculate the specificity\n\nspec(telecom_results, truth = canceled_service,\n    estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 spec    binary         0.870\n```\n:::\n:::\n\n\n## Creating custom metric sets\n\nThe yardstick package also provides the ability to create custom sets of model metrics. In cases where the cost of obtaining false negative errors is different from the cost of false positive errors, it may be important to examine a specific set of performance metrics.\n\nInstead of calculating accuracy, sensitivity, and specificity separately, you can create your own metric function that calculates all three at the same time.\n\nIn this exercise, you will use the results from your logistic regression model, telecom_results, to calculate a custom set of performance metrics. You will also use a confusion matrix to calculate all available binary classification metrics in tidymodelsall at once.\n\nThe telecom_results tibble has been loaded into your session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a custom metric function\ntelecom_metrics <- metric_set(accuracy, sens, spec)\n\n# Calculate metrics using model results tibble\ntelecom_metrics(telecom_results, \n                truth = canceled_service,\n                estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.701\n2 sens     binary         0.366\n3 spec     binary         0.870\n```\n:::\n\n```{.r .cell-code}\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class) %>% \n  # Pass to the summary() function\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary         0.701\n 2 kap                  binary         0.261\n 3 sens                 binary         0.366\n 4 spec                 binary         0.870\n 5 ppv                  binary         0.588\n 6 npv                  binary         0.731\n 7 mcc                  binary         0.274\n 8 j_index              binary         0.236\n 9 bal_accuracy         binary         0.618\n10 detection_prevalence binary         0.209\n11 precision            binary         0.588\n12 recall               binary         0.366\n13 f_meas               binary         0.451\n```\n:::\n:::\n\n\n## Plotting the confusion matrix\n\nCalculating performance metrics with the yardstick package provides insight into how well a classification model is performing on the test dataset. Most yardstick functions return a single number that summarizes classification performance.\n\nMany times, it is helpful to create visualizations of the confusion matrix to more easily communicate your results.\n\nIn this exercise, you will make a heat map and mosaic plot of the confusion matrix from your logistic regression model on the telecom_df dataset.\n\nYour model results tibble, telecom_results, has been loaded into your session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a confusion matrix\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a heat map\n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](tidymodels_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\nconf_mat(telecom_results,\n         truth = canceled_service,\n         estimate = .pred_class)  %>% \n  # Create a mosaic plot\n  autoplot(type = \"mosaic\")\n```\n\n::: {.cell-output-display}\n![](tidymodels_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n:::\n\n\n## ROC curves and area under the ROC curve\n\nROC curves are used to visualize the performance of a classification model across a range of probability thresholds. An ROC curve with the majority of points near the upper left corner of the plot indicates that a classification model is able to correctly predict both the positive and negative outcomes correctly across a wide range of probability thresholds.\n\nThe area under this curve provides a letter grade summary of model performance.\n\nIn this exercise, you will create an ROC curve from your logistic regression model results and calculate the area under the ROC curve with yardstick.\n\nYour model results tibble, telecom_results has been loaded into your session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate metrics across thresholds\nthreshold_df <- telecom_results %>% \n  roc_curve(truth = canceled_service, .pred_yes)\n\n# View results\nthreshold_df %>%\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  .threshold specificity sensitivity\n       <dbl>       <dbl>       <dbl>\n1  -Inf          0                 1\n2     0.0182     0                 1\n3     0.0550     0.00617           1\n4     0.0589     0.0123            1\n5     0.0604     0.0185            1\n6     0.0658     0.0247            1\n```\n:::\n\n```{.r .cell-code}\n# Plot ROC curve\nthreshold_df %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](tidymodels_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Calculate ROC AUC\nroc_auc(telecom_results, truth = canceled_service, .pred_yes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.755\n```\n:::\n:::\n\n\nStreamlining the modeling process The last_fit() function is designed to streamline the modeling workflow in tidymodels. Instead of training your model on the training data and building a results tibble using the test data, last_fit() accomplishes this with one function.\n\nIn this exercise, you will train the same logistic regression model as you fit in the previous exercises, except with the last_fit() function.\n\nYour data split object, telecom_split, and model specification, logistic_model, have been loaded into your session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train model with last_fit()\ntelecom_last_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins +avg_intl_mins+monthly_charges,\n           split = telecom_split)\n\n# View test set metrics\ntelecom_last_fit %>% \n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.701 Preprocessor1_Model1\n2 roc_auc  binary         0.755 Preprocessor1_Model1\n```\n:::\n:::\n\n\n## Collecting predictions and creating custom metrics\n\nUsing the last_fit() modeling workflow also saves time in collecting model predictions. Instead of manually creating a tibble of model results, there are helper functions that extract this information automatically.\n\nIn this exercise, you will use your trained model, telecom_last_fit, to create a tibble of model results on the test dataset as well as calculate custom performance metrics.\n\nYou trained model, telecom_last_fit, has been loaded into this session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Collect predictions\nlast_fit_results <- telecom_last_fit %>% \n  collect_predictions()\n\n# View results\nlast_fit_results %>%\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 7\n  id               .pred_yes .pred_no  .row .pred_class canceled_service .config\n  <chr>                <dbl>    <dbl> <int> <fct>       <fct>            <chr>  \n1 train/test split    0.353     0.647     2 no          yes              Prepro…\n2 train/test split    0.155     0.845     4 no          yes              Prepro…\n3 train/test split    0.108     0.892     8 no          no               Prepro…\n4 train/test split    0.181     0.819    14 no          yes              Prepro…\n5 train/test split    0.281     0.719    20 no          no               Prepro…\n6 train/test split    0.0182    0.982    22 no          no               Prepro…\n```\n:::\n\n```{.r .cell-code}\n# Custom metrics function\nlast_fit_metrics <- metric_set(accuracy, sens,\n                               spec, roc_auc)\n\n# Calculate metrics\nlast_fit_metrics(last_fit_results,\n                 truth = canceled_service,\n                 estimate = .pred_class,\n                 .pred_yes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.701\n2 sens     binary         0.366\n3 spec     binary         0.870\n4 roc_auc  binary         0.755\n```\n:::\n:::\n\n\n## Complete modeling workflow\n\nIn this exercise, you will use the last_fit() function to train a logistic regression model and evaluate its performance on the test data by assessing the ROC curve and the area under the ROC curve.\n\nSimilar to previous exercises, you will predict canceled_service in the telecom_df data, but with an additional predictor variable to see if you can improve model performance.\n\nThe telecom_df tibble, telecom_split, and logistic_model objects from the previous exercises have been loaded into your workspace. The telecom_split object contains the instructions for randomly splitting the telecom_df tibble into training and test sets. The logistic_model object is a parsnip specification of a logistic regression model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train a logistic regression model\nlogistic_fit <- logistic_model %>% \n  last_fit(canceled_service ~ avg_call_mins + avg_intl_mins + monthly_charges + months_with_company, \n           split = telecom_split)\n\n# Collect metrics\nlogistic_fit %>% \n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.795 Preprocessor1_Model1\n2 roc_auc  binary         0.853 Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\n# Collect model predictions\nlogistic_fit %>% \n  collect_predictions() %>% \n  # Plot ROC curve\n  roc_curve(truth = canceled_service, .pred_yes) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](tidymodels_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "tidymodels_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}