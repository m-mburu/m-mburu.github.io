{
  "hash": "e169008630924cff229cbbb8b31445b5",
  "result": {
    "markdown": "---\ntitle: \"Multiple Linear and Logistic Regression in R\"\nauthor: \"Mburu\"\ndate: \"4/13/2021\"\noutput:\n  html_document:\n    toc: true\n    toc_depth: 2\n    toc_float:\n      collapsed: false\n      smooth_scroll: true\n    theme: united\n    highlight: pygments\n---\n\n\n\n\n## Fitting a parallel slopes model\n\nWe use the lm() function to fit linear models to data. In this case, we want to understand how the price of MarioKart games sold at auction varies as a function of not only the number of wheels included in the package, but also whether the item is new or used. Obviously, it is expected that you might have to pay a premium to buy these new. But how much is that premium? Can we estimate its value after controlling for the number of wheels?\n\nWe will fit a parallel slopes model using lm(). In addition to the data argument, lm() needs to know which variables you want to include in your regression model, and how you want to include them. It accomplishes this using a formula argument. A simple linear regression formula looks like y \\~ x, where y is the name of the response variable, and x is the name of the explanatory variable. Here, we will simply extend this formula to include multiple explanatory variables. A parallel slopes model has the form y \\~ x + z, where z is a categorical explanatory variable, and x is a numerical explanatory variable.\n\nThe output from lm() is a model object, which when printed, will show the fitted coefficients.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(openintro)\nlibrary(broom)\nlibrary(pander)\ndata( mariokart, package = \"openintro\")\nmario_kart <- mariokart\n\nmario_kart <- mario_kart %>% mutate(total_pr := ifelse(total_pr > 100, NA, total_pr))\n# Explore the data\nglimpse(mario_kart)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 143\nColumns: 12\n$ id          <dbl> 150377422259, 260483376854, 320432342985, 280405224677, 17…\n$ duration    <int> 3, 7, 3, 3, 1, 3, 1, 1, 3, 7, 1, 1, 1, 1, 7, 7, 3, 3, 1, 7…\n$ n_bids      <int> 20, 13, 16, 18, 20, 19, 13, 15, 29, 8, 15, 15, 13, 16, 6, …\n$ cond        <fct> new, used, new, new, new, new, used, new, used, used, new,…\n$ start_pr    <dbl> 0.99, 0.99, 0.99, 0.99, 0.01, 0.99, 0.01, 1.00, 0.99, 19.9…\n$ ship_pr     <dbl> 4.00, 3.99, 3.50, 0.00, 0.00, 4.00, 0.00, 2.99, 4.00, 4.00…\n$ total_pr    <dbl> 51.55, 37.04, 45.50, 44.00, 71.00, 45.00, 37.02, 53.99, 47…\n$ ship_sp     <fct> standard, firstClass, firstClass, standard, media, standar…\n$ seller_rate <int> 1580, 365, 998, 7, 820, 270144, 7284, 4858, 27, 201, 4858,…\n$ stock_photo <fct> yes, yes, no, yes, yes, yes, yes, yes, yes, no, yes, yes, …\n$ wheels      <int> 1, 1, 1, 1, 2, 0, 0, 2, 1, 1, 2, 2, 2, 2, 1, 0, 1, 1, 2, 2…\n$ title       <fct> \"~~ Wii MARIO KART &amp; WHEEL ~ NINTENDO Wii ~ BRAND NEW …\n```\n:::\n\n```{.r .cell-code}\n# fit parallel slopes\n\nmod_mario <- lm(total_pr ~ wheels + cond, data = mario_kart)\n```\n:::\n\n\n## Reasoning about two intercepts\n\nThe mario_kart data contains several other variables. The totalPr, startPr, and shipPr variables are numeric, while the cond and stockPhoto variables are categorical.\n\nWhich formula will result in a parallel slopes model?\n\n-   totalPr \\~ shipPr + stockPhoto\n\n## Using geom_line() and augment()\n\nParallel slopes models are so-named because we can visualize these models in the data space as not one line, but two parallel lines. To do this, we'll draw two things:\n\na scatterplot showing the data, with color separating the points into groups a line for each value of the categorical variable Our plotting strategy is to compute the fitted values, plot these, and connect the points to form a line. The augment() function from the broom package provides an easy way to add the fitted values to our data frame, and the geom_line() function can then use that data frame to plot the points and connect them.\n\nNote that this approach has the added benefit of automatically coloring the lines appropriately to match the data.\n\nYou already know how to use ggplot() and geom_point() to make the scatterplot. The only twist is that now you'll pass your augment()-ed model as the data argument in your ggplot() call. When you add your geom_line(), instead of letting the y aesthetic inherit its values from the ggplot() call, you can set it to the .fitted column of the augment()-ed model. This has the advantage of automatically coloring the lines for you.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Augment the model\naugmented_mod <- augment(mod_mario)\nglimpse(augmented_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 141\nColumns: 10\n$ .rownames  <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"1…\n$ total_pr   <dbl> 51.55, 37.04, 45.50, 44.00, 71.00, 45.00, 37.02, 53.99, 47.…\n$ wheels     <int> 1, 1, 1, 1, 2, 0, 0, 2, 1, 1, 2, 2, 2, 2, 1, 0, 1, 1, 2, 0,…\n$ cond       <fct> new, used, new, new, new, new, used, new, used, used, new, …\n$ .fitted    <dbl> 49.60260, 44.01777, 49.60260, 49.60260, 56.83544, 42.36976,…\n$ .resid     <dbl> 1.9473995, -6.9777674, -4.1026005, -5.6026005, 14.1645592, …\n$ .hat       <dbl> 0.02103158, 0.01250410, 0.02103158, 0.02103158, 0.01915635,…\n$ .sigma     <dbl> 4.902339, 4.868399, 4.892414, 4.881308, 4.750591, 4.899816,…\n$ .cooksd    <dbl> 1.161354e-03, 8.712334e-03, 5.154337e-03, 9.612441e-03, 5.5…\n$ .std.resid <dbl> 0.40270893, -1.43671086, -0.84838977, -1.15857953, 2.926332…\n```\n:::\n\n```{.r .cell-code}\n# scatterplot, with color\ndata_space <- ggplot(augmented_mod, aes(x = wheels, y = total_pr , color = cond )) + \n  geom_point()\n  \n# single call to geom_line()\ndata_space + \n  geom_line(aes(y = .fitted))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Intercept interpretation\n\nRecall that the cond variable is either new or used. Here are the fitted coefficients from your model:\n\nCall: lm(formula = totalPr \\~ wheels + cond, data = mario_kart)\n\nCoefficients: (Intercept) wheels condused\\\n42.370 7.233 -5.585\\\nChoose the correct interpretation of the coefficient on condused:\n\n-   The expected price of a used MarioKart is \\$5.58 less than that of a new one with the same number of wheels.\n\n-   For each additional wheel, the expected price of a MarioKart increases by \\$7.23 regardless of whether it is new or used.\n\nSyntax from math The babies data set contains observations about the birthweight and other characteristics of children born in the San Francisco Bay area from 1960--1967.\n\nWe would like to build a model for birthweight as a function of the mother's age and whether this child was her first (parity == 0). Use the mathematical specification below to code the model in R.\n\n$$birthweight = \\beta_0 + \\beta_1 * age  + \\beta_2 * parity + \\epsilon$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata( babies, package = \"openintro\")\n\nmod <- lm(bwt~ age+parity, data = babies)\n\ntidy(mod) %>% pander()\n```\n\n::: {.cell-output-display}\n-------------------------------------------------------------\n    term       estimate   std.error   statistic    p.value   \n------------- ---------- ----------- ----------- ------------\n (Intercept)    118.3       2.788       42.43     3.957e-243 \n\n     age       0.06315     0.09577     0.6594       0.5097   \n\n   parity       -1.652      1.271       -1.3        0.1937   \n-------------------------------------------------------------\n:::\n:::\n\n\n## Syntax from plot\n\nThis time, we'd like to build a model for birthweight as a function of the length of gestation and the mother's smoking status. Use the plot to inform your model specification.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(babies, aes(gestation, bwt, color = factor(smoke)))+\n    geom_point()\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmod <- lm(bwt~ gestation + smoke, data = babies)\n\ntidy(mod) %>% pander()\n```\n\n::: {.cell-output-display}\n------------------------------------------------------------\n    term       estimate   std.error   statistic    p.value  \n------------- ---------- ----------- ----------- -----------\n (Intercept)   -0.9317      8.152      -0.1143      0.909   \n\n  gestation     0.4429     0.02902      15.26     3.156e-48 \n\n    smoke       -8.088     0.9527       -8.49     5.963e-17 \n------------------------------------------------------------\n:::\n:::\n\n\nR-squared vs. adjusted R-squared Two common measures of how well a model fits to data are $$R^2$$ (the coefficient of determination) and the adjusted $$R^2$$ . The former measures the percentage of the variability in the response variable that is explained by the model. To compute this, we define\n\n$$R^2 = 1 - \\frac{sse}{sst} $$ where SSE and SST are the sum of the squared residuals, and the total sum of the squares, respectively. One issue with this measure is that the can only decrease as new variable are added to the model, while the SST depends only on the response variable and therefore is not affected by changes to the model. This means that you can increase $$R^2$$ by adding any additional variable to your model---even random noise.\n\nThe adjusted $$R^2$$ includes a term that penalizes a model for each additional explanatory variable (where is the number of explanatory variables). We can see both measures in the output of the summary() function on our model object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# R^2 and adjusted R^2\nsummary(mod_mario)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = total_pr ~ wheels + cond, data = mario_kart)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.0078  -3.0754  -0.8254   2.9822  14.1646 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  42.3698     1.0651  39.780  < 2e-16 ***\nwheels        7.2328     0.5419  13.347  < 2e-16 ***\ncondused     -5.5848     0.9245  -6.041 1.35e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.887 on 138 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7165,\tAdjusted R-squared:  0.7124 \nF-statistic: 174.4 on 2 and 138 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\n# add random noise\nmario_kart_noisy <- mario_kart %>% \nmutate(noise = rnorm(n = nrow(mario_kart)))\n  \n# compute new model\nmod2_mario2 <- lm(total_pr ~ wheels + cond+noise, data = mario_kart_noisy)\n\n# new R^2 and adjusted R^2\nsummary(mod2_mario2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = total_pr ~ wheels + cond + noise, data = mario_kart_noisy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.7376  -3.0423  -0.8488   2.6825  14.3907 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  42.3864     1.0681  39.682  < 2e-16 ***\nwheels        7.2062     0.5453  13.216  < 2e-16 ***\ncondused     -5.5855     0.9268  -6.027 1.46e-08 ***\nnoise        -0.2190     0.3863  -0.567    0.572    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.899 on 137 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7172,\tAdjusted R-squared:  0.711 \nF-statistic: 115.8 on 3 and 137 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Prediction\n\nOnce we have fit a regression model, we can use it to make predictions for unseen observations or retrieve the fitted values. Here, we explore two methods for doing the latter.\n\nA traditional way to return the fitted values (i.e. the y 's) is to run the predict() function on the model object. This will return a vector of the fitted values. Note that predict() will take an optional newdata argument that will allow you to make predictions for observations that are not in the original data.\n\nA newer alternative is the augment() function from the broom package, which returns a data.frame with the response varible (), the relevant explanatory variables (the 's), the fitted value ( ) and some information about the residuals (). augment() will also take a newdata argument that allows you to make predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# return a vector\nlibrary(knitr)\n\npredict(mod_mario)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1        2        3        4        5        6        7        8 \n49.60260 44.01777 49.60260 49.60260 56.83544 42.36976 36.78493 56.83544 \n       9       10       11       12       13       14       15       16 \n44.01777 44.01777 56.83544 56.83544 56.83544 56.83544 44.01777 36.78493 \n      17       18       19       21       22       23       24       25 \n49.60260 49.60260 56.83544 36.78493 56.83544 56.83544 56.83544 44.01777 \n      26       27       28       29       30       31       32       33 \n56.83544 36.78493 36.78493 36.78493 49.60260 36.78493 36.78493 44.01777 \n      34       35       36       37       38       39       40       41 \n51.25061 44.01777 44.01777 36.78493 44.01777 56.83544 56.83544 49.60260 \n      42       43       44       45       46       47       48       49 \n44.01777 51.25061 56.83544 56.83544 44.01777 56.83544 36.78493 36.78493 \n      50       51       52       53       54       55       56       57 \n44.01777 56.83544 36.78493 44.01777 42.36976 36.78493 36.78493 44.01777 \n      58       59       60       61       62       63       64       66 \n44.01777 36.78493 36.78493 56.83544 36.78493 56.83544 36.78493 51.25061 \n      67       68       69       70       71       72       73       74 \n56.83544 44.01777 58.48345 51.25061 49.60260 44.01777 49.60260 56.83544 \n      75       76       77       78       79       80       81       82 \n56.83544 51.25061 44.01777 36.78493 36.78493 36.78493 44.01777 56.83544 \n      83       84       85       86       87       88       89       90 \n44.01777 65.71629 44.01777 56.83544 36.78493 49.60260 49.60260 36.78493 \n      91       92       93       94       95       96       97       98 \n44.01777 36.78493 51.25061 44.01777 36.78493 51.25061 42.36976 56.83544 \n      99      100      101      102      103      104      105      106 \n51.25061 44.01777 51.25061 56.83544 56.83544 56.83544 36.78493 49.60260 \n     107      108      109      110      111      112      113      114 \n51.25061 44.01777 56.83544 49.60260 36.78493 44.01777 51.25061 56.83544 \n     115      116      117      118      119      120      121      122 \n64.06828 44.01777 49.60260 44.01777 49.60260 51.25061 42.36976 44.01777 \n     123      124      125      126      127      128      129      130 \n56.83544 44.01777 49.60260 44.01777 51.25061 56.83544 56.83544 49.60260 \n     131      132      133      134      135      136      137      138 \n56.83544 36.78493 44.01777 44.01777 36.78493 56.83544 36.78493 44.01777 \n     139      140      141      142      143 \n36.78493 51.25061 49.60260 36.78493 56.83544 \n```\n:::\n\n```{.r .cell-code}\n# return a data frame\n\naugment(mod_mario)%>% head() %>% kable()\n```\n\n::: {.cell-output-display}\n|.rownames | total_pr| wheels|cond |  .fitted|    .resid|      .hat|   .sigma|   .cooksd| .std.resid|\n|:---------|--------:|------:|:----|--------:|---------:|---------:|--------:|---------:|----------:|\n|1         |    51.55|      1|new  | 49.60260|  1.947399| 0.0210316| 4.902340| 0.0011614|  0.4027089|\n|2         |    37.04|      1|used | 44.01777| -6.977767| 0.0125041| 4.868399| 0.0087123| -1.4367109|\n|3         |    45.50|      1|new  | 49.60260| -4.102601| 0.0210316| 4.892414| 0.0051543| -0.8483898|\n|4         |    44.00|      1|new  | 49.60260| -5.602601| 0.0210316| 4.881308| 0.0096124| -1.1585795|\n|5         |    71.00|      2|new  | 56.83544| 14.164559| 0.0191563| 4.750591| 0.0557493|  2.9263328|\n|6         |    45.00|      0|new  | 42.36976|  2.630240| 0.0474932| 4.899816| 0.0050537|  0.5514192|\n:::\n:::\n\n\n## Thought experiments\n\nSuppose that after going apple picking you have 12 apples left over. You decide to conduct an experiment to investigate how quickly they will rot under certain conditions. You place six apples in a cool spot in your basement, and leave the other six on the window sill in the kitchen. Every week, you estimate the percentage of the surface area of the apple that is rotten or moldy.\n\nConsider the following models:\n\n$$rot = \\beta_0 + \\beta_1 *t + \\beta_2 * temp + \\epsilon $$\n\nand\n\n$$rot = \\beta_0 + \\beta_1 *t + \\beta_2 * temp + \\beta_2 * temp *t +  \\epsilon $$\n\n-   The rate at which apples rot will vary based on the temperature.\n\n## Fitting a model with interaction\n\nIncluding an interaction term in a model is easy---we just have to tell lm() that we want to include that new variable. An expression of the form\n\n    lm(y ~ x + z + x:z, data = mydata)\n\nwill do the trick. The use of the colon (:) here means that the interaction between and will be a third term in the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# include interaction\n\nmod <- lm(total_pr ~cond + duration + cond:duration, data = mario_kart)\n\ntidy(mod) %>% pander()\n```\n\n::: {.cell-output-display}\n------------------------------------------------------------------\n       term          estimate   std.error   statistic    p.value  \n------------------- ---------- ----------- ----------- -----------\n    (Intercept)       58.27       1.366       42.64     5.832e-81 \n\n     condused         -17.12      2.178       -7.86     1.014e-12 \n\n     duration         -1.966     0.4488       -4.38     2.342e-05 \n\n condused:duration    2.325      0.5484       4.239     4.102e-05 \n------------------------------------------------------------------\n:::\n:::\n\n\n## Visualizing interaction models\n\nInteraction allows the slope of the regression line in each group to vary. In this case, this means that the relationship between the final price and the length of the auction is moderated by the condition of each item.\n\nInteraction models are easy to visualize in the data space with ggplot2 because they have the same coefficients as if the models were fit independently to each group defined by the level of the categorical variable. In this case, new and used MarioKarts each get their own regression line. To see this, we can set an aesthetic (e.g. color) to the categorical variable, and then add a geom_smooth() layer to overlay the regression line for each color.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# interaction plot\nggplot(mario_kart, aes(duration, total_pr, color = cond)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = 0)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## Consequences of Simpson's paradox\n\nIn the simple linear regression model for average SAT score, (total) as a function of average teacher salary (salary), the fitted coefficient was -5.02 points per thousand dollars. This suggests that for every additional thousand dollars of salary for teachers in a particular state, the expected SAT score for a student from that state is about 5 points lower.\n\nIn the model that includes the percentage of students taking the SAT, the coefficient on salary becomes 1.84 points per thousand dollars. Choose the correct interpretation of this slope coefficient.\n\n-   For every additional thousand dollars of salary for teachers in a particular state, the expected SAT score for a student from that state is about 2 points higher, after controlling for the percentage of students taking the SAT.\n\n## Simpson's paradox in action\n\nA mild version of Simpson's paradox can be observed in the MarioKart auction data. Consider the relationship between the final auction price and the length of the auction. It seems reasonable to assume that longer auctions would result in higher prices, since---other things being equal---a longer auction gives more bidders more time to see the auction and bid on the item.\n\nHowever, a simple linear regression model reveals the opposite: longer auctions are associated with lower final prices. The problem is that all other things are not equal. In this case, the new MarioKarts---which people pay a premium for---were mostly sold in one-day auctions, while a plurality of the used MarioKarts were sold in the standard seven-day auctions.\n\nOur simple linear regression model is misleading, in that it suggests a negative relationship between final auction price and duration. However, for the used MarioKarts, the relationship is positive.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslr <- ggplot(mario_kart, aes(y = total_pr, x = duration)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n# model with one slope\nmod <- lm(total_pr ~ duration, data = mario_kart)\n\n# plot with two slopes\nslr + aes(color = cond)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## Fitting a MLR model\n\nIn terms of the R code, fitting a multiple linear regression model is easy: simply add variables to the model formula you specify in the lm() command.\n\nIn a parallel slopes model, we had two explanatory variables: one was numeric and one was categorical. Here, we will allow both explanatory variables to be numeric.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the model using duration and startPr\n\nmod <- lm(total_pr~ start_pr + duration, data = mario_kart)\n\ntidy(mod) %>% pander()\n```\n\n::: {.cell-output-display}\n------------------------------------------------------------\n    term       estimate   std.error   statistic    p.value  \n------------- ---------- ----------- ----------- -----------\n (Intercept)    51.03       1.179       43.28     3.666e-82 \n\n  start_pr      0.233      0.04364      5.339     3.756e-07 \n\n  duration      -1.508     0.2555      -5.902     2.645e-08 \n------------------------------------------------------------\n:::\n:::\n\n\n## Tiling the plane\n\nOne method for visualizing a multiple linear regression model is to create a heatmap of the fitted values in the plane defined by the two explanatory variables. This heatmap will illustrate how the model output changes over different combinations of the explanatory variables.\n\nThis is a multistep process:\n\nFirst, create a grid of the possible pairs of values of the explanatory variables. The grid should be over the actual range of the data present in each variable. We've done this for you and stored the result as a data frame called grid. Use augment() with the newdata argument to find the 's corresponding to the values in grid. Add these to the data_space plot by using the fill aesthetic and geom_tile().\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add predictions to grid\nprice_hats <- augment(mod, newdata = grid)\n\n# tile the plane\ndata_space + \n  geom_tile(data = price_hats, aes(fill = .fitted), alpha = 0.5)\n```\n:::\n\n\n## Models in 3D\n\nAn alternative way to visualize a multiple regression model with two numeric explanatory variables is as a plane in three dimensions. This is possible in R using the plotly package.\n\nWe have created three objects that you will need:\n\nx: a vector of unique values of duration y: a vector of unique values of startPr plane: a matrix of the fitted values across all combinations of x and y Much like ggplot(), the plot_ly() function will allow you to create a plot object with variables mapped to x, y, and z aesthetics. The add_markers() function is similar to geom_point() in that it allows you to add points to your 3D plot.\n\nNote that plot_ly uses the pipe (%\\>%) operator to chain commands together.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# draw the 3D scatterplot\np <- plot_ly(data = mario_kart, z = ~totalPr, x = ~duration, y = ~startPr, opacity = 0.6) %>%\n  add_markers() \n  \n# draw the plane\np %>%\n  add_surface(x = ~x, y = ~y, z = ~plane, showscale = FALSE)\n```\n:::\n",
    "supporting": [
      "regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}