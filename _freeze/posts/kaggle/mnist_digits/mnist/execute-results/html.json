{
  "hash": "9838117117d08d8c111462eb364d0479",
  "result": {
    "markdown": "---\ntitle: \"MNIST Digits\"\nauthor: \"Mburu\"\ndate: \"7/10/2020\"\noutput:\n  html_document:\n    toc: true\n    toc_depth: 2\n    toc_float:\n      collapsed: false\n      smooth_scroll: false\n    theme: united\n    highlight: pygments\n---\n\n\n\n\n\n# MNIST Digits\n\n## Read data and Load libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(keras)\nlibrary(caret)\nlibrary(DT)\nlibrary(caretEnsemble)\nlibrary(tictoc)\n\ntrain_data <- fread(\"data/train.csv\")\n\ntest_data <- fread(\"data/test.csv\")\n```\n:::\n\n\n## Frequency of digits\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(train_data, aes(x = factor(label))) +\n    geom_bar()\n```\n:::\n\n\n\n## Randomly sample 12 digits\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#  image coordinates\nxy_axis <- data.frame(x = expand.grid(1:28, 28:1)[,1],\n                      y = expand.grid(1:28, 28:1)[,2])\n\n\n# get 12 images\nset.seed(100)\nsample_10 <- train_data[sample(1:.N, 12), -1] %>% as.matrix()\n\ndatatable(sample_10, \n          options = list(scrollX = TRUE))\n\nsample_10 <- t(sample_10)\n\nplot_data <- cbind(xy_axis, sample_10 )\n\nsetDT(plot_data, keep.rownames = \"pixel\")\n\n# Observe the first records\nhead(plot_data) %>% datatable(options = list(scrollX = TRUE))\n```\n:::\n\n\n## Plot the 12 digits\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_data_m <- melt(plot_data, id.vars = c(\"pixel\", \"x\", \"y\"))\n\n# Plot the image using ggplot()\nggplot(plot_data_m, aes(x, y, fill = value)) +\n    geom_raster()+\n     facet_wrap(~variable)+\n    scale_fill_gradient(low = \"white\",\n                        high = \"black\", guide = FALSE)+\n    theme(axis.line = element_blank(),\n                  axis.text = element_blank(),\n                  axis.ticks = element_blank(),\n                  axis.title = element_blank(),\n                  panel.background = element_blank(),\n                  panel.border = element_blank(),\n                  panel.grid.major = element_blank(),\n                  panel.grid.minor = element_blank(),\n                  plot.background = element_blank())\n```\n:::\n\n\n## Prepare data for model fitting\n\n- Decided to have a self test set\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnmst <- names(train_data)\nnmst <- nmst[nmst != \"label\"]\nminmax <- function(x) {\n  top =  (x - min(x))\n  bottom = (max(x) - min(x))\n  if(bottom == 0){ \n    return(0)\n  }else{\n      return(top/bottom)\n    }\n}\n#train_data[, (nmst) := lapply(.SD,  function(x) x/255), .SDcols = nmst]\ntrain_data[, (nmst) := lapply(.SD,  minmax), .SDcols = nmst]\nset.seed(100)\nN1 = nrow(train_data)\nsample_one <- sample(N1, 5000)\ntrain_data <- train_data[sample_one]\nN = nrow(train_data)\nsample_train <- sample(N, size = round(0.75 *N ))\ntest_own <- train_data[-sample_train]\ntrain_data2 <- train_data[sample_train, ]\ntrain_y <-to_categorical(train_data2$label, 10)\n\ntrain_x <- train_data2[, -1]\n#convert to matrix\ntrain_x <- train_x %>%\n    as.matrix()\n\n#train_x <- train_x/255\n```\n:::\n\n\n\n## Construct model layers\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() \nmodel %>% \n    layer_dense(units = 784, activation = 'relu', input_shape = 784) %>%\n    layer_dropout(rate = 0.3) %>%\n    layer_dense(units = 784, activation = 'relu') %>%\n    layer_dropout(rate = 0.3) %>%\n    layer_dense(units = 392, activation = 'relu') %>%\n    layer_dropout(rate = 0.2) %>%\n    layer_dense(units = 200, activation = 'tanh') %>%\n    #layer_dropout(rate = 0.) %>%\n    layer_dense(units = 10, activation = 'softmax')\n```\n:::\n\n\n\n## Compile model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% compile(\n    loss = 'categorical_crossentropy',\n    optimizer = 'adam',\n    metrics = c('accuracy'))\n```\n:::\n\n\n## Fit model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist <- model %>% fit(train_x, train_y, \n                      epochs = 9, batch_size = 1000,\n                      validation_split = .2)\n\n\nplot(hist,type = \"b\")\n```\n:::\n\n\n## Own test\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_own_x <- test_own[, -1] %>% as.matrix()\n\ntest_own_pred <- model %>% predict_classes(test_own_x) %>% factor()\n\nconfusionMatrix(data = test_own_pred, reference = factor(test_own$label))\n```\n:::\n\n\n### Save predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_x <- as.matrix(test_data)/255\n\ntest_pred <- model %>% predict_classes(test_x)\n#head(test_pred)\n\ndf_pred1 <- data.frame(ImageId = 1:length(test_pred),\n                       Label = test_pred)\n\nwrite.csv(df_pred1, file = \"sample_submission.csv\", row.names = F)\n```\n:::\n\n\n\n# Compare between tsne amd glmr\n\n## TSNE\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Rtsne)\n\ntsne_output <- Rtsne(train_x, check_duplicates = FALSE)\n\n# Generate a data frame to plot the result\ntsne_train <- data.table(tsne_x = tsne_output$Y[,1],\n                        tsne_y = tsne_output$Y[,2],\n                        label =  train_data2$label)\n\n# Plot the embedding usign ggplot and the label\nggplot(tsne_train,\n       aes(x = tsne_x, y = tsne_y, color = factor(label))) + \n  ggtitle(\"t-SNE of MNIST data set\") + \n  geom_text(aes(label = label)) +\n    theme(legend.position = \"none\")\n```\n:::\n\n\n## Plot tsne group means\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntsne_mean <- tsne_train[, \n                        .(mean_x = mean(tsne_x), mean_y = mean(tsne_y)),\n                        by = label]\n\n\nggplot(tsne_mean,\n       aes(x = mean_x, y = mean_y, color = factor(label))) + \n  ggtitle(\"t-SNE of MNIST data set group means\") + \n  geom_text(aes(label = label)) +\n    theme(legend.position = \"none\")\n```\n:::\n\n## Kmeans to see if tsne and kmeans agree\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nk_means_mnist <- kmeans(train_x, 10)\n\ntsne_train[, cluster := k_means_mnist$cluster]\nggplot(tsne_train,\n       aes(x = tsne_x, y = tsne_y, color = factor(cluster))) + \n  geom_point()+\n  ggtitle(\"t-SNE of MNIST data set\") + \n    theme(legend.position = \"none\")\n\ntsne_train[, cluster := NULL]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(kernlab)\ncol_sum <- colSums(train_data2[, .SD, .SDcols = !\"label\"])\n\nzero_var_cols <- col_sum[col_sum == 0] %>% names()\ntrain_data2 <- train_data2[, .SD, .SDcols = !zero_var_cols]\ndf_nms <-  data.frame(vars = names(train_data2))\nwrite.csv(df_nms, file = \"df_nms.csv\", row.names = F)\nmnist_matrix <- train_data2[, .SD, .SDcols = !\"label\"] %>% na.omit %>% as.matrix()\n\n\nspec_models <- list()\ntot_withinss <- c()\nfor(i in 1:10){\n    \n    spec_fit <- specc(mnist_matrix, centers=i+1)\n    tot_withinss[i] <-withinss(spec_fit) %>% median()\n    spec_models[[i]] <- spec_fit\n}\n\nplot( tot_withinss)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nspec_fit_final <- spec_models[[4]]\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}