{
  "hash": "77c489de9a1486c4e3b5d1d57c5f28b6",
  "result": {
    "markdown": "---\ntitle: \"Introduction to advanced dimensionality reduction\"\nauthor: \"Mburu\"\ndate: \"4/8/2020\"\noutput:\n  html_document:\n    toc: true\n    toc_depth: 2\n    toc_float:\n      collapsed: false\n      smooth_scroll: false\n    theme: united\n    highlight: pygments\n---\n\n\n\n\n## Exploring MNIST dataset\n\nYou will use the MNIST dataset in several exercises through the course. Let's do some data exploration to gain a better understanding. Remember that the MNIST dataset contains a set of records that represent handwritten digits using 28x28 features, which are stored into a 784-dimensional vector.\n\nmnistInput ![degit 3.](MNIST_input_ex.png) Each record of the MNIST dataset corresponds to a handwritten digit and each feature represents one pixel of the digit image. In this exercise, a sample of 200 records of the MNIST dataset named mnist_sample is loaded for you.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"mnist-sample-200.RData\")\n# Have a look at the MNIST dataset names\n#names(mnist_sample)\n\n# Show the first records\n#str(mnist_sample)\n\n# Labels of the first 6 digits\nhead(mnist_sample$label)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5 0 7 0 9 3\n```\n:::\n:::\n\n\n## Digits features\n\nLet's continue exploring the dataset. Firstly, it would be helpful to know how many different digits are present by computing a histogram of the labels. Next, the basic statistics (min, mean, median, maximum) of the features for all digits can be calculated. Finally, you will compute the basic statistics for only those digits with label 0. The MNIST sample data is loaded for you as mnist_sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot the histogram of the digit labels\nhist(mnist_sample$label)\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Compute the basic statistics of all records\n#summary(mnist_sample)\n\n# Compute the basic statistics of digits with label 0\n#summary(mnist_sample[, mnist_sample$label == 0])\n```\n:::\n\n\n## Euclidean distance\n\nEuclidean distance is the basis of many measures of similarity and is the most important distance metric. You can compute the Euclidean distance in R using the dist() function. In this exercise, you will compute the Euclidean distance between the first 10 records of the MNIST sample data.\n\nThe mnist_sample object is loaded for you.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Show the labels of the first 10 records\nmnist_sample$label[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 5 0 7 0 9 3 4 1 2 6\n```\n:::\n\n```{.r .cell-code}\n# Compute the Euclidean distance of the first 10 records\ndistances <- dist(mnist_sample[1:10, -1])\n\n# Show the distances values\ndistances\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          1        2        3        4        5        6        7        8\n2  2185.551                                                               \n3  2656.407 2869.979                                                      \n4  2547.027 2341.249 2936.772                                             \n5  2406.509 2959.108 1976.406 2870.928                                    \n6  2343.982 2759.681 2452.568 2739.470 2125.723                           \n7  2464.388 2784.158 2573.667 2870.918 2174.322 2653.703                  \n8  2149.872 2668.903 1999.892 2585.980 2067.044 2273.248 2407.962         \n9  2959.129 3209.677 2935.262 3413.821 2870.746 3114.508 2980.663 2833.447\n10 2728.657 3009.771 2574.752 2832.521 2395.589 2655.864 2464.301 2550.126\n          9\n2          \n3          \n4          \n5          \n6          \n7          \n8          \n9          \n10 2695.166\n```\n:::\n\n```{.r .cell-code}\n# Plot the numeric matrix of the distances in a heatmap\nheatmap(as.matrix(distances), \n    \tRowv = NA, symm = TRUE, \n        labRow = mnist_sample$label[1:10], \n        labCol = mnist_sample$label[1:10])\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n## Minkowsky distance\n\nThere are other well-known distance metrics besides the Euclidean distance, like the Minkowski distance. This metric can be considered a generalisation of both the Euclidean and Manhattan distance. In R, you can calculate the Minkowsky distance of order p by using dist(..., method = \"minkowski\", p).\n\nThe MNIST sample data is loaded for you as mnist_sample\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Minkowski distance or order 3\ndistances_3 <- dist(mnist_sample[1:10, -1], method = \"minkowski\", p = 3)\n\ndistances_3 \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           1         2         3         4         5         6         7\n2  1002.6468                                                            \n3  1169.6470 1228.8295                                                  \n4  1127.4919 1044.9182 1249.6133                                        \n5  1091.3114 1260.3549  941.1654 1231.7432                              \n6  1063.7026 1194.1212 1104.2581 1189.9558  996.2687                    \n7  1098.4279 1198.8891 1131.4498 1227.7888 1005.7588 1165.4475          \n8  1006.9070 1169.4720  950.6812 1143.3503  980.6450 1056.1814 1083.2255\n9  1270.0240 1337.2068 1257.4052 1401.2461 1248.0777 1319.2768 1271.7095\n10 1186.9620 1268.1539 1134.0371 1219.1388 1084.5416 1166.9129 1096.3586\n           8         9\n2                     \n3                     \n4                     \n5                     \n6                     \n7                     \n8                     \n9  1236.9178          \n10 1133.2929 1180.7970\n```\n:::\n\n```{.r .cell-code}\nheatmap(as.matrix(distances_3 ), \n        Rowv = NA, symm = TRUE, \n        labRow = mnist_sample$label[1:10], \n        labCol = mnist_sample$label[1:10])\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Minkowski distance of order 2\ndistances_2 <- dist(mnist_sample[1:10, -1], method = \"minkowski\", p = 2)\ndistances_2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          1        2        3        4        5        6        7        8\n2  2185.551                                                               \n3  2656.407 2869.979                                                      \n4  2547.027 2341.249 2936.772                                             \n5  2406.509 2959.108 1976.406 2870.928                                    \n6  2343.982 2759.681 2452.568 2739.470 2125.723                           \n7  2464.388 2784.158 2573.667 2870.918 2174.322 2653.703                  \n8  2149.872 2668.903 1999.892 2585.980 2067.044 2273.248 2407.962         \n9  2959.129 3209.677 2935.262 3413.821 2870.746 3114.508 2980.663 2833.447\n10 2728.657 3009.771 2574.752 2832.521 2395.589 2655.864 2464.301 2550.126\n          9\n2          \n3          \n4          \n5          \n6          \n7          \n8          \n9          \n10 2695.166\n```\n:::\n\n```{.r .cell-code}\nheatmap(as.matrix(distances_2), \n        Rowv = NA, symm = TRUE, \n        labRow = mnist_sample$label[1:10], \n        labCol = mnist_sample$label[1:10])\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n:::\n\n\n-   Very Good! As you can see, when using Minkowski distance of order 2 the most similar digits are in positions 3 and 5 of the heatmap grid which corresponds to digits 7 and 9.\n\n## KL divergence\n\nThere are more distance metrics that can be used to compute how similar two feature vectors are. For instance, the philentropy package has the function distance(), which implements 46 different distance metrics. For more information, use ?distance in the console. In this exercise, you will compute the KL divergence and check if the results differ from the previous metrics. Since the KL divergence is a measure of the difference between probability distributions you need to rescale the input data by dividing each input feature by the total pixel intensities of that digit. The philentropy package and mnist_sample data have been loaded.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(philentropy)\nlibrary(tidyverse)\n# Get the first 10 records\nmnist_10 <- mnist_sample[1:10, -1]\n\n# Add 1 to avoid NaN when rescaling\nmnist_10_prep <- mnist_10 + 1 \n\n# Compute the sums per row\nsums <- rowSums(mnist_10_prep)\n\n# Compute KL divergence\ndistances <- distance(mnist_10_prep/sums, method = \"kullback-leibler\")\nheatmap(as.matrix(distances), \n        Rowv = NA, symm = TRUE, \n        labRow = mnist_sample$label[1:10], \n        labCol = mnist_sample$label[1:10])\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Generating PCA from MNIST sample\n\nYou are going to compute a PCA with the previous mnist_sample dataset. The goal is to have a good representation of each digit in a lower dimensional space. PCA will give you a set of variables, named principal components, that are a linear combination of the input variables. These principal components are ordered in terms of the variance they capture from the original data. So, if you plot the first two principal components you can see the digits in a 2-dimensional space. A sample of 200 records of the MNIST dataset named mnist_sample is loaded for you.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the principal components from PCA\npca_output <- prcomp(mnist_sample[, -1])\n\n# Observe a summary of the output\n#summary(pca_output)\n\n# Store the first two coordinates and the label in a data frame\npca_plot <- data.frame(pca_x = pca_output$x[, \"PC1\"], pca_y = pca_output$x[, \"PC2\"], \n                       label = as.factor(mnist_sample$label))\n\n# Plot the first two principal components using the true labels as color and shape\nggplot(pca_plot, aes(x = pca_x, y = pca_y, color = label)) + \n\tggtitle(\"PCA of MNIST sample\") + \n\tgeom_text(aes(label = label)) + \n\ttheme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## t-SNE output from MNIST sample\n\nYou have seen that PCA has some limitations in correctly classifying digits, mainly due to its linear nature. In this exercise, you are going to use the output from the t-SNE algorithm on the MNIST sample data, named tsne_output and visualize the obtained results. In the next chapter, you will focus on the t-SNE algorithm and learn more about how to use it! The MNIST sample dataset mnist_sample as well as the tsne_output are available in your workspace.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Explore the tsne_output structure\nlibrary(Rtsne)\nlibrary(tidyverse)\ntsne_output <- Rtsne(mnist_sample[, -1])\n#str(tsne_output)\n\n# Have a look at the first records from the t-SNE output\n#head(tsne_output)\n\n# Store the first two coordinates and the label in a data.frame\ntsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], \n                        label = as.factor(mnist_sample$label))\n\n# Plot the t-SNE embedding using the true labels as color and shape\nggplot(tsne_plot, aes(x =tsne_x, y = tsne_y, color = label)) + \n\tggtitle(\"T-Sne output\") + \n\tgeom_text(aes(label = label)) + \n\ttheme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Computing t-SNE\n\nAs we have seen, the t-SNE embedding can be computed in R using the Rtsne() function from the Rtsne package in CRAN. Performing a PCA is a common step before running the t-SNE algorithm, but we can skip this step by setting the parameter PCA to FALSE. The dimensionality of the embedding generated by t-SNE can be indicated with the dims parameter. In this exercise, we will generate a three-dimensional embedding from the mnist_sample dataset without doing the PCA step and then, we will plot the first two dimensions. The MNIST sample dataset mnist_sample, as well as the Rtsne and ggplot2 packages, are already loaded.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute t-SNE without doing the PCA step\ntsne_output <- Rtsne(mnist_sample[,-1], PCA = FALSE, dim = 3)\n\n# Show the obtained embedding coordinates\nhead(tsne_output$Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]      [,2]       [,3]\n[1,]  2.797005  5.142908 -6.6520644\n[2,]  4.556940 10.472095 -5.6356933\n[3,]  6.395298 -7.396396  0.2339104\n[4,] -9.464386  9.943015 -4.1290475\n[5,]  2.835100 -6.763885  0.8818647\n[6,]  9.257222 -2.907011 -6.0542834\n```\n:::\n\n```{.r .cell-code}\n# Store the first two coordinates and plot them \ntsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], \n                        digit = as.factor(mnist_sample$label))\n\n# Plot the coordinates\nggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = digit)) + \n\tggtitle(\"t-SNE of MNIST sample\") + \n\tgeom_text(aes(label = digit)) + \n\ttheme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## Understanding t-SNE output\n\nThe most important t-SNE output are those related to the K-L divergence of the points in the original high dimensions and in the new lower dimensional space. Remember that the goal of t-SNE is to minimize the K-L divergence between the original space and the new one. In the returned object, the itercosts structure indicates the total cost from the K-L divergence of all the objects in each 50th iteration and the cost structure indicates the K-L divergence of each record in the final iteration. The Rtsne package and the tsne_output object have been loaded for you.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Inspect the output object's structure\nstr(tsne_output)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 14\n $ N                  : int 200\n $ Y                  : num [1:200, 1:3] 2.8 4.56 6.4 -9.46 2.84 ...\n $ costs              : num [1:200] 0.00342 0.00297 0.00055 0.00277 0.00223 ...\n $ itercosts          : num [1:20] 53.3 52.3 52.9 51.7 53 ...\n $ origD              : int 50\n $ perplexity         : num 30\n $ theta              : num 0.5\n $ max_iter           : num 1000\n $ stop_lying_iter    : int 250\n $ mom_switch_iter    : int 250\n $ momentum           : num 0.5\n $ final_momentum     : num 0.8\n $ eta                : num 200\n $ exaggeration_factor: num 12\n - attr(*, \"class\")= chr [1:2] \"Rtsne\" \"list\"\n```\n:::\n\n```{.r .cell-code}\n# Show total costs after each 50th iteration\ntsne_output$itercosts\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 53.2567159 52.2915239 52.8661957 51.6591221 52.9516520  0.9307949\n [7]  0.6470520  0.5626513  0.5156980  0.5108396  0.5055191  0.4974795\n[13]  0.4942471  0.4922827  0.4931260  0.4904664  0.4914848  0.4898935\n[19]  0.4913327  0.4908019\n```\n:::\n\n```{.r .cell-code}\n# Plot the evolution of the KL divergence at each 50th iteration\nplot(tsne_output$itercosts, type = \"l\")\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Inspect the output object's structure\nstr(tsne_output)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 14\n $ N                  : int 200\n $ Y                  : num [1:200, 1:3] 2.8 4.56 6.4 -9.46 2.84 ...\n $ costs              : num [1:200] 0.00342 0.00297 0.00055 0.00277 0.00223 ...\n $ itercosts          : num [1:20] 53.3 52.3 52.9 51.7 53 ...\n $ origD              : int 50\n $ perplexity         : num 30\n $ theta              : num 0.5\n $ max_iter           : num 1000\n $ stop_lying_iter    : int 250\n $ mom_switch_iter    : int 250\n $ momentum           : num 0.5\n $ final_momentum     : num 0.8\n $ eta                : num 200\n $ exaggeration_factor: num 12\n - attr(*, \"class\")= chr [1:2] \"Rtsne\" \"list\"\n```\n:::\n\n```{.r .cell-code}\n# Show the K-L divergence of each record after the final iteration\ntsne_output$costs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1]  0.0034174757  0.0029728581  0.0005504898  0.0027692258  0.0022288362\n  [6]  0.0057885138  0.0036343563  0.0013069847  0.0015557943  0.0019944906\n [11]  0.0024601940  0.0029579243  0.0026655458  0.0018782248  0.0043633200\n [16]  0.0009990607  0.0032647592  0.0025923357  0.0008498765  0.0022686889\n [21]  0.0017665547  0.0015116845  0.0031226883  0.0021650831  0.0024715843\n [26]  0.0025431443  0.0016450644  0.0021711457  0.0041928263  0.0009438300\n [31]  0.0050829387  0.0032062042  0.0035671215  0.0021694142  0.0020954372\n [36]  0.0033302921  0.0057613944  0.0026565644  0.0011248004  0.0039679638\n [41]  0.0029367949  0.0024785620  0.0024159892  0.0017942388  0.0022840477\n [46]  0.0037765342  0.0025203077  0.0051968590  0.0004972147  0.0044978331\n [51]  0.0020800592  0.0040475671  0.0015066205  0.0056465079  0.0016973046\n [56]  0.0054093156  0.0011852646  0.0058488410  0.0008691591  0.0013817153\n [61]  0.0033213517  0.0013516170  0.0011240949  0.0018977794  0.0022572910\n [66]  0.0010196905  0.0008412208  0.0006216288  0.0015772333  0.0011447258\n [71]  0.0027004229  0.0056891891  0.0006107877  0.0033526510  0.0012266942\n [76]  0.0026682752  0.0005598282  0.0020758530  0.0011308186  0.0012633827\n [81]  0.0027626258  0.0019415110  0.0033466082  0.0012637130  0.0046555588\n [86]  0.0016723685  0.0016770583  0.0034623779  0.0012029961  0.0035918725\n [91]  0.0044978676  0.0007241990  0.0037106577  0.0002940634  0.0077671012\n [96]  0.0030014218  0.0015530451  0.0008403763  0.0019963392  0.0020803698\n[101]  0.0015604525  0.0023659722  0.0015253896  0.0015649411  0.0015653190\n[106]  0.0024791352  0.0020028889  0.0026158041  0.0015421620  0.0010002653\n[111]  0.0012526674  0.0024896172  0.0020835220  0.0011423596  0.0032638947\n[116]  0.0032574106  0.0036779259  0.0031165055  0.0000668607  0.0042271907\n[121]  0.0027297369  0.0050504469  0.0005103500  0.0015037387  0.0040764548\n[126]  0.0023880081  0.0044988762  0.0020478148  0.0034122314  0.0020406245\n[131]  0.0083893037  0.0020684726  0.0026419824  0.0019747799  0.0008668788\n[136]  0.0025096476  0.0048179466  0.0015292831  0.0020501421  0.0028643908\n[141]  0.0013399658  0.0029231555  0.0024378603  0.0020372578  0.0016122528\n[146]  0.0052698817  0.0017881580  0.0008024673  0.0018592243  0.0021153887\n[151] -0.0001172438  0.0020217826  0.0024551064  0.0028175871  0.0046163864\n[156]  0.0030941460  0.0059331118  0.0028702886  0.0002709510  0.0008989285\n[161]  0.0017600385  0.0008235261  0.0028759197  0.0023707087  0.0018275774\n[166]  0.0018605250  0.0015037026  0.0017111415  0.0039444059  0.0015747368\n[171]  0.0008325786  0.0009607417  0.0031155447  0.0029885027  0.0037049660\n[176]  0.0049675154  0.0029984435  0.0032886225  0.0041335089  0.0023761950\n[181]  0.0013478639  0.0018146625  0.0026450723  0.0011213476  0.0021787760\n[186]  0.0012619862  0.0014239765  0.0030828049  0.0036936194  0.0016160208\n[191]  0.0035105621  0.0013495031  0.0026488494  0.0024515092  0.0022165538\n[196]  0.0005791620  0.0019845642  0.0020360660  0.0025363715  0.0013035413\n```\n:::\n\n```{.r .cell-code}\n# Plot the K-L divergence of each record after the final iteration\nplot(tsne_output$costs, type = \"l\")\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n:::\n\n\n## Reproducing results\n\nt-SNE is a stochastic algorithm, meaning there is some randomness inherent to the process. To ensure reproducible results it is necessary to fix a seed before every new execution. This way, you can tune the algorithm hyper-parameters and isolate the effect of the randomness. In this exercise, the goal is to generate two embeddings and check that they are identical. The mnist_sample dataset is available in your workspace.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate a three-dimensional t-SNE embedding without PCA\ntsne_output <- Rtsne(mnist_sample[, -1], PCA = FALSE, dim = 3)\n\n# Generate a new t-SNE embedding with the same hyper-parameter values\ntsne_output_new <- Rtsne(mnist_sample[, -1], PCA = FALSE, dim = 3)\n\n# Check if the two outputs are identical\nidentical(tsne_output, tsne_output_new)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n\n```{.r .cell-code}\n# Generate a three-dimensional t-SNE embedding without PCA\nset.seed(1234)\ntsne_output <- Rtsne(mnist_sample[, -1], PCA = FALSE, dims = 3)\n\n# Generate a new t-SNE embedding with the same hyper-parameter values\nset.seed(1234)\ntsne_output_new <- Rtsne(mnist_sample[, -1], PCA = FALSE, dims = 3)\n\n# Check if the two outputs are identical\nidentical(tsne_output, tsne_output_new)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\n## Optimal number of iterations\n\nA common hyper-parameter to optimize in t-SNE is the optimal number of iterations. As you have seen before it is important to always use the same seed before you can compare different executions. To optimize the number of iterations, you can increase the max_iter parameter of Rtsne() and observe the returned itercosts to find the minimum K-L divergence. The mnist_sample dataset and the Rtsne package have been loaded for you.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed to ensure reproducible results\nset.seed(1234)\n\n# Execute a t-SNE with 2000 iterations\ntsne_output <- Rtsne(mnist_sample[, -1], max_iter = 2000,PCA = TRUE, dims = 2)\n\n# Observe the output costs \ntsne_output$itercosts\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 53.3290786 53.2244985 52.2552243 53.0855081 53.2869529  1.0962201\n [7]  0.8253614  0.7929360  0.7815495  0.7787592  0.7786192  0.7713815\n[13]  0.7651633  0.7471433  0.7454131  0.7458568  0.7475261  0.7453661\n[19]  0.7439994  0.7298887  0.6989778  0.6993673  0.7012341  0.6989279\n[25]  0.6974807  0.6975004  0.6955154  0.6984345  0.7003433  0.7004715\n[31]  0.7002442  0.7002176  0.7003945  0.6978038  0.6970268  0.6995675\n[37]  0.6978879  0.6997786  0.6996063  0.7009296\n```\n:::\n\n```{.r .cell-code}\n# Get the 50th iteration with the minimum K-L cost\nwhich.min(tsne_output$itercosts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 27\n```\n:::\n:::\n\n\n## Perplexity of MNIST sample\n\nThe perplexity parameter indicates the balance between the local and global aspect of the input data. The parameter is an estimate of the number of close neighbors of each original point. Typical values of this parameter fall in the range of 5 to 50. We will generate three different t-SNE executions with the same number of iterations and perplexity values of 5, 20, and 50 and observe the differences in the K-L divergence costs. The optimal number of iterations we found in the last exercise (1200) will be used here. The mnist_sample dataset and the Rtsne package have been loaded for you.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed to ensure reproducible results\npar(mfrow = c(3, 1))\nset.seed(1234)\n\nperp <- c(5, 20, 50)\nmodels <- list()\nfor (i in 1:length(perp)) {\n        \n        # Execute a t-SNE with perplexity 5\n        perplexity  = perp[i]\n        tsne_output <- Rtsne(mnist_sample[, -1], perplexity = perplexity, max_iter = 1300)\n        # Observe the returned K-L divergence costs at every 50th iteration\n        models[[i]] <- tsne_output\n        plot(tsne_output$itercosts,\n             main = paste(\"Perplexity\", perplexity),\n             type = \"l\", ylab = \"itercosts\")\n}\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\nnames(models) <- paste0(\"perplexity\",perp)\n```\n:::\n\n\n## Perplexity of bigger MNIST dataset\n\nNow, let's investigate the effect of the perplexity values with a bigger MNIST dataset of 10.000 records. It would take a lot of time to execute t-SNE for this many records on the DataCamp platform. This is why the pre-loaded output of two t-SNE embeddings with perplexity values of 5 and 50, named tsne_output_5 and tsne_output_50 are available in the workspace. We will look at the K-L costs and plot them using the digit label from the mnist_10k dataset, which is also available in the environment. The Rtsne and ggplot2 packages have been loaded.\n\n-   I used mnist smaller data set\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Observe the K-L divergence costs with perplexity 5 and 50\ntsne_output_5 <- models$perplexity5\ntsne_output_50  <- models$perplexity50\n# Generate the data frame to visualize the embedding\ntsne_plot_5 <- data.frame(tsne_x = tsne_output_5$Y[, 1], tsne_y = tsne_output_5$Y[, 2], digit = as.factor(mnist_sample$label))\n\ntsne_plot_50 <- data.frame(tsne_x = tsne_output_50$Y[, 1], tsne_y = tsne_output_50$Y[, 2], digit = as.factor(mnist_sample$label))\n\n# Plot the obtained embeddings\nggplot(tsne_plot_5, aes(x = tsne_x, y = tsne_y, color = digit)) + \n\tggtitle(\"MNIST t-SNE with 1300 iter and Perplexity=5\") +\n\tgeom_text(aes(label = digit)) + \n\ttheme(legend.position=\"none\")\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(tsne_plot_50, aes(x = tsne_x, y = tsne_y, color = digit)) + \n\tggtitle(\"MNIST t-SNE with 1300 iter and Perplexity=50\") + \n\tgeom_text(aes(label = digit)) + \n\ttheme(legend.position=\"none\")\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n:::\n\n\n## Plotting spatial distribution of true classes\n\nAs seen in the video, you can use the obtained representation of t-SNE in a lower dimension space to classify new digits based on the Euclidean distance to known clusters of digits. For this task, let's start with plotting the spatial distribution of the digit labels in the embedding space. You are going to use the output of a t-SNE execution of 10K MNIST records named tsne and the true labels can be found in a dataset named mnist_10k. In this exercise, you will use the first 5K records of tsne and mnist_10k datasets and the goal is to visualize the obtained t-SNE embedding. The ggplot2 package has been loaded for you.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\nmnist_10k <- readRDS(\"mnist_10k.rds\") %>% setDT()\ntsne <- Rtsne(mnist_10k[, -1], perplexity = 50, max_iter = 1500)\n# Prepare the data.frame\ntsne_plot <- data.frame(tsne_x = tsne$Y[1:5000, 1], \n                        tsne_y = tsne$Y[1:5000, 2], \n                        digit = as.factor(mnist_10k[1:5000, ]$label))\n\n# Plot the obtained embedding\nggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = digit)) + \n\tggtitle(\"MNIST embedding of the first 5K digits\") + \n\tgeom_text(aes(label = digit)) + \n\ttheme(legend.position=\"none\")\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n## Computing the centroids of each class\n\nSince the previous visual representation of the digit in a low dimensional space makes sense, you want to compute the centroid of each class in this lower dimensional space. This centroid can be used as a prototype of the digit and you can classify new digits based on their Euclidean distance to these ones. The MNIST data mnist_10k and t-SNE output tsne are available in the workspace. The data.table package has been loaded for you.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the first 5K records and set the column names\ndt_prototypes <- as.data.table(tsne$Y[1:5000,])\nsetnames(dt_prototypes, c(\"X\",\"Y\"))\n\n# Paste the label column as factor\ndt_prototypes[, label := as.factor(mnist_10k[1:5000,]$label)]\n\n# Compute the centroids per label\ndt_prototypes[, mean_X := mean(X), by = label]\ndt_prototypes[, mean_Y := mean(Y), by = label]\n\n# Get the unique records per label\ndt_prototypes <- unique(dt_prototypes, by = \"label\")\ndt_prototypes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             X          Y label     mean_X     mean_Y\n 1:  34.835576  16.996845     7  30.167171  13.979335\n 2:  16.417303   3.914098     4  23.015486  -4.668565\n 3:   8.523417  24.050519     1   1.162703  33.822984\n 4:   9.346855 -26.402177     6   3.616021 -27.402390\n 5: -11.365115  -6.049854     5  -6.425186  -8.957745\n 6:  -5.704105   7.608415     8  -8.415119   6.278887\n 7: -34.408062   2.209488     3 -25.853224  -1.354711\n 8: -29.504694  16.977549     2 -20.059132  21.122323\n 9: -19.469866 -30.616292     0 -17.952830 -32.331439\n10:  27.605614   4.103024     9  20.453515  -3.364576\n```\n:::\n:::\n\n\n## Computing similarities of digits 1 and 0\n\nOne way to measure the label similarity for each digit is by computing the Euclidean distance in the lower dimensional space obtained from the t-SNE algorithm. You need to use the previously calculated centroids stored in dt_prototypes and compute the Euclidean distance to the centroid of digit 1 for the last 5000 records from tsne and mnist_10k datasets that are labeled either as 1 or 0. Note that the last 5000 records of tsne were not used before. The MNIST data mnist_10k and t-SNE output tsne are available in the workspace. The data.table package has been loaded for you.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Store the last 5000 records in distances and set column names\ndistances <- as.data.table(tsne$Y[5001:10000,])\nsetnames(distances, c(\"X\", \"Y\"))\n# Paste the true label\ndistances[, label := mnist_10k[5001:10000,]$label]\ndistances[, mean_X := mean(X), by = label]\ndistances[, mean_Y := mean(Y), by = label]\n\n\n# Filter only those labels that are 1 or 0 \ndistances_filtered <- distances[label == 1 | label == 0]\n\n# Compute Euclidean distance to prototype of digit 1\ndistances_filtered[, dist_1 := sqrt( (X - dt_prototypes[label == 1,]$mean_X)^2 + \n                             (Y - dt_prototypes[label == 1,]$mean_Y)^2)]\n```\n:::\n\n\n## Plotting similarities of digits 1 and 0\n\nIn distances, the distances of 1108 records to the centroid of digit 1 are stored in dist_1. Those records correspond to digits you already know are 1's or 0's. You can have a look at the basic statistics of the distances from records that you know are 0 and 1 (label column) to the centroid of class 1 using summary(). Also, if you plot a histogram of those distances and fill them with the label you can check if you are doing a good job identifying the two classes with this t-SNE classifier. The data.table and ggplot2 packages, as well as the distances object, have been loaded for you.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute the basic statistics of distances from records of class 1\nsummary(distances_filtered[label == 1]$dist_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.9694  6.5282  8.8059  9.0494 11.6443 54.7206 \n```\n:::\n\n```{.r .cell-code}\n# Compute the basic statistics of distances from records of class 0\nsummary(distances_filtered[label == 0]$dist_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  49.54   64.67   68.42   68.86   73.02   78.97 \n```\n:::\n\n```{.r .cell-code}\n# Plot the histogram of distances of each class\nggplot(distances_filtered, \n       aes(x = dist_1, fill = as.factor(label))) +\n    geom_histogram(binwidth = 5, alpha = .5, \n                   position = \"identity\", show.legend = FALSE) + \n  \tggtitle(\"Distribution of Euclidean distance 1 vs 0\")\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n## Exploring credit card fraud dataset\n\nIn this exercise, you will do some data exploration on a sample of the credit card fraud detection dataset from Kaggle. For any problem, starting with some data exploration is a good practice and helps us better understand the characteristics of the data.\n\nThe credit card fraud dataset is already loaded in the environment as a data table with the name creditcard. As you saw in the video, it consists of 30 numerical variables. The Class column indicates if the transaction is fraudulent. The ggplot2 package has been loaded for you.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"creditcard.RData\") \n\nsetDT(creditcard)\n# Look at the data dimensions\ndim(creditcard)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 28923    31\n```\n:::\n\n```{.r .cell-code}\n# Explore the column names\n#names(creditcard)\n\n# Explore the structure\n#str(creditcard)\n\n# Generate a summary\nsummary(creditcard)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Time              V1                  V2                  V3          \n Min.   :    26   Min.   :-56.40751   Min.   :-72.71573   Min.   :-31.1037  \n 1st Qu.: 54230   1st Qu.: -0.96058   1st Qu.: -0.58847   1st Qu.: -0.9353  \n Median : 84512   Median : -0.02400   Median :  0.08293   Median :  0.1659  \n Mean   : 94493   Mean   : -0.08501   Mean   :  0.05955   Mean   : -0.1021  \n 3rd Qu.:139052   3rd Qu.:  1.30262   3rd Qu.:  0.84003   3rd Qu.:  1.0119  \n Max.   :172788   Max.   :  2.41150   Max.   : 22.05773   Max.   :  3.8771  \n       V4                  V5                  V6           \n Min.   :-5.071241   Min.   :-31.35675   Min.   :-26.16051  \n 1st Qu.:-0.824978   1st Qu.: -0.70869   1st Qu.: -0.78792  \n Median : 0.007618   Median : -0.06071   Median : -0.28396  \n Mean   : 0.073391   Mean   : -0.04367   Mean   : -0.02722  \n 3rd Qu.: 0.789293   3rd Qu.:  0.61625   3rd Qu.:  0.37911  \n Max.   :16.491217   Max.   : 34.80167   Max.   : 20.37952  \n       V7                  V8                  V9           \n Min.   :-43.55724   Min.   :-50.42009   Min.   :-13.43407  \n 1st Qu.: -0.57404   1st Qu.: -0.21025   1st Qu.: -0.66974  \n Median :  0.02951   Median :  0.01960   Median : -0.06343  \n Mean   : -0.08873   Mean   : -0.00589   Mean   : -0.04295  \n 3rd Qu.:  0.57364   3rd Qu.:  0.33457   3rd Qu.:  0.58734  \n Max.   : 29.20587   Max.   : 20.00721   Max.   :  8.95567  \n      V10                 V11                V12                V13           \n Min.   :-24.58826   Min.   :-4.11026   Min.   :-18.6837   Min.   :-3.844974  \n 1st Qu.: -0.54827   1st Qu.:-0.75404   1st Qu.: -0.4365   1st Qu.:-0.661168  \n Median : -0.09843   Median :-0.01036   Median :  0.1223   Median :-0.009685  \n Mean   : -0.08468   Mean   : 0.06093   Mean   : -0.0943   Mean   :-0.002110  \n 3rd Qu.:  0.44762   3rd Qu.: 0.77394   3rd Qu.:  0.6172   3rd Qu.: 0.664794  \n Max.   : 15.33174   Max.   :12.01891   Max.   :  4.8465   Max.   : 4.569009  \n      V14                 V15                 V16                 V17          \n Min.   :-19.21432   Min.   :-4.498945   Min.   :-14.12985   Min.   :-25.1628  \n 1st Qu.: -0.44507   1st Qu.:-0.595272   1st Qu.: -0.48770   1st Qu.: -0.4951  \n Median :  0.04865   Median : 0.045992   Median :  0.05736   Median : -0.0742  \n Mean   : -0.09653   Mean   :-0.007251   Mean   : -0.06186   Mean   : -0.1046  \n 3rd Qu.:  0.48765   3rd Qu.: 0.646584   3rd Qu.:  0.52147   3rd Qu.:  0.3956  \n Max.   :  7.75460   Max.   : 5.784514   Max.   :  5.99826   Max.   :  7.2150  \n      V18                V19                 V20            \n Min.   :-9.49875   Min.   :-4.395283   Min.   :-20.097918  \n 1st Qu.:-0.51916   1st Qu.:-0.462158   1st Qu.: -0.211663  \n Median :-0.01595   Median : 0.010494   Median : -0.059160  \n Mean   :-0.04344   Mean   : 0.009424   Mean   :  0.006943  \n 3rd Qu.: 0.48634   3rd Qu.: 0.471172   3rd Qu.:  0.141272  \n Max.   : 3.88618   Max.   : 5.228342   Max.   : 24.133894  \n      V21                  V22                 V23           \n Min.   :-22.889347   Min.   :-8.887017   Min.   :-36.66600  \n 1st Qu.: -0.230393   1st Qu.:-0.550210   1st Qu.: -0.16093  \n Median : -0.028097   Median :-0.000187   Median : -0.00756  \n Mean   :  0.004995   Mean   :-0.006271   Mean   :  0.00418  \n 3rd Qu.:  0.190465   3rd Qu.: 0.516596   3rd Qu.:  0.15509  \n Max.   : 27.202839   Max.   : 8.361985   Max.   : 13.65946  \n      V24                 V25                 V26           \n Min.   :-2.822684   Min.   :-6.712624   Min.   :-1.658162  \n 1st Qu.:-0.354367   1st Qu.:-0.319410   1st Qu.:-0.328496  \n Median : 0.038722   Median : 0.011815   Median :-0.054131  \n Mean   : 0.000741   Mean   :-0.002847   Mean   :-0.002546  \n 3rd Qu.: 0.440797   3rd Qu.: 0.351797   3rd Qu.: 0.237782  \n Max.   : 3.962197   Max.   : 5.376595   Max.   : 3.119295  \n      V27                 V28               Amount            Class          \n Min.   :-8.358317   Min.   :-8.46461   Min.   :    0.00   Length:28923      \n 1st Qu.:-0.071275   1st Qu.:-0.05424   1st Qu.:    5.49   Class :character  \n Median : 0.002727   Median : 0.01148   Median :   22.19   Mode  :character  \n Mean   :-0.000501   Mean   : 0.00087   Mean   :   87.90                     \n 3rd Qu.: 0.095974   3rd Qu.: 0.08238   3rd Qu.:   78.73                     \n Max.   : 7.994762   Max.   :33.84781   Max.   :11898.09                     \n```\n:::\n\n```{.r .cell-code}\n# Plot a histogram of the transaction time\nggplot(creditcard, aes(x = Time)) + \n\tgeom_histogram()\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n## Generating training and test sets\n\nBefore we can apply the t-SNE algorithm to perform a dimensionality reduction, we need to split the original data into a training and test set. Next, we will perform an under-sampling of the majority class and generate a balanced training set. Generating a balanced dataset is a good practice when we are using tree-based models. In this exercise you already have the creditcard dataset loaded in the environment. The ggplot2 and data.table packages are already loaded.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract positive and negative instances of fraud\ncreditcard_pos <- creditcard[Class == 1]\ncreditcard_neg <- creditcard[Class == 0]\n\n# Fix the seed\nset.seed(1234)\n\n# Create a new negative balanced dataset by undersampling\ncreditcard_neg_bal <- creditcard_neg[sample(1:nrow(creditcard_neg), nrow(creditcard_pos))]\n\n# Generate a balanced train set\ncreditcard_train <- rbind(creditcard_pos, creditcard_neg_bal)\n```\n:::\n\n\n## Training a random forest with original features\n\nIn this exercise, we are going to train a random forest model using the original features from the credit card dataset. The goal is to detect new fraud instances in the future and we are doing that by learning the patterns of fraud instances in the balanced training set. Remember that a random forest can be trained with the following piece of code: randomForest(x = features, y = label, ntree = 100) The only pre-processing that has been done to the original features was to scale the Time and Amount variables. You have the balanced training dataset available in the environment as creditcard_train. The randomForest package has been loaded.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fix the seed\nset.seed(1234)\nlibrary(randomForest)\n# Separate x and y sets\ntrain_x <- creditcard_train[,-31]\ntrain_y <- creditcard_train$Class %>% as.factor()\n\n# Train a random forests\nrf_model <- randomForest(x = train_x, y = train_y, ntree = 100)\n\n# Plot the error evolution and variable importance\nplot(rf_model, main = \"Error evolution vs number of trees\")\n# Fix the seed\nset.seed(1234)\n\n# Separate x and y sets\ntrain_x <- creditcard_train[,-31]\ntrain_y <- creditcard_train$Class %>%as.factor()\n\n# Train a random forests\nrf_model <- randomForest(x = train_x, y = train_y, ntree = 100)\n\n# Plot the error evolution and variable importance\nplot(rf_model, main = \"Error evolution vs number of trees\")\nlegend(\"topright\", colnames(rf_model$err.rate),col=1:3,cex=0.8,fill=1:3)\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n\n```{.r .cell-code}\nvarImpPlot(rf_model, main = \"Variable importance\")\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-20-2.png){width=672}\n:::\n:::\n\n\n## Computing and visualising the t-SNE embedding\n\nIn this exercise, we are going to generate a t-SNE embedding using only the balanced training set creditcard_train. The idea is to train a random forest using the two coordinates of the generated embedding instead of the original 30 dimensions. Due to computational restrictions, we are going to compute the embedding of the training data only, but note that in order to generate predictions from the test set we should compute the embedding of the test set together with the train set. Then, we will visualize the obtained embedding highlighting the two classes in order to clarify if we can differentiate between fraud and non-fraud transactions. The creditcard_train data, as well as the Rtsne and ggplot2 packages, have been loaded.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set the seed\n#set.seed(1234)\n\n# Generate the t-SNE embedding \ncreditcard_train[, Time := scale(Time)]\nnms <- names(creditcard_train)\npred_nms <- nms[nms != \"Class\"]\nrange01 <- function(x){(x-min(x))/(max(x)-min(x))}\ncreditcard_train[, (pred_nms) := lapply(.SD ,range01), .SDcols = pred_nms]\n\ntsne_output <- Rtsne(as.matrix(creditcard_train[, -31]), check_duplicates = FALSE, PCA = FALSE)\n\n# Generate a data frame to plot the result\ntsne_plot <- data.frame(tsne_x = tsne_output$Y[,1],\n                        tsne_y = tsne_output$Y[,2],\n                        Class = creditcard_train$Class)\n\n# Plot the embedding usign ggplot and the label\nggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = factor(Class))) + \n  ggtitle(\"t-SNE of credit card fraud train set\") + \n  geom_text(aes(label = Class)) + theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n## Training a random forest with embedding features\n\nIn this exercise, we are going to train a random forest model using the embedding features from the previous t-SNE embedding. So, in this case, we are going to use a two-dimensional dataset that has been generated from the original input features. In the rest of the chapter, we are going to verify if we have a worse, similar, or better performance for this model in comparison to the random forest trained with the original features. In the environment two objects named train_tsne_x and train_tsne_y that contain the features and the Class variable are available. The randomForest package has been loaded as well.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fix the seed\nset.seed(1234)\ntrain_tsne_x <- tsne_output$Y\n# Train a random forest\nrf_model_tsne <- randomForest(x = train_tsne_x, y = train_y, ntree = 100)\n\n# Plot the error evolution\n\nplot(rf_model_tsne)\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot the variable importance\nvarImpPlot(rf_model_tsne)\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-22-2.png){width=672}\n:::\n:::\n\n\n## Predicting data using original features\n\nIn this exercise, we are using the random forest trained with the original features and generate predictions using the test set. These predictions will be plotted to see the distribution and will be evaluated using the ROCR package by considering the area under the curve.\n\nThe random forest model, named rf_model, and the test set, named creditcard_test, are available in the environment. The randomForest and ROCR packages have been loaded for you\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict on the test set using the random forest \ncreditcard_test <- creditcard\npred_rf <- predict(rf_model, creditcard_test, type = \"prob\")\n\n# Plot a probability distibution of the target class\nhist(pred_rf[,2])\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlibrary(ROCR)\n# Compute the area under the curve\npred <-  prediction(pred_rf[,2], creditcard_test$Class)\nperf <- performance(pred, measure = \"auc\") \nperf@y.values\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[1] 0.9995958\n```\n:::\n:::\n\n\n## Predicting data using embedding random forest\n\nNow, we are going to do the same analysis, but instead of using the random forest trained with the original features, we will make predictions using the random forest trained with the t-SNE embedding coordinates. The random forest model is pre-loaded in an object named rf_model_tsne and the t-SNE embedding features from the original test set are stored in the object test_x. Finally, the test set labels are stored in creditcard_test. The randomForest and ROCR packages have been loaded for you.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncreditcard_test[, (pred_nms) := lapply(.SD ,range01), .SDcols = pred_nms]\n\ntsne_output <- Rtsne(as.matrix(creditcard_test[, -31]), check_duplicates = FALSE, PCA = FALSE)\n\ntest_x <- tsne_output$Y\n# Predict on the test set using the random forest generated with t-SNE features\npred_rf <- predict(rf_model_tsne, test_x, type = \"prob\")\n\n# Plot a probability distibution of the target class\nhist(pred_rf[, 2])\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Compute the area under the curve\npred <- prediction(pred_rf[, 2] , creditcard_test$Class)\nperf <- performance(pred, measure = \"auc\") \nperf@y.values\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[1] 0.3418133\n```\n:::\n:::\n\n\n## Exploring neural network layer output\n\nIn this exercise, we will have a look at the data that is being generated in a specific layer of a neural network. In particular, this data corresponds to the third layer, composed of 128 neurons, of a neural network trained with the balanced credit card fraud dataset generated before. The goal of the exercise is to perform an exploratory data analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Observe the dimensions\n#dim(layer_128_train)\n\n# Show the first six records of the last ten columns\n#head(layer_128_train[, 119:128])\n\n# Generate a summary of all columns\n#summary(layer_128_train)\n```\n:::\n\n\n## Using t-SNE to visualise a neural network layer\n\nNow, we would like to visualize the patterns obtained from the neural network model, in particular from the last layer of the neural network. As we mentioned before this last layer has 128 neurons and we have pre-loaded the weights of these neurons in an object named layer_128_train. The goal is to compute a t-SNE embedding using the output of the neurons from this last layer and visualize the embedding colored according to the class. The Rtsne and ggplot2 packages as well as the layer_128_train and creditcard_train have been loaded for you\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set the seed\nset.seed(1234)\n\n# Generate the t-SNE\n#tsne_output <- Rtsne(as.matrix(layer_128_train), check_duplicates = FALSE, max_iter = 400, perplexity = 50)\n\n# Prepare data.frame\n#tsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], \n#                        Class = creditcard_train$Class)\n\n# Plot the data \n# ggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = Class)) + \n# \tgeom_point() + \n# \tggtitle(\"Credit card embedding of Last Neural Network Layer\")\n```\n:::\n\n\n## Using t-SNE to visualise a neural network layer\n\nNow, we would like to visualize the patterns obtained from the neural network model, in particular from the last layer of the neural network. As we mentioned before this last layer has 128 neurons and we have pre-loaded the weights of these neurons in an object named layer_128_train. The goal is to compute a t-SNE embedding using the output of the neurons from this last layer and visualize the embedding colored according to the class. The Rtsne and ggplot2 packages as well as the layer_128_train and creditcard_train have been loaded for you\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set the seed\n# set.seed(1234)\n# \n# # Generate the t-SNE\n# tsne_output <- Rtsne(as.matrix(layer_128_train), check_duplicates = FALSE, max_iter = 400, perplexity = 50)\n# \n# # Prepare data.frame\n# tsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], \n#                         Class = creditcard_train$Class)\n# \n# # Plot the data \n# ggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = Class)) + \n# \tgeom_point() + \n# \tggtitle(\"Credit card embedding of Last Neural Network Layer\")\n```\n:::\n\n\n## Exploring fashion MNIST\n\nThe Fashion MNIST dataset contains grayscale images of 10 clothing categories. The first thing to do when you are analyzing a new dataset is to perform an exploratory data analysis in order to understand the data. A sample of the fashion MNIST dataset fashion_mnist, with only 500 records, is pre-loaded for you.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\n#load(\"fashion_mnist_500.RData\")\nload(\"fashion_mnist.rda\")\nset.seed(100)\n\nind <- sample(1:nrow(fashion_mnist), 1000)\n\nfashion_mnist <- fashion_mnist[ind, ]\n# Show the dimensions\ndim(fashion_mnist)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1000  785\n```\n:::\n\n```{.r .cell-code}\n# Create a summary of the last six columns \nsummary(fashion_mnist[, 780:785])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    pixel779         pixel780         pixel781        pixel782      \n Min.   :  0.00   Min.   :  0.00   Min.   :  0.0   Min.   :  0.000  \n 1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:  0.0   1st Qu.:  0.000  \n Median :  0.00   Median :  0.00   Median :  0.0   Median :  0.000  \n Mean   : 22.16   Mean   : 18.64   Mean   : 10.6   Mean   :  3.781  \n 3rd Qu.:  1.00   3rd Qu.:  0.00   3rd Qu.:  0.0   3rd Qu.:  0.000  \n Max.   :236.00   Max.   :255.00   Max.   :231.0   Max.   :188.000  \n    pixel783          pixel784     \n Min.   :  0.000   Min.   : 0.000  \n 1st Qu.:  0.000   1st Qu.: 0.000  \n Median :  0.000   Median : 0.000  \n Mean   :  0.934   Mean   : 0.043  \n 3rd Qu.:  0.000   3rd Qu.: 0.000  \n Max.   :147.000   Max.   :39.000  \n```\n:::\n\n```{.r .cell-code}\n# Table with the class distribution\ntable(fashion_mnist$label)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  0   1   2   3   4   5   6   7   8   9 \n100  94 115  90  97  98  97 105 102 102 \n```\n:::\n:::\n\n\n## Visualizing fashion MNIST\n\nIn this exercise, we are going to visualize an example image of the fashion MNIST dataset. Basically, we are going to plot the 28x28 pixels values. To do this we use:\n\nA custom ggplot theme named plot_theme. A data structure named xy_axis where the pixels values are stored. A character vector named class_names with the names of each class. The fashion_mnist dataset with 500 examples is available in the workspace. The \\`ggplot2 package is loaded. Note that you can access the definition of the custom theme by typing plot_theme in the console.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nplot_theme <- list(\n    raster = geom_raster(hjust = 0, vjust = 0),\n    gradient_fill = scale_fill_gradient(low = \"white\",\n                                        high = \"black\", guide = FALSE),\n    theme = theme(axis.line = element_blank(),\n                  axis.text = element_blank(),\n                  axis.ticks = element_blank(),\n                  axis.title = element_blank(),\n                  panel.background = element_blank(),\n                  panel.border = element_blank(),\n                  panel.grid.major = element_blank(),\n                  panel.grid.minor = element_blank(),\n                  plot.background = element_blank()))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nclass_names <-  c(\"T-shirt/top\", \"Trouser\", \"Pullover\", \n                  \"Dress\", \"Coat\", \"Sandal\", \"Shirt\",\n                  \"Sneaker\", \"Bag\", \"Ankle\", \"boot\")\n\n\nxy_axis <- data.frame(x = expand.grid(1:28, 28:1)[,1],\n                      y = expand.grid(1:28, 28:1)[,2])\n\n# Get the data from the last image\nplot_data <- cbind(xy_axis, fill = as.data.frame(t(fashion_mnist[500, -1]))[,1])\n\n# Observe the first records\nhead(plot_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  x  y fill\n1 1 28    0\n2 2 28    0\n3 3 28    0\n4 4 28    0\n5 5 28    0\n6 6 28    0\n```\n:::\n\n```{.r .cell-code}\n# Plot the image using ggplot()\nggplot(plot_data, aes(x, y, fill = fill)) + \n  ggtitle(class_names[as.integer(fashion_mnist[500, 1])]) + \n  plot_theme \n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\n## Reducing data with GLRM\n\nWe are going to reduce the dimensionality of the fashion MNIST sample data using the GLRM implementation of h2o. In order to do this, in the next steps we are going to: Start a connection to a h2o cluster by invoking the method h2o.init(). Store the fashion_mnist data into the h2o cluster with as.h2o(). Launch a GLRM model with K=2 (rank-2 model) using the h2o.glrm() function. As we have discussed in the video session, it is important to check the convergence of the objective function. Note that here we are also fixing the seed to ensure the same results. The h2o package and fashion_mnist data are pre-loaded in the environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(h2o)\n# Start a connection with the h2o cluster\nh2o.init()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nH2O is not running yet, starting it now...\n\nNote:  In case of errors look at the following log files:\n    /tmp/RtmpxtoQk8/file944d21a59e761/h2o_mburu_started_from_r.out\n    /tmp/RtmpxtoQk8/file944d2232141a5/h2o_mburu_started_from_r.err\n\n\nStarting H2O JVM and connecting: .. Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         1 seconds 126 milliseconds \n    H2O cluster timezone:       Africa/Nairobi \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.38.0.1 \n    H2O cluster version age:    4 months and 23 days !!! \n    H2O cluster name:           H2O_started_from_R_mburu_cvk380 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   3.40 GB \n    H2O cluster total cores:    8 \n    H2O cluster allowed cores:  8 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.2.2 Patched (2022-11-10 r83330) \n```\n:::\n\n```{.r .cell-code}\n# Store the data into h2o cluster\nfashion_mnist.hex <- as.h2o(fashion_mnist, \"fashion_mnist.hex\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\n# Launch a GLRM model over fashion_mnist data\nmodel_glrm <- h2o.glrm(training_frame = fashion_mnist.hex,\n                       cols = 2:ncol(fashion_mnist), \n                       k = 2,\n                       seed = 123,\n                       max_iterations = 2100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\n# Plotting the convergence\nplot(model_glrm)\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\n## Improving model convergence\n\nIn the previous exercise, we didn't get good convergence values for the GLRM model. Improving convergence values can sometimes be achieved by applying a transformation to the input data. In this exercise, we are going to normalize the input data before we start building the GLRM model. This can be achieved by setting the transform parameter of h2o.glrm() equal to \"NORMALIZE\". The h2o package and fashion_mnist dataset are pre-loaded.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Start a connection with the h2o cluster\n#h2o.init()\n\n# Store the data into h2o cluster\n#fashion_mnist.hex <- as.h2o(fashion_mnist, \"fashion_mnist.hex\")\n\n# Launch a GLRM model with normalized fashion_mnist data  \nmodel_glrm <- h2o.glrm(training_frame = fashion_mnist.hex, \n                       transform = \"NORMALIZE\",\n                       cols = 2:ncol(fashion_mnist), \n                       k = 2, \n                       seed = 123,\n                       max_iterations = 2100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\n# Plotting the convergence\nplot(model_glrm)\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n## Visualizing the output of GLRM\n\nA GLRM model generates the X and Y matrixes. In this exercise, we are going to visualize the obtained low-dimensional representation of the input records in the new K-dimensional space. The output of the X matrix from the previous GLRM model has been loaded with the name X_matrix. This matrix has been obtained by calling:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX_matrix <- as.data.table(h2o.getFrame(model_glrm@model$representation_name))\n# Dimension of X_matrix\ndim(X_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1000    2\n```\n:::\n\n```{.r .cell-code}\n# First records of X_matrix\nhead(X_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Arch1       Arch2\n1: -1.4562898  0.28620951\n2:  0.7132191  1.18922464\n3:  0.5600450 -1.29628758\n4:  0.9997013 -0.51894405\n5:  1.3377989 -0.05616662\n6:  1.0687898  0.07447071\n```\n:::\n\n```{.r .cell-code}\n# Plot the records in the new two dimensional space\nggplot(as.data.table(X_matrix), aes(x= Arch1, y = Arch2, color =  fashion_mnist$label)) + \n\tggtitle(\"Fashion Mnist GLRM Archetypes\") + \n\tgeom_text(aes(label =  fashion_mnist$label)) + \n\ttheme(legend.position=\"none\")\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n## Visualizing the prototypes\n\nNow, we are going to compute the centroids of the coordinates for each of the two archetypes for each label. We did something similar before for the t-SNE embedding. The goal is to have a representation or prototype of each label in this new two-dimensional space.\n\nThe ggplot2 and data.table packages are pre-loaded, as well as the X_matrix object and the fashion_mnist dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Store the label of each record and compute the centroids\nX_matrix[, label := as.numeric(fashion_mnist$label)]\nX_matrix[, mean_x := mean(Arch1), by = label]\nX_matrix[, mean_y := mean(Arch2), by = label]\n\n# Get one record per label and create a vector with class names\nX_mean <- unique(X_matrix, by = \"label\")\n\nlabel_names <- c(\"T-shirt/top\", \"Trouser\", \"Pullover\",\n                 \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \n                 \"Sneaker\", \"Bag\", \"Ankle boot\")\n\n# Plot the centroids\nX_mean[, label := factor(label, levels = 0:9, labels = label_names)]\nggplot(X_mean, aes(x = mean_x, y = mean_y, color = label_names)) + \n\tggtitle(\"Fashion Mnist GLRM class centroids\") + \n\tgeom_text(aes(label = label_names)) +\n\ttheme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](introduction_dimensionality_reduction_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n## Imputing missing data\n\nIn this exercise, we will use GLRM to impute missing data. We are going to build a GLRM model from a dataset named fashion_mnist_miss, where 20% of values are missing. The goal is to fill these values by making a prediction using h2o.predict() with the GLRM model. In this exercise an h2o instance is already running, so it is not necessary to call h2o.init(). The h2o package and fashion_mnist_miss have been loaded\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfashion_mnist_miss <- h2o.insertMissingValues(fashion_mnist.hex, \n                                              fraction = 0.2, seed = 1234)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\n# Store the input data in h2o\nfashion_mnist_miss.hex <- as.h2o(fashion_mnist_miss, \"fashion_mnist_miss.hex\")\n\n# Build a GLRM model\nmodel_glrm <- h2o.glrm(training_frame = fashion_mnist_miss.hex,\n                       k = 2,\n                       transform = \"NORMALIZE\",\n                       max_iterations = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\n# Impute missing values\nfashion_pred <- predict(model_glrm, fashion_mnist_miss.hex)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\n# Observe the statistics of the first 5 pixels\nsummary(fashion_pred[, 1:5])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n reconstr_label     reconstr_pixel2      reconstr_pixel3     \n Min.   :-0.47771   Min.   :-0.0015096   Min.   :-0.0048449  \n 1st Qu.:-0.26505   1st Qu.:-0.0008691   1st Qu.:-0.0022916  \n Median :-0.09920   Median :-0.0003405   Median :-0.0003435  \n Mean   :-0.08239   Mean   :-0.0002889   Mean   :-0.0002108  \n 3rd Qu.: 0.10662   3rd Qu.: 0.0002168   3rd Qu.: 0.0018435  \n Max.   : 0.35054   Max.   : 0.0015996   Max.   : 0.0057716  \n reconstr_pixel4      reconstr_pixel5     \n Min.   :-0.0058540   Min.   :-0.0034778  \n 1st Qu.:-0.0032125   1st Qu.:-0.0018475  \n Median :-0.0013600   Median : 0.0003356  \n Mean   :-0.0011934   Mean   : 0.0001850  \n 3rd Qu.: 0.0008461   3rd Qu.: 0.0019639  \n Max.   : 0.0058340   Max.   : 0.0044750  \n```\n:::\n:::\n\n\n##Training a random forest with original data\n\nIn this exercise, we are going to train a random forest using the original fashion MNIST dataset with 500 examples. This dataset is preloaded in the environment with the name fashion_mnist. We are going to train a random forest with 20 trees and we will look at the time it takes to compute the model and the out-of-bag error in the 20th tree. The randomForest package is loaded.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the starting timestamp\nlibrary(randomForest)\n\ntime_start <- proc.time()\n\n# Train the random forest\nfashion_mnist[, label := factor(label)]\nrf_model <- randomForest(label~., ntree = 20,\n                         data = fashion_mnist)\n\n# Get the end timestamp\ntime_end <- timetaken(time_start)\n\n# Show the error and the time\nrf_model$err.rate[20]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2512513\n```\n:::\n\n```{.r .cell-code}\ntime_end\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"0.641s elapsed (0.641s cpu)\"\n```\n:::\n:::\n\n\n## Training a random forest with compressed data\n\nNow, we are going to train a random forest using a compressed representation of the previous 500 input records, using only 8 dimensions!\n\nIn this exercise, you a dataset named train_x that contains the compressed training data and another one named train_y that contains the labels are pre-loaded. We are going to calculate computation time and accuracy, similar to what was done in the previous exercise. Since the dimensionality of this dataset is much smaller, we can train a random forest using 500 trees in less time. The randomForest package is already loaded.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_glrm <- h2o.glrm(training_frame = fashion_mnist.hex, \n                       transform = \"NORMALIZE\",\n                       cols = 2:ncol(fashion_mnist), \n                       k = 8, \n                       seed = 123,\n                       max_iterations = 1000)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\ntrain_x <- as.data.table(h2o.getFrame(model_glrm@model$representation_name))\ntrain_y <- fashion_mnist$label %>% as.factor()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\n# Get the starting timestamp\ntime_start <- proc.time()\n\n# Train the random forest\nrf_model <- randomForest(x = train_x, y = train_y, ntree = 500)\n\n# Get the end timestamp\ntime_end <- timetaken(time_start)\n\n\n# Show the error and the time\nrf_model$err.rate[500]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.259\n```\n:::\n\n```{.r .cell-code}\ntime_end\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"0.320s elapsed (0.315s cpu)\"\n```\n:::\n:::\n",
    "supporting": [
      "introduction_dimensionality_reduction_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}