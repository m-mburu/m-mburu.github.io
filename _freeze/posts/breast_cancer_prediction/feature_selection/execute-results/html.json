{
  "hash": "dc813a7a110715a15be02a3610594ffa",
  "result": {
    "markdown": "---\ntitle: \"Feature Selection Comparison\"\nauthor: ''\ndate: \"9/22/2020\"\noutput:\n  word_document: default\n  pdf_document: default\n  html_document: default\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(knitr)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(ggthemes)\n\ncancer <- fread(\"data.csv\")\n\ncancer[, V33 := NULL]\ncancer[, diagnosis := factor(diagnosis)]\nnms <- names(cancer)\nnms <- gsub(\" \", \"_\", nms)\nnames(cancer) <- nms\nstr(cancer)\ncancer[, id := NULL]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncancer[, .(freq = .N),\n       by = diagnosis] %>% \n    .[, perc := round(100 * freq/sum(freq), 2)] %>%\n  \nggplot(aes(x=diagnosis, y=perc, fill = diagnosis)) + \n    geom_bar(stat = \"identity\", width  = 0.5)+ theme_hc() +\n    geom_text(aes(x=diagnosis, y=perc, label = paste(perc, \"%\")),\n              position =  position_dodge(width = 0.5),\n              vjust = 0.05, hjust = 0.5, size = 5)+\n    scale_fill_hc(name = \"\")+\n    labs(x = \"Cancer Type\",\n         y = \"Percentage\",\n         title = \"Percentage of women with benign or malignant breast bancer\")+\n    theme(legend.position = \"none\",\n          axis.title = element_text(size =12))\n```\n:::\n\n\n## Test train\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(100)\ntrain_sample <- sample(1:nrow(cancer), round(0.7*nrow(cancer)))\ntrain_set <- cancer[train_sample,]\ntest_set <- cancer[-train_sample,]\n```\n:::\n\n\n## Fit model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nglm_mod <- glm(diagnosis ~ .,\n               data = train_set, \n               family = binomial())\n\n\ntidy(glm_mod) %>% kable\n```\n:::\n\n\n## Forward\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforward_select <- step(glm_mod, direction = \"forward\")\n```\n:::\n\n\n## Backward\n\n\n::: {.cell}\n\n```{.r .cell-code}\nback_select <- step(glm_mod, direction = \"backward\")\n```\n:::\n\n\n## Using Entropy-Based Feature Selection Algorithms\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(FSelectorRcpp)\nx <- information_gain(diagnosis ~ ., train_set)\nx %>% arrange(desc(importance)) %>%\n  kable()\n```\n:::\n\n\n## Recursive Feature Elimination (RFE)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nctrl <- rfeControl(functions = rfFuncs,\n                   method = \"repeatedcv\",\n                   repeats = 5,\n                   verbose = FALSE)\n\nlmProfile <- rfe(diagnosis ~ ., \n                 data = train_set,\n                 rfeControl = ctrl)\n\nlmProfile\n\nlmProfile$optVariables\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvar\n```\n:::\n\n\n## Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv_fold <- createFolds(train_set$diagnosis, k = 5)\n\ntrain_ctrl <- trainControl(method = \"cv\",\n                        number = 5,\n                        summaryFunction = twoClassSummary,\n                        classProbs = TRUE,\n                        allowParallel=T,\n                        index = cv_fold,\n                        verboseIter = FALSE,\n                        savePredictions = TRUE,\n                        search = \"grid\")\nglm_grid <- expand.grid(\n                       alpha = 0:1,\n                       lambda = seq(0.0001, 1, length = 10)\n                      )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_model <- train(\n    diagnosis~.,\n    data = train_set,\n    method = \"glmnet\",\n    metric = \"ROC\",\n    trControl = train_ctrl,\n    tuneGrid = glm_grid\n)\n\nfull_model\n```\n:::\n\n\n## Forward model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforward_model <- train(\n    forward_select$formula,\n    data = train_set,\n    method = \"glmnet\",\n    metric = \"ROC\",\n    trControl = train_ctrl,\n    tuneGrid = glm_grid\n)\n\nforward_model\n```\n:::\n\n\n## Fit model with variables selected from backward selection\n\n\n::: {.cell}\n\n```{.r .cell-code}\nback_model <- train(\n    back_select$formula,\n    data = train_set,\n    method = \"glmnet\",\n    metric = \"ROC\",\n    trControl = train_ctrl,\n    tuneGrid = glm_grid\n)\n\nback_model\n```\n:::\n\n\n## Fit model with variables selected from backward selection\n\n\n::: {.cell}\n\n```{.r .cell-code}\nback_model <- train(\n    back_select$formula,\n    data = train_set,\n    method = \"glmnet\",\n    metric = \"ROC\",\n    trControl = train_ctrl,\n    tuneGrid = glm_grid\n)\n\nback_model\n```\n:::\n\n\n## Fit model with variables selected from entropy\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsetDT(x)\n#selector predictors with importance of more than 0.05\npredictors <- x[importance > 0.05, attributes]\n\nentropy_predctors <- train_set[, ..predictors]\nentropy_y <- train_set$diagnosis\nentropy_model <- train(\n    entropy_predctors,\n    entropy_y,\n    method = \"glm\",\n    metric = \"ROC\",\n    trControl = train_ctrl\n)\n\nentropy_model \n```\n:::\n\n\n## Fit model with variables selected Recursive Feature Elimination\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecu_pred <- lmProfile$optVariables\nrecursive_predctors <- train_set[, ..recu_pred]\nrecursive_y <- train_set$diagnosis\nrecu_model <- train(\n    recursive_predctors,\n    recursive_y,\n    method = \"glm\",\n    metric = \"ROC\",\n    trControl = train_ctrl\n)\n\nrecu_model \n```\n:::\n\n\n## Full model test accuracy\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor_glm <- predict(full_model, test_set, type = \"prob\")\n\n\nfor_glm1 <- ifelse(for_glm[, \"M\"] > 0.5, \"M\", \"B\")\nfor_glm1 <- factor(for_glm1, levels = levels(test_set$diagnosis))\n\n\n\nconfusionMatrix(for_glm1, test_set$diagnosis,positive = \"M\") \n```\n:::\n\n\n## Forward test accuracy\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor_glm <- predict(forward_model, test_set, type = \"prob\")\n\n\nfor_glm1 <- ifelse(for_glm[, \"M\"] > 0.5, \"M\", \"B\")\nfor_glm1 <- factor(for_glm1, levels = levels(test_set$diagnosis))\n\n\n\nconfusionMatrix(for_glm1, test_set$diagnosis,positive = \"M\") \n```\n:::\n\n\n## Backward test accuracy\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor_glm <- predict(back_model, test_set, type = \"prob\")\n\n\nfor_glm1 <- ifelse(for_glm[, \"M\"] > 0.5, \"M\", \"B\")\nfor_glm1 <- factor(for_glm1, levels = levels(test_set$diagnosis))\n\n\n\nconfusionMatrix(for_glm1, test_set$diagnosis,positive = \"M\") \n```\n:::\n\n\n## entropy method test accuracy\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor_glm <- predict(entropy_model, test_set, type = \"prob\")\n\n\nfor_glm1 <- ifelse(for_glm[, \"M\"] > 0.5, \"M\", \"B\")\nfor_glm1 <- factor(for_glm1, levels = levels(test_set$diagnosis))\n\n\n\nconfusionMatrix(for_glm1, test_set$diagnosis,positive = \"M\") \n```\n:::\n\n\n## Recursive Feature Elimination method test accuracy\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor_glm <- predict(recu_model, test_set, type = \"prob\")\n\n\nfor_glm1 <- ifelse(for_glm[, \"M\"] > 0.5, \"M\", \"B\")\nfor_glm1 <- factor(for_glm1, levels = levels(test_set$diagnosis))\n\n\n\nconfusionMatrix(for_glm1, test_set$diagnosis,positive = \"M\") \n```\n:::\n",
    "supporting": [
      "feature_selection_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}