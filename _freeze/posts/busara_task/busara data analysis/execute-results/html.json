{
  "hash": "94cfa24bb36f4f099f49ef6a65c8807a",
  "result": {
    "markdown": "---\ntitle: \"Busara Data Analysis\"\nauthor: \"mburu\"\ndate: \"February 16, 2019\"\noutput:\n  html_document:\n    toc: yes\n    toc_depth: 2\n    toc_float:\n      collapsed: no\n      smooth_scroll: no\n    highlight: pygments\n    css: style.css\n  pdf_document:\n    toc: yes\n    toc_depth: '2'\n  word_document:\n    toc: yes\n    toc_depth: '2'\ncategories: [news, code, analysis]\n---\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(data.table)\nlibrary(knitr)\nlibrary(ggthemes)\n#library(kableExtra)\n```\n:::\n\n\n-   I was not sure if the presentation is about insights from the data or how I solved the problem.\n-   I decided to to combine both with R presentation.\n\n# ***Task 1***\n\n-   Understanding the demographics of company xyz.\n\n-   Atleast half are youth average age = 33.5\n\n-   Atleast half earn 5557\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxyz <- setDT(read_csv(\"XYZ.csv\"))\n\nxyz_sub <- xyz[, .(Gender, Age, Income)]\n\nxyz_subm <- melt(xyz_sub, id.vars = \"Gender\")\n```\n:::\n\n\n# ***Summary Statistics Age and Income***\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxyz_subm %>% group_by(variable) %>%\n    summarise(Average = mean(value), Median = median(value),\n              Min = min(value), Max = max(value)) %>%\n    \n    kable() #%>% kable_styling() %>%\n```\n\n::: {.cell-output-display}\n|variable |  Average| Median|  Min|  Max|\n|:--------|--------:|------:|----:|----:|\n|Age      |   33.506|     33|   18|   50|\n|Income   | 5498.844|   5557| 1000| 9897|\n:::\n\n```{.r .cell-code}\n    #scroll_box(width = \"100%\", height = \"30%\")\n```\n:::\n\n\n# ***% gender Gap***\n\n-   Male/Female % al most equal\n-   There are 5.2% more men than women\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngender <- xyz %>% group_by(Gender) %>%\n    summarise(freq = n()) %>%\n    mutate(Perc = round(freq/sum(freq) * 100, 2))\n\ngender %>% kable() #%>% kable_styling() %>%\n```\n\n::: {.cell-output-display}\n|Gender | freq| Perc|\n|:------|----:|----:|\n|Female |  237| 47.4|\n|Male   |  263| 52.6|\n:::\n\n```{.r .cell-code}\n    #scroll_box(width = \"100%\", height = \"30%\")\n```\n:::\n\n\n# ***% gender Gap***\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngender[, -2] %>% spread(Gender, Perc) %>% \n    mutate(Percentage_Gender_Gap = Male - Female) %>% \n    kable() #%>% kable_styling() %>%\n```\n\n::: {.cell-output-display}\n| Female| Male| Percentage_Gender_Gap|\n|------:|----:|---------------------:|\n|   47.4| 52.6|                   5.2|\n:::\n\n```{.r .cell-code}\n    #scroll_box(width = \"100%\", height = \"30%\")\n```\n:::\n\n\n\\#***Single Ladies Nyeri***\n\n-   12(2.4% of the company employees) single ladies from Nyeri county\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsingle_nyeri <- xyz[Gender == \"Female\" & Marital_Status == \"Single\" & County == \"Nyeri\",]\nnrow(xyz)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 500\n```\n:::\n\n```{.r .cell-code}\ncat(\"The Number of single ladies in Nyeri is \", nrow(single_nyeri))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe Number of single ladies in Nyeri is  12\n```\n:::\n:::\n\n\n# ***Summary Statistics Single Ladies Nyeri***\n\n-   Average age 36 and medium income is about \\$50\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsingle_nyeri %>%\n    summarise(Average_Age = mean(Age), Median_Income = median(Income)) %>%\n    kable() #%>%  kable_styling() %>%\n```\n\n::: {.cell-output-display}\n| Average_Age| Median_Income|\n|-----------:|-------------:|\n|          36|          5557|\n:::\n\n```{.r .cell-code}\n    #scroll_box(width = \"100%\", height = \"30%\")\n```\n:::\n\n\n# ***Number of Juniors***\n\n-   28 juniors\n\n\n::: {.cell}\n\n```{.r .cell-code}\njuniors_26 <-xyz[!grepl(\"Operartions|Data\",Department)  & xyz$Age < 26 & grepl(\"Junior\", Role),]\n\ncat(\"The Number of juniors \", nrow(juniors_26))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe Number of juniors  28\n```\n:::\n\n```{.r .cell-code}\njuniors_26 %>% group_by(Department) %>%\n    summarise(freq = n()) %>%\n    mutate(Perc = round(freq/sum(freq) * 100, 2)) %>%\n    kable() #%>% kable_styling() %>%\n```\n\n::: {.cell-output-display}\n|Department       | freq|  Perc|\n|:----------------|----:|-----:|\n|Associate        |    5| 17.86|\n|Finance          |   11| 39.29|\n|Operations       |    8| 28.57|\n|Research Analyst |    4| 14.29|\n:::\n\n```{.r .cell-code}\n    #scroll_box(width = \"100%\", height = \"40%\")\n```\n:::\n\n\n# ***Difference in mean income between male and female***\n\n-   The Operations has the biggest difference in mean income\n-   Female/Males average earnings in different departments\n\n\n::: {.cell}\n\n```{.r .cell-code}\nincome_gender <- xyz %>% group_by(Gender, Department) %>%\n    summarise(Average = mean(Income))\n\n\nincome_gender_dcast <- dcast(Department ~ Gender, data = income_gender) \n\nincome_gender_dcast %>% mutate( Difference = Male - Female) %>%\n    kable()#%>% kable_styling() %>%\n```\n\n::: {.cell-output-display}\n|Department       |   Female|     Male| Difference|\n|:----------------|--------:|--------:|----------:|\n|Associate        | 5345.047| 5071.941|  -273.1053|\n|Data             | 5613.420| 5270.396|  -343.0238|\n|Finance          | 5574.714| 5936.750|   362.0357|\n|Operations       | 5043.286| 6284.854|  1241.5685|\n|Research Analyst | 5264.522| 5533.327|   268.8055|\n:::\n\n```{.r .cell-code}\n    #scroll_box(width = \"100%\", height = \"45%\")\n```\n:::\n\n\n# ***Function to plot categorical variables***\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbar_plot <- function(data, title,...) {\n    #load ggplot2\n    #function takes a data frame\n    #and other arguments that ggplot\n    #function from ggplot2 takes\n    # the other arguments are aesthetic mappings\n    require(ggplot2)\n    ggplot(data) + geom_bar(aes(...))+\n        ggtitle(title)+\n        ggthemes::theme_hc()+\n        ggthemes::scale_fill_hc()+\n        theme(legend.position = \"none\")\n        \n}\n```\n:::\n\n\n# ***Function to plot categorical variables test 1***\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbar_plot(xyz, Department, title = \"Department Distribution\", fill = Department)\n```\n\n::: {.cell-output-display}\n![](busara-data-analysis_files/figure-html/unnamed-chunk-12-1.png){width=624}\n:::\n:::\n\n\n# ***Function to plot categorical variables test 2***\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbar_plot(xyz, Gender, title = \"Gender Distibution\", fill = Gender)\n```\n\n::: {.cell-output-display}\n![](busara-data-analysis_files/figure-html/unnamed-chunk-13-1.png){width=624}\n:::\n:::\n\n\n# ***Task 2***\n\n***Read Files***\n\n-   Read files using the patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_files <- dir(path = \"Education\",pattern = \"^Chi|^Sch|^Persi|Secon|^Progr|Pri\")\n\nmy_files <- paste0(\"Education/\", my_files)\n\nlibrary(readxl)\n\nlist_files <- list()\n\nfor (i in 1:length(my_files)) {\n    \n    \n    x = read_excel(my_files[i]) \n    id = grep(\"Country Name\", x$`Data Source`)\n    nms <- x[id,]\n    names(x) <- nms %>% as.character()\n    list_files[[i]] <- x[-c(1:id),] \n    cat(\"...\")\n    \n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n......................................................\n```\n:::\n:::\n\n\n# Combine Files\n\n-   Since files are stored in a list combine them\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_world <- rbindlist(list_files) %>% setDT()\n\ndf_world_melt <- melt(df_world, id.vars = names(df_world)[1:4])\n\nnms2 <- Hmisc::Cs(Country_Name,\tCountry_Code,\t\n          Indicator_Name,\tIndicator_Code,\tYear,\tIndicator_value)\n\nnames(df_world_melt) <- nms2\n\ndf_world_melt[,  Year := as.numeric(as.character(df_world_melt$Year))]\n```\n:::\n\n\n# ***Head output data frame***\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(df_world_melt) %>% kable() #%>% \n```\n\n::: {.cell-output-display}\n|Country_Name |Country_Code |Indicator_Name                          |Indicator_Code | Year|Indicator_value |\n|:------------|:------------|:---------------------------------------|:--------------|----:|:---------------|\n|Aruba        |ABW          |Children out of school, primary, female |SE.PRM.UNER.FE | 1960|NA              |\n|Afghanistan  |AFG          |Children out of school, primary, female |SE.PRM.UNER.FE | 1960|NA              |\n|Angola       |AGO          |Children out of school, primary, female |SE.PRM.UNER.FE | 1960|NA              |\n|Albania      |ALB          |Children out of school, primary, female |SE.PRM.UNER.FE | 1960|NA              |\n|Andorra      |AND          |Children out of school, primary, female |SE.PRM.UNER.FE | 1960|NA              |\n|Arab World   |ARB          |Children out of school, primary, female |SE.PRM.UNER.FE | 1960|NA              |\n:::\n\n```{.r .cell-code}\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n```\n:::\n\n\n# ***Head output kenya data***\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkenya_2011 <- df_world_melt[Country_Name == \"Kenya\" & Year >= 2011]\n\nhead(kenya_2011) #%>% kable() %>%\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Country_Name Country_Code\n1:        Kenya          KEN\n2:        Kenya          KEN\n3:        Kenya          KEN\n4:        Kenya          KEN\n5:        Kenya          KEN\n6:        Kenya          KEN\n                                               Indicator_Name    Indicator_Code\n1:                    Children out of school, primary, female    SE.PRM.UNER.FE\n2: Persistence to last grade of primary, female (% of cohort) SE.PRM.PRSL.FE.ZS\n3:   Persistence to last grade of primary, male (% of cohort) SE.PRM.PRSL.MA.ZS\n4:  Primary completion rate, female (% of relevant age group) SE.PRM.CMPT.FE.ZS\n5:    Primary completion rate, male (% of relevant age group) SE.PRM.CMPT.MA.ZS\n6:                Progression to secondary school, female (%) SE.SEC.PROG.FE.ZS\n   Year Indicator_value\n1: 2011            <NA>\n2: 2011            <NA>\n3: 2011            <NA>\n4: 2011            <NA>\n5: 2011            <NA>\n6: 2011            <NA>\n```\n:::\n\n```{.r .cell-code}\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"70%\")\n```\n:::\n\n\n# ***Head output kenya data and saving files***\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite.csv(head(kenya_2011, 15), file = \"kenya data.csv\", row.names = F)\n\nkenya_2011_na <- kenya_2011[!is.na(kenya_2011$Indicator_value),]\nwrite.csv(head(kenya_2011_na, 15), file = \"kenya data without na.csv\", row.names = F)\n\nhead(kenya_2011_na) %>% kable() #%>%\n```\n\n::: {.cell-output-display}\n|Country_Name |Country_Code |Indicator_Name                                                |Indicator_Code    | Year|Indicator_value    |\n|:------------|:------------|:-------------------------------------------------------------|:-----------------|----:|:------------------|\n|Kenya        |KEN          |Children out of school, primary, female                       |SE.PRM.UNER.FE    | 2012|537736             |\n|Kenya        |KEN          |School enrollment, primary (gross), gender parity index (GPI) |SE.ENR.PRIM.FM.ZS | 2012|1.0080599784851101 |\n|Kenya        |KEN          |School enrollment, primary, female (% gross)                  |SE.PRM.ENRR.FE    | 2012|112.41464233398401 |\n|Kenya        |KEN          |School enrollment, primary, male (% gross)                    |SE.PRM.ENRR.MA    | 2012|111.51609802246099 |\n|Kenya        |KEN          |Primary completion rate, female (% of relevant age group)     |SE.PRM.CMPT.FE.ZS | 2014|100.183967590332   |\n|Kenya        |KEN          |Primary completion rate, male (% of relevant age group)       |SE.PRM.CMPT.MA.ZS | 2014|98.815101623535199 |\n:::\n\n```{.r .cell-code}\n   # kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"70%\")\n```\n:::\n\n\n\\#***Task 3***\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfigari_sheet1 <- read_excel(\"Figari Bank.xlsx\" ) %>% setDT()\n\nfigari_sheet2 <- read_excel(\"Figari Bank.xlsx\", sheet = 2 ) %>% setDT()\n\nfigari_sheet2[,  Dates := as.Date(Dates, origin = \"1900-01-01\")]\n\nfigari_sheet2[,  year := year(Dates)]\n\n\nfigari_sheet2[,  month := month(Dates)]\nfigari_sheet2[, month := ifelse(nchar(month) == 1 ,paste(0, month), month)]\n\nfigari_sheet2[,  week_day := as.POSIXlt(Dates)$wday+1]\nfigari_sheet2[, week_day := ifelse(nchar(week_day) == 1 ,paste(0, week_day), week_day)]\nfigari_sheet2[,  week_no := week(Dates)]\n\nfigari_sheet2[, week_no := ifelse(nchar(week_no) == 1 ,paste(0, week_no), week_no)]\n\nfigari_sheet2[,  day_month := format(Dates, \"%d\")]\n\nfigari_sheet2_m <- melt(figari_sheet2[, c(3:9), with = F], id.vars = c(\"Amount\", \"Saving Mode\"))\n```\n:::\n\n\n# Task 3 Plots\n\n-   Time series will enable us too see if there is seasonal/cyclic effects/trend\n-   week number after every two weeks, maybe end month\n-   Smoothing/decoposing often needed to see trend\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfigari_dat <- figari_sheet2_m %>% group_by(`Saving Mode`,variable, value) %>%\n    summarise(Average = mean(Amount)) \ntitles <- levels(as.factor(figari_dat$`Saving Mode`))\ntitles <- paste(\"Average Savings for\", titles)\nfigari_dat_split <- split(figari_dat, figari_dat$`Saving Mode`)\nplots_figari <- list()\nfor ( i in 1:length(figari_dat_split)) {\n    this = figari_dat_split[[i]]\n    #write.csv(this, file = \"this.csv\", row.names = F)\n   plots_figari[[i]] <- ggplot(this, aes(value, Average)) +\n       facet_wrap(~variable, scales = \"free_x\", ncol = 1)+\n       geom_line(data = this, aes(value, Average, group = 1)) +\n       ggthemes::theme_hc()+\n       labs(x = \"\", y = \"Average amount saved (KES)\", title = titles[i])# +\n       #theme(axis.text.x = element_text(angle = 30, vjust = 1, hjust = 1))\n    \n}\n```\n:::\n\n\n# ***Average Amount saved at the Bank***\n\n-   Year no visible trend/ few years\n-   first two months higher savings\n-   lowest between month 3 to month 8\n-   Save more from day 2 - day 4\n-   Week 1- 3 more savings drops to week 10\n-   more save less in around day 10 of the month\n-   decreasing trend trend from day 4 -10\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplots_figari[[2]]\n```\n\n::: {.cell-output-display}\n![](busara-data-analysis_files/figure-html/unnamed-chunk-21-1.png){width=1152}\n:::\n:::\n\n\n# ***Average Amount saved at the Agent***\n\n-   Save more from March to June\n-   Increasing trend from week 1 to 23 then decreasing\n-   Save less towards end of a month\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplots_figari[[1]]\n```\n\n::: {.cell-output-display}\n![](busara-data-analysis_files/figure-html/unnamed-chunk-22-1.png){width=1152}\n:::\n:::\n\n\n# ***Average Amount saved Mobile money***\n\n-   on average\n-   save less from month 3 to 6\n-   save less from week 9 to 22\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplots_figari[[3]]\n```\n\n::: {.cell-output-display}\n![](busara-data-analysis_files/figure-html/unnamed-chunk-23-1.png){width=1152}\n:::\n:::\n\n\n# ***End Month Savings Favourite tool***\n\n-   I'm thinking about the number of times someone saves. Average maybe skewed.\n-   Women prefer to save using agent\n-   In regions no Nyeri\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(figari_sheet2)[1] = names(figari_sheet1)[1]\n\nfigari_comb <- merge(figari_sheet2, figari_sheet1, by = \"CustomerID\")\n\nend_month <- figari_comb %>% \n    group_by(day_month, `Saving Mode`) %>%\n    summarise(Freq = n()) %>%\n    mutate(perc = round(100 * Freq/sum(Freq), 2)) %>% ungroup()\n#The number of times one deposits\nggplot(end_month, aes(day_month, Freq )) +\n    geom_line(aes(color =`Saving Mode`, group =`Saving Mode` ), size = 1)+\n    theme_hc()+\n    scale_color_hc(name = \"\") +\n    theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](busara-data-analysis_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n# Histogram Deposits\n\n-   What you would expect.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndeposits <- figari_sheet2[, .(freq = .N), by = CustomerID] \n\n#approximmately poison\nhist(deposits$freq, col = \"black\",\n     main = \"Deposits\", \n     xlab = \"Deposits\")\n```\n\n::: {.cell-output-display}\n![](busara-data-analysis_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n# ***Subset People who have made one deposit***\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfigari_deposits <- merge(deposits, figari_sheet1, by = \"CustomerID\")\nfigari_deposits_one <- figari_deposits[freq == 1] \n```\n:::\n\n\n# ***Demographic characteristics of those who have only made one deposit***\n\n# ***Gender***\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfigari_deposits_one %>% group_by(Gender) %>%\n    summarise(freq= n()) %>%\n    mutate(Perc =round(freq/sum(freq) *100, 2) ) %>%\n    kable()# %>%\n```\n\n::: {.cell-output-display}\n|Gender | freq| Perc|\n|:------|----:|----:|\n|Female |  141| 49.3|\n|Male   |  145| 50.7|\n:::\n\n```{.r .cell-code}\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n```\n:::\n\n\n# ***Region***\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfigari_deposits_one %>% group_by(Region) %>%\n    summarise(freq= n()) %>% \n    mutate(Perc =round(freq/sum(freq) *100, 2) ) %>%\n    kable() #%>%\n```\n\n::: {.cell-output-display}\n|Region     | freq|  Perc|\n|:----------|----:|-----:|\n|Bondo      |   25|  8.74|\n|Gatitu     |   53| 18.53|\n|Kawangware |   21|  7.34|\n|Kayole     |   13|  4.55|\n|Kibera     |   31| 10.84|\n|Kilimani   |   41| 14.34|\n|Kirinyaga  |   18|  6.29|\n|Rongai     |   10|  3.50|\n|Ruai       |   45| 15.73|\n|Taita      |   29| 10.14|\n:::\n\n```{.r .cell-code}\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"100%\")\n```\n:::\n\n\n# ***Age***\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfigari_deposits_one %>% \n    summarise(Mean= round(mean(Age), 2), Median  = median(Age)) %>%\n    kable() #%>%\n```\n\n::: {.cell-output-display}\n|  Mean| Median|\n|-----:|------:|\n| 54.71|     56|\n:::\n\n```{.r .cell-code}\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n```\n:::\n\n\n# ***Task 4***\n\n# ***Project Motivation***\n\nData has the potential to transform business and drive the creation of business value. It can be used for a range of tasks such visualization relationships between variables to predicting if an event will occur. The later is one of the heavily reaserched areas in recent times. The reason for this is that data has grown exponentially and so does the computing power. Banks and financial institutions used data analytics for a range of value such as fraud detetction customer segment, recruiting, credit scoring and so on.\n\nIn this study I will use Bogoza data set to build a credit model where an applicat will be avaluated on whether they will default or not.\n\nHigh accuracy for this model will be required because predicting false positives will eventually cause a business to make a loss and false negatives means that the financial instituion looses business.\n\n# ***Data Cleaning***\n\nFirst step is data cleaning. This ensures that columns are consistent. For instance the target variable had values such as Y y yes where all of them represent yes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#some algorithms like xgboost take numeric data\n#you can convert binary vars to 1,0\n# and form dummie variables using library dummies\n#for variables with more than 2 categories\nborogoza <- setDT(read_csv( \"Bagorogoza Loan.csv\"))\n\nborogoza[, Target := ifelse(grepl(\"y|Y\", Target), 1, 0)]\n\nborogoza[, Gender := ifelse(grepl(\"^m$|^male$\", tolower(Gender)), 0, 1)]\n\nborogoza[, Married := ifelse(grepl(\"Yes\",Married), 1, 0)]\n\nborogoza[, Education := ifelse(grepl(\"not\", tolower(Education)), 0, 1)]\n\nborogoza[, Self_Employed := ifelse(grepl(\"Yes\",Self_Employed), 1, 0)]\n\nborogoza[, Property_Area := ifelse(grepl(\"rural\",tolower(Property_Area)), \"Rural\", Property_Area)]\n\nborogoza[, Property_Area := ifelse(grepl(\"semi\",tolower(Property_Area)), \"Semi-urban\", Property_Area)]\n\nborogoza[, Property_Area := ifelse(grepl(\"^urban$\",tolower(Property_Area)), \"Urban\", Property_Area)]\n```\n:::\n\n\n# ***Variable Selection***\n\n-   Where we run descripte statistics\n\n# ***Visualize Categorical variables***\n\nVisualization and summary statistics is an impostant step before fitting any model as this will give you a glimpse of how the variables are associated with target variable. In this case I will use stacked barplot as from them you can see if the prorpotions of defaulters and non defaulters is equal in defferent categories of a variable. From the graphs we can see that the prorpotion of defaulters and non defaulters is defferent for the different credit history categories. This is aslo seen in the prorpety area. From the categorical variables we can therefore conclude that one of the best predictors is credit history.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnumeric_vars <- Hmisc::Cs(ApplicantIncome,CoapplicantIncome, LoanAmount )\n\nnms_bo <- names(borogoza)[-1]\n\ncat_vars <- nms_bo[!nms_bo %in% numeric_vars]\n\n\nborogoza_catm <- melt(borogoza[, cat_vars, with = F], id.vars = \"Target\")\n\nborogoza_catm_perc <-borogoza_catm  %>%  group_by(variable, value, Target) %>%\n    summarise(freq= n()) %>% mutate(perc =round(freq/sum(freq) *100, 2) )\n\nlibrary(ggthemes)\nggplot(borogoza_catm_perc, aes(value, perc, fill = factor(Target) )) +\n    geom_bar(stat = \"identity\") +facet_wrap(~variable, scales = \"free_x\")+\n    scale_fill_economist(name = \"\")+\n    theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](busara-data-analysis_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\n# ***Visualize numeric variables***\n\nFor the numeric variables boxplot help us visualize which distribution is different from the other. Non overlapping boxplot for defaulters and non defaulters may indicate that the mean/median values in the two groups was significantly different. From this we can see that it's unlikely that education and self employment affect loan repayment and for this we drop this two variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nborogoza_numm <- melt(borogoza[, c(numeric_vars, \"Target\"), with = F], id.vars = \"Target\")\n\nggplot(borogoza_numm, aes(as.factor(Target), scale(value ))) +\n    geom_boxplot() +facet_wrap(~variable, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](busara-data-analysis_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n# ***One-Hot Encoding for categorical variables with more than 2 levels***\n\nIn this step variables with more than two categories are converted to dummies variables. The first column in each category is dropped as it's linearly depedent with the second column.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchars <- unlist(lapply(borogoza[, -1, with = F], is.character)) \n\nchars <- nms_bo[chars]\n\nlibrary(dummies)\nborogoza_dummy <- dummy.data.frame(borogoza, names = c(chars, \"Loan_Amount_Term\")) %>%\n    setDT()\n\n\nborogoza_dummy[, Loan_ID := NULL]\nborogoza_dummy[, Loan_Amount_Term36 := NULL]\nborogoza_dummy[, `Property_AreaSemi-urban` := NULL]\nborogoza_dummy[, `Dependents1` := NULL]\n```\n:::\n\n\n# ***Scale variables***\n\nIt's important to scale your variables since it leads to faster convergence and since some algorithm use distances to find decision boundary this means that variables with big values will have a big influence.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxvars <- names(borogoza_dummy)[!names(borogoza_dummy) %in% \"Target\"]\nborogoza_dummy[, (xvars) := lapply(.SD, function(x) scale(x)), .SDcols = xvars ]\n```\n:::\n\n\n# ***Split test and train sets***\n\nThis is important as it helps evaluate your model on data it has never seen. The model will be trained on one set(training set) and tested using test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(200) # for reproducibility\ntrain_sample <- sample(1:nrow(borogoza_dummy), round(0.7*nrow(borogoza_dummy)))\n\ntrain <- borogoza_dummy[train_sample,]\ntest <- borogoza_dummy[-train_sample,]\n```\n:::\n\n\n# ***Fit Logistic Regression***\n\nLogistic regression was fit to predict the probability of someone defaulting. The advantages of logistic regression is interprettable, ie you can see the association between a predictor and response value, it also gives a probability. This is very improtant when you want to have your own cut off point eg you want to label someone as a defulter if you the predicted probability is more than 0.7. This increases precision but lowers recall. Using stepwise selection the model was used to select the variables that best predict loan deafult.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_glm <- glm(Target ~ Married + CoapplicantIncome + Loan_Amount_Term60 + \n    Loan_Amount_Term180 + Loan_Amount_Term300 + \n     Loan_Amount_Term360 + Credit_History + Property_AreaRural + \n     Property_AreaUrban ,data = train, family = binomial)\n\nborogoza_dummy <- borogoza_dummy[, .(Target,Married , CoapplicantIncome , Loan_Amount_Term60 , \n     Loan_Amount_Term180 , Loan_Amount_Term300 , \n     Loan_Amount_Term360 , Credit_History , Property_AreaRural , \n     Property_AreaUrban)]\n\n\ntrain <- borogoza_dummy[train_sample,]\ntest <- borogoza_dummy[-train_sample,]\nsummary(fit_glm) %>% xtable::xtable() %>% kable()# %>%\n```\n\n::: {.cell-output-display}\n|                    |   Estimate| Std. Error|    z value| Pr(>&#124;z&#124;)|\n|:-------------------|----------:|----------:|----------:|------------------:|\n|(Intercept)         |  1.1415592|  5.3056959|  0.2151573|          0.8296447|\n|Married             |  0.1436465|  0.1721719|  0.8343199|          0.4041008|\n|CoapplicantIncome   | -0.1281571|  0.1406808| -0.9109779|          0.3623070|\n|Loan_Amount_Term60  |  1.1450622| 73.3785540|  0.0156049|          0.9875496|\n|Loan_Amount_Term180 |  0.5524587|  0.2784012|  1.9843977|          0.0472115|\n|Loan_Amount_Term300 |  0.0344328|  0.1576171|  0.2184586|          0.8270718|\n|Loan_Amount_Term360 |  0.5348545|  0.2558935|  2.0901448|          0.0366048|\n|Credit_History      |  1.4385841|  0.2260642|  6.3636094|          0.0000000|\n|Property_AreaRural  | -0.4079017|  0.2058167| -1.9818693|          0.0474939|\n|Property_AreaUrban  | -0.4665362|  0.2048849| -2.2770650|          0.0227823|\n:::\n\n```{.r .cell-code}\n    # kable_styling() %>%\n    # scroll_box(width = \"100%\", height = \"100%\")\n\n#MASS::stepAIC(fit_glm)\n```\n:::\n\n\nThe estimate column shows the log odds. Positive values means that the variable makes it more likely for a person to repay their loan negative values means that the person is less likely to repay.\n\n# ***Confusion Matrix Logistic regression***\n\nThe confusion matrix evaluate correctly classified cases. A perfect fit will have all values in the main diagnol while the entries of lower/upper triangulars should be zeros. In this case we have 14 cases of false positives and 7 cases of false negatives the accuracy of the model is 0.82 with and f1 score of 0.87. F1 score is a very important evaluation metric where there is unbalanced classes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\npred_glm <- predict(fit_glm,newdata = test)\n\npred_glm <- ifelse(pred_glm>0.7, 1 , 0)\n\ntable(test$Target, pred_glm) %>% kable()# %>%\n```\n\n::: {.cell-output-display}\n|   |  0|  1|\n|:--|--:|--:|\n|0  | 20| 19|\n|1  |  3| 73|\n:::\n\n```{.r .cell-code}\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n```\n:::\n\n\n# ***Accuracy Logistic regression***\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nlibrary(pROC)\ntable(test$Target, pred_glm) %>% \n    confusionMatrix(positive = \"1\") %>% \n    tidy() %>% kable()# %>%\n```\n\n::: {.cell-output-display}\n|term                 |class |  estimate| conf.low| conf.high|   p.value|\n|:--------------------|:-----|---------:|--------:|---------:|---------:|\n|accuracy             |NA    | 0.8086957| 0.724814| 0.8760546| 0.4628434|\n|kappa                |NA    | 0.5258621|       NA|        NA|        NA|\n|mcnemar              |NA    |        NA|       NA|        NA| 0.0013838|\n|sensitivity          |1     | 0.7934783|       NA|        NA|        NA|\n|specificity          |1     | 0.8695652|       NA|        NA|        NA|\n|pos_pred_value       |1     | 0.9605263|       NA|        NA|        NA|\n|neg_pred_value       |1     | 0.5128205|       NA|        NA|        NA|\n|precision            |1     | 0.9605263|       NA|        NA|        NA|\n|recall               |1     | 0.7934783|       NA|        NA|        NA|\n|f1                   |1     | 0.8690476|       NA|        NA|        NA|\n|prevalence           |1     | 0.8000000|       NA|        NA|        NA|\n|detection_rate       |1     | 0.6347826|       NA|        NA|        NA|\n|detection_prevalence |1     | 0.6608696|       NA|        NA|        NA|\n|balanced_accuracy    |1     | 0.8315217|       NA|        NA|        NA|\n:::\n\n```{.r .cell-code}\n    # kable_styling() %>%\n    # scroll_box(width = \"100%\", height = \"100%\")\n```\n:::\n\n\n# ***Area under curve***\n\nThis is important as it will help you know if the sufferes from high false negatives or false positives. A value greater than 0.8 is normally desired in this case we achieve 0.74.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc(as.numeric(test$Target), pred_glm, print.auc=T, print.auc.y=0.5, levels =0:1 ) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nroc.default(response = as.numeric(test$Target), predictor = pred_glm,     levels = 0:1, print.auc = T, print.auc.y = 0.5)\n\nData: pred_glm in 39 controls (as.numeric(test$Target) 0) < 76 cases (as.numeric(test$Target) 1).\nArea under the curve: 0.7367\n```\n:::\n:::\n\n\n# ***Cross Validation SVM***\n\nNext we fit Support vector machine model. We start by finding the best parameters using cross validation. We use 10 fold this where train set is randomly split into 10 sets. In each cases one of the 1 set is used as a valiadation/test set while the other 9 are used to train the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(e1071)\ntune.out = tune(svm, as.factor(Target)~., data = train, kernel =\"radial\", \n                type =\"C-classification\",\n                ranges =list (cost=c(0.01, 0.1, 1 ,5 ,  10),\n                              gamma = c(0.01,  0.1, 1 ,5 )))\n\nsummary(tune.out)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost gamma\n    5  0.01\n\n- best performance: 0.1595442 \n\n- Detailed performance results:\n    cost gamma     error dispersion\n1   0.01  0.01 0.2747863 0.06706580\n2   0.10  0.01 0.2747863 0.06706580\n3   1.00  0.01 0.1596866 0.06510749\n4   5.00  0.01 0.1595442 0.06956599\n5  10.00  0.01 0.1595442 0.06956599\n6   0.01  0.10 0.2747863 0.06706580\n7   0.10  0.10 0.1967236 0.07767111\n8   1.00  0.10 0.1670940 0.06780670\n9   5.00  0.10 0.1633903 0.06543082\n10 10.00  0.10 0.1633903 0.06543082\n11  0.01  1.00 0.2747863 0.06706580\n12  0.10  1.00 0.2747863 0.06706580\n13  1.00  1.00 0.2041311 0.06754684\n14  5.00  1.00 0.2004274 0.07183672\n15 10.00  1.00 0.2078348 0.07591537\n16  0.01  5.00 0.2747863 0.06706580\n17  0.10  5.00 0.2747863 0.06706580\n18  1.00  5.00 0.2078348 0.06963224\n19  5.00  5.00 0.2152422 0.07506115\n20 10.00  5.00 0.2189459 0.08024003\n```\n:::\n:::\n\n\n# ***Confusion Matrix SVM***\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_svm <- svm(as.factor(Target)~., data = train, cost =5  , gamma = .01, \n               kernel = \"radial\", type =\"C-classification\")\n\n\npred_svm <-predict(fit_svm, newdata = test)\ntable(test$Target, pred_svm) %>% kable() #%>%\n```\n\n::: {.cell-output-display}\n|   |  0|  1|\n|:--|--:|--:|\n|0  | 16| 23|\n|1  |  2| 74|\n:::\n\n```{.r .cell-code}\n   # kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"30%\")\n```\n:::\n\n\n# ***Area under curve***\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc(test$Target, as.numeric(pred_svm), print.auc=T, print.auc.y=0.5, levels =0:1 ) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nroc.default(response = test$Target, predictor = as.numeric(pred_svm),     levels = 0:1, print.auc = T, print.auc.y = 0.5)\n\nData: as.numeric(pred_svm) in 39 controls (test$Target 0) < 76 cases (test$Target 1).\nArea under the curve: 0.692\n```\n:::\n:::\n\n\n# ***Accuracy SVM***\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(test$Target, pred_svm) %>% \n    confusionMatrix(positive = \"1\") %>% \n    tidy() %>% kable() #%>%\n```\n\n::: {.cell-output-display}\n|term                 |class |  estimate|  conf.low| conf.high|   p.value|\n|:--------------------|:-----|---------:|---------:|---------:|---------:|\n|accuracy             |NA    | 0.7826087| 0.6960357| 0.8541027| 0.9685321|\n|kappa                |NA    | 0.4418560|        NA|        NA|        NA|\n|mcnemar              |NA    |        NA|        NA|        NA| 0.0000633|\n|sensitivity          |1     | 0.7628866|        NA|        NA|        NA|\n|specificity          |1     | 0.8888889|        NA|        NA|        NA|\n|pos_pred_value       |1     | 0.9736842|        NA|        NA|        NA|\n|neg_pred_value       |1     | 0.4102564|        NA|        NA|        NA|\n|precision            |1     | 0.9736842|        NA|        NA|        NA|\n|recall               |1     | 0.7628866|        NA|        NA|        NA|\n|f1                   |1     | 0.8554913|        NA|        NA|        NA|\n|prevalence           |1     | 0.8434783|        NA|        NA|        NA|\n|detection_rate       |1     | 0.6434783|        NA|        NA|        NA|\n|detection_prevalence |1     | 0.6608696|        NA|        NA|        NA|\n|balanced_accuracy    |1     | 0.8258877|        NA|        NA|        NA|\n:::\n\n```{.r .cell-code}\n    #kable_styling() %>%\n    #scroll_box(width = \"100%\", height = \"100%\")\n```\n:::\n\n\n# ***Validation Curves***\n\nThe two models almost give equal results based on accuracy, f1 score and area under the curve. In this section we will evaluate the models using learning curves to see if they suffer from high variance or bias. In this case the model sufferes from high bias. It's evident that adding more data won't solve accuracy problems. In this case additional features would help.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsets <- seq(from = 50, to = nrow(train), by = 50)\nsets[length(sets)] <-nrow(train) \ntrain.err <- c()\ntest.err<- c()\nfor (i in 1:length(sets)) {\n    \n    traini = train[1:sets[i],]\n    fit_svm <- svm(as.factor(Target)~., data = traini, cost =5  , gamma = .01, \n               kernel = \"radial\", type =\"C-classification\")\n\n    \n    pred_train = predict(fit_svm, newdata = traini)\n    train.err[i] =1 -  mean(pred_train == traini$Target)\n    pred_test <- predict(fit_svm, newdata = test)\n    test.err[i] = 1 - mean(test$Target == pred_test)\n    \n    cat(i,\" \")\n    \n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1  2  3  4  5  \n```\n:::\n\n```{.r .cell-code}\nmatplot(sets, cbind(test.err, train.err), pch = 19, col = c(\"red\", \"blue\"),\n        type = \"b\", ylab = \"Error\", xlab = \"Train sample size\", main = \"SVM Training and Validation errors\")\nlegend(\"topright\", legend = c(\"Test\", \"Train\"), pch = 19, col = c(\"red\", \"blue\"))\n```\n\n::: {.cell-output-display}\n![](busara-data-analysis_files/figure-html/unnamed-chunk-44-1.png){width=720}\n:::\n:::\n\n\n# ***Deployment***\n\nOther model like Xgboost which uses boosting and bagging could first be used to see if the model performs better on this data. The problem could after this be intergrated with a loan evaluation software where it can help loan officers decide if the will award a loan.\n",
    "supporting": [
      "busara-data-analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}