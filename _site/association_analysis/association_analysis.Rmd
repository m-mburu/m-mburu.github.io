---
title: "Association analysis"
author: "Mburu"
date: "November 27, 2018"
output:
     html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Association Analysis

Maybe you have heard of a grocery store that carried out analysis and found out that men who buy diapers between 5pm to 7pm were more likely to buy beer. The grocery store then moved the beer isle closer to the diaper isle and beer sales increased by 35%. This is called association analysis which was motivated by retail store data. 

In this blog I will explore the basics of association analysis. 
The goal is to find out:

- Items frequently bought together in association analysis this is called support. Let say you have ten transactions and in those ten 3 transactions have maize floor, rice and bread the the support for maize floor, rice and bread is 3/10 = 0.3. This is just marginal probability. In other terms the percentage of transactions these items were bought together.

- In this example the support is written as Support({bread, maize floor} --> {rice} ). In general this is written as Support of item one and item 2 is Support({item1} --> {item 2}). Item 1 and item 2 may contain one or more items.
    
- We also want to find out if someone bought a set of items what other set of item(s) were they likely to buy. In association analysis this is called confidence. In our above example let say that you find the proportion of transactions that contained maize floor and bread are 0.4. Then the confidence is the proportion of those transactions with maize floor, bread and rice/proportion of transactions that contained maize floor and bread. Then the confidence is 0.3/0.4 which is 0.75. In other word 75% of those who bought maize floor and bread also bought rice.
    - Confidence in this example is denoted as Confidence({bread, maize floor} --> {rice} ) and in general this is
      Confidence({item 1} --> {item 2} ).

- The lift refers to how the chances of rice being purchased increased given that maize floor and bread are purchased. So the lift of rice is confidence of rice/support(rice). Support of rice is the number of transactions that contain rice.
    - Lift({Item 1} -> {Item 2 }) = (Confidence(Item1 -> Item2)) / (Support(Item2))

To make sense of all these I'm going to use a bakery to find association rules between items bought manually and then towards the end I will use r package arules which uses apriori algorithm to find association between items bought. The data set is available on kaggle as BreadBasket_DMS. We start by first having a glimpse of this data set. 

```{r}

library(tidyverse)
library(knitr)
library(data.table)
library(DT)

dat <- setDT(read.csv( "BreadBasket_DMS.csv" ))

dat <- dat[Item != "NONE"]
head(dat[sample(1:nrow(dat), 10)]) %>% datatable()


```

# Data Cleaning and Exploratory Data Analysis 
-----

First step is to transform the data set into wide format. Column headers will be items sold in the bakery and the rows will be populated with 1 and 0 indicating whether that item was bought for that transaction. 

```{r}

dat2 <- dcast(Date+Time+Transaction~Item, data = dat, fun.aggregate = length)
#dat2[, NONE := NULL]

sample_cols <- sample(4:ncol(dat2), 5)

item_names <- names(dat2)[4:97]

dat2[, (item_names) := lapply(.SD, function(x) ifelse(x==0, 0, 1)), .SDcols = item_names]

head(dat2[, c(1:3, sample_cols), with = F]) %>% datatable()
```

# How many items each customer buy?
-----
On average a transaction has 2 items. The median is also 2, this shows that atleast 50% of the transactions contained 2 or more items and atleast 25% of the transactions have  1 item.

```{r}

number_items <- rowSums(dat2[, 4:97, with =F])

dat2[, number_items := number_items]

hist(number_items, col = "black", main = "Number of items bought")

sumStats <- dat2 %>%
    summarise(Average = round(mean(number_items), 2), Stdev = round(sd(number_items), 2),
              Median= median(number_items),
              Minimum = min(number_items), Maximum = max(number_items),
              First_qurtile = quantile(number_items,0.25,  na.rm = T),
              Third_qurtile=quantile(number_items,0.75,  na.rm = T))

datatable(sumStats)
```


# Top 10 bought items
-----
Table below shows top ten most bought items and about 47.84% of the transactions contained coffee. Coffee was the most popular item in this bakery followed by bread.

```{r}



n_transacts_item_in <- colSums(dat2[, item_names, with = F])

data.frame(item = names(n_transacts_item_in),
           number =n_transacts_item_in) %>%
    mutate(Percentage = round(number/nrow(dat2)*100, 2)) %>%
    arrange(desc(number)) %>% head(10) %>% datatable()


```

Since we have transformed the data in the wide format and every transaction is in it's row we can visualize how the baskets look like. This is done by extracting the column names for the transactions where the value is 1. For each transaction, 1 represent that item being in that transaction.

```{r}


items_bought <- apply(dat2[, 4:97, with =F], 1, paste, collapse = "", sep = "")

list_items <- vector(mode = "list", length = length(items_bought))
for (i in 1:length(items_bought)) {
    index <- unlist(gregexpr("1", items_bought[i]))
    items_transaction_i <- item_names[index]
    items_transaction_i <- paste(items_transaction_i, sep = " ", collapse = " , ")
    list_items[[i]] <- items_transaction_i
}

head(unlist(list_items))
```

Data frame below shows how the baskets look like. Only 10 randomly selected rows are displayed. Items_bought column shows the baskets.

```{r}


dat2[, items_bought := unlist(list_items) ]

head(dat2[sample(1:nrow(dat2), 10),
          .(Transaction, number_items,items_bought)]) %>%
  datatable()

```


A small example which I will work out manually  to see what is the support for ({coffee, bread} --> {jam}). Generally I want to see how many transactions contained these 3 items. 0.12% of the transactions contained {bread, coffee, jam}

```{r}


my_item_set <- Hmisc::Cs(Coffee , Jam , Bread)

idx_sample <- grep( "Transactio|^Coffee$|^Jam$|^Bread$", names(dat2))



item_set_dat <- dat2[, idx_sample, with = F] 

#some transaction bought more thanone of 
item_set_dat[, (my_item_set) := lapply(.SD, function(x) ifelse(x==0, 0, 1)), .SDcols = my_item_set]

item_set_dat[, total_items := rowSums(item_set_dat[, 2:4, with = F]) ]

support_coffee_bread_jam <- table(item_set_dat$total_items)["3"]/9531 

support_coffee_bread_jam
```

To calculate confidence({bread, coffee} --> {jam}) we should also calculate the support of ({bread, coffee}) which is the prorpotion of bread and coffee appearing together in the transactions which is about 8.9% of the transactions.

```{r}
item_set_dat[, coffee_bread := rowSums(item_set_dat[, c("Bread", "Coffee"), with = F]) ]

head(item_set_dat, 2)
support_coffee_bread <- table(item_set_dat$coffee_bread)["2"]/9531 

support_coffee_bread
```


1.3% of the people who bought bread and coffee also bought jam. This is the confidence of({bread, coffee} --> {jam})
For statisticians this can be translated as conditional probability. In conditional probability notations P(Jam/bread, coffee) which is probability you will buy jam given that you have already bought bread and coffee. In association analysis we have {bread, coffee} >>{jam} bread and coffee implies jam. So the confidence measures the strength/probability of this implication.

```{r}
confidence <- support_coffee_bread_jam/support_coffee_bread 

confidence * 100
```

Using package arules we find the 10 rules with the highest confidence in descending order. Confidence({Toast} -->{Coffee}) had the highest confidence of 0.70440252. About 70.44% of the transactions that contained toast also contained coffee.

```{r}
library(arules)

transactions <- as(split(dat$Item, dat$Transaction), "transactions")

assoc_rules <- apriori(transactions,
                 parameter = list(supp = 0.02, conf = 0.04, target = "rules"))

assoc_rules <- sort(assoc_rules, by='confidence', decreasing = TRUE)


inspect(assoc_rules[1:30]) #%>% broom::tidy() %>% kable
```

I hope with this small example you can now understand how association analysis works.